[
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "indexing",
            "counting"
        ],
        "owner": {
            "account_id": 5806199,
            "reputation": 879,
            "user_id": 9811964,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/dTuBz.jpg?s=128&g=1",
            "display_name": "PParker",
            "link": "https://stackoverflow.com/users/9811964/pparker"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 68245456,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625407195,
        "creation_date": 1625407093,
        "question_id": 68245443,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68245443/simplest-way-to-add-repeating-counter-column-to-pandas-dataframe",
        "title": "Simplest way to add repeating counter column to pandas dataframe",
        "answer_body": "<p>try:</p>\n<pre><code>df['counter']=df.index//3\n</code></pre>\n<p>OR</p>\n<p>If you have a custom index then you can use:</p>\n<pre><code>df['counter']=[x//3 for x in range(len(df))]\n</code></pre>\n<p>output of <code>df</code>:</p>\n<pre><code>    key     team    counter\n0   K0      a       0\n1   K1      d       0\n2   K2      w       0\n3   K3      a       1\n4   K4      c       1\n5   K5      s       1\n6   K6      x       2\n7   K7      d       2\n8   K8      a       2\n9   K9      f       3\n10  K10     e       3\n11  K11     r       3\n</code></pre>\n",
        "question_body": "<p>I have a datafame:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame(\n {\n &quot;key&quot;: [&quot;K0&quot;, &quot;K1&quot;, &quot;K2&quot;, &quot;K3&quot;, &quot;K4&quot;, &quot;K5&quot;, &quot;K6&quot;, &quot;K7&quot;, &quot;K8&quot;, &quot;K9&quot;,'K10', 'K11',],\n &quot;team&quot;: ['a', 'd', 'w', 'a', 'c', 's', 'x', 'd', 'a', 'f', 'e', 'r'],\n }\n)\n\ndf = \n    key team\n0   K0  a\n1   K1  d\n2   K2  w\n3   K3  a\n4   K4  c\n5   K5  s\n6   K6  x\n7   K7  d\n8   K8  a\n9   K9  f\n10  K10 e\n11  K11 r\n</code></pre>\n<p>What is the shortes / simples way to add a repeating counter column like this?:</p>\n<pre><code>    key team    counter\n0    K0  a      0\n1    K1  d      0\n2    K2  w      0\n3    K3  a      1\n4    K4  c      1\n5    K5  s      1\n6    K6  x      2\n7    K7  d      2\n8    K8  a      2\n9    K9  f      3\n10   K10 e      3\n11   K11 r      3\n</code></pre>\n<p>My feeling tells me, that there must be a one-line solution (maybe a bit longer). But all I can think of would be much longer and complex.</p>\n<p>How would you approach this?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "account_id": 5725404,
            "reputation": 2179,
            "user_id": 4522512,
            "user_type": "registered",
            "accept_rate": 43,
            "profile_image": "https://www.gravatar.com/avatar/dacad3598d0ccc1f6e580be2da403dcd?s=128&d=identicon&r=PG&f=1",
            "display_name": "Shivkumar Mallesappa",
            "link": "https://stackoverflow.com/users/4522512/shivkumar-mallesappa"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 68243106,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1625395151,
        "creation_date": 1625387950,
        "question_id": 68243003,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68243003/python-pandas-findall-not-returning-all-columns",
        "title": "Python Pandas - findall() not returning all columns",
        "answer_body": "<p><strong>Input df</strong></p>\n<pre><code>    Name    date    address\n0   A   2021-07-03  X\n1   B   2021-07-03  Y\n2   C   2021-07-03  Z\n3   D   2021-08-01  M\n4   E   2021-08-01  N\n5   F   2021-08-01  O\n</code></pre>\n<p><strong>If date col is object/string type and we want to keep it as is</strong></p>\n<pre><code>search = '2021-07'\nmonth_dataframe = df[df[&quot;date&quot;].str.contains(str(search))]\nmonth_dataframe\n</code></pre>\n<p><strong>If date col is object/str type and we are okay with converting it to datetime</strong></p>\n<p>Benefit in this case is we don't need to define variable <code>search = '2021-07'</code> and this solution will work for every(current) month.</p>\n<pre><code>df['date'] = pd.to_datetime(df.date)\nmonth_dataframe =  df[df.date.dt.month == pd.Timestamp('today').month]\nmonth_dataframe\n</code></pre>\n<p><strong>If date col is datetime type</strong></p>\n<p>(Just an option, not the best way)</p>\n<pre><code>search = '2021-07'\nmonth_dataframe = df[df[&quot;date&quot;].astype(str).str.contains(str(search))]\nmonth_dataframe\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>    Name    date    address\n0   A   2021-07-03  X\n1   B   2021-07-03  Y\n2   C   2021-07-03  Z\n</code></pre>\n",
        "question_body": "<p>I have the following data in my DataFrame <br></p>\n<p><a href=\"https://i.stack.imgur.com/Zgy4f.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Zgy4f.png\" alt=\"enter image description here\" /></a></p>\n<p>I want to create another DataFrame by applying one of the filter ( Get all data which belongs to current month ).<br>\nTo achieve this I am using the following code :</p>\n<pre><code>search = '2021-07'\nmonth_dataframe=all_dataframe[&quot;date&quot;].str.findall(str(search))\n</code></pre>\n<p>I think the filter is running properly , because when I print <code>month_dataframe</code>, I can see the data, but it is only returning <code>date</code> column data, I want all of the columns/data from <code>all_dataframe</code>.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "grouping"
        ],
        "owner": {
            "account_id": 4811278,
            "reputation": 375,
            "user_id": 3884701,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://www.gravatar.com/avatar/ee520bbfd1cd6ec632bdf0eb7cd9ae10?s=128&d=identicon&r=PG",
            "display_name": "TuoCuggino",
            "link": "https://stackoverflow.com/users/3884701/tuocuggino"
        },
        "is_answered": true,
        "view_count": 10314,
        "accepted_answer_id": 40569207,
        "answer_count": 1,
        "score": 9,
        "last_activity_date": 1625394951,
        "creation_date": 1478990060,
        "last_edit_date": 1592644375,
        "question_id": 40568438,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/40568438/python-pandas-dataframe-find-max-for-each-unique-values-of-an-another-column",
        "title": "Python pandas dataframe: find max for each unique values of an another column",
        "answer_body": "<p>Sample data (note that you posted an image which can't be used by potential answerers without retyping, so I'm making a simple example in its place):</p>\n\n<pre><code>df=pd.DataFrame({ 'id':[1,1,1,1,2,2,2,2],\n                   'a':range(8), 'b':range(8,0,-1) })\n</code></pre>\n\n<p>The key to this is just using <code>idxmax</code> and <code>idxmin</code> and then futzing with the indexes so that you can merge things in a readable way.  Here's the whole answer and you may wish to examine intermediate dataframes to see how this is working.</p>\n\n<pre><code>df_max = df.groupby('id').idxmax()\ndf_max['type'] = 'max'\ndf_min = df.groupby('id').idxmin()\ndf_min['type'] = 'min'\n\ndf2 = df_max.append(df_min).set_index('type',append=True).stack().rename('index')\n\ndf3 = pd.concat([ df2.reset_index().drop('id',axis=1).set_index('index'), \n                  df.loc[df2.values] ], axis=1 )\n\ndf3.set_index(['id','level_2','type']).sort_index()\n\n                 a  b\nid level_2 type      \n1  a       max   3  5\n           min   0  8\n   b       max   0  8\n           min   3  5\n2  a       max   7  1\n           min   4  4\n   b       max   4  4\n           min   7  1\n</code></pre>\n\n<p>Note in particular that df2 looks like this:</p>\n\n<pre><code>id  type   \n1   max   a    3\n          b    0\n2   max   a    7\n          b    4\n1   min   a    0\n          b    3\n2   min   a    4\n          b    7\n</code></pre>\n\n<p>The last column there holds the index values in <code>df</code> that were derived with <code>idxmax</code> &amp; <code>idxmin</code>.  So basically all the information you need is in <code>df2</code>.  The rest of it is just a matter of merging back with <code>df</code> and making it more readable.</p>\n",
        "question_body": "<p>I have a large dataframe (from 500k to 1M rows) which contains for example these 3 numeric columns: ID, A, B</p>\n<p>I want to filter the results in order to obtain a table like the one in the image below, where, for each unique value of column id, i have the maximum and minimum value of A and B.\nHow can i do?</p>\n<p>EDIT: i have updated the image below in order to be more clear: when i get the max or min from a column i need to get also the data associated to it of the others columns</p>\n<p><a href=\"https://i.stack.imgur.com/xuinR.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/xuinR.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 17258259,
            "reputation": 235,
            "user_id": 12496869,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c07accea77bc7cb3a24f5822d39e3a6f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Sun",
            "link": "https://stackoverflow.com/users/12496869/sun"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68243655,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625393691,
        "creation_date": 1625392796,
        "question_id": 68243538,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68243538/replace-na-in-dataframe-with-value-from-a-list",
        "title": "replace Na in dataframe with value from a list",
        "answer_body": "<p>You can use <code>loc</code> to get the locations where <code>id</code> column equals to <code>&quot;Na&quot;</code> and put your list in there:</p>\n<pre><code>df.loc[df.id.eq(&quot;Na&quot;), &quot;id&quot;] = correct_id\n</code></pre>\n<p>If you want them integers at the end, you can use <code>astype</code>:</p>\n<pre><code>df.id = df.id.astype(int)\n</code></pre>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; df\n\n           movieName    id  Year\n0            birdman  1987  2010\n1  avengers: endgame  4563  2020\n2           deadpool  3452  2012\n3      The bird King  1230  2018\n4            Bla bla  6547  2013\n5          Lion King  9384  2020\n</code></pre>\n",
        "question_body": "<p>I have a dataframe</p>\n<pre><code>          movieName    id  Year\n            birdman  1987  2010\n  avengers: endgame    Na  2020\n           deadpool    Na  2012\n      The bird King  1230  2018\n            Bla bla    Na  2013\n          Lion King    Na  2020\n</code></pre>\n<p>and i have a list :</p>\n<pre><code>correct_id = [4563,3452,6547,9384]\n</code></pre>\n<p>I want to replace the value <strong>Na</strong> with value from <strong>the list above</strong> in order to get:</p>\n<pre><code>           movieName    id  Year\n            birdman  1987  2010\n  avengers: endgame  4563  2020\n           deadpool  3452  2012\n      The bird King  1230  2018\n            Bla bla  6547  2013\n          Lion King  9384  2020\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 13994697,
            "reputation": 11,
            "user_id": 10107828,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ff2a54ee3ee776081f9f9d265bfd67a4?s=128&d=identicon&r=PG&f=1",
            "display_name": "bewilderd",
            "link": "https://stackoverflow.com/users/10107828/bewilderd"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68243644,
        "answer_count": 2,
        "score": -2,
        "last_activity_date": 1625393666,
        "creation_date": 1625392837,
        "question_id": 68243543,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68243543/convert-comma-seperated-string-to-pandas-dataframe",
        "title": "Convert Comma Seperated String to Pandas Dataframe",
        "answer_body": "<p>You can try <code>regex</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>import re\nimport pandas as pd\n\ns = &quot;Key=xxxx, age=11, key=yyyy , age=22,Key=zzzz, age=01, key=qqqq, age=21,Key=wwwww, age=91, key=pppp, age=22&quot;\n\ndf = pd.DataFrame(zip(re.findall(r'Key=([^,\\s]+)', s, re.IGNORECASE), re.findall(r'age=([^,\\s]+)', s, re.IGNORECASE)),\n                 columns=['key', 'age'])\n\ndf\n</code></pre>\n<pre><code>     key    age\n0   xxxx    11\n1   yyyy    22\n2   zzzz    01\n3   qqqq    21\n4   wwwww   91\n5   pppp    22\n</code></pre>\n",
        "question_body": "<p>I have a string which is -</p>\n<pre><code>str=&quot;Key=xxxx, age=11, key=yyyy , age=22,Key=zzzz, age=01, key=qqqq, age=21,Key=wwwww, age=91, key=pppp, age=22&quot;\n</code></pre>\n<p>I want to convert this string to Python DataFrame with KEY and AGE as Column names.\nThe given key and age are in pair.\nHow could I achieve this conversion?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21977797,
            "reputation": 124,
            "user_id": 16253345,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/4227312227290363/picture?type=large",
            "display_name": "Ahmed Chater",
            "link": "https://stackoverflow.com/users/16253345/ahmed-chater"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68243166,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625389829,
        "creation_date": 1625389469,
        "question_id": 68243146,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas",
        "title": "replace zero with value of an other column using pandas",
        "answer_body": "<p>via <code>mask()</code>:</p>\n<pre><code>df['id']=df['id'].mask(df['id'].eq(0),df['ref'])\n</code></pre>\n<p>OR</p>\n<p>via numpy's <code>where()</code>:</p>\n<pre><code>#import numpy as np\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe df1:</p>\n<pre><code>    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n</code></pre>\n<p>I want to replace 0 in the <strong>id column</strong> with value from <strong>ref column</strong> of the same row So it will become:</p>\n<pre><code>   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n</code></pre>\n",
        "input_data_frames": [
            "    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n",
            "   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n"
        ],
        "output_codes": [
            "#import numpy as np\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n"
        ],
        "ques_desc": "I have a dataframe df1: I want to replace 0 in the id column with value from ref column of the same row So it will become: ",
        "ans_desc": "via : OR via numpy's : ",
        "formatted_input": {
            "qid": 68243146,
            "link": "https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas",
            "question": {
                "title": "replace zero with value of an other column using pandas",
                "ques_desc": "I have a dataframe df1: I want to replace 0 in the id column with value from ref column of the same row So it will become: "
            },
            "io": [
                "    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n",
                "   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n"
            ],
            "answer": {
                "ans_desc": "via : OR via numpy's : ",
                "code": [
                    "#import numpy as np\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "counter",
            "data-manipulation"
        ],
        "owner": {
            "account_id": 16152320,
            "reputation": 37,
            "user_id": 11660409,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-Z9REP6T6mFg/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfMwWQ1YYLWV35aCWf3OxptEFurJw/mo/photo.jpg?sz=128",
            "display_name": "rylynn_mcbos",
            "link": "https://stackoverflow.com/users/11660409/rylynn-mcbos"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 68242911,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625388139,
        "creation_date": 1625386647,
        "last_edit_date": 1625386801,
        "question_id": 68242870,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68242870/how-to-make-a-new-column-with-counter-of-the-number-of-times-a-word-from-a-prede",
        "title": "How to make a new column with counter of the number of times a word from a predefined list appears in a text column of the dataframe?",
        "answer_body": "<p>As you do <code>count = </code>, you erase the previous value, you want to sum up the different counts</p>\n<pre><code>def func(stringans):\n    count = 0\n    for x in ai_tech:\n        count += stringans.count(x)\n    return count\n\n# with sum and generator \ndef func(stringans):\n    return sum(stringans.count(x) for x in ai_tech)\n</code></pre>\n<p>Fixing some typos in <code>ai_tech</code> and setting all to <code>.lower()</code> gives <code>2,1,0</code> in the counter col, the last row has no value in common</p>\n<pre><code>import pandas as pd\n\nai_tech = [&quot;natural language processing&quot;, &quot;nlp&quot;, &quot;A I &quot;, &quot;Artificial intelligence&quot;,\n           &quot;stemming&quot;, &quot;lemmatization&quot;, &quot;information extraction&quot;,\n           &quot;text mining&quot;, &quot;text analytics&quot;, &quot;data - mining&quot;]\n\ndf = pd.DataFrame([[&quot;1. More details  A I   Artificial Intelligence&quot;], [&quot;2. NLP works very well these days&quot;],\n                   [&quot;3. receiving information at the right time&quot;]], columns=[&quot;text&quot;])\n\ndef func(stringans):\n    return sum(stringans.lower().count(x.lower()) for x in ai_tech)\n\ndf['counter'] = df['text'].apply(func)\nprint(df)\n\n# ------------------\n                                             text  counter\n0  1. More details  A I   Artificial Intelligence        2\n1               2. NLP works very well these days        1\n2      3. receiving information at the right time        0\n</code></pre>\n",
        "question_body": "<p>I want to build a new column which contains the count of the number of times a word from ai_functional list occurs in a text column.</p>\n<p>List given is:</p>\n<pre><code>&gt; ai_functional = [&quot;natural language\n&gt; processing&quot;,&quot;nlp&quot;,&quot;A I &quot;,&quot;Aritificial intelligence&quot;, &quot;stemming&quot;,&quot;lemmatization&quot;,&quot;lemmatization&quot;,&quot;information\n&gt; extraction&quot;,&quot;text mining&quot;,&quot;text analytics&quot;,&quot;data-mining&quot;]\n</code></pre>\n<p>the result I want is as follows:</p>\n<pre><code>&gt; text                                                                     counter\n&gt; \n&gt; 1. More details  A I   Artificial Intelligence                             2\n&gt; 2. NLP works very well these days                                          1         \n&gt; 3. receiving information at the right time                                 1\n</code></pre>\n<p>The code i have been using is</p>\n<pre><code>def func(stringans):\n  for x in ai_tech:\n    count = stringans.count(x)\n  \n  return count\n\ndf['counter']=df['text'].apply(func)\n</code></pre>\n<p>Please can someone help me with this. I am really stuck because everytime i apply this i get result as 0 in the counter column</p>\n"
    },
    {
        "tags": [
            "python",
            "csv",
            "pandas",
            "dataframe",
            "chunks"
        ],
        "owner": {
            "account_id": 9084008,
            "reputation": 2027,
            "user_id": 6762788,
            "user_type": "registered",
            "accept_rate": 38,
            "profile_image": "https://i.stack.imgur.com/Pcz8P.jpg?s=128&g=1",
            "display_name": "Geet",
            "link": "https://stackoverflow.com/users/6762788/geet"
        },
        "is_answered": true,
        "view_count": 17923,
        "accepted_answer_id": 39386767,
        "answer_count": 2,
        "score": 8,
        "last_activity_date": 1625380793,
        "creation_date": 1473324464,
        "last_edit_date": 1473353618,
        "question_id": 39386458,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/39386458/how-to-read-data-in-python-dataframe-without-concatenating",
        "title": "How to read data in Python dataframe without concatenating?",
        "answer_body": "<p>As you are trying to process 85GB CSV file, if you will try to read all the data by breaking it into chunks and converting it into dataframe then it will hit memory limit for sure. You can try to solve this problem by using different approach. In this case, you can use filtering operations on your data. For example, if there are 600 columns in your dataset and you are interested only in 50 columns. Try to read only 50 columns from the file. This way you will save lot of memory. Process your rows as you read them. If you need to filter the data first, use a generator function. <code>yield</code> makes a function a generator function, which means it won't do any work until you start looping over it.</p>\n<p>For more information regarding generator function:\n<a href=\"https://stackoverflow.com/questions/17444679/reading-a-huge-csv-in-python\">Reading a huge .csv file</a></p>\n<p>For efficient filtering refer: <a href=\"https://codereview.stackexchange.com/questions/88885/efficiently-filter-a-large-100gb-csv-file-v3\">https://codereview.stackexchange.com/questions/88885/efficiently-filter-a-large-100gb-csv-file-v3</a></p>\n<p>For processing smaller dataset:</p>\n<p><strong>Approach 1: To convert reader object to dataframe directly:</strong></p>\n<pre><code>full_data = pd.concat(TextFileReader, ignore_index=True)\n</code></pre>\n<p>It is necessary to add parameter <a href=\"http://pandas.pydata.org/pandas-docs/stable/merging.html#ignoring-indexes-on-the-concatenation-axis\" rel=\"nofollow noreferrer\">ignore index</a> to function concat, because avoiding duplicity of indexes.</p>\n<p><strong>Approach 2:</strong> <strong>Use Iterator or get_chunk to convert it into dataframe.</strong></p>\n<p>By specifying a chunksize to read_csv,return value will be an iterable object of type TextFileReader.</p>\n<pre><code>df=TextFileReader.get_chunk(3)\n\nfor chunk in TextFileReader:\n    print(chunk)\n</code></pre>\n<p>Source : <a href=\"http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking\" rel=\"nofollow noreferrer\">http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking</a></p>\n<p><code>df= pd.DataFrame(TextFileReader.get_chunk(1))</code></p>\n<p>This will convert one chunk to dataframe.</p>\n<p><strong>Checking total number of chunks in TextFileReader</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>for chunk_number, chunk in enumerate(TextFileReader):\n    # some code here, if needed\n    pass\n\nprint(&quot;Total number of chunks is&quot;, chunk_number+1)\n</code></pre>\n<p>If file size is bigger,I won't recommend second approach. For example, if csv file consist of 100000 records then chunksize=5 will create 20,000 chunks.</p>\n",
        "question_body": "<p>I want to read the file f (file size:85GB) in chunks to a dataframe. Following code is suggested.</p>\n\n<pre><code>chunksize = 5\nTextFileReader = pd.read_csv(f, chunksize=chunksize)\n</code></pre>\n\n<p>However, this code gives me TextFileReader, not dataframe. Also, I don't want to concatenate these chunks to convert TextFileReader to dataframe because of the memory limit. Please advise.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 16134187,
            "reputation": 1544,
            "user_id": 11647025,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e8f78049f82e583722b25c6a8edd7417?s=128&d=identicon&r=PG&f=1",
            "display_name": "crissal",
            "link": "https://stackoverflow.com/users/11647025/crissal"
        },
        "is_answered": true,
        "view_count": 52,
        "accepted_answer_id": 68242225,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625379551,
        "creation_date": 1625352171,
        "last_edit_date": 1625378846,
        "question_id": 68240448,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68240448/replace-a-pandas-subrow-with-an-array-of-values",
        "title": "Replace a Pandas subrow with an array of values?",
        "answer_body": "<h3>Simple pandas solution</h3>\n<pre><code>s = df.stack()\ns[s.str.contains(r'\\d?,\\d+', na=False)] = np.hstack(values)\nout = s.unstack().reindex(df.columns, axis=1)\n</code></pre>\n<h3>Explanation</h3>\n<ul>\n<li><code>stack</code> the dataframe to reshape. Note: Stacking operation also drops the <code>NaN</code> values by default.</li>\n</ul>\n<pre><code>&gt;&gt;&gt; df.stack()\n\n0  Unnamed: 1    Ore\n   2Gi           ,30\n   3Ve           ,46\n   4Sa           ,50\n   ...\n   9Me           ,07\n   0Gi           ,11\n   1Ve           ,59\n1  Unnamed: 1    Ore\n   6Lu           ,53\ndtype: object\n</code></pre>\n<ul>\n<li>Match the regular expression pattern (<code>\\d?,\\d+</code>) against the stacked frame using <code>str.contains</code>, this essentially creates a boolean mask</li>\n</ul>\n<pre><code>&gt;&gt;&gt; s.str.contains(r'\\d?,\\d+', na=False)\n\n0  Unnamed: 1    False\n   2Gi            True\n   3Ve            True\n   4Sa            True\n   ...\n   9Me            True\n   0Gi            True\n   1Ve            True\n1  Unnamed: 1    False\n   6Lu            True\ndtype: bool\n</code></pre>\n<ul>\n<li>Using <code>hstack</code> flatten the <code>values</code> then assign these values to the matched strings in the stacked frame</li>\n</ul>\n<pre><code>&gt;&gt;&gt; s[s.str.contains(r'\\d?,\\d+', na=False)] = np.hstack(values)\n&gt;&gt;&gt; s\n\n0  Unnamed: 1     Ore\n   2Gi           6,30\n   3Ve           5,46\n   4Sa           4,50\n   ...\n   9Me           5,07\n   0Gi           6,11\n   1Ve           2,59\n1  Unnamed: 1     Ore\n   6Lu           2,53\ndtype: object\n</code></pre>\n<ul>\n<li>Now <code>unstack</code> to reshape back into a dataframe and <code>reindex</code> the columns</li>\n</ul>\n<pre><code>&gt;&gt;&gt; s.unstack().reindex(df.columns, axis=1)\n\n   Unnamed: 0 Unnamed: 1  1Me   2Gi   3Ve   4Sa  5Do   6Lu   7Ma   8Me   9Gi   0Ve   1Sa   2Do  3Lu   4Ma   5Me   6Gi   7Ve   8Sa  9Do   0Lu   1Ma   2Me   3Gi   4Ve   5Sa  6Do   7Lu   8Ma   9Me   0Gi   1Ve  Unnamed: 2\n0         NaN        Ore  NaN  6,30  5,46  4,50  NaN   NaN  5,20  7,48  5,41  2,07  3,52  3,11  NaN  4,53  4,51  5,14  4,28  3,33  NaN  5,32  3,10  5,03  4,44  4,39  5,04  NaN  5,26  7,15  5,07  6,11  2,59         NaN\n1         NaN        Ore  NaN   NaN   NaN   NaN  NaN  2,53   NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN   NaN   NaN   NaN   NaN         NaN\n</code></pre>\n",
        "question_body": "<p>I have a (simple?) problem, but that cannot understand how to solve in a pandas way.</p>\n<p>I have this CSV:</p>\n<pre><code>,Unnamed: 0,Unnamed: 1,1Me,2Gi,3Ve,4Sa,5Do,6Lu,7Ma,8Me,9Gi,0Ve,1Sa,2Do,3Lu,4Ma,5Me,6Gi,7Ve,8Sa,9Do,0Lu,1Ma,2Me,3Gi,4Ve,5Sa,6Do,7Lu,8Ma,9Me,0Gi,1Ve,Unnamed: 2\n0,,Ore,,&quot;,30&quot;,&quot;,46&quot;,&quot;,50&quot;,,,&quot;,20&quot;,&quot;,48&quot;,&quot;,41&quot;,&quot;,07&quot;,&quot;,52&quot;,&quot;,11&quot;,,&quot;,53&quot;,&quot;,51&quot;,&quot;,14&quot;,&quot;,28&quot;,&quot;,33&quot;,,&quot;,32&quot;,&quot;,10&quot;,&quot;,03&quot;,&quot;,44&quot;,&quot;,39&quot;,&quot;,04&quot;,,&quot;,26&quot;,&quot;,15&quot;,&quot;,07&quot;,&quot;,11&quot;,&quot;,59&quot;,\n1,,Ore,,,,,,&quot;,53&quot;,,,,,,,,,,,,,,,,,,,,,,,,,,\n</code></pre>\n<p>That, when loaded, results in this dataframe:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; df\n  Unnamed: 0 Unnamed: 1  1Me  2Gi  3Ve  4Sa  ...  7Lu  8Ma  9Me  0Gi  1Ve Unnamed: 2\n0        NaN        Ore  NaN  ,30  ,46  ,50  ...  ,26  ,15  ,07  ,11  ,59        NaN\n1        NaN        Ore  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN        NaN\n</code></pre>\n<p>And also, I have <code>values</code>, that is a numpy array of two lists.</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; values\narray([list(['6,30', '5,46', '4,50', '5,20', '7,48', '5,41', '2,07', '3,52', '3,11', '4,53', '4,51', '5,14', '4,28', '3,33', '5,32', '3,10', '5,03', '4,44', '4,39', '5,04', '5,26', '7,15', '5,07', '6,11', '2,59']),\n       list(['2,53'])], dtype=object)\n</code></pre>\n<p>My question is, I want to replace all elements in Dataframe that match a specific regex to be replaced with the corresponding element of the <code>values</code> list.</p>\n<p>I assume that <code>df</code> and <code>values</code> have the same length (in this case 2) and also that &quot;wrong&quot; numbers to be replaced inside df are the same of the corresponding row in the <code>values</code> array.</p>\n<p>In my case, I tried using <code>df.replace()</code>, but it didn't work; I got this error:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; df_lattice2.replace(r&quot;\\d?,\\d+&quot;, values)\nTypeError: Invalid &quot;to_replace&quot; type: 'str'\n</code></pre>\n<p>After I while, I came out with an iterative algorithm, using <code>df.iterrows()</code>, counters and checking the elements one by one; I think, however, that a Pandas solution to a problem like this must exist, but I didn't find anything.</p>\n<p>My expected output is:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; expected_df\n  Unnamed: 0 Unnamed: 1  1Me   2Gi   3Ve   4Sa  ...   7Lu   8Ma   9Me   0Gi   1Ve Unnamed: 2\n0        NaN        Ore  NaN  6,30  5,46  4,50  ...  5,26  7,15  5,07  6,11  2,59        NaN\n1        NaN        Ore  NaN   NaN   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN        NaN\n</code></pre>\n<p>A precondition is that any function should work row-to-row (so no <code>applymap</code>) because some values are found in the second row - and the corresponding value in the first row is <code>NaN</code> -, while <code>applymap</code> works column by column.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 19614527,
            "reputation": 27,
            "user_id": 14356332,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f5ca28f850fe4c776932d1c310250e9c?s=128&d=identicon&r=PG&f=1",
            "display_name": "beginnerprogrammerforever",
            "link": "https://stackoverflow.com/users/14356332/beginnerprogrammerforever"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 68241939,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625376700,
        "creation_date": 1625373687,
        "last_edit_date": 1625376178,
        "question_id": 68241764,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68241764/how-do-i-create-a-column-in-a-dataframe-that-is-the-sum-of-another-column-in-ano",
        "title": "How do I create a column in a dataframe that is the sum of another column in another dataframe based on a common column?",
        "answer_body": "<p>try via <code>map()</code> and <code>groupby()</code>:</p>\n<pre><code>df2['Word Count']=df2['Character'].map(df1.groupby('Character')['Word Count'].sum())\n#you can also use replace() method in place of map()\n</code></pre>\n<p>output of <code>df2</code>:</p>\n<pre><code>    Character       Line Count  Word Count\n0   Leslie Knope    81          71\n1   Child           1           72\n</code></pre>\n",
        "question_body": "<p>I have a dataframe (df1) that looks like this:</p>\n<pre><code>Character       Word Count\nLeslie Knope    58\nChild           9\nLeslie Knope    13\nChild           63\n</code></pre>\n<p>and another dataframe (df2) that looks like this:</p>\n<pre><code>Character      Line Count\nLeslie Knope   81\nChild          1\n</code></pre>\n<p>I'm trying to create a new column in df2 that is the sum of the 'Word Count' column in df1 based on the common 'Character Name' column in both datasets. The output should look like this:</p>\n<pre><code>Character     Line Count   Word Count\nLeslie Knope  81           71\nChild         1            72\n</code></pre>\n<p>I've tried using groupby() but can't figure out how to create the Word Count column in df2 that is based on the condition that the Character Name matches between the two dataframes.</p>\n<p>Thanks in advance for any help!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 14382056,
            "reputation": 77,
            "user_id": 10388970,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/0W6iB.jpg?s=128&g=1",
            "display_name": "William.D",
            "link": "https://stackoverflow.com/users/10388970/william-d"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 68240855,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625362924,
        "creation_date": 1625356780,
        "question_id": 68240743,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68240743/how-to-pandas-rows-into-column",
        "title": "How to pandas rows into column?",
        "answer_body": "<p>Your dataframe has the dates as an index, so reset index to pull them out into a column.  If the index was previously named something other than 'index', use that name instead of 'index' if you choose to rename it.</p>\n<pre><code>df = pd.DataFrame([[2], [4]], columns=['order'])\n\ndf = df.reset_index()\ndf = df.rename(columns={'index':'date'})\nprint(df)\n#    date  order\n# 0     0      2\n# 1     1      4\n\n</code></pre>\n",
        "question_body": "<p>I have a dataset that looks like this:</p>\n<p><a href=\"https://i.stack.imgur.com/XGXAG.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/XGXAG.png\" alt=\"enter image description here\" /></a></p>\n<p>And I want to have a column that's called &quot;Date&quot; and has all the dates that I currently have as rows. Does anyone have any idea how to do that?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22121113,
            "reputation": 13,
            "user_id": 16374337,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5027088ecbae7f723679d379a7af38bd?s=128&d=identicon&r=PG&f=1",
            "display_name": "JoshPickel",
            "link": "https://stackoverflow.com/users/16374337/joshpickel"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 68240672,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625355847,
        "creation_date": 1625355205,
        "last_edit_date": 1625355847,
        "question_id": 68240645,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68240645/manipulating-pandas-data-frame-to-get-into-long-format",
        "title": "Manipulating Pandas data frame to get into long? format",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html\" rel=\"nofollow noreferrer\"><code>wide_to_long</code></a>:</p>\n<pre><code>pd.wide_to_long(\n  df, \n  ['Quan1', 'Quan2'], \n  i=['Year', 'code', 'Country'], \n  j='month', \n  suffix='\\w+'\n).reset_index()\n\n#   Year  code Country month  Quan1  Quan2\n#0  2020  8123   Japan   jan    500     26\n#1  2020  8123   Japan   feb    400     28\n#2  2020  8123  Taiwan   jan    450    245\n#3  2020  8123  Taiwan   feb   4500     87\n</code></pre>\n",
        "question_body": "<p>So I have a df that looks like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Year</th>\n<th>code</th>\n<th>Country</th>\n<th>Quan1jan</th>\n<th>Quan2jan</th>\n<th>Quan1feb</th>\n<th>Quan2feb</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2020</td>\n<td>08123</td>\n<td>Japan</td>\n<td>500</td>\n<td>26</td>\n<td>400</td>\n<td>28</td>\n</tr>\n<tr>\n<td>2020</td>\n<td>08123</td>\n<td>Taiwan</td>\n<td>450</td>\n<td>245</td>\n<td>4500</td>\n<td>87</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>And I would like for it to look like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Year</th>\n<th>month</th>\n<th>code</th>\n<th>Country</th>\n<th>Quan1</th>\n<th>Quan2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2020</td>\n<td>jan</td>\n<td>08123</td>\n<td>Japan</td>\n<td>500</td>\n<td>26</td>\n</tr>\n<tr>\n<td>2020</td>\n<td>feb</td>\n<td>08123</td>\n<td>Japan</td>\n<td>400</td>\n<td>28</td>\n</tr>\n<tr>\n<td>2020</td>\n<td>jan</td>\n<td>08123</td>\n<td>Taiwan</td>\n<td>450</td>\n<td>245</td>\n</tr>\n<tr>\n<td>2020</td>\n<td>feb</td>\n<td>08123</td>\n<td>Taiwan</td>\n<td>4500</td>\n<td>87</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>It doesn\u2019t matter if the data follows this same order, but I need it to be in this format.</p>\n<p>Ive tried to play around with melt, and unstack with no luck. Any help is very much appreciated.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-science",
            "eda"
        ],
        "owner": {
            "account_id": 21594975,
            "reputation": 17,
            "user_id": 15962699,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJzVpowOF-wBUeUE2eDGzvlMJlqF1zg_drNTqm_9=k-s128",
            "display_name": "leahnanno",
            "link": "https://stackoverflow.com/users/15962699/leahnanno"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68238169,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625330400,
        "creation_date": 1625255773,
        "question_id": 68230652,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68230652/how-to-map-two-column-values-in-a-pandas-dataframe-location-id-location-name",
        "title": "How to map two column values in a pandas dataframe (location-id, location-name) and spot errors in the dataframe?",
        "answer_body": "<p>Use group by and then filter on those countries something like below:</p>\n<pre><code>t=df.groupby(['location-name']).count().reset_index()\ndf_filtr=df[df['location-name'].isin(t[t['location-id']&gt;1]['location-name'])]\n\n</code></pre>\n<p>You can sort by countries to find the correct entry</p>\n",
        "question_body": "<p>My dataset have two columns name location-id and location-name. Each location name is given a unique location id.</p>\n<pre><code>location-id   location-name\n 234            SL\n 456            IN\n 234            SL\n 123            EN\n</code></pre>\n<p>As each location has a unique id, unique values in location-id column and location-name column need to be equal. But there seems to be an error in the df and my location-id has 1863 unique values and location-name has 1800 unique values.</p>\n<p>Is there a way to spot in which entries the error is made?</p>\n<p>I thought of a way. Iterate through these two columns and create a dictionary with key-value pairs.</p>\n<pre><code>dict_a = {234:&quot;SL&quot;, 456:&quot;IN&quot;, 123:&quot;EN&quot;}\n</code></pre>\n<p>For each sample, get the location-id, check if it is already a key in the dict. If it is already in, check the value related to it. If that value == location-id for the current sample, then go to the next sample. If the value and location-name is different, add that new name as another value to the same key.\nAfter iterating through the complete dataset, get key-value pairs with more than one value to spot the errors in the dataset.</p>\n<p>Is there a more efficient way to do this?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "if-statement"
        ],
        "owner": {
            "account_id": 19260754,
            "reputation": 19,
            "user_id": 14076709,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ea880d4412fd7991bad4cb158fb9f1e9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Qfin",
            "link": "https://stackoverflow.com/users/14076709/qfin"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68237224,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625325086,
        "creation_date": 1625321859,
        "question_id": 68237066,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68237066/adding-a-value-to-a-column-if-the-company-id-matches-between-two-dataframes-and",
        "title": "Adding a value to a column IF the company ID matches between two dataframes AND the year is lower or equal to the year for the same row",
        "answer_body": "<p>You can merge the datasets:</p>\n<pre><code>df = Dataset_1.merge(Dataset_2, left_on='company_id', right_on='org_number', how='left')\n\ndf['lower_year']=np.where(df.yr&lt;=df.year, 1, 0)\n\ndf = df[(df.lower_year==1) | (pd.isna(df.year))]\n\ndf.reset_index(drop=True, inplace=True)\n\ndf.drop(['org_number', 'year'], axis=1)\n\nprint(df)\n</code></pre>\n<p>Output:</p>\n<pre><code>  company_id    yr  lower_year\n0        111  2012           1\n1        111  2014           1\n2        223  2020           0\n3        444  1843           1\n</code></pre>\n<p>Let me know if you need explanation about the code</p>\n",
        "question_body": "<p>I have two datasets:\nDataset_1 has about 140 variables, one of which is &quot;company id&quot;, &quot;yr&quot; and a column for &quot;lower_year&quot;\nThis dataset has multiple rows for each year for each company id.</p>\n<p>Dataset_2 has ids (some of which match the &quot;company_id&quot;) named &quot;org_number&quot;, and &quot;year&quot; with only a single row and year for each id.</p>\n<p>I want to add a 1 in the column called &quot;lower_year&quot; for all the rows of Dataset_1 where the company ID matches one of the IDs in Dataset_2 and the &quot;yr&quot; value in Dataset_1 is equal to or lower than the &quot;year&quot; value in Dataset_2 at the specific row of the ID.</p>\n<p>Down the road I would like to delete the matching IDs that are not of a lower than or equal year (but not values where the ids do not match) but that should be simpler when this first step is done.</p>\n<p>I have made some attempts and tried combining if and and functions but have had no luck and am not entirely sure I am going in the right direction.</p>\n<pre><code># assign data of lists.\nDataset_1 = {'company_id': ['111', '111', '223', '444'], 'yr': [2012, 2014, 2020, 1843], 'lower_year': [0, 0, 0, 0]}\nDataset_2 = {'org_number': ['111', '444'], 'year': [2015, 2020]}\nDataset_1 = pd.DataFrame(Dataset_1)\nDataset_2 = pd.DataFrame(Dataset_2)\nprint(Dataset_1)\nprint(Dataset_2)\ndata.insert(1, 'investment', '0')\n\n\n#one attempt that doesn't work\nif Dataset_2['org_number'] == Dataset_1['company_id'] and Dataset_1['yr'] &lt;= Dataset_2['year']:\n    Dataset_2['lower_year'] = Dataset_1['lower_year'] = 1\nelse:\n    Dataset_1['lower_year'] = Dataset_1['lower_year'] = 0\n\n\n#alternatively: This runs but I think it is not filtering for the year needing to be the same in the same row as the company ID of Dataset_2\nDataset_1['lower_year'] = np.where((Dataset_1['company_id'].isin(Dataset_2['org_number'])) &amp; (Dataset_1['yr'].isin((Dataset_2)['year'])), '1', '2')\n</code></pre>\n<p>Here is a reproductable sample and my two various attempts that didn't work. My issue seems to be that I need it to be from the same row as the company ID in Dataset_2.</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "nlp"
        ],
        "owner": {
            "account_id": 15001531,
            "reputation": 4181,
            "user_id": 10829044,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XkVFAldxdns/AAAAAAAAAAI/AAAAAAAAAAA/AKxrwcYw7mFQ9BqeKrYJYmarsmObRK9Lsw/mo/photo.jpg?sz=128",
            "display_name": "The Great",
            "link": "https://stackoverflow.com/users/10829044/the-great"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 68237244,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1625323590,
        "creation_date": 1625322837,
        "last_edit_date": 1625323202,
        "question_id": 68237180,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68237180/how-to-filter-elements-within-a-list-and-generate-frequency-table",
        "title": "How to filter elements within a list and generate frequency table",
        "answer_body": "<p>We can use <code>value_counts</code> after flattening the columns <code>tokens</code> and <code>labels</code> using <code>hstack</code></p>\n<pre><code>t = np.hstack(df['tokens'])\nl = np.hstack(df['labels'])\n</code></pre>\n<p>Count of each label</p>\n<pre><code>pd.value_counts(l)\n\nB    3\nA    3\nD    1\nC    1\ndtype: int64\n</code></pre>\n<p>Count tokens under each label</p>\n<pre><code>pd.DataFrame(zip(l, t)).value_counts()\n\n0  1   \nA  Hi      2\n   fine    1\nB  am      1\n   how     1\n   say     1\nC  I       1\nD  Ila     1\ndtype: int64\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like as shown below</p>\n<pre><code>df = pd.DataFrame({'text': [&quot;Hi how&quot;,&quot;I am fine&quot;,&quot;Ila say Hi&quot;],\n                   'tokens':[['Hi','how'],['I','am','fine'],['Ila','say','Hi']],\n                    'labels':[['A','B'],['C','B','A'],['D','B','A']]})\n</code></pre>\n<p>I would like to do two things</p>\n<p>a) count of each labels</p>\n<p>b) count tokens under each label</p>\n<p>I was trying something like below</p>\n<pre><code>flattened = [] \nop = itertools.zip_longest(df['tokens'],df['labels'])\nfor i in op:\n    for la in df['labels']: \n        if la == 'A' : \n        flattened.append(val)\n</code></pre>\n<p>But this is incorrect and going no-where</p>\n<p>I expect my output to have two dataframes/tables like as shown below</p>\n<p><a href=\"https://i.stack.imgur.com/BrdRX.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/BrdRX.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "account_id": 10713144,
            "reputation": 316,
            "user_id": 7884793,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/870e1f9544f3175b86dc097bbcec535f?s=128&d=identicon&r=PG&f=1",
            "display_name": "NLeeT",
            "link": "https://stackoverflow.com/users/7884793/nleet"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68236863,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625320222,
        "creation_date": 1625319729,
        "question_id": 68236800,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68236800/how-to-use-map-function-to-compare-and-fill-in-dataframe",
        "title": "How to use map function to compare and fill in dataframe?",
        "answer_body": "<p>To <a href=\"https://stackoverflow.com/questions/28457149/how-to-map-a-function-using-multiple-columns-in-pandas\">map a function using multiple columns</a> you may use <code>DataFrame.apply(,axis=1)</code></p>\n<pre><code>df = pd.DataFrame([[None, &quot;A&quot;, &quot;Galaxy Puzzle&quot;, &quot;TRUE&quot;],\n                   [None, &quot;B&quot;, &quot;Car Toy&quot;, &quot;TRUE&quot;],\n                   [&quot;4EB3D322-7A25-4A67-9053-28955383DD1F&quot;, &quot;B&quot;, &quot;Car Toy&quot;, &quot;FALSE&quot;]],\n                  columns=[&quot;ID&quot;, &quot;Type&quot;, &quot;Name&quot;, &quot;Enabled&quot;])\n\nlookup = [{&quot;ID&quot;: &quot;70B52DA6-F099-4D01-BBD0-03EA97292C26&quot;, &quot;Type&quot;: &quot;A&quot;, &quot;Name&quot;: &quot;Galaxy Puzzle&quot;},\n          {&quot;ID&quot;: &quot;442B1598-20CF-4425-8A28-0438FBF77C46&quot;, &quot;Type&quot;: &quot;B&quot;, &quot;Name&quot;: &quot;Car&quot;}]\n\nlookup = {(entity[&quot;Name&quot;], entity['Type']): entity[&quot;ID&quot;] for entity in lookup}\n\ndf.loc[df[&quot;ID&quot;].isnull(), &quot;ID&quot;] = df[df[&quot;ID&quot;].isnull()] \\\n    .apply(lambda row: lookup.get((row['Name'], row['Type'])), axis=1)\nprint(df)\n</code></pre>\n<p>Giving the following (I changed a type in the <code>lookup</code> so you see it is well handled)</p>\n<pre><code>                                     ID Type           Name Enabled\n0  70B52DA6-F099-4D01-BBD0-03EA97292C26    A  Galaxy Puzzle    TRUE\n1                                  None    B        Car Toy    TRUE\n2  4EB3D322-7A25-4A67-9053-28955383DD1F    B        Car Toy   FALSE\n</code></pre>\n",
        "question_body": "<p>I have the json object like below:</p>\n<pre><code>lookup = [\n  {\n    &quot;ID&quot;: &quot;70B52DA6-F099-4D01-BBD0-03EA97292C26&quot;,\n    &quot;Type&quot;: &quot;A&quot;\n    &quot;Name&quot;: &quot;Galaxy Puzzle&quot;\n  },\n  {\n    &quot;ID&quot;: &quot;442B1598-20CF-4425-8A28-0438FBF77C46&quot;,\n    &quot;Type&quot;: &quot;B&quot;\n    &quot;Name&quot;: &quot;Car Toy&quot;\n  }\n]\n</code></pre>\n<p>And my dataframe is:</p>\n<pre><code>df = pd.DataFrame([[None, &quot;A&quot;, &quot;Galaxy Puzzle&quot;,&quot;TRUE&quot;], [None, &quot;B&quot;, &quot;Car Toy&quot;,&quot;TRUE&quot;],[&quot;4EB3D322-7A25-4A67-9053-28955383DD1F&quot;, &quot;B&quot;, &quot;Car Toy&quot;, &quot;FALSE&quot;]], columns=[&quot;ID&quot;, &quot;Type&quot;, &quot;Name&quot;,&quot;Enabled&quot;])\n</code></pre>\n<p>Now I have to mapping ID from json object to dataframe base on Name and Type. Also it will ignore rows have ID in dataframe.</p>\n<p>Below code from my last post help me to compare Name and ignore rows have ID but how can I <strong>combine with other conditions</strong> (in this case is Type)</p>\n<pre><code>df.loc[df[&quot;ID&quot;].isnull(), &quot;ID&quot;] = df.loc[df[&quot;ID&quot;].isnull(), &quot;Name&quot;].map({entity[&quot;Name&quot;]: entity[&quot;ID&quot;] for entity in lookup}) \n</code></pre>\n<p>Please help me, many thanks to your help &lt;3</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "nlp"
        ],
        "owner": {
            "account_id": 15001531,
            "reputation": 4181,
            "user_id": 10829044,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XkVFAldxdns/AAAAAAAAAAI/AAAAAAAAAAA/AKxrwcYw7mFQ9BqeKrYJYmarsmObRK9Lsw/mo/photo.jpg?sz=128",
            "display_name": "The Great",
            "link": "https://stackoverflow.com/users/10829044/the-great"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68236099,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625314202,
        "creation_date": 1625313612,
        "question_id": 68236070,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68236070/convert-multiple-rows-of-text-into-a-single-row-using-pandas",
        "title": "Convert multiple rows of text into a Single row using Pandas",
        "answer_body": "<p>try via <code>agg()</code>,<code>to_frame()</code> and <code>reset_index()</code>:</p>\n<pre><code>out=(df.agg(' '.join)\n        .to_frame('text')\n        .reset_index(drop=True))\n</code></pre>\n<p>output of out:</p>\n<pre><code>    text\n0   Hi how are you I am fine I love you I hate you\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like as shown below</p>\n<pre><code>df = pd.DataFrame({'text': [&quot;Hi how are you&quot;,&quot;I am fine&quot;,&quot;I love you&quot;,&quot;I hate you&quot;]})\n</code></pre>\n<p>I would like to convert all these individual rows into a single row</p>\n<p>I tried the below but it is incorrect and doesn't work</p>\n<pre><code>df['text'].apply(' '.join).reset_index()\n</code></pre>\n<p>I expect my output to be like as shown below</p>\n<p><a href=\"https://i.stack.imgur.com/ChSOa.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ChSOa.png\" alt=\"enter image description here\" /></a></p>\n<p>How can I do this?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "getlocation"
        ],
        "owner": {
            "account_id": 19475708,
            "reputation": 47,
            "user_id": 14247142,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-yp_hor1kHZQ/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckWZe2-xcw7i1y5JFakUs2x91TkzA/photo.jpg?sz=128",
            "display_name": "Ren Ningen",
            "link": "https://stackoverflow.com/users/14247142/ren-ningen"
        },
        "is_answered": true,
        "view_count": 58,
        "accepted_answer_id": 68179161,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625314038,
        "creation_date": 1624971292,
        "last_edit_date": 1625170116,
        "question_id": 68179089,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68179089/python-how-to-get-position-of-pandas-series-element-where-conditions-exist",
        "title": "Python: How to get position of pandas.series element where conditions exist?",
        "answer_body": "<p>Use:</p>\n<pre><code>up_edge_x = x[edge_or_not &gt; 0]\nup_edge_y = y[edge_or_not &gt; 0]\n\ndown_edge_x = x[edge_or_not &lt; 0]\ndown_edge_y = y[edge_or_not &lt; 0]\n\nall_edges_x = x[edge_or_not != 0]\nall_edges_y = y[edge_or_not != 0]\n</code></pre>\n<p>Create <code>Series</code> by ranges with index by <code>up_edge_x, down_edge_x</code> first:</p>\n<pre><code>up_edge = pd.Series(range(len(up_edge_x)), index=up_edge_x, name='pos')\ndown_edge = pd.Series(range(len(down_edge_x)), index=down_edge_x, name='pos')\nprint (up_edge)\n0.9394    0\n0.8955    1\nName: pos, dtype: int64\n\nprint (down_edge)\n0.8574    0\n0.9196    1\n0.9388    2\n0.9602    3\nName: pos, dtype: int64\n</code></pre>\n<p>Then join together:</p>\n<pre><code>pos = pd.concat([up_edge, down_edge])\nprint (pos)\n0.9394    0\n0.8955    1\n0.8574    0\n0.9196    1\n0.9388    2\n0.9602    3\nName: pos, dtype: int64\n</code></pre>\n<p>And last map new column:</p>\n<pre><code>all_edges = pd.DataFrame({'y':all_edges_y,\n                          'edge':edge_or_not[edge_or_not != 0].to_numpy(), \n                          'pos': pd.Index(all_edges_x).map(pos)},\n                          index=all_edges_x)\n\n\nprint (all_edges)\n            y  edge  pos\n0.9394  0.884     1    0\n0.8574  0.880    -1    0\n0.8955  0.861     1    1\n0.9196  0.817    -1    1\n0.9388  0.771    -1    2\n0.9602  0.727    -1    3\n</code></pre>\n",
        "question_body": "<p>I have a <code>numpy.ndarrays: x,y</code>:</p>\n<pre><code>&gt;&gt;&gt; x = np.ndarray(shape=(10,), buffer=np.array([0.9902, 0.9394, 0.839,  0.8574, 0.9174, 0.8742, 0.8955, 0.9196, 0.9388, 0.9602]), dtype=float)\n\n[0.9902 0.9394 0.839  0.8574 0.9174 0.8742 0.8955 0.9196 0.9388 0.9602]\n\n&gt;&gt;&gt; y = np.ndarray(shape=(10,), buffer=np.array([0.956, 0.884, 0.875, 0.880, 0.865, 0.870, 0.861, 0.817, 0.771, 0.727]), dtype=float)\n\n[0.956, 0.884, 0.875, 0.880, 0.865, 0.870, 0.861, 0.817, 0.771, 0.727]\n</code></pre>\n<p>and series <code>edge_or_not</code>:</p>\n<pre><code>&gt;&gt;&gt; d = {'2020-03-17 04:39:00+03:00': 0,\n          '2020-03-17 04:40:00+03:00': 1,\n          '2020-03-17 04:41:00+03:00': 0,\n          '2020-03-17 04:42:00+03:00': -1,\n          '2020-03-17 04:43:00+03:00': 0,\n          '2020-03-17 04:44:00+03:00': 0,\n          '2020-03-17 04:45:00+03:00': 1,\n          '2020-03-17 04:46:00+03:00': -1,\n          '2020-03-17 04:47:00+03:00': -1,\n          '2020-03-17 04:48:00+03:00': -1}\n\n&gt;&gt;&gt; edge_or_not = pd.Series(data=d)\n\n2020-03-17 04:39:00+03:00    0\n2020-03-17 04:40:00+03:00    1\n2020-03-17 04:41:00+03:00    0\n2020-03-17 04:42:00+03:00   -1\n2020-03-17 04:43:00+03:00    0\n2020-03-17 04:44:00+03:00    0\n2020-03-17 04:45:00+03:00    1\n2020-03-17 04:46:00+03:00   -1\n2020-03-17 04:47:00+03:00   -1\n2020-03-17 04:48:00+03:00   -1\ndtype: int64\n</code></pre>\n<p>And I'm getting <code>up_edge_x</code>, <code>up_edge_y</code>, <code>down_edge_x</code>, <code>down_edge_y</code> like this:</p>\n<pre><code>&gt;&gt;&gt; up_edge_x = x[edge_or_not &gt; 0]\n\narray([0.9394, 0.8955])\n\n&gt;&gt;&gt; up_edge_y = y[edge_or_not &gt; 0]\n\narray([0.884, 0.861])\n\n&gt;&gt;&gt; down_edge_x = x[edge_or_not &lt; 0]\n\narray([0.8574, 0.9196, 0.9388, 0.9602])\n\n&gt;&gt;&gt; down_edge_y = y[edge_or_not &lt; 0]\n\narray([0.88 , 0.817, 0.771, 0.727])\n</code></pre>\n<p>And <code>all_edges_x</code>, <code>all_edges_y</code>:</p>\n<pre><code>&gt;&gt;&gt; all_edges_x = x[edge_or_not != 0]\n\narray([0.9394, 0.8574, 0.8955, 0.9196, 0.9388, 0.9602])\n\n&gt;&gt;&gt; all_edges_y = y[edge_or_not != 0]\n\narray([0.884, 0.88 , 0.861, 0.817, 0.771, 0.727])\n</code></pre>\n<p>And then creating DataFrames:</p>\n<pre><code>&gt;&gt;&gt; up_edge = pd.DataFrame({'y':up_edge_y}, index=up_edge_x)\n\n            y   (pos)\n0.9394  0.884       0\n0.8955  0.861       1\n\n&gt;&gt;&gt; down_edge = pd.DataFrame({'y':down_edge_y}, index=down_edge_x)\n\n            y   (pos)\n0.8574  0.880       0\n0.9196  0.817       1\n0.9388  0.771       2\n0.9602  0.727       3\n</code></pre>\n<p>All I need is creating <code>all_edges DataFrame</code> where will be 3 columns: <code>'y'</code>, <code>'edge'</code>, <code>'pos'</code></p>\n<pre><code>&gt;&gt;&gt; all_edges = pd.DataFrame({'y':all_edges_y, 'edge':edge_or_not[edge_or_not != 0].to_numpy(), \n                         'pos':???},\n                          index=all_edges_x)\n</code></pre>\n<p>So that after all <code>all_edges DataFrame</code> must look like this:</p>\n<pre><code>            y  edge  pos\n0.9394  0.884     1    0\n0.8574  0.880    -1    0\n0.8955  0.861     1    1\n0.9196  0.817    -1    1\n0.9388  0.771    -1    2\n0.9602  0.727    -1    3\n</code></pre>\n<p>How to calculate 3rd column <code>pos</code>, that I can links to <code>all_edges</code> from <code>up_edge</code> and <code>down_edge</code> DataFrames like in below stupid example:</p>\n<pre><code>&gt;&gt;&gt; down_x1 = 0.9602\n&gt;&gt;&gt; loc = down_edge.index.get_loc(down_x1)\n&gt;&gt;&gt; edges = all_edges.loc[all_edges['pos']==loc]['edge']\n&gt;&gt;&gt; print(edges)\n\n0.9602   -1\nName: edge, dtype: int64\n</code></pre>\n<p>And I've got a second question: How to get array of locations another DataFrame?\nLike this:</p>\n<pre><code>&gt;&gt;&gt; locations = down_edge.index.get_loc(#mb all indexes)\n\n[0, 1, 2, 3]\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "networkx"
        ],
        "owner": {
            "account_id": 15498653,
            "reputation": 684,
            "user_id": 14045537,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/hUZ4q.jpg?s=128&g=1",
            "display_name": "Pluviophile",
            "link": "https://stackoverflow.com/users/14045537/pluviophile"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68235670,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625310430,
        "creation_date": 1625307886,
        "question_id": 68235334,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68235334/how-to-generate-the-component-id-in-the-networkx-graph",
        "title": "How to generate the component id in the Networkx graph?",
        "answer_body": "<p>Create triplets of <code>Node</code>, <code>ComponentId</code> and <code>Component</code> by enumerating over the connected component list, then create a new dataframe from these triplets and <code>merge</code> it with the given dataframe on <code>Node</code></p>\n<pre><code>df = pd.DataFrame([(n, i, c) for i,c in enumerate(Gcc, 1) for n in c], \n                        columns=['Node', 'ComponentID', 'Components'])\n\ndata = data.merge(df, on='Node')\n</code></pre>\n<p>Alternatively you can use <code>map</code> instead of <code>merge</code> to individually create <code>ComponentID</code> and <code>Components</code> columns</p>\n<pre><code>d = dict(enumerate(Gcc, 1))\ndata['ComponentID'] = data['Node'].map({n:i for i,c in d.items() for n in c})\ndata['Components']  = data['ComponentID'].map(d)\n</code></pre>\n<hr />\n<pre><code>print(data)\n\n   Node  degree  ComponentID    Components\n1     1       2            1  {0, 1, 2, 3}\n2     2       2            1  {0, 1, 2, 3}\n5    11       2            2  {10, 11, 12}\n0     0       1            1  {0, 1, 2, 3}\n3     3       1            1  {0, 1, 2, 3}\n4    10       1            2  {10, 11, 12}\n6    12       1            2  {10, 11, 12}\n</code></pre>\n",
        "question_body": "<p>I have a large Graph Network generated using <code>Networkx</code> package.</p>\n<p><a href=\"https://i.stack.imgur.com/OGoTl.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/OGoTl.jpg\" alt=\"enter image description here\" /></a></p>\n<p>Here I'm adding a sample</p>\n<pre><code>import networkx as nx\nimport pandas as pd\n\nG = nx.path_graph(4)\nnx.add_path(G, [10, 11, 12])\n</code></pre>\n<p>I'm trying to create a <code>dataframe</code> with Node, degrees, component id, component.</p>\n<p>Created degrees using</p>\n<pre><code>degrees = list(nx.degree(G))\n\ndata = pd.DataFrame([list(d) for d in degrees], columns=['Node', 'degree']).sort_values('degree', ascending=False)\n</code></pre>\n<p>extracted components using</p>\n<pre><code>Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n\nGcc\n</code></pre>\n<pre><code>[{0, 1, 2, 3}, {10, 11, 12}]\n</code></pre>\n<p>And not sure how I can create the <code>Component ID</code> and <code>components</code> in the data.</p>\n<p>Required output:</p>\n<pre><code>  Node  degree  ComponentID  Components\n1   1   2           1         {0, 1, 2, 3}\n2   2   2           1         {0, 1, 2, 3}\n5   11  2           2         {10, 11, 12}\n0   0   1           1         {0, 1, 2, 3}\n3   3   1           1         {0, 1, 2, 3}\n4   10  1           2         {10, 11, 12}\n6   12  1           2         {10, 11, 12}\n</code></pre>\n<p>How to generate the component ids and add them to the nodes and degrees?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 19895303,
            "reputation": 93,
            "user_id": 14576516,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/269c791d68e9e7c2e6f51170cc269f52?s=128&d=identicon&r=PG&f=1",
            "display_name": "sinha-shaurya",
            "link": "https://stackoverflow.com/users/14576516/sinha-shaurya"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 68234871,
        "answer_count": 1,
        "score": -2,
        "last_activity_date": 1625305645,
        "creation_date": 1625303892,
        "last_edit_date": 1625304879,
        "question_id": 68234845,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68234845/conditional-sum-in-pandas",
        "title": "Conditional sum in Pandas",
        "answer_body": "<p>try via <code>pivot_table()</code>:</p>\n<pre><code>out=df.pivot_table('Amount','ID','Type',fill_value=0,aggfunc=['sum','count'])\n</code></pre>\n<p>Then:</p>\n<pre><code>out=(out.swaplevel(axis=1)\n        .rename(columns=lambda x:'_FREQ' if x=='count' else '',level=1)\n        .rename_axis(columns=[None,None])\n        .reset_index())\n</code></pre>\n<p>Finally:</p>\n<pre><code>out.columns=out.columns.map(''.join)\n</code></pre>\n<p>output of <code>out</code>:</p>\n<pre><code>    ID  CR  DB  CR_FREQ     DB_FREQ\n0   1   63  40  2           1\n1   2   99  0   1           0\n</code></pre>\n",
        "question_body": "<p>I am using Pandas to process some financial data in csv form.I have a table as follows:</p>\n<pre><code>ID Amount Type \n001 33 CR\n001 40 DB\n001 30 CR\n002 99 CR\n</code></pre>\n<p>I want to add it based on ID and Type both. Answer should resemble something like this:</p>\n<pre><code>ID CR DB CR_FREQ DB_FREQ\n001 63 40 2 1\n002 99 0 1 0\n</code></pre>\n<p>This means I need to add the amounts and split them accordingly. Any idea how to achieve this?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "timestamp"
        ],
        "owner": {
            "account_id": 19256289,
            "reputation": 237,
            "user_id": 14073111,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-v9-fmI_5DnM/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckI-maisNcyjtLpr8roYSBRbOvrhw/photo.jpg?sz=128",
            "display_name": "user14073111",
            "link": "https://stackoverflow.com/users/14073111/user14073111"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 68232113,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625301132,
        "creation_date": 1625261258,
        "last_edit_date": 1625262116,
        "question_id": 68231389,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas",
        "title": "Compare two columns that contains timestamps in pandas",
        "answer_body": "<p>A straightforward way with boolean mask:</p>\n<pre><code>dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n</code></pre>\n<pre><code>&gt;&gt;&gt; df\n    Col0                Col1                Col2 Col3 Col4\n0  1.txt 2021-06-23 15:04:30                 NaT  NaT  NaT\n1  2.txt 2021-06-23 14:25:30 2021-06-23 15:30:30  NaT  NaT\n</code></pre>\n",
        "question_body": "<p>Lets say I have a dataframe like this one:</p>\n<pre><code>  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n</code></pre>\n<p>I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4).</p>\n<p>I tried this one:</p>\n<pre><code>df['Col1'] = pd.to_datetime(df['Col1'])\ndf['Col2'] = pd.to_datetime(df['Col2'])\ndf['Col3'] = pd.to_datetime(df['Col3'])\nk= (df['Col1'] &gt; df['Col2']).astype(int)\np=(df['Col2'] &gt; df['Col3']).astype(int)\n\nif k&gt;0:\n    df['Col2']=np.nan\n    df['Col3']=np.nan\n    df['Col4']=np.nan\nelif p&gt;0:\n    df['Col3']=np.nan\n    df['Col4']=np.nan \n</code></pre>\n<p>But it is showing me this error:</p>\n<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n<p>My desirable output would look like this:</p>\n<pre><code>  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n</code></pre>\n<p>EDITED:\nAdded Col0</p>\n",
        "input_data_frames": [
            "  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n",
            "  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n"
        ],
        "output_codes": [
            "dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n",
            ">>> df\n    Col0                Col1                Col2 Col3 Col4\n0  1.txt 2021-06-23 15:04:30                 NaT  NaT  NaT\n1  2.txt 2021-06-23 14:25:30 2021-06-23 15:30:30  NaT  NaT\n"
        ],
        "ques_desc": "Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0 ",
        "ans_desc": "A straightforward way with boolean mask: ",
        "formatted_input": {
            "qid": 68231389,
            "link": "https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas",
            "question": {
                "title": "Compare two columns that contains timestamps in pandas",
                "ques_desc": "Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0 "
            },
            "io": [
                "  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n",
                "  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n"
            ],
            "answer": {
                "ans_desc": "A straightforward way with boolean mask: ",
                "code": [
                    "dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 13559905,
            "reputation": 37,
            "user_id": 9782031,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/814d44757366ef9b19baf8cdeeec11d6?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dan",
            "link": "https://stackoverflow.com/users/9782031/dan"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 68196162,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625295958,
        "creation_date": 1625059334,
        "last_edit_date": 1625295958,
        "question_id": 68195637,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68195637/if-two-columns-satisfy-a-condition-return-calculated-value-from-other-columns-p",
        "title": "If two columns satisfy a condition return calculated value from other columns. Pandas / Windows",
        "answer_body": "<p>I found a quite tricky solution that works.</p>\n<pre><code>import pandas as pd\n\n# define groups between two LDE\ndf['Group'] = (df['K'] == 'LDE').cumsum().shift(1, fill_value=0)\n\n# custom function to perform your subtraction\ndef f(x):\n    if x.loc[x['J'] == 'FDE', 'A'].size == 0:\n        return None\n    else:\n        return x.loc[x['K'] == 'LDE', 'D'].iloc[0] - x.loc[x['J'] == 'FDE', 'A'].iloc[0]\n\n# get list of numerical results\nresults = df.groupby('Group').apply(f).tolist()\n\n# input the list into the specified LDE rows\ndf.loc[df['K'] == 'LDE', 'Results'] = results\n</code></pre>\n<hr />\n<p>Results</p>\n<p><a href=\"https://i.stack.imgur.com/GWnod.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/GWnod.png\" alt=\"enter image description here\" /></a></p>\n<hr />\n<p>Starting data</p>\n<pre><code>df = pd.DataFrame([['03-01-2011', 523, 698, 284, 33, 416, 675, 300, 690, 314, '', '', 'FDM', ''], ['27-01-2011', 353, 1, 50, 547, 514, 957, 804, 490, 108, '', 'LDE', '', ''],\n                   ['28-01-2011', 307, 837, 656, 755, 792, 568, 119, 439, 943, 'FDE', '', '', ''], ['31-01-2011', 327, 409, 155, 358, 120, 401, 385, 965, 888, '', '', '', 'LDM'], ['01-02-2011', 686, 313, 714, 12, 140, 112, 589, 908, 605, '', '', 'FDM', ''], ['24-02-2011', 161, 846, 816, 223, 387, 566, 435, 567, 36, '', 'LDE', '', ''], ['25-02-2011', 889, 652, 190, 324, 947, 778, 575, 604, 314, 'FDE', '', '', ''], ['28-02-2011', 704, 33, 232, 630, 344, 796, 331, 409, 597, '', '', '', 'LDM'], ['01-03-2011', 592, 148, 974, 540, 848, 393, 505, 699, 315, '', '', 'FDM', ''], ['31-03-2011', 938, 768, 325, 756, 971, 644, 546, 238, 376, '', 'LDE', '', 'LDM'], ['01-04-2011', 385, 298, 654, 655, 2, 112, 960, 306, 477, 'FDE', '', 'FDM', ''], ['28-04-2011', 704, 516, 785, 152, 355, 348, 106, 611, 426, '', 'LDE', '', ''], ['29-04-2011', 753, 719, 776, 826, 756, 370, 660, 536, 903, 'FDE', '', '', 'LDM'], ['02-05-2011', 222, 28, 102, 363, 952, 860, 48, 976, 478, '', '', 'FDM', ''], ['26-05-2011', 361, 588, 866, 884, 809, 662, 801, 843, 668, '', 'LDE', '', '']],\n                  columns=['Date'] + list(map(chr, range(65, 78))))\n</code></pre>\n",
        "question_body": "<p><a href=\"https://i.stack.imgur.com/iEbZS.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/iEbZS.png\" alt=\"Refer Image\" /></a></p>\n<p>Refer yellow highlighted cells:</p>\n<p>If K = LDE then look for FDE in column J (above LDE's row), in Result column return (D from LDE minus A from FDE) (ie 223-307 = -84)</p>\n<p>Refer green highlighted cells: 152-385 = -233 and so on.</p>\n<p>How to solve ?</p>\n<p>Data:</p>\n<pre><code>['03-01-2011', 523, 698, 284, 33, 416, 675, 300, 690, 314, '', '', 'FDM', ''] ['27-01-2011', 353, 1, 50, 547, 514, 957, 804, 490, 108, '', 'LDE', '', ''] ['28-01-2011', 307, 837, 656, 755, 792, 568, 119, 439, 943, 'FDE', '', '', ''] ['31-01-2011', 327, 409, 155, 358, 120, 401, 385, 965, 888, '', '', '', 'LDM'] ['01-02-2011', 686, 313, 714, 12, 140, 112, 589, 908, 605, '', '', 'FDM', ''] ['24-02-2011', 161, 846, 816, 223, 387, 566, 435, 567, 36, '', 'LDE', '', ''] ['25-02-2011', 889, 652, 190, 324, 947, 778, 575, 604, 314, 'FDE', '', '', ''] ['28-02-2011', 704, 33, 232, 630, 344, 796, 331, 409, 597, '', '', '', 'LDM'] ['01-03-2011', 592, 148, 974, 540, 848, 393, 505, 699, 315, '', '', 'FDM', ''] ['31-03-2011', 938, 768, 325, 756, 971, 644, 546, 238, 376, '', 'LDE', '', 'LDM'] ['01-04-2011', 385, 298, 654, 655, 2, 112, 960, 306, 477, 'FDE', '', 'FDM', ''] ['28-04-2011', 704, 516, 785, 152, 355, 348, 106, 611, 426, '', 'LDE', '', ''] ['29-04-2011', 753, 719, 776, 826, 756, 370, 660, 536, 903, 'FDE', '', '', 'LDM'] ['02-05-2011', 222, 28, 102, 363, 952, 860, 48, 976, 478, '', '', 'FDM', ''] ['26-05-2011', 361, 588, 866, 884, 809, 662, 801, 843, 668, '', 'LDE', '', '']\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 88863,
            "reputation": 99223,
            "user_id": 245549,
            "user_type": "registered",
            "accept_rate": 66,
            "profile_image": "https://www.gravatar.com/avatar/eec7e09c7d0731a44e91b403dd0bc347?s=128&d=identicon&r=PG",
            "display_name": "Roman",
            "link": "https://stackoverflow.com/users/245549/roman"
        },
        "is_answered": true,
        "view_count": 3938374,
        "protected_date": 1549951536,
        "accepted_answer_id": 16476974,
        "answer_count": 28,
        "score": 2827,
        "last_activity_date": 1625295504,
        "creation_date": 1368169489,
        "last_edit_date": 1611227189,
        "question_id": 16476924,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas",
        "title": "How to iterate over rows in a DataFrame in Pandas",
        "answer_body": "<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html#pandas-dataframe-iterrows\" rel=\"noreferrer\"><code>DataFrame.iterrows</code></a> is a generator which yields both the index and row (as a Series):</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\n\nfor index, row in df.iterrows():\n    print(row['c1'], row['c2'])\n</code></pre>\n\n<pre><code>10 100\n11 110\n12 120\n</code></pre>\n",
        "question_body": "<p>I have a <code>DataFrame</code> from Pandas:</p>\n\n<pre><code>import pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\nprint df\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>   c1   c2\n0  10  100\n1  11  110\n2  12  120\n</code></pre>\n\n<p>Now I want to iterate over the rows of this frame. For every row I want to be able to access its elements (values in cells) by the name of the columns. For example:</p>\n\n<pre><code>for row in df.rows:\n   print row['c1'], row['c2']\n</code></pre>\n\n<p>Is it possible to do that in Pandas?</p>\n\n<p>I found this <a href=\"https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas\">similar question</a>. But it does not give me the answer I need. For example, it is suggested there to use:</p>\n\n<pre><code>for date, row in df.T.iteritems():\n</code></pre>\n\n<p>or</p>\n\n<pre><code>for row in df.iterrows():\n</code></pre>\n\n<p>But I do not understand what the <code>row</code> object is and how I can work with it.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 15001531,
            "reputation": 4181,
            "user_id": 10829044,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XkVFAldxdns/AAAAAAAAAAI/AAAAAAAAAAA/AKxrwcYw7mFQ9BqeKrYJYmarsmObRK9Lsw/mo/photo.jpg?sz=128",
            "display_name": "The Great",
            "link": "https://stackoverflow.com/users/10829044/the-great"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 68233650,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625293175,
        "creation_date": 1625289943,
        "question_id": 68233487,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68233487/how-to-elegantly-drop-the-identified-records-using-pandas",
        "title": "How to elegantly drop the identified records using pandas?",
        "answer_body": "<p>Try:</p>\n<pre><code>d1=df.groupby('subject_id').filter(lambda x: x['test_id'].count() == 0)\nd2=df.groupby('test_id').filter(lambda x: x['test_name'].nunique() &gt; 1)\nd3=df[df['invalid_condition']==1]\n#your conditions\n\ndata_inconsistencies_df = pd.DataFrame([[(len(d1)*100/len(df)),(len(d2)*100/len(df)),(len(d3)*100/len(df))]],columns = ['subject_with_no_test_info','test_id_diff_names','invalid_condition'])\n#created dataframe to show percentages\n\nd2=d2.drop_duplicates('test_id',keep='last')\n          #^your sub condition\n          #(We don't drop  101, because we keep = first of duplicate test ids)\nto_drop=pd.concat([d1,d2,d3]).index\n#concatinating 3 dataframes to grab the index of the rows which are going to drop\n</code></pre>\n<p>Finally:</p>\n<pre><code>df=df.drop(to_drop)\n#dropping those indexes\n</code></pre>\n<p>Output of <code>df</code>:</p>\n<pre><code>  subject_id    test_id     test_name   invalid_condition\n0   101         21.0            A           0\n3   201         24.0            D           0\n</code></pre>\n<p>Output of <code>data_inconsistencies_df</code>:</p>\n<pre><code>  subject_with_no_test_info     test_id_diff_names  invalid_condition\n0          20.0                         40.0            20.0\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe like as shown below</p>\n<pre><code>df = pd.DataFrame({'subject_id': [101,102,103,201,202],\n                  'test_id':[21,21,np.nan,24,25],\n                  'test_name':['A','B',np.nan,'D','E'],\n                  'invalid_condition':[0,0,0,0,1]})\n</code></pre>\n<p>I would like to identify the issues in the data based on below conditions and drop them</p>\n<p>While I am able to identify them using the code below, I am not sure how to drop them</p>\n<pre><code>subject_with_no_test_info = len(df.groupby('subject_id').filter(lambda x: x['test_id'].count() == 0))*100/len(df)\ntest_id_diff_names = len(df.groupby('test_id').filter(lambda x: x['test_name'].nunique() &gt; 1))*100/len(df)\ninvalid_condition = len(df[df['invalid_condition']==1])*100/len(df)\ndata_inconsistencies_df = pd.DataFrame([[subject_with_no_test_info,test_id_diff_names,invalid_condition]],columns = ['subject_with_no_test_info','test_id_diff_names','invalid_condition'])\n</code></pre>\n<p>This gives me an output like below</p>\n<p><a href=\"https://i.stack.imgur.com/6sAc2.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/6sAc2.png\" alt=\"enter image description here\" /></a></p>\n<p>but now I want to drop those records which contribute to the <code>20%, 40% and 20%</code> under each column in the data_incosistencies_df?</p>\n<p>Any elegant and efficient way to drop these records from the dataframe?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 19347989,
            "reputation": 5,
            "user_id": 14146065,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/55d05c0e6b0f115af3d2176754ec8c71?s=128&d=identicon&r=PG&f=1",
            "display_name": "Karina",
            "link": "https://stackoverflow.com/users/14146065/karina"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 68232816,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625280514,
        "creation_date": 1625279405,
        "last_edit_date": 1625279441,
        "question_id": 68232782,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68232782/extracting-from-a-2d-df-and-adding-value-to-1d-df-in-python",
        "title": "Extracting from a 2D df and adding value to 1D df in python",
        "answer_body": "<p>One option is to <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html\" rel=\"nofollow noreferrer\"><code>stack</code></a> <code>df1</code> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reindex.html\" rel=\"nofollow noreferrer\"><code>reindex</code></a> from <code>well</code>:</p>\n<pre><code>df3 = df1.stack()\ndf3.index = df3.index.map(lambda s: ''.join(map(str, s)))\ndf3 = df3.reindex(df2['well']).reset_index(name='Value')\n</code></pre>\n<p><code>df3</code>:</p>\n<pre><code>  Well  Value\n0   A1    237\n1   A2    543\n2   A3    300\n3   B1    435\n4   B2    313\n5   B3    150\n</code></pre>\n<p>DataFrames used:</p>\n<pre><code>import pandas as pd\n\ndf1 = pd.DataFrame({\n    1: {'A': 237, 'B': 435}, 2: {'A': 543, 'B': 313},\n    3: {'A': 300, 'B': 150}, 4: {'A': 256, 'B': 635},\n    5: {'A': 343, 'B': 847}, 6: {'A': 122, 'B': 321}\n})\n\ndf2 = pd.DataFrame({'well': ['A1', 'A2', 'A3', 'B1', 'B2', 'B3']})\n</code></pre>\n<hr />\n<p>Explanations:</p>\n<ol>\n<li>Stack produces a series:</li>\n</ol>\n<pre><code>df3 = df1.stack()\n</code></pre>\n<p><code>df3</code>:</p>\n<pre><code>A  1    237\n   2    543\n   3    300\n   4    256\n   5    343\n   6    122\nB  1    435\n   2    313\n   3    150\n   4    635\n   5    847\n   6    321\ndtype: int64\n</code></pre>\n<hr />\n<ol start=\"2\">\n<li>Collapse the MultiIndex into a single index with <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Index.map.html\" rel=\"nofollow noreferrer\"><code>Index.map</code></a>:</li>\n</ol>\n<pre><code>df3.index = df3.index.map(lambda s: ''.join(map(str, s)))\n</code></pre>\n<p><code>df3</code>:</p>\n<pre><code>A1    237\nA2    543\nA3    300\nA4    256\nA5    343\nA6    122\nB1    435\nB2    313\nB3    150\nB4    635\nB5    847\nB6    321\ndtype: int64\n</code></pre>\n<hr />\n<ol start=\"3\">\n<li>Grab <code>well</code> from <code>df2</code> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reindex.html\" rel=\"nofollow noreferrer\"><code>reindex</code></a>:</li>\n</ol>\n<pre><code>df3 = df3.reindex(df2['well'])\n</code></pre>\n<pre><code>Well\nA1    237\nA2    543\nA3    300\nB1    435\nB2    313\nB3    150\ndtype: int64\n</code></pre>\n<p>*If well is a <code>list</code> as it appears in the OP turn it into a <code>Series</code> with the name <code>well</code> and <code>reindex</code> from that instead:</p>\n<pre><code>well = ['A1', 'A2', 'A3', 'B1', 'B2', 'B3']\ndf3 = df3.reindex(pd.Series(well, name='well'))\n</code></pre>\n<p>Or <code>reindex</code> directly from the <code>list</code> but then the axis will need renamed before resetting the index:</p>\n<pre><code>well = ['A1', 'A2', 'A3', 'B1', 'B2', 'B3']\ndf3 = df3.reindex(well).rename_axis(index='Well')\n</code></pre>\n<hr />\n<ol start=\"4\">\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reset_index.html\" rel=\"nofollow noreferrer\"><code>reset_index</code></a> to convert to a DataFrame (and give the <code>Value</code> column a name):</li>\n</ol>\n<pre><code>df3 = df3.reindex(df2['well']).reset_index(name='Value')\n</code></pre>\n<p><code>df3</code>:</p>\n<pre><code>  Well  Value\n0   A1    237\n1   A2    543\n2   A3    300\n3   B1    435\n4   B2    313\n5   B3    150\n</code></pre>\n",
        "question_body": "<p>I am trying to extract information from a 2d data frame where I have rows A through H and columns 1-12. Each cell has a different number. My second dataframe only has one column with the combined information, for example A1, A2, etc. I want to set a for loop so that I can do df1.iloc and select the coordinates. But I don't want to type 96 rows, so I am thinking a for loop would be helpful but I don't know how to do it.\nI am fairly new so I don't know much of the tricks.</p>\n<p>df1 would be:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th></th>\n<th>1</th>\n<th>2</th>\n<th>3</th>\n<th>4</th>\n<th>5</th>\n<th>6</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>237</td>\n<td>543</td>\n<td>300</td>\n<td>256</td>\n<td>343</td>\n<td>122</td>\n</tr>\n<tr>\n<td>B</td>\n<td>435</td>\n<td>313</td>\n<td>150</td>\n<td>635</td>\n<td>847</td>\n<td>321</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>df2 would be:</p>\n<pre><code>well=['A1','A2','A3','B1','B2','B3']\n</code></pre>\n<p>I would like to make a df3 that looks like:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Well</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A1</td>\n<td>237</td>\n</tr>\n<tr>\n<td>A2</td>\n<td>543</td>\n</tr>\n<tr>\n<td>A3</td>\n<td>300</td>\n</tr>\n<tr>\n<td>B1</td>\n<td>435</td>\n</tr>\n<tr>\n<td>B2</td>\n<td>313</td>\n</tr>\n<tr>\n<td>B3</td>\n<td>150</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>So far what I have is:</p>\n<pre><code>    df3=pd.DataFrame()\n    for i in df2:\n        if i.contains('A'):\n           row=df1.iloc[1]\n           if i.contains('1'):\n              value=row.iloc[1]\n        df3.append(i,value)\n</code></pre>\n<p>I know that col # will always be the same for iloc.\nSo can I assign A-H as 1-8 and then somehow select the number after the letter in df2 as the iloc value?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "algorithmic-trading"
        ],
        "owner": {
            "account_id": 20537248,
            "reputation": 23,
            "user_id": 15073029,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b91b48b8afad3287bbd67bcde645f9bd?s=128&d=identicon&r=PG&f=1",
            "display_name": "MoonBoi9001",
            "link": "https://stackoverflow.com/users/15073029/moonboi9001"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 68232150,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625269508,
        "creation_date": 1625268162,
        "last_edit_date": 1625268583,
        "question_id": 68232120,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68232120/how-can-i-simplify-code-that-runs-the-same-calculation-with-different-parameters",
        "title": "How can I simplify code that runs the same calculation with different parameters into different output variables?",
        "answer_body": "<p>Instead of having <code>Short1</code> and <code>Short5</code> be separate variables, have there just be <em>one</em> <code>Shorts</code> dictionary, and have <code>1</code>, <code>5</code>, etc be keys. Thus:</p>\n<pre><code>MySet = [1, 5, 15, 30, 60, 240, 360, 720, 1440, 10080]\nShorts = {}\nLongs = {}\nMACDs = {}\nSignals = {}\n\nfor val in MySet:\n  Shorts[val] = df.Close.ewm(span=12 * val, adjust=False).mean()\n  Longs[val] = df.Close.ewm(span=26 * val, adjust=False).mean()\n  MACDs[val] = Shorts[val] - Longs[val]\n  Signals[val] = MACDs[val].ewm(span=9 * val, adjust=False).mean()\n\n  df[f'MACD{val}'] = MACDs[val]\n  df[f'Signal Line{val}'] = Signals[val]\n</code></pre>\n",
        "question_body": "<p>I have a dataframe, df, that contains 1.4 million rows of data, where each row represents 1 minute of open, high, low and close prices for BTC from 2018 to 2020. I want to add the MACD (Popular trading indicator) to my df, but instead of only calculating the macd for the 1 minute time frame like this:</p>\n<pre><code>ShortEMA = df.Close.ewm(span=12, adjust=False).mean()\nLongEMA = df.Close.ewm(span=26, adjust=False).mean()\nMACD = ShortEMA - LongEMA\nsignal = MACD.ewm(span=9, adjust=False).mean()\n\ndf[&quot;MACD&quot;] = MACD\ndf[&quot;Signal Line&quot;] = signal\n</code></pre>\n<p>I want to calculate the MACD for each of the time frames 1 minute, 15 minutes, 30 minutes, 1 hour, ect...</p>\n<p>I did this with the following code (which took ages):</p>\n<pre><code>MySet = [1, 5, 15, 30, 60, 240, 360, 720, 1440, 10080]\n\nShortEMA1 = df.Close.ewm(span=12 * MySet[0], adjust=False).mean()\nLongEMA1 = df.Close.ewm(span=26 * MySet[0], adjust=False).mean()\nMACD1 = ShortEMA1 - LongEMA1\nsignal1 = MACD.ewm(span=9 * MySet[0], adjust=False).mean()\n\nShortEMA5 = df.Close.ewm(span=12 * MySet[1], adjust=False).mean()\nLongEMA5 = df.Close.ewm(span=26 * MySet[1], adjust=False).mean()\nMACD5 = ShortEMA5 - LongEMA5\nsignal5 = MACD.ewm(span=9 * MySet[1], adjust=False).mean()\n\nShortEMA15 = df.Close.ewm(span=12 * MySet[2], adjust=False).mean()\nLongEMA15 = df.Close.ewm(span=26 * MySet[2], adjust=False).mean()\nMACD15 = ShortEMA15 - LongEMA15\nsignal15 = MACD.ewm(span=9 * MySet[2], adjust=False).mean()\n\nShortEMA30 = df.Close.ewm(span=12 * MySet[3], adjust=False).mean()\nLongEMA30 = df.Close.ewm(span=26 * MySet[3], adjust=False).mean()\nMACD30 = ShortEMA30 - LongEMA30\nsignal30 = MACD.ewm(span=9 * MySet[3], adjust=False).mean()\n\nShortEMA60 = df.Close.ewm(span=12 * MySet[4], adjust=False).mean()\nLongEMA60 = df.Close.ewm(span=26 * MySet[4], adjust=False).mean()\nMACD60 = ShortEMA60 - LongEMA60\nsignal60 = MACD.ewm(span=9 * MySet[4], adjust=False).mean()\n\nShortEMA240 = df.Close.ewm(span=12 * MySet[5], adjust=False).mean()\nLongEMA240 = df.Close.ewm(span=26 * MySet[5], adjust=False).mean()\nMACD240 = ShortEMA240 - LongEMA240\nsignal240 = MACD.ewm(span=9 * MySet[5], adjust=False).mean()\n\nShortEMA360 = df.Close.ewm(span=12 * MySet[6], adjust=False).mean()\nLongEMA360 = df.Close.ewm(span=26 * MySet[6], adjust=False).mean()\nMACD360 = ShortEMA360 - LongEMA360\nsignal360 = MACD.ewm(span=9 * MySet[6], adjust=False).mean()\n\nShortEMA720 = df.Close.ewm(span=12 * MySet[7], adjust=False).mean()\nLongEMA720 = df.Close.ewm(span=26 * MySet[7], adjust=False).mean()\nMACD720 = ShortEMA720 - LongEMA720\nsignal720 = MACD.ewm(span=9 * MySet[7], adjust=False).mean()\n\nShortEMA1440 = df.Close.ewm(span=12 * MySet[8], adjust=False).mean()\nLongEMA1440 = df.Close.ewm(span=26 * MySet[8], adjust=False).mean()\nMACD1440 = ShortEMA1440 - LongEMA1440\nsignal1440 = MACD.ewm(span=9 * MySet[8], adjust=False).mean()\n\nShortEMA10080 = df.Close.ewm(span=12 * MySet[9], adjust=False).mean()\nLongEMA10080 = df.Close.ewm(span=26 * MySet[9], adjust=False).mean()\nMACD10080 = ShortEMA10080 - LongEMA10080\nsignal10080 = MACD.ewm(span=9 * MySet[9], adjust=False).mean()\n\n\ndf[&quot;MACD1&quot;] = MACD1\ndf[&quot;Signal Line1&quot;] = signal1\n\ndf[&quot;MACD5&quot;] = MACD1\ndf[&quot;Signal Line5&quot;] = signal5\n\ndf[&quot;MACD15&quot;] = MACD1\ndf[&quot;Signal Line15&quot;] = signal15\n\ndf[&quot;MACD30&quot;] = MACD1\ndf[&quot;Signal Line30&quot;] = signal30\n\ndf[&quot;MACD60&quot;] = MACD60\ndf[&quot;Signal Line60&quot;] = signal60\n\ndf[&quot;MACD240&quot;] = MACD240\ndf[&quot;Signal Line240&quot;] = signal240\n\ndf[&quot;MACD360&quot;] = MACD360\ndf[&quot;Signal Line360&quot;] = signal360\n\ndf[&quot;MACD720&quot;] = MACD720\ndf[&quot;Signal Line720&quot;] = signal720\n\ndf[&quot;MACD1440&quot;] = MACD1440\ndf[&quot;Signal Line1440&quot;] = signal1440\n\ndf[&quot;MACD10080&quot;] = MACD10080\ndf[&quot;Signal Line10080&quot;] = signal10080\n</code></pre>\n<p>How can I simplify this whole process?</p>\n"
    },
    {
        "tags": [
            "python",
            "excel",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22032077,
            "reputation": 11,
            "user_id": 16299184,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GiNU37EfjkI5lTDdheiYqEcy91Nwb3r3oxr8unA=k-s128",
            "display_name": "Adam Demo_Fighter",
            "link": "https://stackoverflow.com/users/16299184/adam-demo-fighter"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68232133,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625268280,
        "creation_date": 1625263083,
        "last_edit_date": 1625264356,
        "question_id": 68231600,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68231600/pandas-i-get-dataframe-full-of-nan-when-reading-from-xlsx",
        "title": "Pandas, I get dataframe full of nan when reading from xlsx",
        "answer_body": "<p>The <strong>column</strong> parameter of <code>pd.Dataframe()</code> function doesn't set column names in result dataframe, but selects columns from the original file.</p>\n<p>See <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\" rel=\"nofollow noreferrer\">pandas documentation</a> :</p>\n<blockquote>\n<p>Column labels to use for resulting frame when data does not have them, defaulting to RangeIndex(0, 1, 2, \u2026, n). If data contains column labels, will perform column selection instead.</p>\n</blockquote>\n<p>So you shouldn't provide <code>column</code> parameter and after the file is read, rename columns of the dataframe:</p>\n<pre><code>df = pd.DataFrame(data)\ndf.columns = ['subreddit_group', 'links/caption', 'def']\n</code></pre>\n",
        "question_body": "<p>I am reading from an Excel file &quot;.xslx&quot;, it's consist of 3 columns, but when I read from it, I get a DF full of nans, I checked the table in Excel, it consists of normal cells no formulas no hyperlinks.</p>\n<p>My code:</p>\n<pre><code>data = pd.read_excel(&quot;Data.xlsx&quot;)\ndf = pd.DataFrame(data, columns=[&quot;subreddit_group&quot;, &quot;links/caption&quot;, &quot;subreddits/flair&quot;])\nprint(df)\n</code></pre>\n<p>Here is the excel file:</p>\n<p><img src=\"https://i.stack.imgur.com/tY3T9.png\" alt=\"Excel file.\" /></p>\n<p>Here is the output:</p>\n<p><img src=\"https://i.stack.imgur.com/deeth.png\" alt=\"output\" /></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22053207,
            "reputation": 49,
            "user_id": 16317158,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyBUnbLv2PI4_Agtm14oXQyk_ozZyrNZkI9w4DE=k-s128",
            "display_name": "Jewel_R",
            "link": "https://stackoverflow.com/users/16317158/jewel-r"
        },
        "is_answered": true,
        "view_count": 20,
        "accepted_answer_id": 68231157,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625259165,
        "creation_date": 1625258774,
        "question_id": 68231104,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe",
        "title": "Extract part of a 3 D dataframe",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.isin.html#pandas-index-isin\" rel=\"nofollow noreferrer\"><code>Index.isin</code></a> on the level 1 values of columns then select with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>loc</code></a>:</p>\n<pre><code>filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n</code></pre>\n<p><code>filtered_df</code>:</p>\n<pre><code>   d1      d2    \n    A   B   A   B\n0   1   2   5   6\n1   9  10  13  14\n2  17  18  21  22\n</code></pre>\n<hr />\n<p>Sample Data Used:</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n</code></pre>\n<pre><code>   d1              d2            \n    A   B   C   D   A   B   C   D\n0   1   2   3   4   5   6   7   8\n1   9  10  11  12  13  14  15  16\n2  17  18  19  20  21  22  23  24\n</code></pre>\n",
        "question_body": "<p>I have a 3d dataframe. looks like this:</p>\n<pre><code>     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n</code></pre>\n<p>How could I extract only column  A &amp; B from every d1,d2.....? I desire to take the dataframe like this:</p>\n<pre><code>    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n</code></pre>\n",
        "input_data_frames": [
            "     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n",
            "    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n"
        ],
        "output_codes": [
            "filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n",
            "import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n"
        ],
        "ques_desc": "I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: ",
        "ans_desc": "Use on the level 1 values of columns then select with : : Sample Data Used: ",
        "formatted_input": {
            "qid": 68231104,
            "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe",
            "question": {
                "title": "Extract part of a 3 D dataframe",
                "ques_desc": "I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: "
            },
            "io": [
                "     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n",
                "    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n"
            ],
            "answer": {
                "ans_desc": "Use on the level 1 values of columns then select with : : Sample Data Used: ",
                "code": [
                    "filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n",
                    "import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 16382741,
            "reputation": 3,
            "user_id": 11833378,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-cRmTvV13nk0/AAAAAAAAAAI/AAAAAAAAAOM/fk53byCeV9A/photo.jpg?sz=128",
            "display_name": "oamoralest",
            "link": "https://stackoverflow.com/users/11833378/oamoralest"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 68230973,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625257821,
        "creation_date": 1625257415,
        "question_id": 68230916,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68230916/add-update-merge-original-dataframe-into-a-grouped-dataframe",
        "title": "Add/Update/Merge original DataFrame into a grouped DataFrame",
        "answer_body": "<p>You can achieve this dynamically creating a dictionary with list comprehension like this:</p>\n<pre><code>df.groupby(['group', 'code'], as_index=False).agg({col : 'sum' for col in df.columns[3:]}\n</code></pre>\n<p>If <code>item_no</code> is your index, then change <code>df.columns[3:]</code> to <code>df.columns[2:]</code></p>\n",
        "question_body": "<p><strong>How can I merge, update, join, concat, or filter the original DF correctly so that I can have the complete 78 columns?</strong></p>\n<p>I have a DataFrame with 22 rows and 78 columns. An internet-friendly version of the file can be found <a href=\"https://www.dropbox.com/s/x0w628bzi9gegr8/original_df.csv?dl=0\" rel=\"nofollow noreferrer\">here</a>. This a sample:</p>\n<pre><code>item_no     code            group   gross_weight    net_weight  value   ... ... +70 columns more\n1           7417.85.24.25   0       18              17          13018.74\n2           1414.19.00.62   1       35              33          0.11\n3           7815.80.99.96   0       49              48          1.86\n4           1414.19.00.62   1       30              27          2.7\n5           5867.21.36.92   1       31              24          94\n6           9227.71.84.12   1       24              17          56.4\n7           1414.19.00.62   0       42              35          0.56\n8           4465.58.84.31   0       50              42          0.94\n9           1596.09.32.64   1       20              13          0.75\n10          2194.64.27.41   1       38              33          1.13\n11          1596.09.32.64   1       53              46          1.9\n12          1596.09.32.64   1       18              15          10.44\n13          1596.09.32.64   1       35              33          15.36\n14          4835.09.81.44   1       55              47          10.44\n15          5698.44.72.13   1       51              49          15.36\n16          5698.44.72.13   1       49              45          2.15\n17          5698.44.72.13   0       41              33          16\n18          3815.79.80.69   1       25              21          4\n19          3815.79.80.69   1       35              30          2.4\n20          4853.40.53.94   1       53              46          3.12\n21          4853.40.53.94   1       50              47          3.98\n22          4853.40.53.94   1       16              13          6.53\n</code></pre>\n<p>The column group gives me the instruction that I should group all similar values in the code column and add the values in the columns: 'gross_weight', 'net_weight', 'value', and 'item_quantity'. Additionally, I have to modify 2 additional columns as shown below:</p>\n<pre><code>#Group DF\ngrouped_df = df.groupby(['group', 'code'], as_index=False).agg({'item_quantity':'sum', 'gross_weight':'sum','net_weight':'sum', 'value':'sum'}).copy()\n\n#Total items should be equal to the length of the DF\ngrouped_df['total_items'] = len(grouped_df)\n\n#Item No.\ngrouped_df['item_no'] = [x+1 for x in range(len(grouped_df))]\n</code></pre>\n<p>This is the result:</p>\n<pre><code>    group   code            item_quantity   gross_weight    net_weight  value       total_items     item_no\n0   0       1414.19.00.62   75.0            42              35          0.56        14              1\n1   0       4465.58.84.31   125.0           50              42          0.94        14              2\n2   0       5698.44.72.13   200.0           41              33          16.0        14              3\n3   0       7417.85.24.25   1940.2          18              17          13018.74    14              4\n4   0       7815.80.99.96   200.0           49              48          1.86        14              5\n5   1       1414.19.00.62   275.0           65              60          2.81        14              6\n6   1       1596.09.32.64   515.0           126             107         28.45       14              7\n7   1       2194.64.27.41   151.0           38              33          1.13        14              8\n8   1       3815.79.80.69   400.0           60              51          6.4 18      14              9\n9   1       4835.09.81.44   87.0            55              47          10.44       14              10\n10  1       4853.40.53.94   406.0           119             106         13.63       14              11\n11  1       5698.44.72.13   328.0           100             94          17.51       14              12\n12  1       5867.21.36.92   1000.0          31              24          94.0        14              13\n13  1       9227.71.84.12   600.0           24              17          56.4        14              14\n</code></pre>\n<p>All of the columns in the grouped DF exist in the original DF but some have different values.</p>\n<p><strong>How can I merge, update, join, concat, or filter the original DF correctly so that I can have the complete 78 columns?</strong></p>\n<ul>\n<li>The objective DataFrame is the grouped DF.</li>\n<li>The columns in the original DF that already exists in the Grouped DF should be omitted.</li>\n<li>I should be able to take the first value of the columns in the original DF that aren't in the Grouped DF.</li>\n<li>The column code does not have unique values.</li>\n<li>The column part_number in the complete file does not have unique values.</li>\n</ul>\n<p>I tried:</p>\n<ul>\n<li>pd.Merge(how='left') after creating a unique ID; it duplicates existing columns instead of updating values or overwriting.</li>\n<li>join, concat, update: does not yield the expected results.</li>\n<li><code>.agg({lambda x: x.iloc[0]})</code> adds all the columns but I don't know how to add it to the current <code>.agg({'item_quantity':'sum', 'gross_weight':'sum','net_weight':'sum', 'value':'sum'})</code></li>\n<li>I know that <code>.agg({'column_name':'first'])</code> returns the first value, but I don't know how to make it work for over 70 columns automatically.</li>\n</ul>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "date",
            "valueerror"
        ],
        "owner": {
            "account_id": 7583125,
            "reputation": 93,
            "user_id": 5753204,
            "user_type": "registered",
            "accept_rate": 43,
            "profile_image": "https://i.stack.imgur.com/fKNZ3.jpg?s=128&g=1",
            "display_name": "Asma",
            "link": "https://stackoverflow.com/users/5753204/asma"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 68230654,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625256943,
        "creation_date": 1625241184,
        "last_edit_date": 1625256593,
        "question_id": 68228154,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68228154/select-rows-between-two-dates-recent-3-month-period",
        "title": "Select rows between two dates - recent 3 month period -",
        "answer_body": "<p>By following your code, I found that the data type of both 'start_date' and 'end_date' is Series (NOT Timestamp like df['week']). Check by:</p>\n<pre><code>type(df['week'][0]), type(start_date)\n</code></pre>\n<p>Then they cannot be compared.\nYou may try this code:<br />\n<code>id = start_date.index[0]</code></p>\n<p><code>start_date = pd.to_datetime(start_date[id])</code></p>\n<p>Note: &quot;id&quot; stores the index (based on your data, it is 199.)</p>\n",
        "question_body": "<p>I have a data frame, and I'm trying to select rows between two dates (recent 3 months period), and I found this solution <a href=\"https://www.geeksforgeeks.org/select-pandas-dataframe-rows-between-two-dates/\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>this is the head of my dataframe</p>\n<pre><code>        week    storeA  storeB  storeC  storeD  storeE\n0   2014-05-04  2643    8257    3893    6231    1294\n1   2014-05-11  6444    5736    5634    7092    2907\n2   2014-05-18  9646    2552    4253    5447    4736\n3   2014-05-25  5960    10740   8264    6063    949\n4   2014-06-01  7412    7374    3208    3985    3023\n</code></pre>\n<p>First, I convert the column 'week' of my dataframe to datetime64[ns] format:</p>\n<pre class=\"lang-py prettyprint-override\"><code>    df['week'] = pd.to_datetime(df['week'])\n</code></pre>\n<p>I get the start and end dates like this</p>\n<pre class=\"lang-py prettyprint-override\"><code>    start_date = pd.to_datetime(df.tail(1)['week'] - pd.DateOffset(months=3))\n    end_date = pd.to_datetime(df.tail(1)['week']) \n\n    # start_date : 199   2017-11-25 Name: week, dtype: datetime64[ns] \n    #end_date : 199   2018-02-25 Name: week, dtype: datetime64[ns]\n</code></pre>\n<p>Then I tried to select rows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>    mask = (df['week'] &gt; start_date) &amp; (df['week'] &lt;= end_date)\n    df.loc[mask]\n</code></pre>\n<p>Here I get this error:</p>\n<pre><code>ValueError: Can only compare identically-labeled Series objects\n</code></pre>\n<p>I tried to write dates as a string and it works:</p>\n<pre class=\"lang-py prettyprint-override\"><code>    mask = (df['week'] &gt; '2017-11-25') &amp; (df['week'] &lt;= '2018-02-25')\n    df.loc[mask]\n</code></pre>\n<p>I tried to convert start date to string and it works</p>\n<pre class=\"lang-py prettyprint-override\"><code>    start_date = str(start_date)[6:16]\n    end_date = str(end_date)[6:16]\n</code></pre>\n<p>So, what causes the error?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 16025893,
            "reputation": 37,
            "user_id": 11566142,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d2e69536157f140a0c82dc8f714c295d?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ivan7",
            "link": "https://stackoverflow.com/users/11566142/ivan7"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 68229969,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1625254302,
        "creation_date": 1625250274,
        "last_edit_date": 1625251248,
        "question_id": 68229806,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe",
        "title": "Insert values from variable and DataFrame into another DataFrame",
        "answer_body": "<p>Your mistake is on this string <code>df1[df1['id']==id]['col0']</code> when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value.</p>\n<p>To solve this issue is very very very simple, you just have to call the first item at the Series object like this: <code>df1[df1['id']==id]['col0'][0]</code></p>\n<p>Your code with the ajustment must look like this</p>\n<pre><code>import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n</code></pre>\n<p>Then your new df2 is like this:</p>\n<pre><code>   id  col0  col1  col2\n0   1     3    13    23\n1   1     3    14    24\n2   1     3    15    25\n</code></pre>\n",
        "question_body": "<p>On start I have two DataFrames and one variable:</p>\n<pre><code>id=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n</code></pre>\n<p>I have to map <strong>id</strong> variable and the corresponding <strong>col0</strong> cell from <strong>df1</strong> DataFrame to all rows in <strong>df2</strong> DataFrame. I tryed and as the result I made the code below:</p>\n<pre><code>df2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'])\n</code></pre>\n<p>It seems to me that the code should work correctly, but unfortunatelly I have a <strong>NaN</strong> value in the <strong>col0</strong> column.</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n</code></pre>\n<p>The expected result was:</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n</code></pre>\n<p>I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please:</p>\n<ol>\n<li>explain briefly why I am getting the error</li>\n<li>fix my mistake in the code</li>\n</ol>\n",
        "input_data_frames": [
            "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n",
            "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n"
        ],
        "output_codes": [
            "import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, \"id\", id)\ndf2.insert(1, \"col0\", df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n"
        ],
        "ques_desc": "On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code ",
        "ans_desc": "Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: ",
        "formatted_input": {
            "qid": 68229806,
            "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe",
            "question": {
                "title": "Insert values from variable and DataFrame into another DataFrame",
                "ques_desc": "On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code "
            },
            "io": [
                "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n",
                "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n"
            ],
            "answer": {
                "ans_desc": "Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: ",
                "code": [
                    "import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, \"id\", id)\ndf2.insert(1, \"col0\", df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 16328360,
            "reputation": 225,
            "user_id": 11792394,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/22d16718aeb83422c1151da929f4ee7d?s=128&d=identicon&r=PG&f=1",
            "display_name": "suresh",
            "link": "https://stackoverflow.com/users/11792394/suresh"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 68181888,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625248355,
        "creation_date": 1624981031,
        "last_edit_date": 1625248087,
        "question_id": 68181818,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68181818/how-to-set-the-values-of-a-row-based-on-similar-rows-in-pandas-dataframe",
        "title": "how to set the values of a row based on similar rows in pandas dataframe?",
        "answer_body": "<p>Use <code>groupby</code> (on the first column) + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html\" rel=\"nofollow noreferrer\"><code>cumcount</code></a>, add 1 (since we start counting at zero), and multiply by 10:</p>\n<pre><code>df['newcol'] = (df.groupby('col1').cumcount() + 1) * 10\n\n    col1 col2 col3  newcol\n0  data1   s1   k1      10\n1  data1   s2   k2      20\n2  data2   s4   k4      10\n3  data2   s5   k5      20\n4  data3   s6   k6      10\n5  data3   s7   k7      20\n6  data1   s8   k8      30\n7  data1   s9   k9      40\n</code></pre>\n<p>EDIT (After Question Update). You have to merge in the original database dataframe, so that you can know where to start counting with <code>(df.groupby('col1')['newcol'].transform('first')</code> and then add it to my first solution:</p>\n<pre><code>df = df.merge(db_df, on=['col1', 'col2', 'col3'], how='left')\ndf['newcol'] = df['newcol'].fillna(0).astype(int)\ndf['newcol'] = (df.groupby('col1')['newcol'].transform('max') \n             + (df.groupby('col1').cumcount()+ 1) * 10)\ndf\n</code></pre>\n",
        "question_body": "<p>I have a dataframe where i want to add a column based on duplicate values in the 1st column.Here is my dataframe:</p>\n<pre><code>df\n</code></pre>\n<pre><code>col1    col2   col3\n\ndata1    s1     k1\ndata1    s2     k2\ndata2    s4     k4\ndata2    s5     k5\ndata3    s6     k6\ndata3    s7     k7\ndata1    s8     k8\ndata1    s9     k9\n</code></pre>\n<pre><code>output I want is\n</code></pre>\n<pre><code>col1    col2   col3  newcol\n\ndata1    s1     k1    10\ndata1    s2     k2    20\ndata2    s4     k4    10\ndata2    s5     k5    20\ndata3    s6     k6    10\ndata3    s7     k7    20\ndata1    s8     k8    30\ndata1    s9     k9    40\n</code></pre>\n<pre><code>so in row :7 data1 again comes &amp; is already there in row :2 so i get set it to 30 (10 increament). I tried something like outputdf[&quot;code&quot;] = [i for i in range(10,10+len(outputdf),10)], but it doesn't work, please help me how to achieve the output.\n</code></pre>\n<pre><code>db_df = made a dataframe from the database\n</code></pre>\n<pre><code>col1    col2   col3  newcol\n\ndata1    s1     k1    30\ndata1    s2     k2    40\ndata2    s4     k4    10\n</code></pre>\n<p>In this db_df i already have data : col1(data1,data1,data2) of newcol(30,40,10) , when I create newcol in df , I want the data1 to become 40+10 &amp; data2 10+10( 40,10 are the max value of newcol in data1 &amp; data2 rows of db_df). I want to compare the df with db_df, if data1 is not there in db_df then create data1 rows 10/20... else existing max newcol value + 10, example: if db_df exists then out should be</p>\n<pre><code>col1    col2   col3  newcol\n\ndata1    s1     k1    50 \ndata1    s2     k2    60 \ndata2    s4     k4    20\ndata2    s5     k5    30\ndata3    s6     k6    10\ndata3    s7     k7    20\ndata1    s8     k8    70\ndata1    s9     k9    80\n\n</code></pre>\n<p>Now what is happening is , it is not checking whether data1 or data2 is present in db_df , so instead of Row(data1,data1,data2,data2 -- 50,60,20,30) I am getting Row(data1,data1,data2,data2 -- 10,20,10,20)</p>\n<pre><code>my output after edit code is\n0  data1   s1   k1      40\n1  data1   s2   k2      50\n2  data2   s4   k4      20\n3  data2   s5   k5      30\n4  data3   s6   k6      10\n5  data3   s7   k7      20\n6  data1   s8   k8      60\n7  data1   s9   k9      70\n</code></pre>\n<pre><code>expecting this \ndata1    s1     k1    50 \ndata1    s2     k2    60 \ndata2    s4     k4    20\ndata2    s5     k5    30\ndata3    s6     k6    10\ndata3    s7     k7    20\ndata1    s8     k8    70\ndata1    s9     k9    80\n\n</code></pre>\n<pre><code>.transform('first') returns the first non NaN value, i want to start counting from the largest value of 'newcol' in db_df , is there anyway, i tried df['newcol'] = (df.groupby('col1')['newcol'].transform(max) + (df.groupby('col1').cumcount()+ 1) * 10) but not working ,\n</code></pre>\n<p>largest values of newcol for the rows data1 is 40 &amp; data2 is 10 , so i want to start from 50 for data1 &amp; 20 for data2</p>\n<p>1 last help, this works only when 1st dataframe's(df)  col2 &amp; col3 values are same as 2nd dataframe's(db_df) col2 &amp; col3, if i change values of col2 &amp; col3 for df_df to something else , i think it will not work? please have a look</p>\n<pre><code>when db_df = \ncol1 col2 col3 newcol\n0  data1   m1   n1     20\n1  data1   m2   n2     90\n2  data2   m4   m4     50\n</code></pre>\n<p>then it's not giving the output using .transform(max).Will it only work when each row have same value in the col2 &amp; col3 column of both the DataFrame?Kindly verify</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 16501529,
            "reputation": 1760,
            "user_id": 11922765,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ed69b624cdb86e52caf0010e274df7b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mainland",
            "link": "https://stackoverflow.com/users/11922765/mainland"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 68229348,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625247933,
        "creation_date": 1625247086,
        "question_id": 68229245,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68229245/python-dataframe-filter-data-using-linear-relation",
        "title": "Python Dataframe Filter data using linear relation",
        "answer_body": "<p>You can do a linear fit first then filter out the data that is outside of a certain threshold.\nSample code below:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\n\ndf = pd.DataFrame({'ip':[10,20,30,40],'op':[105,195,500,410]})\n\n# do a linear fit on ip and op\nf = np.polyfit(df.ip,df.op,1)\n\nfl = np.poly1d(f)\n\n# you will have to determine this threshold in some way\nthreshold = 100\n\noutput = df[(df.op - fl(df.ip)).abs()&lt;threshold]\n\n</code></pre>\n",
        "question_body": "<p>I have a data frame with input and output columns. They have a linear relation. So, I want to remove data that does not fit this relation. My actual df is big and has many samples. Here, I am giving an example.</p>\n<p>My code:</p>\n<p><a href=\"https://i.stack.imgur.com/1S2CQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/1S2CQ.png\" alt=\"enter image description here\" /></a></p>\n<pre><code>xdf = pd.DataFrame({'ip':[10,20,30,40],'op':[105,195,500,410]})\n</code></pre>\n<p>I am not getting any idea on how to proceed.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "fuzzy-logic",
            "fuzzy-comparison"
        ],
        "owner": {
            "account_id": 19672854,
            "reputation": 29,
            "user_id": 14401723,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-mMaJZqFaWGg/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckHOl7aOPEE2PrNvK7sancXcf4Nyg/photo.jpg?sz=128",
            "display_name": "Sunny Reddy",
            "link": "https://stackoverflow.com/users/14401723/sunny-reddy"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68229173,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625246745,
        "creation_date": 1625243778,
        "last_edit_date": 1625246282,
        "question_id": 68228682,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68228682/compare-each-row-in-column-with-every-row-in-the-same-column-and-remove-the-row",
        "title": "Compare each row in column with every row in the same column and remove the row if match ratio is &gt; 90 with fuzzy logic in python",
        "answer_body": "<ul>\n<li>use <code>itertools.combinations</code> to get all combinations of values</li>\n<li>then <code>apply()</code> <code>fuzz.ratio()</code></li>\n<li>analyse results and select rows that don't have a strong match to another combination</li>\n</ul>\n<pre><code>import pandas as pd\nimport io\nimport itertools\nfrom fuzzywuzzy import fuzz\n\ndf = pd.read_csv(\n    io.StringIO(\n        &quot;&quot;&quot;    Page_no\n0   Hello\n2   Hey\n3   Helloo\n4   Heyy\n5   Hellooo&quot;&quot;&quot;\n    ),\n    sep=&quot;\\s+&quot;,\n)\n\n# find combinations that have greater than 80 match\ndfx = pd.DataFrame(itertools.combinations(df[&quot;Page_no&quot;].values, 2)).assign(\n    ratio=lambda d: d.apply(lambda t: fuzz.ratio(t[0], t[1]), axis=1)\n).loc[lambda d: d[&quot;ratio&quot;].gt(80)]\n\n# exclude rows that have big match to another row...\ndf.loc[~df[&quot;Page_no&quot;].isin(dfx[1])]\n\n</code></pre>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: right;\"></th>\n<th style=\"text-align: left;\">Page_no</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: left;\">Hello</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">2</td>\n<td style=\"text-align: left;\">Hey</td>\n</tr>\n</tbody>\n</table>\n</div>",
        "question_body": "<p><strong>Compare each row in column with every row in the same column and remove the row if match ratio is &gt; 90 with fuzzy logic in python. I tried removing using duplicates, but there are some rows with same content with some extra information. The data is like below</strong></p>\n<pre><code>print(df)\n</code></pre>\n<p>Output is :</p>\n<pre><code>    Page no\n0   Hello\n2   Hey\n3   Helloo\n4   Heyy\n5   Hellooo\n</code></pre>\n<p>I'm trying to compare each row with every row and remove if row matches the content with ratio greater than 90 using fuzzy logic. The expected output is :</p>\n<pre><code>    Page no\n0   Hello\n2   Hey\n</code></pre>\n<p>The code i tried is :</p>\n<pre><code>def func(name):\n    matches = df.apply(lambda row: (fuzz.ratio(row['Content'], name) &gt;= 90), axis=1)\n    print(matches)\n    return [i for i, x in enumerate(matches) if x]\n\nfunc(&quot;Hey&quot;)\n</code></pre>\n<p><strong>The above code only checks for one row with sentence Hey</strong></p>\n<p><strong>Can anyone please help me with code? It would be really helpful</strong></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 15001531,
            "reputation": 4181,
            "user_id": 10829044,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XkVFAldxdns/AAAAAAAAAAI/AAAAAAAAAAA/AKxrwcYw7mFQ9BqeKrYJYmarsmObRK9Lsw/mo/photo.jpg?sz=128",
            "display_name": "The Great",
            "link": "https://stackoverflow.com/users/10829044/the-great"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 68228377,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625242352,
        "creation_date": 1625241718,
        "last_edit_date": 1625242250,
        "question_id": 68228284,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68228284/elegant-way-to-get-duplicate-statistics-like-missing-values-using-pandas",
        "title": "Elegant way to get duplicate statistics like missing values using pandas",
        "answer_body": "<p>You can use a list comprehension:</p>\n<pre><code>df_len = len(df)\npercent_duplicates = [df[col].duplicated(keep=False).sum() * 100 / df_len\n                      for col in df]\n</code></pre>\n<p>or with <code>apply</code>:</p>\n<pre><code>percent_duplicates = df.apply(lambda col:\n                              col.duplicated(keep=False).sum() * 100 / df_len)\n</code></pre>\n<p>where we pass <code>keep=False</code> so that all the duplicates are marked as <code>True</code>,</p>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; pd.DataFrame({&quot;column_name&quot;: df.columns,\n                  &quot;percent_duplicates&quot;: percent_duplicates})\n\n  column_name  percent_duplicates\n0  subject_id                40.0\n1     test_id                40.0\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like as shown below</p>\n<pre><code>df = pd.DataFrame({'subject_id': [101,101,np.nan,201,202],\n                  'test_id':['A1','A1','A3',np.nan,'A4']})\n</code></pre>\n<p>This has a both of missing values and duplicate values in the dataframe</p>\n<p>I would like to get the statistics of missing and duplicate values across columns.</p>\n<p>It works fine for missing columns when I try the below</p>\n<pre><code>percent_missing = df.isna().sum() * 100 / len(df)\npd.DataFrame({'column_name': df.columns,\n              'percent_missing': percent_missing})\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/GsBzr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/GsBzr.png\" alt=\"enter image description here\" /></a></p>\n<p>but for duplicates, I would like to write like as above but it is clearly incorrect. It doesn't return me output column-wise (instead it provides me dataframe level)</p>\n<p>I cannot use <code>subset</code> because I would like to check for duplicate values across all columns at once like we did above for <code>missing</code> using isna()</p>\n<pre><code>percent_duplicates = df.duplicated().sum() * 100 / len(df)\npd.DataFrame({'column_name': df.columns,\n              'percent_duplicates': percent_duplicates})\n</code></pre>\n<p>I expect my output to be like as shown below</p>\n<p><a href=\"https://i.stack.imgur.com/nIDSD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/nIDSD.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 5496408,
            "reputation": 3021,
            "user_id": 4367371,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://i.stack.imgur.com/dCKEb.jpg?s=128&g=1",
            "display_name": "Mustard Tiger",
            "link": "https://stackoverflow.com/users/4367371/mustard-tiger"
        },
        "is_answered": true,
        "view_count": 216,
        "accepted_answer_id": 68227972,
        "answer_count": 5,
        "score": 8,
        "last_activity_date": 1625240386,
        "creation_date": 1624237311,
        "last_edit_date": 1624589725,
        "question_id": 68061201,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68061201/pandas-sql-co-occurrence-count",
        "title": "Pandas/SQL co-occurrence count",
        "answer_body": "<p>Only because the question is well written and it seemed like a nice puzzle, here's some magic.</p>\n<p>Potentially you'll have to store a lot of data, so you need to compress the frame as much as possible and do several passes through the base. If the database contains not primitive objects, convert those into integers, if you do multiprocessing, the dataframe will be copied into subprocesses, so keeping it contents small helps.</p>\n<p>The runtime depends on the length of the dataframe but also on the number of unique stores, unique products and the size of a chunk of pairs to count. Spreading the work to many subprocesses can speed up things but there is constant cost to all the functions which will accumulate. For example, pandas' own methods will run faster on a single ten thousand rows dataframe than on a dozen of thousand row frames. And when you're running nested calls on sub dataframes of unpredictable size things get complicated. You'll probably have to experiment a bit to find a chunksize with optimal speed\\memory usage.</p>\n<p>Test runtimes with smaller numbers first. Including less shops and products. That being said, this is not a quick task. On high end machine it completes in about ten minutes.</p>\n<pre><code>import pandas as pd, numpy as np\ndf = pd.DataFrame({\n  'store':np.random.randint(0,int(2e4),int(5e6)),\n  'product':np.random.randint(0,int(5e4),int(5e6))\n  }).sort_values('store')\n\nproducts = df['product'].unique()\nN, chunksize, Ntop = len(products), int(1e4), 200\ndtype = np.min_scalar_type(max(products.max(),N))\ndf = df.astype(dtype)\n\ndef store_cats(df):\n    df = df.astype('category')\n    cats = [df[x].cat.categories for x in df.columns]\n    for col in df.columns:\n        df[col] = df[col].cat.codes\n    return df, cats    \ndef restore_cats(summary,cats):\n    for col in ['product_x','product_y']:\n        summary[col] = pandas.Categorical.from_codes(summary[col], cats)\n\ndef subsets(n = chunksize):\n    n = int(n)\n    res = [frozenset(products[i:i+n]) for i in range(0,N,n)]\n    info = 'In total there will be {:.1E} pairs, per pass {:.1E} will be checked, thats up to around {} mb per pass, {} passes'\n    print(info.format((N**2),(n*N),(n*N*3*8/1e6),len(res)))\n    return res\n\ndef count(df,subset):\n    res = df.merge(df,on = 'store')\\\n        .query('(product_x &lt; product_y) and product_x in @subset')\\\n        .groupby(['product_x','product_y'])\\\n        .count()\\\n        .astype(dtype)\\\n        .reset_index()\n    return res \ndef one_pass(gr,subset):\n    per_group = gr.apply(count,subset)\n    total_counts = per_group.sort_values(['product_x','product_y'])\\\n        .groupby(['product_x','product_y'])\\\n        .agg('sum')\\\n        .sort_values('store',ascending=False)[:Ntop]\\\n        .copy().reset_index()\n    return total_counts\ndef merge_passes(dfs):\n    res = pd.concat(dfs,ignore_index=True)\n    res = res.append(res.rename(columns={'product_x':'product_y','product_y':'product_x'}),ignore_index=True)\n    res = res.sort_values('store',ascending=False)[:Ntop]\n    return res\n\nfrom concurrent.futures import as_completed, ProcessPoolExecutor as Pool\n\ngr = df.groupby('store',as_index = False)\ndef worker(subset):\n    return one_pass(gr,subset)\ndef run_progress(max_workers=2,chunksize=chunksize):\n    from tqdm.auto import tqdm \n    with Pool(max_workers = max_workers) as p:\n        futures = [p.submit(worker,subset) for subset in subsets(chunksize)]\n        summaries = [x.result() for x in tqdm(as_completed(futures),total=len(futures))]\n        return merge_passes(summaries)\n</code></pre>\n",
        "question_body": "<p>Lets say I have the following table/data frame:</p>\n<pre><code>d = {'store': ['s1', 's1', 's2', 's2',], 'product': ['a', 'c', 'a', 'c']}\n    df = pd.DataFrame(data=d)\n\n\nprint(df)\n    store  product\n0     s1      a                 \n1     s1      c                     \n3     s2      a                  \n4     s2      c                \n</code></pre>\n<p>I would like to find, for each pair of products the number of times they co-occur in a store.</p>\n<p>Since the data is <strong>very large</strong> (5M rows and about 50K individual products &amp; 20K individual stores) and there are many potential co-occurrence pairs, I would just like to get the top n (example: 10) co-occurrences for each product and the count of the cooccurrence. The example result is below:</p>\n<pre><code>    product_1  product_2     cooccurrence_count\n0      a           c                  2 \n1      c           a                  2\n</code></pre>\n<p><strong>An effective and efficient solution in SQL instead of pandas would also be acceptable</strong></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 15001531,
            "reputation": 4181,
            "user_id": 10829044,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XkVFAldxdns/AAAAAAAAAAI/AAAAAAAAAAA/AKxrwcYw7mFQ9BqeKrYJYmarsmObRK9Lsw/mo/photo.jpg?sz=128",
            "display_name": "The Great",
            "link": "https://stackoverflow.com/users/10829044/the-great"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 68227920,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625240203,
        "creation_date": 1625234896,
        "last_edit_date": 1625236920,
        "question_id": 68226605,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68226605/apply-multiple-quality-checks-on-pandas-dataframe-across-columns",
        "title": "Apply multiple quality checks on pandas dataframe across columns",
        "answer_body": "<p>We can iterate over the column names in the given list, then for each column check the given conditions and create the corresponding flag columns</p>\n<pre><code>cols = ['subject_id', 'test_id']\n\nfor c in cols:\n    df[c + '_missing'] = df[c].isna()\n    df[c + '_duplicated'] = df[c].duplicated(keep=False)\n    df[c + '_numeric'] = pd.to_numeric(df[c], errors='coerce') % 1 == 0\n</code></pre>\n<hr />\n<pre><code>print(df)\n\n   subject_id               test_id  subject_id_missing  subject_id_duplicated  subject_id_numeric  test_id_missing  test_id_duplicated  test_id_numeric\n0         101         A1:123,A25668               False                  False                True            False               False             True\n1         102  B1:TEST,B2456,B3#123               False                  False                True            False               False             True\n2         103                B3:456               False                  False                True            False               False             True\n3         201         B3:678,C1:345               False                  False                True            False               False             True\n4         202             C2:367,C3               False                  False                True            False               False             True\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like as shown below</p>\n<pre><code>df = pd.DataFrame({'subject_id': [101,102,103,201,202],\n                  'test_id':['A1:123,A25668','B1:TEST,B2456,B3#123','B3:456','B3:678,C1:345','C2:367,C3']})\n</code></pre>\n<p>Currently, I would like to validate whether selected columns in the dataframe meets the quality criteria</p>\n<p>a) Columns should not contain duplicates</p>\n<p>b) Column should not contain missing values</p>\n<p>c) Column should contain only numeric/integer values. There should be no string/float values</p>\n<p>So, I tried the below quality checks</p>\n<pre><code>q_1 = np.where(df['subject_id'].isna(),&quot;No&quot;,&quot;Yes&quot;)\nq_2 = np.where(df['subject_id'].duplicated(keep=False),&quot;No&quot;,&quot;Yes&quot;)\nq_3 = np.where(df['subject_id'].str.isdigit(),&quot;Yes&quot;, &quot;No&quot;) #but this throws error due int64.\nq_4 = np.where(df['test_id'].isna(),&quot;No&quot;,&quot;Yes&quot;)\nq_5 = np.where(df['test_id'].duplicated(keep=False),&quot;No&quot;,&quot;Yes&quot;)\nq_6 = np.where(df['test_id'].str.isdigit(),&quot;Yes&quot;, &quot;No&quot;) #but this throws error due int64.\n</code></pre>\n<p>How to do this elegantly and efficiently across different columns in the dataframe? you can see that I am repeating same line of code multiple times for different columns</p>\n<p>How do I verify whether the subject_id contains only integer and not string/float? I would like to check it row by row.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 4119877,
            "reputation": 61,
            "user_id": 3380537,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f158b7466ae617c659dc5c5d176f6a4d?s=128&d=identicon&r=PG&f=1",
            "display_name": "Kandarpa",
            "link": "https://stackoverflow.com/users/3380537/kandarpa"
        },
        "is_answered": true,
        "view_count": 23,
        "accepted_answer_id": 68227419,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625239498,
        "creation_date": 1625237986,
        "question_id": 68227378,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68227378/how-to-deal-with-multiple-lists-inside-multiple-column-in-df",
        "title": "how to deal with multiple lists inside multiple column in df?",
        "answer_body": "<p>try via <code>DataFrame()</code> method and <code>apply()</code>:</p>\n<pre><code>out=pd.DataFrame(df['data_list'].tolist()).apply(pd.Series.explode)\n#OR(you can also use agg() method in place of apply() method)\nout=pd.DataFrame(df['data_list'].tolist()).agg(pd.Series.explode)\n</code></pre>\n<p>Finally:</p>\n<pre><code>out.columns=['token','ext_t','symbol','name']\n</code></pre>\n<p>Now If you print <code>out</code> you will get your expected output</p>\n",
        "question_body": "<p>I have a df like this</p>\n<pre><code>                                            data_list\n0   [['13878018', '13878274'], ['54211', '54212'], ['AARTIIND21JUL850PE', 'AARTIIND21JUL860CE'], ['AARTIIND', 'AARTIIND']]\n1   [['13099778', '13100034'], ['51171', '51172'], ['ABFRL21JUL210PE', 'ABFRL21JUL215CE'], ['ABFRL', 'ABFRL']]\n2   [['13910018', '13910274'], ['54336', '54337'], ['ACC21JUL1980PE', 'ACC21JUL2000CE'], ['ACC', 'ACC']]\n</code></pre>\n<p>and I want to convert it to</p>\n<pre><code>    name           token    ext_t      symbol\n0   AARTIIND    13878018    54211       AARTIIND21JUL850PE\n1   AARTIIND    13878274    54212       AARTIIND21JUL860CE\n2   ABFRL       13099778    51171       ABFRL21JUL210PE\n3   ABFRL       13100034    51172       ABFRL21JUL215CE\n4   ACC         13910018    54336       ACC21JUL1980PE\n5   ACC         13910274    54337       ACC21JUL2000CE\n</code></pre>\n<p>How can I achieve this?</p>\n<p>I tried to apply pd.series and I got an output like this</p>\n<pre><code>                      0               1                                               2                         3\n0  [13878018, 13878274]  [54211, 54212]        [AARTIIND21JUL850PE, AARTIIND21JUL860CE]      [AARTIIND, AARTIIND]\n1  [13099778, 13100034]  [51171, 51172]              [ABFRL21JUL210PE, ABFRL21JUL215CE]            [ABFRL, ABFRL]\n2  [13910018, 13910274]  [54336, 54337]                [ACC21JUL1980PE, ACC21JUL2000CE]                [ACC, ACC]\n</code></pre>\n<p>I am not sure how to proceed next. Can anyone help please?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "parquet"
        ],
        "owner": {
            "account_id": 8077990,
            "reputation": 407,
            "user_id": 9904934,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/7da1540967426a07d9b0dc48f754bb32?s=128&d=identicon&r=PG&f=1",
            "display_name": "ronald mcdolittle",
            "link": "https://stackoverflow.com/users/9904934/ronald-mcdolittle"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68226812,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625236881,
        "creation_date": 1625235231,
        "last_edit_date": 1625236881,
        "question_id": 68226692,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68226692/pandas-dataframe-concat-upsert-on-a-combination-key",
        "title": "pandas Dataframe Concat/Upsert on a Combination Key",
        "answer_body": "<p>Is this expected output? Based on your</p>\n<blockquote>\n<p>Basically, if the ID and season are the same update the existing\nrecord, and if they're different add a new record.</p>\n</blockquote>\n<p>We concatenate two dataframes, group by <code>ID</code> and <code>Year</code> and leave the last (thus, coming from <code>df2</code>) element in each group.</p>\n<pre><code>&gt;&gt;&gt; pd.concat([df1, df2]).groupby([&quot;ID&quot;, &quot;Year&quot;], as_index=False).last()\n    ID  Year       Name balance\n0  112  2020  Johnstown    $321\n1  112  2021  Johnstown    $321\n2  121  2020    Jackson    $254\n3  321  2020     Oregon    $216\n</code></pre>\n",
        "question_body": "<p>Let's say I have the following dataset</p>\n<pre><code>ID | Name | balance | Year\n112 Johnstown $321 2020\n321 Oregon $214 2020\n121 Jackson $254 2020\n</code></pre>\n<p>and the following incoming dataset</p>\n<pre><code>112 Johnstown $321 2021\n321 Oregon $216 2020\n121 Jackson $254 2020\n</code></pre>\n<p>What I want to do is combine these two datasets while retaining a concept of seasonality. Basically, if a record has any field other than the year changed, update that record. However, if the record had the year changed, then make a new record.</p>\n<p>So in our case, the result dataset would look like this</p>\n<pre><code>112 Johnstown $321 2021\n112 Johnstown $321 2020\n321 Oregon $216 2020\n121 Jackson $254 2020\n</code></pre>\n<p>This is essentially an upsert operation, the way I'm thinking about it is as an upsert on a combination key of ID and season. Basically, if the ID and season are the same update the existing record, and if they're different add a new record. In other words</p>\n<ol>\n<li>If a record is exactly the same do nothing</li>\n<li>If a record is different but year/id are the same take the record\nfrom the new dataset</li>\n</ol>\n<p>3.If a record is different and year/id are\ndifferent make a new record</p>\n<p>Is this possible with dataframes? If not, is there another structure I should look at for implementing this? Our datasets are just parquet files, so we're free to manipulate them however we like</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21703267,
            "reputation": 21,
            "user_id": 16016164,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c5b5ca86a797c3ed0ddafd04735a857c?s=128&d=identicon&r=PG&f=1",
            "display_name": "Work_fl0w",
            "link": "https://stackoverflow.com/users/16016164/work-fl0w"
        },
        "is_answered": true,
        "view_count": 21,
        "accepted_answer_id": 68226341,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625236839,
        "creation_date": 1625233144,
        "last_edit_date": 1625233916,
        "question_id": 68226202,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68226202/df-analyse-with-pandas-in-python-filter-data",
        "title": "DF analyse with Pandas in Python filter data",
        "answer_body": "<p>you can try:</p>\n<pre><code>m=(df['day'].isin([7,14,21,28])) &amp; (df['countriesAndTerritories'].isin(['USA','Germany']))\n#If the names are exact 'USA' and 'Germany'\n#OR\nm=(df['day'].isin([7,14,21,28])) &amp; (df['countriesAndTerritories'].str.contains('USA|Germany',case=False))\n#IF the names are in irregular case i.e some are in uppercase and some are in lowercase\n</code></pre>\n<p>Finally:</p>\n<pre><code>df[m]\n#OR\ndf.loc[m]\n</code></pre>\n",
        "question_body": "<p>hey stack overflow users,</p>\n<p>i have the following problem. i have a table with informations about the incidence values of the individual countries.</p>\n<p>I want to display the data in such a way that I can compare the incidence values of the USA with Germany, for example.</p>\n<p>my problem is that the incidence values are accumulated. How can I filter out only the values of USA &amp; Germany from the column day = 14.</p>\n<p>As a result I want to see only the 14 days values in the respective rows, so that I can draw a temporal comparison of the incidence values.</p>\n<p>DATA PREVIEW:</p>\n<p><a href=\"https://i.stack.imgur.com/MS3vQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/MS3vQ.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 15001531,
            "reputation": 4181,
            "user_id": 10829044,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XkVFAldxdns/AAAAAAAAAAI/AAAAAAAAAAA/AKxrwcYw7mFQ9BqeKrYJYmarsmObRK9Lsw/mo/photo.jpg?sz=128",
            "display_name": "The Great",
            "link": "https://stackoverflow.com/users/10829044/the-great"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68225663,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625230890,
        "creation_date": 1625230638,
        "question_id": 68225629,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68225629/pandas-access-list-element-within-a-dataframe",
        "title": "Pandas access list element within a dataframe?",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html\" rel=\"nofollow noreferrer\"><code>Series.str.extract</code></a> if need values before first <code>:</code> with <code>^</code> for match start of string and <code>.*</code> for any value before <code>:</code>:</p>\n<pre><code>df['new_test_id'] = df['new_test'].str.extract(&quot;^(.*):&quot;)\nprint (df)\n   person_id               test_id new_test new_test_id\n0        101         A1:123,A25668   A1:123          A1\n0        101         A1:123,A25668   A25668         NaN\n1        102  B1:TEST,B2456,B3#123  B1:TEST          B1\n1        102  B1:TEST,B2456,B3#123    B2456         NaN\n1        102  B1:TEST,B2456,B3#123   B3#123         NaN\n2        103                B3:456   B3:456          B3\n3        201         B3:678,C1:345   B3:678          B3\n3        201         B3:678,C1:345   C1:345          C1\n4        202             C2:367,C3   C2:367          C2\n4        202             C2:367,C3       C3         NaN\n</code></pre>\n<p>Your solution is with selectting by <code>str[0]</code>, but need also set <code>NaN</code>s if no match <code>:</code>:</p>\n<pre><code>df['new_test_id'] = df['new_test'].str.split(&quot;:&quot;).str[0].where(df['new_test'].str.contains(&quot;:&quot;))\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like as shown below</p>\n<pre><code>df = pd.DataFrame({'person_id': [101,102,103,201,202],\n                  'test_id':['A1:123,A25668','B1:TEST,B2456,B3#123','B3:456','B3:678,C1:345','C2:367,C3']})\n</code></pre>\n<p>I would like to extract the portion before <code>:</code> character to a new column.</p>\n<p>I tried the below but it doesn't work well</p>\n<pre><code> df['new_test'] = df['test_id'].str.split(&quot;,&quot;)\n df= df.explode('new_test')\n df['new_test_id'] = df['new_test'].str.split(&quot;:&quot;)[:0]  #what index should I give here?\n</code></pre>\n<p>Whatever I give in the start and slice operator, am not able to get the 1st item from the list to the <code>new_test_id</code></p>\n<p>I expect my output to be like as shown below</p>\n<pre><code>df['new_test_id']\nA1\nNaN\nB1\nNaN\nB3\nB3\nB3\nC3\nNaN\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21072066,
            "reputation": 43,
            "user_id": 15488129,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/66fe9503838debd29c35f0c14d9bee44?s=128&d=identicon&r=PG&f=1",
            "display_name": "girolamo",
            "link": "https://stackoverflow.com/users/15488129/girolamo"
        },
        "is_answered": true,
        "view_count": 20,
        "accepted_answer_id": 68225485,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625229974,
        "creation_date": 1625229892,
        "question_id": 68225470,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68225470/best-way-to-impute-multiple-columns-nan-values-with-their-mean",
        "title": "Best way to impute multiple columns NaN values with their mean",
        "answer_body": "<p>If need replace <code>NaN</code>s in all columns (and all columns are numeric):</p>\n<pre><code>df = df.fillna(df.mean())\nprint (df)\n   one  two  three\n0  1.0  2.5      1\n1  2.0  2.0      2\n2  3.0  3.0      3\n3  2.0  2.5      4\n</code></pre>\n<p>If need specify columns by names in list:</p>\n<pre><code>c = ['one','two']\ndf[c] = df[c].fillna(df[c].mean())\n</code></pre>\n<p>Or if need replace only numeric columns:</p>\n<pre><code>c = df.select_dtypes(np.number).columns\ndf[c] = df[c].fillna(df[c].mean())\n</code></pre>\n",
        "question_body": "<p>I'm still new to Python</p>\n<p>I need to write a function that imputes the NaN values of 2+ df columns with their mean.\nI've tried several ways that work on the single column but don't work when combined.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\nexample = {'one':[1,2,3,np.nan],\n           'two':[np.nan,2,3,np.nan],\n           'three':[1,2,3,4]}\n\ndf = pd.DataFrame(example)\n</code></pre>\n<p>What I need is to impute NaN values with the mean of the series.</p>\n<p>I've already tried:</p>\n<ul>\n<li>Calculating the mean of each column, then filling the NaN values with the calculated mean. It works but seems that It can't be automated.</li>\n</ul>\n<pre><code>df['one'] = df['one'].fillna(df['one].mean(), inplace=True)\n</code></pre>\n<ul>\n<li>Imputing every single column with sklearn.SimpleImputer, but even if I reshape the fit and transformed array, can't find a way to automate to multiple columns.</li>\n</ul>\n<pre><code>from sklearn.impute import SimpleImputer\nSI = SimpleImputer(strategy='mean')\n\nX = np.array(df['one'])\nSI.fit_transform(X.reshape(-1,1)\n</code></pre>\n<p>Could you help?</p>\n<p>Thanks in advance.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "in-place"
        ],
        "owner": {
            "account_id": 9754244,
            "reputation": 1858,
            "user_id": 7231526,
            "user_type": "registered",
            "accept_rate": 70,
            "profile_image": "https://www.gravatar.com/avatar/1326dc7779cdb7364af2b50b063dce36?s=128&d=identicon&r=PG&f=1",
            "display_name": "theprowler",
            "link": "https://stackoverflow.com/users/7231526/theprowler"
        },
        "is_answered": true,
        "view_count": 80669,
        "accepted_answer_id": 45147491,
        "answer_count": 4,
        "score": 56,
        "last_activity_date": 1625229777,
        "creation_date": 1500302292,
        "last_edit_date": 1568072312,
        "question_id": 45147100,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/45147100/pandas-drop-columns-with-all-nans",
        "title": "Pandas: drop columns with all NaN&#39;s",
        "answer_body": "<p>From the <code>dropna</code> docstring:</p>\n<h5>Drop the columns where all elements are NaN:</h5>\n<pre><code>df.dropna(axis=1, how='all')\n\n\n   A    B    D\n0  NaN  2.0  0\n1  3.0  4.0  1\n2  NaN  NaN  5\n</code></pre>\n",
        "question_body": "<p>I realize that dropping <code>NaN</code>s from a dataframe is as easy as <code>df.dropna</code> but for some reason that isn't working on mine and I'm not sure why.</p>\n\n<p>Here is my original dataframe:</p>\n\n<pre><code>fish_frame1:                       0   1   2         3   4       5   6          7\n0               #0915-8 NaN NaN       NaN NaN     NaN NaN        NaN\n1                   NaN NaN NaN  LIVE WGT NaN  AMOUNT NaN      TOTAL\n2               GBW COD NaN NaN     2,280 NaN   $0.60 NaN  $1,368.00\n3               POLLOCK NaN NaN     1,611 NaN   $0.01 NaN     $16.11\n4                 WHAKE NaN NaN       441 NaN   $0.70 NaN    $308.70\n5           GBE HADDOCK NaN NaN     2,788 NaN   $0.01 NaN     $27.88\n6           GBW HADDOCK NaN NaN    16,667 NaN   $0.01 NaN    $166.67\n7               REDFISH NaN NaN       932 NaN   $0.01 NaN      $9.32\n8    GB WINTER FLOUNDER NaN NaN       145 NaN   $0.25 NaN     $36.25\n9   GOM WINTER FLOUNDER NaN NaN    25,070 NaN   $0.35 NaN  $8,774.50\n10        GB YELLOWTAIL NaN NaN        26 NaN   $1.75 NaN     $45.50\n</code></pre>\n\n<p>The code that follows is an attempt to drop all <code>NaN</code>s as well as any columns with more than 3 <code>NaN</code>s (either one, or both, should work I think):</p>\n\n<pre><code>fish_frame.dropna()\nfish_frame.dropna(thresh=len(fish_frame) - 3, axis=1)\n</code></pre>\n\n<p>This produces: </p>\n\n<pre><code>fish_frame1 after dropna:                       0   1   2         3   4       5   6          7\n0               #0915-8 NaN NaN       NaN NaN     NaN NaN        NaN\n1                   NaN NaN NaN  LIVE WGT NaN  AMOUNT NaN      TOTAL\n2               GBW COD NaN NaN     2,280 NaN   $0.60 NaN  $1,368.00\n3               POLLOCK NaN NaN     1,611 NaN   $0.01 NaN     $16.11\n4                 WHAKE NaN NaN       441 NaN   $0.70 NaN    $308.70\n5           GBE HADDOCK NaN NaN     2,788 NaN   $0.01 NaN     $27.88\n6           GBW HADDOCK NaN NaN    16,667 NaN   $0.01 NaN    $166.67\n7               REDFISH NaN NaN       932 NaN   $0.01 NaN      $9.32\n8    GB WINTER FLOUNDER NaN NaN       145 NaN   $0.25 NaN     $36.25\n9   GOM WINTER FLOUNDER NaN NaN    25,070 NaN   $0.35 NaN  $8,774.50\n10        GB YELLOWTAIL NaN NaN        26 NaN   $1.75 NaN     $45.50\n</code></pre>\n\n<p>I am a novice with <code>Pandas</code> so I'm not sure if this isn't working because I'm doing something wrong or I'm misunderstanding something or misusing a command. Any help is appreciated thanks. </p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 14016043,
            "reputation": 33,
            "user_id": 10123553,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c7093be39ce653917ece2b82676ea0a4?s=128&d=identicon&r=PG&f=1",
            "display_name": "Marra",
            "link": "https://stackoverflow.com/users/10123553/marra"
        },
        "is_answered": true,
        "view_count": 53,
        "accepted_answer_id": 68209735,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1625226378,
        "creation_date": 1625139171,
        "last_edit_date": 1625225682,
        "question_id": 68209418,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68209418/calculate-cumulative-sum-based-on-threshold-and-condition-in-another-column-nump",
        "title": "Calculate cumulative sum based on threshold and condition in another column numpy",
        "answer_body": "<p>I hope I've understood your question right. This example will substract necessary value (&quot;reset&quot;) when cumulative sum of sale is greater than 5 and IsSuccess==True:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df[&quot;SumSale&quot;] = df[&quot;Sale&quot;].cumsum()\n\n# &quot;reset&quot; when SumSale&gt;5 and IsSuccess==True\nm = df[&quot;SumSale&quot;].gt(5) &amp; df[&quot;IsSuccess&quot;].eq(True)\ndf.loc[m, &quot;to_remove&quot;] = df[&quot;SumSale&quot;]\ndf[&quot;to_remove&quot;] = df[&quot;to_remove&quot;].ffill().shift().fillna(0)\ndf[&quot;SumSale&quot;] -= df[&quot;to_remove&quot;]\n\ndf = df.drop(columns=&quot;to_remove&quot;)\n\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>   Sale  IsSuccess  SumSale\n0     1      False      1.0\n1     2       True      3.0\n2     3      False      6.0\n3     2      False      8.0\n4     4       True     12.0\n5     3      False      3.0\n6     5       True      8.0\n7     5      False      5.0\n</code></pre>\n<hr />\n<p>EDIT:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def fn():\n    sale, success = yield\n    cum = sale\n    while True:\n        sale, success = yield cum\n        if success and cum &gt; 5:\n            cum = sale\n        else:\n            cum += sale\n\n\ns = fn()\nnext(s)\ndf[&quot;ss&quot;] = df[&quot;IsSuccess&quot;].shift()\ndf[&quot;SumSale&quot;] = df.apply(lambda x: s.send((x[&quot;Sale&quot;], x[&quot;ss&quot;])), axis=1)\ndf = df.drop(columns=&quot;ss&quot;)\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>   Sale  IsSuccess  SumSaleExpected  SumSale\n0    10      False               10       10\n1     2       True               12       12\n2     2      False                2        2\n3     1      False                3        3\n4     3       True                6        6\n5     2      False                2        2\n6     1       True                3        3\n7     3      False                6        6\n8     5      False               11       11\n9     5      False               16       16\n</code></pre>\n",
        "question_body": "<p>I have a data frame and I'd like to calculate cumulative sum based on 2 conditions:</p>\n<ul>\n<li>1st which is a boolean already in the table</li>\n<li>and a fixed threshold that checks what's the cumulative sum.</li>\n</ul>\n<p>I've succeed with 1st or 2nd but I find it hard to combine both.</p>\n<p>For the first one I used groupby</p>\n<pre><code>df['group'] = np.cumsum((df['IsSuccess'] != df['IsSuccess'].shift(1)))\ndf['SumSale'] = df[['Sale', 'group']].groupby('group').cumsum()\n</code></pre>\n<p>For the 2nd frompyfunc</p>\n<pre><code>sumlm = np.frompyfunc(lambda a,b: b if (a+b&gt;5) else a+b, 2, 1)\ndf['SumSale'] = sumlm.accumulate(df['Sale'], dtype=object)\n</code></pre>\n<p>My df is, and the SumSale is the result I'm looking for.</p>\n<pre><code>df2 = pd.DataFrame({'Sale': [10, 2, 2, 1, 3, 2, 1, 3, 5, 5],\n                 'IsSuccess': [False, True, False, False, True, False, True, False, False, False],\n                 'SumSaleExpected': [10, 12, 2, 3, 6, 2, 3, 6, 11, 16]})\n</code></pre>\n<p>So to summarize I'd like to start cumulating the sum once that sum is over 5 and the row IsSuccess is True. I'd like to avoid for loop if possible as well.</p>\n<p>Thank you for help!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nlp",
            "spacy"
        ],
        "owner": {
            "account_id": 14803986,
            "reputation": 271,
            "user_id": 10691504,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/zSc2n.png?s=128&g=1",
            "display_name": "loving_guy",
            "link": "https://stackoverflow.com/users/10691504/loving-guy"
        },
        "is_answered": true,
        "view_count": 948,
        "accepted_answer_id": 58852371,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625221858,
        "creation_date": 1573664230,
        "last_edit_date": 1625221858,
        "question_id": 58841995,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/58841995/using-nlp-pipe-in-spacy-to-get-doc-objects-for-dataframe-column",
        "title": "Using nlp.pipe() in Spacy to get doc objects for Dataframe column",
        "answer_body": "<p>According to the Spacy Documentation of <code>Doc</code> object <a href=\"https://spacy.io/api/doc\" rel=\"nofollow noreferrer\">here</a>, the <code>__len__</code> operator gets \"the number of tokens in the document.\".</p>\n\n<p>The last text in your data is: </p>\n\n<pre><code>&gt;&gt;&gt; df['text'].values[-1]\n@AmericanAir we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?\n</code></pre>\n\n<p>After running the <code>nlp.pipe()</code> method, this sentence will be tokenized into 32 tokens which what you're asking for. To verfiy that, try runn the following code after <code>len(text)</code> and will get the exact result:</p>\n\n<pre><code>&gt;&gt;&gt; last_tokens = [token for token in text]\n&gt;&gt;&gt; last_tokens\n[@AmericanAir, we, have, 8, ppl, so, we, need, 2, know, how, many, seats, are, on, the, next, flight, ., Plz, put, us, on, standby, for, 4, people, on, the, next, flight, ?]\n\n&gt;&gt;&gt; len(last_tokens)\n32\n</code></pre>\n\n<h2>EDIT</h2>\n\n<p>You can iterate over the tokens of each <code>doc</code> returned from the pipeline like so:</p>\n\n<pre><code>nlp = spacy.load(\"en_core_web_sm\")\nfor text in nlp.pipe(iter(df['text']), batch_size = 1000, n_threads=-1):\n    for token in text:\n        print(token)\n    print('\\n')\n</code></pre>\n",
        "question_body": "<p>I am using Spacy nlp.pipe() for getting doc objects for text data in pandas Dataframe column but the parsed text returned as &quot;text&quot; in the code has <strong>length</strong> of only <strong>32</strong>. However, <strong>the shape of dataframe</strong> is <strong>(14640, 16)</strong>.\nHere is the data <a href=\"https://www.kaggle.com/crowdflower/twitter-airline-sentiment\" rel=\"nofollow noreferrer\">link</a> if someone wants to read the data.</p>\n<pre><code>nlp = spacy.load(&quot;en_core_web_sm&quot;)\nfor text in nlp.pipe(iter(df['text']), batch_size = 1000, n_threads=-1):\n  print(text)\n\nlen(text)\n</code></pre>\n<p>Result:</p>\n<pre><code>32\n</code></pre>\n<p>Can someone help me with this what is going on? What I am doing wrong?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary",
            "listview"
        ],
        "owner": {
            "account_id": 14311695,
            "reputation": 388,
            "user_id": 10337789,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-QT110ytXo9Q/AAAAAAAAAAI/AAAAAAAAAAA/APUIFaNJIZI_t0bnH5mpDqgCjRW21yqcCg/mo/photo.jpg?sz=128",
            "display_name": "\u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440 \u041a\u0443\u0437\u043e\u0432\u043a\u0438\u043d",
            "link": "https://stackoverflow.com/users/10337789/%d0%92%d0%bb%d0%b0%d0%b4%d0%b8%d0%bc%d0%b8%d1%80-%d0%9a%d1%83%d0%b7%d0%be%d0%b2%d0%ba%d0%b8%d0%bd"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68223460,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625221035,
        "creation_date": 1625219897,
        "question_id": 68223193,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68223193/convert-dataframe-with-to-hierarchical-dictionary-in-python",
        "title": "convert dataframe with to hierarchical dictionary in python",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_tree(df, parent=0):\n    out = []\n    for _, row in df[df.parent_id == parent].iterrows():\n        out.append({&quot;id&quot;: row[&quot;id&quot;], &quot;title&quot;: row[&quot;title&quot;]})\n\n        subs = df[df.parent_id == row[&quot;id&quot;]]\n        if len(subs) &gt; 0:\n            out[-1][&quot;subs&quot;] = get_tree(df, parent=row[&quot;id&quot;])\n\n    return out\n\n\nprint(get_tree(df))\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-json prettyprint-override\"><code>[\n    {\n        &quot;id&quot;: 1,\n        &quot;title&quot;: &quot;Math&quot;,\n        &quot;subs&quot;: [\n            {\n                &quot;id&quot;: 2,\n                &quot;title&quot;: &quot;Algebra&quot;,\n                &quot;subs&quot;: [\n                    {\n                        &quot;id&quot;: 3,\n                        &quot;title&quot;: &quot;Polynomials&quot;,\n                        &quot;subs&quot;: [{&quot;id&quot;: 4, &quot;title&quot;: &quot;sum of polynomials&quot;}],\n                    }\n                ],\n            }\n        ],\n    },\n    {\n        &quot;id&quot;: 5,\n        &quot;title&quot;: &quot;Physics&quot;,\n        &quot;subs&quot;: [\n            {\n                &quot;id&quot;: 6,\n                &quot;title&quot;: &quot;Mechanics&quot;,\n                &quot;subs&quot;: [\n                    {&quot;id&quot;: 7, &quot;title&quot;: &quot;Kinematics &quot;},\n                    {&quot;id&quot;: 8, &quot;title&quot;: &quot;Dynamics&quot;},\n                ],\n            }\n        ],\n    },\n]\n</code></pre>\n",
        "question_body": "<p>I have dataframe with hierarchical structure:</p>\n<pre><code>df = pd.DataFrame(\n    {\n        'parent_id': [0,1,2,3,0,5,6,6],\n        'id': [1,2,3,4,5,6,7,8],\n        'title':['Math',\n            'Algebra',\n            'Polynomials',\n            'sum of polynomials',\n            'Physics',\n            'Mechanics',\n            'Kinematics ',\n            'Dynamics']\n    }\n)\n</code></pre>\n<p>I table the structure is:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">id</th>\n<th style=\"text-align: center;\">parent_id</th>\n<th style=\"text-align: right;\">title</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: right;\">Math</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">2</td>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: right;\">Algebra</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">3</td>\n<td style=\"text-align: center;\">2</td>\n<td style=\"text-align: right;\">Polynomials</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">4</td>\n<td style=\"text-align: center;\">3</td>\n<td style=\"text-align: right;\">sum of polynomials</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">5</td>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: right;\">Physics</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">6</td>\n<td style=\"text-align: center;\">5</td>\n<td style=\"text-align: right;\">Mechanics</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">7</td>\n<td style=\"text-align: center;\">6</td>\n<td style=\"text-align: right;\">Kinematics</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">8</td>\n<td style=\"text-align: center;\">6</td>\n<td style=\"text-align: right;\">Dynamics</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I need to get hierarchical dictionary:</p>\n<pre><code>SampleJSONData_2 = [\n{\n    'id': 1,\n    'title': 'Math',\n    'subs': [\n        {\n            'id': 2,\n            'title': 'Algebra',\n            'subs': [\n                {\n                    'id': 3,\n                    'title': 'Polynomials',\n                    'subs':\n                        [\n                            {\n                                'id': 4,\n                                'title': 'sum of polynomials',\n                            }\n                        ]\n                }]\n        }]\n},\n{\n    'id': 5,\n    'title': 'Physics',\n    'subs': [\n        {\n            'id': 6,\n            'title': 'Mechanics',\n            'subs': [\n                {\n                    'id': 7,\n                    'title': 'Kinematics'\n                },\n                {\n                    'id': 8,\n                    'title': 'Dynamics'\n                }\n\n            ]\n        }]\n}\n</code></pre>\n<p>]</p>\n<p>How should I solve my problem?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-visualization",
            "analytics"
        ],
        "owner": {
            "account_id": 19707837,
            "reputation": 15,
            "user_id": 14429067,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d29f1bd5e2be0852d3a3df6dd7746f5a?s=128&d=identicon&r=PG&f=1",
            "display_name": "Shreshth Narayan Singh",
            "link": "https://stackoverflow.com/users/14429067/shreshth-narayan-singh"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 68222693,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1625218923,
        "creation_date": 1625216625,
        "question_id": 68222462,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68222462/what-does-the-following-statement-do-df-groupbylevelattempt-mean",
        "title": "What does the following statement do - df.groupby(&quot;level&quot;)[&quot;attempt&quot;].mean()?",
        "answer_body": "<p>Here's the top-level description from the pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>pandas.DataFrame.groupby</code></a> docs:</p>\n<p>A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups.</p>\n<p>In this context, you are using the values of the column 'level' to split the dataframe.</p>\n<p>You then select only the column &quot;attempt&quot; and apply the mean function, the values of which are then combined back together.</p>\n<p>So in english, your results are the mean value for all records of each 'level'. That is, in the example below, the mean attempt value for all records of level 1 is 0.75144</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport io\nimport requests\nresp = requests.get('https://raw.githubusercontent.com/whitehatjr/Data-Analysis-by-visualisation/master/data.csv')\ndf = pd.read_csv(io.BytesIO(resp.content))\ndf.groupby(&quot;level&quot;)[&quot;attempt&quot;].mean().reset_index()\n</code></pre>\n<p>output:</p>\n<pre><code>     level   attempt\n0  Level 1  0.751445\n1  Level 2  0.863281\n2  Level 3  0.698113\n3  Level 4  0.734694\n</code></pre>\n",
        "question_body": "<p>Ok, so I was looking through some data analysis (very <em>basic</em>) projects. I came across this line-</p>\n<pre><code>print(df.groupby(&quot;level&quot;)[&quot;attempt&quot;].mean())\n</code></pre>\n<p>Where df is the dataframe of the file <a href=\"https://raw.githubusercontent.com/whitehatjr/Data-Analysis-by-visualisation/master/data.csv\" rel=\"nofollow noreferrer\">https://raw.githubusercontent.com/whitehatjr/Data-Analysis-by-visualisation/master/data.csv</a></p>\n<p>Basically, as far as I can tell, It is the data of Grade 3 students who attempted a quiz, which had levels. Now, the only usages of groupby() I knew were -</p>\n<pre><code>#First Usage\nq = df.groupby('')\n\n#Second Usage\nw = df.groupby(['', ''])\n</code></pre>\n<p>Can someone please explain to me, what the statement print(df.groupby(&quot;level&quot;)[&quot;attempt&quot;].mean()) actually is?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "join",
            "merge"
        ],
        "owner": {
            "account_id": 22036818,
            "reputation": 25,
            "user_id": 16303180,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxmRjS_p0GtNCtVKhl5scPV68BUbaa5fsXwOOid=k-s128",
            "display_name": "Hoi Lun Sin ",
            "link": "https://stackoverflow.com/users/16303180/hoi-lun-sin"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 68222684,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625217708,
        "creation_date": 1625215316,
        "question_id": 68222155,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68222155/how-to-join-two-dataframe-with-same-category",
        "title": "How to join two dataframe with same category?",
        "answer_body": "<pre><code>&gt;&gt;&gt; categories\n['ATIDS', 'BasicCrane', 'LLP', 'Beam Sensor', 'CLPS', 'SPR']\n\n&gt;&gt;&gt; pd.merge(pd.DataFrame({'Cat': categories}), df, how='outer')\n           Cat    UR3   VR1    VR   VR3\n0        ATIDS  137.0  99.0  40.0  84.0\n1   BasicCrane    2.0   8.0   3.0   1.0\n2          LLP    NaN   NaN   NaN   NaN\n3  Beam Sensor   27.0  12.0  13.0  14.0\n4         CLPS    1.0   NaN   NaN   1.0\n5          SPR    NaN   NaN   NaN   NaN\n</code></pre>\n",
        "question_body": "<p>Now, I have two dataframe. I have use groupby. and count() function to export this dataframe(df1). When I used groupby. to count the total number of each category. It filtered out the category which the count is 0. How can I use Python to get the outcome?</p>\n<p>However,I will like to have a dataframe which also required categories.</p>\n<p>Original dataframe:</p>\n<pre><code>    Cat           UR3     VR1    VR    VR3\n0   ATIDS         137.0   99.0   40.0  84.0\n1   BasicCrane    2.0     8.0    3.0   1.0\n2   Beam Sensor   27.0    12.0   13.0  14.0\n3   CLPS          1.0     NaN    NaN   1.0\n</code></pre>\n<p>However,I will like to have a dataframe which also required categories.\n(required categories: ATIDS, BasicCrane, LLP, Beam Sensor, CLPS, SPR)</p>\n<p>Expected dataframe (The count number of 'LLP' and 'SPR' is 0)</p>\n<pre><code>    Cat           UR3     VR1    VR    VR3\n0   ATIDS         137.0   99.0   40.0  84.0\n1   BasicCrane    2.0     8.0    3.0   1.0\n2   LLP           NaN     NaN    NaN   NaN\n3   Beam Sensor   27.0    12.0   13.0  14.0\n4   CLPS          1.0     NaN    NaN   1.0\n5   SPR           NaN     NaN    NaN   NaN\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "account_id": 74159,
            "reputation": 26249,
            "user_id": 213216,
            "user_type": "registered",
            "accept_rate": 98,
            "profile_image": "https://www.gravatar.com/avatar/bbe98ae51e25f9816052540b75929469?s=128&d=identicon&r=PG",
            "display_name": "user7289",
            "link": "https://stackoverflow.com/users/213216/user7289"
        },
        "is_answered": true,
        "view_count": 555711,
        "protected_date": 1562191904,
        "accepted_answer_id": 19913845,
        "answer_count": 10,
        "score": 380,
        "last_activity_date": 1625213847,
        "creation_date": 1384195926,
        "last_edit_date": 1595722711,
        "question_id": 19913659,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/19913659/pandas-conditional-creation-of-a-series-dataframe-column",
        "title": "Pandas conditional creation of a series/dataframe column",
        "answer_body": "<p><strong>If you only have two choices to select from:</strong></p>\n\n<pre><code>df['color'] = np.where(df['Set']=='Z', 'green', 'red')\n</code></pre>\n\n<p>For example,</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\ndf['color'] = np.where(df['Set']=='Z', 'green', 'red')\nprint(df)\n</code></pre>\n\n<p>yields</p>\n\n<pre><code>  Set Type  color\n0   Z    A  green\n1   Z    B  green\n2   X    B    red\n3   Y    C    red\n</code></pre>\n\n<hr>\n\n<p><strong>If you have more than two conditions then use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.select.html\" rel=\"noreferrer\"><code>np.select</code></a></strong>. For example, if you want <code>color</code> to be </p>\n\n<ul>\n<li><code>yellow</code> when <code>(df['Set'] == 'Z') &amp; (df['Type'] == 'A')</code></li>\n<li>otherwise <code>blue</code> when <code>(df['Set'] == 'Z') &amp; (df['Type'] == 'B')</code> </li>\n<li>otherwise <code>purple</code> when <code>(df['Type'] == 'B')</code></li>\n<li>otherwise <code>black</code>,</li>\n</ul>\n\n<p>then use</p>\n\n<pre><code>df = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\nconditions = [\n    (df['Set'] == 'Z') &amp; (df['Type'] == 'A'),\n    (df['Set'] == 'Z') &amp; (df['Type'] == 'B'),\n    (df['Type'] == 'B')]\nchoices = ['yellow', 'blue', 'purple']\ndf['color'] = np.select(conditions, choices, default='black')\nprint(df)\n</code></pre>\n\n<p>which yields</p>\n\n<pre><code>  Set Type   color\n0   Z    A  yellow\n1   Z    B    blue\n2   X    B  purple\n3   Y    C   black\n</code></pre>\n",
        "question_body": "<p>I have a dataframe along the lines of the below:</p>\n<pre><code>    Type       Set\n1    A          Z\n2    B          Z           \n3    B          X\n4    C          Y\n</code></pre>\n<p>I want to add another column to the dataframe (or generate a series) of the same length as the dataframe (equal number of records/rows) which sets a colour <code>'green'</code> if <code>Set == 'Z'</code> and <code>'red'</code> if <code>Set</code> equals anything else.</p>\n<p>What's the best way to do this?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 408343,
            "reputation": 15753,
            "user_id": 778942,
            "user_type": "registered",
            "accept_rate": 53,
            "profile_image": "https://www.gravatar.com/avatar/f98033f7f2031b092d5ff33d0f32c9da?s=128&d=identicon&r=PG",
            "display_name": "sam",
            "link": "https://stackoverflow.com/users/778942/sam"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 68221821,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625213729,
        "creation_date": 1625199721,
        "last_edit_date": 1625207051,
        "question_id": 68219578,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68219578/capture-a-webpage-table-directly-into-a-dataframe",
        "title": "Capture a webpage table directly into a dataframe",
        "answer_body": "<p>This runs in 4 seconds for me:</p>\n<pre><code>import requests\nimport pandas as pd\n\nheaders = {\n    'Connection': 'keep-alive',\n    'Cache-Control': 'max-age=0',\n    'sec-ch-ua': '&quot; Not;A Brand&quot;;v=&quot;99&quot;, &quot;Google Chrome&quot;;v=&quot;91&quot;, &quot;Chromium&quot;;v=&quot;91&quot;',\n    'sec-ch-ua-mobile': '?0',\n    'Upgrade-Insecure-Requests': '1',\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n    'Sec-Fetch-Site': 'cross-site',\n    'Sec-Fetch-Mode': 'navigate',\n    'Sec-Fetch-User': '?1',\n    'Sec-Fetch-Dest': 'document',\n    'Accept-Language': 'nl-NL,nl;q=0.9,en-US;q=0.8,en;q=0.7',\n}\n\nresponse = requests.get('https://www1.nseindia.com/products/content/derivatives/equities/fo_underlyinglist.htm', headers=headers)\ndf = pd.read_html(response.content)\n</code></pre>\n<p>The table can be found under <code>df[0]</code>.</p>\n",
        "question_body": "<p>I am fetching data from a url.</p>\n<pre><code>import pandas as pd\nimport os\nfrom webdriver_manager.chrome import ChromeDriverManager\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\noptions = Options()\noptions.add_argument('--ignore-certificate-errors')\noptions.add_argument('--start-maximized')\noptions.page_load_strategy = 'eager'\noptions.add_argument(&quot;--headless&quot;);\ndriver = webdriver.Chrome(options=options)\nwait = WebDriverWait(driver, 20)   \n\nurl = &quot;https://www1.nseindia.com/products/content/derivatives/equities/fo_underlying_home.htm&quot;\ndriver.get(url)\n</code></pre>\n<p>I want to capture this table into df.</p>\n<p><a href=\"https://i.stack.imgur.com/H7cCG.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/H7cCG.png\" alt=\"enter image description here\" /></a></p>\n<p>What is the best way to capture a web table directly into a dataframe?</p>\n"
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 191833,
            "reputation": 1217,
            "user_id": 432648,
            "user_type": "registered",
            "accept_rate": 78,
            "profile_image": "https://www.gravatar.com/avatar/d4099512c20f5be4819512c680a1ea71?s=128&d=identicon&r=PG&f=1",
            "display_name": "Moh",
            "link": "https://stackoverflow.com/users/432648/moh"
        },
        "is_answered": true,
        "view_count": 23839,
        "protected_date": 1622002899,
        "accepted_answer_id": 59330040,
        "answer_count": 5,
        "score": 43,
        "last_activity_date": 1625212693,
        "creation_date": 1503512017,
        "last_edit_date": 1543799092,
        "question_id": 45846765,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/45846765/efficient-way-to-unnest-explode-multiple-list-columns-in-a-pandas-dataframe",
        "title": "Efficient way to unnest (explode) multiple list columns in a pandas DataFrame",
        "answer_body": "<h1><strong>pandas &gt;= 0.25</strong></h1>\n<p>Assuming all columns have the same number of lists, you can call <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.explode.html\" rel=\"noreferrer\"><strong><code>Series.explode</code></strong></a> on each column.</p>\n<pre><code>df.set_index(['A']).apply(pd.Series.explode).reset_index()\n\n    A   B   C   D   E\n0  x1  v1  c1  d1  e1\n1  x1  v2  c2  d2  e2\n2  x2  v3  c3  d3  e3\n3  x2  v4  c4  d4  e4\n4  x3  v5  c5  d5  e5\n5  x3  v6  c6  d6  e6\n6  x4  v7  c7  d7  e7\n7  x4  v8  c8  d8  e8\n</code></pre>\n<p>The idea is to set as the index all columns that must <strong>NOT</strong> be exploded first, then reset the index after.</p>\n<hr />\n<p>It's also <strong>faster</strong>.</p>\n<pre><code>%timeit df.set_index(['A']).apply(pd.Series.explode).reset_index()\n%%timeit\n(df.set_index('A')\n   .apply(lambda x: x.apply(pd.Series).stack())\n   .reset_index()\n   .drop('level_1', 1))\n\n\n2.22 ms \u00b1 98.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n9.14 ms \u00b1 329 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre>\n",
        "question_body": "<p>I am reading multiple JSON objects into one DataFrame. The problem is that some of the columns are lists. Also, the data is very big and because of that I cannot use the available solutions on the internet. They are very slow and memory-inefficient </p>\n\n<p>Here is how my data looks like:</p>\n\n<pre><code>df = pd.DataFrame({'A': ['x1','x2','x3', 'x4'], 'B':[['v1','v2'],['v3','v4'],['v5','v6'],['v7','v8']], 'C':[['c1','c2'],['c3','c4'],['c5','c6'],['c7','c8']],'D':[['d1','d2'],['d3','d4'],['d5','d6'],['d7','d8']], 'E':[['e1','e2'],['e3','e4'],['e5','e6'],['e7','e8']]})\n    A       B          C           D           E\n0   x1  [v1, v2]    [c1, c2]    [d1, d2]    [e1, e2]\n1   x2  [v3, v4]    [c3, c4]    [d3, d4]    [e3, e4]\n2   x3  [v5, v6]    [c5, c6]    [d5, d6]    [e5, e6]\n3   x4  [v7, v8]    [c7, c8]    [d7, d8]    [e7, e8]\n</code></pre>\n\n<p>And this is the shape of my data: (441079, 12)</p>\n\n<p>My desired output is:</p>\n\n<pre><code>    A       B          C           D           E\n0   x1      v1         c1         d1          e1\n0   x1      v2         c2         d2          e2\n1   x2      v3         c3         d3          e3\n1   x2      v4         c4         d4          e4\n.....\n</code></pre>\n\n<p>EDIT: After being marked as duplicate, I would like to stress on the fact that in this question I was looking for an <em>efficient</em> method of exploding multiple columns. Therefore the approved answer is able to explode an arbitrary number of columns on very large datasets efficiently. Something that the answers to the other question failed to do (and that was the reason I asked this question after testing those solutions).</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 2939034,
            "reputation": 383,
            "user_id": 2517352,
            "user_type": "registered",
            "accept_rate": 67,
            "profile_image": "https://www.gravatar.com/avatar/a1732b8d0bf8e59328369fb156a17427?s=128&d=identicon&r=PG",
            "display_name": "spriore",
            "link": "https://stackoverflow.com/users/2517352/spriore"
        },
        "is_answered": true,
        "view_count": 30357,
        "accepted_answer_id": 40597684,
        "answer_count": 2,
        "score": 20,
        "last_activity_date": 1625209013,
        "creation_date": 1479152273,
        "question_id": 40596518,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/40596518/writing-a-python-pandas-dataframe-to-word-document",
        "title": "Writing a Python Pandas DataFrame to Word document",
        "answer_body": "<p>You can write the table straight into a <code>.docx</code> file using the <code>python-docx</code> library.</p>\n\n<p>If you are using the Conda or installed Python using Anaconda, you can run the command from the command line:</p>\n\n<pre><code>conda install python-docx --channel conda-forge\n</code></pre>\n\n<p>Or to pip install from the command line:</p>\n\n<pre><code>pip install python-docx\n</code></pre>\n\n<p>After that is installed, we can use it to open the file, add a table, and then populate the table's cell text with the data frame data.</p>\n\n<pre><code>import docx\nimport pandas as pd\n\n# i am not sure how you are getting your data, but you said it is a\n# pandas data frame\ndf = pd.DataFrame(data)\n\n# open an existing document\ndoc = docx.Document('./test.docx')\n\n# add a table to the end and create a reference variable\n# extra row is so we can add the header row\nt = doc.add_table(df.shape[0]+1, df.shape[1])\n\n# add the header rows.\nfor j in range(df.shape[-1]):\n    t.cell(0,j).text = df.columns[j]\n\n# add the rest of the data frame\nfor i in range(df.shape[0]):\n    for j in range(df.shape[-1]):\n        t.cell(i+1,j).text = str(df.values[i,j])\n\n# save the doc\ndoc.save('./test.docx')\n</code></pre>\n",
        "question_body": "<p>I'm working on creating a Python generated report that uses Pandas DataFrames. Currently I am using the <code>DataFrame.to_string()</code> method. However this writes to the file as a string. Is there a way for me to achieve this while keeping it as a table so I can use table formating.</p>\n\n<p>Code:</p>\n\n<pre><code>SEMorgkeys = client.domain_organic(url, database = \"us\", display_limit = 10, export_columns=[\"Ph,Pp,Pd,Nq,Cp,Ur,Tr\"])\norg_df = pd.DataFrame(SEMorgkeys)\n\nf = open(name, 'w')\nf.write(\"\\nOrganic:\\n\")\nf.write(org_df.to_string(index=False,justify=\"left\"))\nf.close()\n</code></pre>\n\n<p>Current Printout (as string):</p>\n\n<pre><code>CPC    Keyword                        Position Difference Previous Position Search Volume Traffic (%) Url                                               \n75.92       small business factoring   0                   1                 210          11.69       https://www..com/small-business-f...\n80.19              factoring company   0                   8                1600           5.72       https://www..com/factoring-vs-ban...\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "fillna"
        ],
        "owner": {
            "account_id": 4812169,
            "reputation": 33,
            "user_id": 3885321,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/800aff179ebca26f595c76f7400f8a63?s=128&d=identicon&r=PG&f=1",
            "display_name": "Donovin",
            "link": "https://stackoverflow.com/users/3885321/donovin"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68219905,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625203652,
        "creation_date": 1625202504,
        "last_edit_date": 1625203652,
        "question_id": 68219878,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68219878/df-fillna-not-working-when-updating-multiple-columns-from-a-slice",
        "title": "df.fillna() not working when updating multiple columns from a slice",
        "answer_body": "<p>If need set all <code>Id</code> if contain at least one row of data with only missing values to string <code>None</code> use:</p>\n<pre><code>garage_cat_columns = ['GarageType','GarageYrBlt','GarageQual','GarageCond','GarageFinish']\n\nno_garage = testing.loc[testing[garage_cat_columns].isnull().all(axis=1), 'Id'].tolist()\ntesting.loc[df['Id'].isin(no_garage), garage_cat_columns] = 'None'\n</code></pre>\n<p>If need set only rows with all <code>None</code>s like Nonetype:</p>\n<pre><code>mask = testing[garage_cat_columns].isnull().all(axis=1)\ntesting.loc[mask, garage_cat_columns] = 'None'\n</code></pre>\n",
        "question_body": "<p>Having issues with fillna() and df slices. Still have my python training wheels on and would appreciate any assistance. I've found lots of close examples on SE, but because of the conditionals and multiple columns I haven't found anything that worked for this case.</p>\n<p>Data:\nAdvanced regression comp: <a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\" rel=\"nofollow noreferrer\">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a></p>\n<p>However, no need to pull down and combine the train and test data like I have because this slice only contains ~150 np.nan rows across the columns/factors listed in garage_cat_columns.</p>\n<p>The Id column is int and between 1 and 3000</p>\n<pre><code># Columns to update and criteria to slice\n\ngarage_cat_columns = ['GarageType','GarageYrBlt','GarageQual','GarageCond','GarageFinish']\nno_garage = testing.Id[((testing['GarageType'].isnull()) &amp;\n                        (testing['GarageYrBlt'].isnull()) &amp;\n                        (testing['GarageQual'].isnull()) &amp;\n                        (testing['GarageCond'].isnull()) &amp;\n                        (testing['GarageFinish'].isnull()))].tolist() # are all null?\n\n# Best of my knowledge this is textbook (old text book) fillna()\n\ntesting[testing.Id.where(df.Id.isin(no_garage)).notnull()][garage_cat_columns].fillna(&quot;None&quot;, inplace=True)\n</code></pre>\n<p>I'm getting the &quot;copy of a slice&quot; warning (to be expected), but can't seem to find a way to write this back to the original dataframe so my changes will &quot;commit.&quot;</p>\n<p>Some failed attempts:</p>\n<pre><code># 1 tried with and without double brackets\n\ntesting[[garage_cat_columns]] = testing[testing['Id'].where(df['Id']isin(no_garage)).notnull()] \\\n        [garage_cat_columns] = 'None'\n\n# 2 \n\ntesting[testing['Id'].where(df['Id']isin(no_garage)).notnull()][garage_cat_columns] = 'None'      \n\n# 3\n\ntesting[garage_cat_columns] = testing[testing['Id'].where(df['Id']isin(no_garage)).notnull()] \\\n        [garage_cat_columns].fillna(&quot;None&quot;)\n\n# 4 tried with and without double brackets\n\ntesting[[garage_cat_columns]] = testing[testing['Id'].where(df['Id']isin(no_garage)).notnull()] \\\n        [garage_cat_columns].fillna(&quot;None&quot;, inplace=True)\n\n# 5 Hail Mary\n\ntesting[testing['Id'].where(df['Id'].isin(no_garage)).notnull()] \\\n        [garage_cat_columns].fillna(&quot;None&quot;, inplace=True) = \\ # equality here\ntesting[testing['Id'].where(df['Id'].isin(no_garage)).notnull()] \\\n        [garage_cat_columns].fillna(&quot;None&quot;, inplace=True)\n</code></pre>\n<p>Any help is much appreciated!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 16716816,
            "reputation": 445,
            "user_id": 12082666,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/2366260153411164/picture?type=large",
            "display_name": "Vinson Ciawandy",
            "link": "https://stackoverflow.com/users/12082666/vinson-ciawandy"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 68218751,
        "answer_count": 4,
        "score": 1,
        "last_activity_date": 1625195841,
        "creation_date": 1625189348,
        "last_edit_date": 1625192373,
        "question_id": 68218610,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68218610/calculate-aggregate-of-numpy-array-with-pandas-groupby",
        "title": "calculate aggregate of numpy array with pandas groupby",
        "answer_body": "<p>Try with <code>apply</code> <code>np.mean</code></p>\n<pre><code>df.groupby('group')['embedding'].apply(np.mean).reset_index()\n  group   embedding\n0     a  [0.5, 0.5]\n1     b  [0.5, 0.5]\n</code></pre>\n",
        "question_body": "<p>I have dataframe with 2 columns, one is group and second one is vector embeddings. The data is already like that so I don't want to argue about the embedding columns. The embedding columns all share the same number of dimension.</p>\n<p>Basically I want to calculate the average of embedding for each group. By average I mean is axis level average. So [1,2] and [4,8] got average to [2.5,5]</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({&quot;group&quot;:[&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;],&quot;embedding&quot;:[[0,1],[1,0],[0,0],[1,1]]})\ndf['embedding'] = df['embedding'].apply(np.array)\n\ndf.groupby(&quot;group&quot;).agg({&quot;embedding&quot;:&quot;mean&quot;}) #This raise error\n</code></pre>\n<pre><code>/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in mean(self, numeric_only)\n   1497             &quot;mean&quot;,\n   1498             alt=lambda x, axis: Series(x).mean(numeric_only=numeric_only),\n-&gt; 1499             numeric_only=numeric_only,\n   1500         )\n   1501 \n\n/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in _cython_agg_general(self, how, alt, numeric_only, min_count)\n   1079 \n   1080         if not output:\n-&gt; 1081             raise DataError(&quot;No numeric types to aggregate&quot;)\n   1082 \n   1083         return self._wrap_aggregated_output(output, index=self.grouper.result_index)\n\nDataError: No numeric types to aggregate\n</code></pre>\n<p>Expected Output :</p>\n<pre><code>pd.DataFrame({&quot;group&quot;:[&quot;a&quot;,&quot;b&quot;],&quot;embedding&quot;:[[0.5,0.5],[0.5,0.5]]})\n</code></pre>\n<p>Fast solution is very appreciated since my data is quite huge.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "conditional-statements"
        ],
        "owner": {
            "account_id": 11369563,
            "reputation": 405,
            "user_id": 8335487,
            "user_type": "registered",
            "accept_rate": 89,
            "profile_image": "https://i.stack.imgur.com/Y2d67.jpg?s=128&g=1",
            "display_name": "Adrian Y",
            "link": "https://stackoverflow.com/users/8335487/adrian-y"
        },
        "is_answered": true,
        "view_count": 3586,
        "accepted_answer_id": 51034079,
        "answer_count": 4,
        "score": 9,
        "last_activity_date": 1625192172,
        "creation_date": 1529979979,
        "last_edit_date": 1625192172,
        "question_id": 51034054,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/51034054/syntax-to-select-previous-row-in-pandas-after-filtering",
        "title": "Syntax to select previous row in pandas after filtering",
        "answer_body": "<p>Shift the mask up by 1.</p>\n\n<pre><code>df[(df['Value'] &lt; 15).shift(-1).fillna(False)]\n\n   Row  Value\n1    2     25\n</code></pre>\n\n<hr>\n\n<p>More generally, if you're trying to find all rows greater than 15, whose next row is lesser than 15, you can compute two separate masks and AND them:</p>\n\n<pre><code>df[(df['Value'].shift(-1) &lt; 15) &amp; (df['Value'] &gt; 15)]\n\n   Row  Value\n1    2     25\n</code></pre>\n",
        "question_body": "<p>I have a pd.DataFrame <code>df</code> with 5 lines, say:</p>\n<pre><code>Row    Value\n 1      32\n 2      25\n 3      10\n 4      18\n 5      21\n</code></pre>\n<p>Lets say I use the filter <code>df[df['Value'] &lt; 15]</code> and this should return</p>\n<pre><code>Row    Value\n 3      10\n</code></pre>\n<p>My question is, I want to access the value for the row before this filter returns True, in this example I want to function to return the value 25 (from Row 2).</p>\n<p>My end goal is to apply the condition, get the row above, and test this row for an additional condition.</p>\n<p>What pandas functions can I use?</p>\n<p>Thanks!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "timezone",
            "timestamp-with-timezone"
        ],
        "owner": {
            "account_id": 513910,
            "reputation": 3265,
            "user_id": 1653571,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://www.gravatar.com/avatar/269f6813c6224833e86f6dfd5a2ee14b?s=128&d=identicon&r=PG",
            "display_name": "Dave X",
            "link": "https://stackoverflow.com/users/1653571/dave-x"
        },
        "is_answered": true,
        "view_count": 52488,
        "accepted_answer_id": 49210679,
        "answer_count": 3,
        "score": 62,
        "last_activity_date": 1625186922,
        "creation_date": 1520612710,
        "last_edit_date": 1520618028,
        "question_id": 49198068,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/49198068/how-to-remove-timezone-from-a-timestamp-column-in-a-pandas-dataframe",
        "title": "How to remove timezone from a Timestamp column in a pandas dataframe",
        "answer_body": "<p>The column must be a <code>datetime</code> dtype, as from using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html\" rel=\"noreferrer\"><code>pd.to_datetime</code></a>, then you can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.tz_localize.html\" rel=\"noreferrer\"><code>tz_localize</code></a> to change the time zone, a na\u00efve timestamp corresponds to time zone <code>None</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>testdata['time'].dt.tz_localize(None)\n</code></pre>\n<p>Unless the column is an index (<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html\" rel=\"noreferrer\"><code>DatetimeIndex</code></a>), the <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#dt-accessor\" rel=\"noreferrer\"><code>.dt</code> accessor</a> must be used to access <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\" rel=\"noreferrer\">pandas datetime functions</a>.</p>\n",
        "question_body": "<p>I read <a href=\"https://stackoverflow.com/questions/40533647/pandas-change-timezone-for-forex-dataframe\">Pandas change timezone for forex DataFrame</a> but I'd like to make the time column of my dataframe timezone naive for interoperability with an sqlite3 database.</p>\n\n<p>The data in my pandas dataframe is already converted to UTC data, but I do not want to have to maintain this UTC timezone information in the database.</p>\n\n<p>Given a sample of the data derived from other sources, it looks like this:</p>\n\n<pre><code>print(type(testdata))\nprint(testdata)\nprint(testdata.applymap(type))\n</code></pre>\n\n<p>gives:</p>\n\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n                        time  navd88_ft  station_id  new\n0  2018-03-07 01:31:02+00:00  -0.030332          13    5\n1  2018-03-07 01:21:02+00:00  -0.121653          13    5\n2  2018-03-07 01:26:02+00:00  -0.072945          13    5\n3  2018-03-07 01:16:02+00:00  -0.139917          13    5\n4  2018-03-07 01:11:02+00:00  -0.152085          13    5\n                                     time        navd88_ft     station_id  \\\n0  &lt;class 'pandas._libs.tslib.Timestamp'&gt;  &lt;class 'float'&gt;  &lt;class 'int'&gt;   \n1  &lt;class 'pandas._libs.tslib.Timestamp'&gt;  &lt;class 'float'&gt;  &lt;class 'int'&gt;   \n2  &lt;class 'pandas._libs.tslib.Timestamp'&gt;  &lt;class 'float'&gt;  &lt;class 'int'&gt;   \n3  &lt;class 'pandas._libs.tslib.Timestamp'&gt;  &lt;class 'float'&gt;  &lt;class 'int'&gt;   \n4  &lt;class 'pandas._libs.tslib.Timestamp'&gt;  &lt;class 'float'&gt;  &lt;class 'int'&gt;   \n\n             new  \n0  &lt;class 'int'&gt;  \n1  &lt;class 'int'&gt;  \n2  &lt;class 'int'&gt;  \n3  &lt;class 'int'&gt;  \n4  &lt;class 'int'&gt;  \n</code></pre>\n\n<p>but </p>\n\n<pre><code>newstamp = testdata['time'].tz_convert(None)\n</code></pre>\n\n<p>gives an eventual error:</p>\n\n<pre><code>TypeError: index is not a valid DatetimeIndex or PeriodIndex\n</code></pre>\n\n<p>What do I do to replace the column with a timezone naive timestamp?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 14728901,
            "reputation": 374,
            "user_id": 14829523,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/074b5d61968152cfa3fdaf216aff7cc7?s=128&d=identicon&r=PG&f=1",
            "display_name": "Exa",
            "link": "https://stackoverflow.com/users/14829523/exa"
        },
        "is_answered": true,
        "view_count": 225,
        "accepted_answer_id": 66570407,
        "answer_count": 4,
        "score": 11,
        "last_activity_date": 1625181004,
        "creation_date": 1615399244,
        "question_id": 66570375,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/66570375/output-all-rows-with-word-count-in-a-column-greater-than-3",
        "title": "Output all rows with word count in a column greater than 3",
        "answer_body": "<p>If the seperator is <code>' '</code> ,you can try <code>series.str.count</code> , else you can replace the <code>sep</code></p>\n<pre><code>n=3\ndf[df['answer'].str.count(' ').gt(n-1)]\n</code></pre>\n<p>To include Multiple spaces <code>#credits @piRSquared</code></p>\n<pre><code>df['answer'].str.count('\\s+').gt(2)\n</code></pre>\n<p>Or using list comprehension:</p>\n<pre><code>n= 3\ndf[[len(i.split())&gt;n for i in df['answer']]] #should be faster than above\n</code></pre>\n<hr />\n<pre><code>                    answer some_number\n0  hello how are you doing         1.0\n2          bye bye bye bye         0.0\n5     Who let the dogs out         0.0\n6            1 + 1 + 1 + 2         1.0\n</code></pre>\n",
        "question_body": "<p>I have this dummy df:</p>\n<pre><code>columns = ['answer', 'some_number']\ndata = [['hello how are you doing','1.0'],\n       ['hello', '1.0'],\n       ['bye bye bye bye', '0.0'],\n        ['no', '0.0'],\n        ['yes', '1.0'],\n        ['Who let the dogs out', '0.0'],\n        ['1 + 1 + 1 + 2', '1.0']]\ndf = pd.DataFrame(columns=columns, data=data)\n</code></pre>\n<p>I want to output the rows with a word count greater than 3.\nHere that would the rows <code>'hello how are you doing', 'bye bye bye bye', 'Who let the dogs out', '1 + 1 + 1 + 2'</code></p>\n<p>My approach doesn't work: <code>df[len(df.answer) &gt; 3]</code></p>\n<p>Output: <code>KeyError: True</code></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "datetime"
        ],
        "owner": {
            "account_id": 22105816,
            "reputation": 23,
            "user_id": 16361499,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d678dcd771291167943f9e6b0e24fd3d?s=128&d=identicon&r=PG&f=1",
            "display_name": "Andres Perez",
            "link": "https://stackoverflow.com/users/16361499/andres-perez"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68217580,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625179028,
        "creation_date": 1625176721,
        "question_id": 68217404,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68217404/using-np-select-in-datetime-series-pandas-python",
        "title": "Using np.select in DateTime series - Pandas Python",
        "answer_body": "<p>use <code>np.where</code> when you have one condition that creates exactly two mutually exclusive groups:</p>\n<pre><code>dataframe['DATE1'] = dt.datetime.now()\ndataframe['DATE2'] = dt.datetime.now()\ndataframe['DATE3'] = np.where(dataframe['FLOAT']&gt;0,dataframe['DATE1'],dataframe['DATE2'])\ndataframe\n</code></pre>\n<p>Regarding the error with <code>np.select</code>. Initially store the date as an object datatype with <code>str()</code> and then convert <code>to_datetime()</code> later. Per the error, there is an issue with the datetime data type in the np.select statement</p>\n<pre><code>dataframe['DATE1'] = str(dt.datetime.now())\ndataframe['DATE2'] = str(dt.datetime.now())\ndataframe['DATE3'] = pd.to_datetime(np.select([(dataframe['FLOAT']&gt;0),(dataframe['FLOAT']&lt;=0)],\n                                              [dataframe['DATE1'],dataframe['DATE2']]))\ndataframe\n</code></pre>\n",
        "question_body": "<p>I have a problem using np.select in DateTime series.\nImagine you have a DataFrame: dateframe['id','FLOAT','DATE1','DATE2']</p>\n<pre><code>dateframe['DATE1'] = dt.datetime.now()\ndateframe['DATE2'] = dt.datetime.now()\n\ndateframe['DATE3'] = np.select(\n    [\n        dateframe['FLOAT']&gt;0,\n        dateframe['FLOAT']&lt;=0,\n    ],\n    [\n        dateframe['DATE1'],\n        dateframe['DATE2'],\n    ]\n)\n</code></pre>\n<p>I get this error:</p>\n<p><strong>TypeError: The DTypes &lt;class 'numpy.dtype[uint8]'&gt; and &lt;class 'numpy.dtype[datetime64]'&gt; do not have a common DType. For example they cannot be stored in a single array unless the dtype is <code>object</code>.</strong></p>\n<p>Thanks in advance.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "account_id": 20429027,
            "reputation": 3,
            "user_id": 14988287,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d399de2194d281fb305951aecebe5046?s=128&d=identicon&r=PG&f=1",
            "display_name": "Priti",
            "link": "https://stackoverflow.com/users/14988287/priti"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68216942,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625173534,
        "creation_date": 1624973834,
        "last_edit_date": 1624975624,
        "question_id": 68179824,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68179824/is-there-any-way-i-can-apply-another-style-on-a-styler-in-pandas",
        "title": "is there any way i can apply another style on a styler in pandas",
        "answer_body": "<p>In the latest version of pandas 1.3.0 <code>set_na_rep()</code> is deprecated and you should use <code>format(na_rep=&quot;xx&quot;)</code> instead.</p>\n<p>If the values are truly missing, i.e. they are <code>pd.nan</code> and  <code>pd.isna(val)</code> returns <code>True</code> then the value will be substitututed for the display value of <code>na_rep</code>.</p>\n<p>On the other hand if you have previously performed some manipulation and the value is a string representation of 'NAN', then this is not a missing value (it is a string), and the <code>na_rep</code> argument will have no effect.</p>\n<p>Actually you can see the relevant issue to this here (<a href=\"https://github.com/pandas-dev/pandas/pull/40060\" rel=\"nofollow noreferrer\">https://github.com/pandas-dev/pandas/pull/40060</a>) and the ultimate solution (<a href=\"https://github.com/pandas-dev/pandas/pull/40134\" rel=\"nofollow noreferrer\">https://github.com/pandas-dev/pandas/pull/40134</a>)</p>\n",
        "question_body": "<p>I have NAN values in my dataframe columns and i need to replace it with empty string. I need to perform subtraction on these columns, so when replacing with empty string i am getting error:</p>\n<blockquote>\n<p>unsupported operand types for -:'str' and 'str'.</p>\n</blockquote>\n<p>so while applying style to my dataframe i have added this line, and it is working as well:</p>\n<pre><code>df.style.set_na_rep('')\n</code></pre>\n<p>But when i am adding format function to the style, to concat '%' symbol in the columns, the NAN values are reappearing along withe the '%' symbol.</p>\n<p>Then syntax is:</p>\n<pre><code>df = (df.style.apply(highlight_cells, axis = None).set_na_rep('')).format({'B':'{:}%','C':'{:}%'}).set_table_styles(...)\n</code></pre>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">A</th>\n<th style=\"text-align: center;\">B</th>\n<th style=\"text-align: right;\">C</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">ab</td>\n<td style=\"text-align: center;\">8.3%</td>\n<td style=\"text-align: right;\">4.7%</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">cd</td>\n<td style=\"text-align: center;\">9.0%</td>\n<td style=\"text-align: right;\">NAN%</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">ef</td>\n<td style=\"text-align: center;\">NAN%</td>\n<td style=\"text-align: right;\">NAN%</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">gh</td>\n<td style=\"text-align: center;\">NAN%</td>\n<td style=\"text-align: right;\">13.9%</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>can anyone please suggest, I am pretty new in pandas and dataframes.\nThanks</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "uint64"
        ],
        "owner": {
            "account_id": 21873758,
            "reputation": 13,
            "user_id": 16162951,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/35edf95e3e546b3191a72c37615b0b0d?s=128&d=identicon&r=PG&f=1",
            "display_name": "r4t31",
            "link": "https://stackoverflow.com/users/16162951/r4t31"
        },
        "is_answered": true,
        "view_count": 17,
        "accepted_answer_id": 68216605,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625171608,
        "creation_date": 1625136424,
        "question_id": 68208793,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68208793/python-pandas-optimization-for-ubyte-data-0-255",
        "title": "python Pandas optimization for ubyte data (0..255)",
        "answer_body": "<p>For unsigned integer data in range 0..255, you can reduce the memory storage from default <code>int64</code> (8 bytes) to use <code>uint8</code> (1 byte).  You can refer to <a href=\"https://www.educative.io/edpresso/reduce-the-memory-usage-when-loading-a-file-in-pandas\" rel=\"nofollow noreferrer\">this article</a> for an example where the memory usage is substantially reduced from 1.5MB to 332KB (around one fifth).</p>\n<p>For Categorical type, as Pandas stores categorical columns as objects, this storage is not optimal. One of the reason is that it creates a list of pointers to the memory address of each value of your column.  Refer to <a href=\"https://vincentteyssier.medium.com/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e\" rel=\"nofollow noreferrer\">this article</a> for more information.</p>\n<p>To use <code>uint8</code>, either you can do it when you input your data, e.g. during <code>pd.read_csv</code> call, you specify the dtype of input columns with <code>uint8</code> type. (See the <a href=\"https://www.educative.io/edpresso/reduce-the-memory-usage-when-loading-a-file-in-pandas\" rel=\"nofollow noreferrer\">first article</a> for an example).  If you already have your data loaded and you want to convert the dataframe columns to use <code>uint8</code>, you can use the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.astype.html\" rel=\"nofollow noreferrer\"><code>Series.astype()</code></a> or <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html#\" rel=\"nofollow noreferrer\"><code>DataFrame.astype()</code></a> function with syntax like <code>.astype('uint8')</code>.</p>\n",
        "question_body": "<p>How is it possible to optimize Pandas df to ubyte data type (0..255)? (by default is int64 for integer)</p>\n<p>If I will convert data to Categorical type, will df use less memory?</p>\n<p>Or the only way to optimize it - use NumPy instead of Pandas?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 4912129,
            "reputation": 897,
            "user_id": 3957154,
            "user_type": "registered",
            "accept_rate": 88,
            "profile_image": "https://www.gravatar.com/avatar/f443271ec0dec47d3ab3c2357f0a705b?s=128&d=identicon&r=PG&f=1",
            "display_name": "Adam",
            "link": "https://stackoverflow.com/users/3957154/adam"
        },
        "is_answered": true,
        "view_count": 2786,
        "accepted_answer_id": 40474890,
        "answer_count": 4,
        "score": 13,
        "last_activity_date": 1625170664,
        "creation_date": 1478553433,
        "last_edit_date": 1592947751,
        "question_id": 40474799,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/40474799/remove-reverse-duplicates-from-dataframe",
        "title": "Remove reverse duplicates from dataframe",
        "answer_body": "<p>You can sort each row of the data frame before dropping the duplicates:</p>\n\n<pre><code>data.apply(lambda r: sorted(r), axis = 1).drop_duplicates()\n\n#   A    B\n#0  0   50\n#1  10  22\n#2  11  35\n#3  5   21\n</code></pre>\n\n<p>If you prefer the result to be sorted by column <code>A</code>:</p>\n\n<pre><code>data.apply(lambda r: sorted(r), axis = 1).drop_duplicates().sort_values('A')\n\n#   A    B\n#0  0   50\n#3  5   21\n#1  10  22\n#2  11  35\n</code></pre>\n",
        "question_body": "<p>I have a data frame with two columns, <code>A</code> and <code>B</code>. The order of <code>A</code> and <code>B</code> is unimportant in this context; for example, I would consider <code>(0,50)</code> and <code>(50,0)</code> to be duplicates. In pandas, what is an efficient way to remove these duplicates from a dataframe?</p>\n\n<pre><code>import pandas as pd\n\n# Initial data frame.\ndata = pd.DataFrame({'A': [0, 10, 11, 21, 22, 35, 5, 50], \n                     'B': [50, 22, 35, 5, 10, 11, 21, 0]})\ndata\n    A   B\n0   0  50\n1  10  22\n2  11  35\n3  21   5\n4  22  10\n5  35  11\n6   5  21\n7  50   0\n\n# Desired output with \"duplicates\" removed. \ndata2 = pd.DataFrame({'A': [0, 5, 10, 11], \n                      'B': [50, 21, 22, 35]})\ndata2\n    A   B\n0   0  50\n1   5  21\n2  10  22\n3  11  35\n</code></pre>\n\n<p>Ideally, the output would be sorted by values of column <code>A</code>.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime",
            "timezone"
        ],
        "owner": {
            "account_id": 17215356,
            "reputation": 499,
            "user_id": 12463547,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2a1e8623ae1c326f9412f9b68c869758?s=128&d=identicon&r=PG&f=1",
            "display_name": "Joehat",
            "link": "https://stackoverflow.com/users/12463547/joehat"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68216272,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625169678,
        "creation_date": 1625169150,
        "question_id": 68216193,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68216193/python-match-same-date-format-in-two-date-time-columns",
        "title": "Python: Match same date format in two date time columns",
        "answer_body": "<p>You can do one of two things:</p>\n<pre><code>df[&quot;date1&quot;]= pd.to_datetime(df[&quot;date1&quot;], format=&quot;%Y-%m-%d %H:%M:%S.%f UTC&quot;)\ndf[&quot;date2&quot;]= pd.to_datetime(df[&quot;date2&quot;], format=&quot;%Y-%m-%d %H:%M:%S.%f&quot;)\n&gt;&gt;&gt; df\n  col1                   date1                   date2\n0   10 2021-06-13 12:08:52.311 2021-03-29 12:44:33.468\n1   36 2019-12-07 12:18:02.311 2011-10-15 10:14:32.118\n</code></pre>\n<p>Or:</p>\n<pre><code>df[&quot;date1&quot;]= pd.to_datetime(df[&quot;date1&quot;].str.replace(&quot; UTC&quot;, &quot;&quot;))\ndf[&quot;date2&quot;]= pd.to_datetime(df[&quot;date2&quot;])\n&gt;&gt;&gt; df\n col1                   date1                   date2\n0   10 2021-06-13 12:08:52.311 2021-03-29 12:44:33.468\n1   36 2019-12-07 12:18:02.311 2011-10-15 10:14:32.118\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe with two dates where one of them has the timezone included.</p>\n<pre><code>df = pd.DataFrame(np.array([[10, &quot;2021-06-13 12:08:52.311 UTC&quot;, &quot;2021-03-29 12:44:33.468&quot;], \n                            [36, &quot;2019-12-07 12:18:02.311 UTC&quot;, &quot;2011-10-15 10:14:32.118&quot;]\n                           ]),\n                   columns=['col1', 'date1', 'date2'])\ndf\n</code></pre>\n<p>Here's how I am converting them from a string to datetime:</p>\n<pre><code>df[&quot;date1&quot;]= pd.to_datetime(df[&quot;date1&quot;])\ndf[&quot;date2&quot;]= pd.to_datetime(df[&quot;date2&quot;])\n</code></pre>\n<p>which returns:</p>\n<pre><code>   col1  date1                              date2\n0   10  2021-06-13 12:08:52.311000+00:00    2021-03-29 12:44:33.468\n1   36  2019-12-07 12:18:02.311000+00:00    2011-10-15 10:14:32.118\n\n</code></pre>\n<p>At some point, I need to compare these two dates to look for the same values. For this, I need them to be written in the same format with the same number of digits. This said,\n<strong>how could I remove the time zone from date1 so it matches the same format in date2?</strong></p>\n<p><code>utc=None</code> as default in <code>pd.to_datetime</code>, so that didn't work...</p>\n<p>I am assuming both dates are in UTC. In the original data, these are part of different datasets, that's why they have a different format.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "boolean"
        ],
        "owner": {
            "account_id": 17215356,
            "reputation": 499,
            "user_id": 12463547,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2a1e8623ae1c326f9412f9b68c869758?s=128&d=identicon&r=PG&f=1",
            "display_name": "Joehat",
            "link": "https://stackoverflow.com/users/12463547/joehat"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68215819,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625167034,
        "creation_date": 1625166001,
        "question_id": 68215642,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68215642/python-return-entire-row-when-two-data-frames-share-a-common-value-in-two-diffe",
        "title": "Python: Return entire row when two data frames share a common value in two different columns",
        "answer_body": "<p>You can use boolean indexing:</p>\n<pre class=\"lang-py prettyprint-override\"><code>mask = df1[&quot;col2&quot;].isin(df2[&quot;col1&quot;])\nprint(df1[mask])\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>   col1  col2  col3\n1     4    55     6\n2     1     8    88\n</code></pre>\n",
        "question_body": "<p>I would like to print the entire row when two different dataframes share a common value in two different columns. I can identify them but instead of returning the entire row, I could only figure out how to return the boolean as shown below:</p>\n<pre><code>df1 = pd.DataFrame(np.array([[1, 2, 1], \n                             [4, 55, 6], \n                             [1, 8, 88]]),\n                   columns=['col1', 'col2', 'col3'])\ndf1\n\n&gt;&gt;\ncol1    col2    col3\n0   1   2        1\n1   4   55       6\n2   1   8        88\n\n\ndf2 = pd.DataFrame(np.array([[333, 1, 2], \n                             [55, 8, 88], \n                             [8, 5, 6]]),\n                   columns=['col1', 'col2', 'col3'])\ndf2\n\n&gt;&gt;\n    col1    col2    col3\n0   333        1    2\n1   55         8    88\n2   8          5    6\n\n\n# Return row where df1[&quot;col2&quot;] and df2[&quot;col1&quot;] have the same values\n# the output should print the rows of index 1 and 2\n\ndf1['col2'].isin(df2['col1'])\n\n&gt;&gt;\n0    False\n1     True\n2     True\nName: col2, dtype: bool\n\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 7423739,
            "reputation": 1379,
            "user_id": 5647038,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/mfe5E.jpg?s=128&g=1",
            "display_name": "Narendra Prasath",
            "link": "https://stackoverflow.com/users/5647038/narendra-prasath"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68214906,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625162265,
        "creation_date": 1625161744,
        "question_id": 68214806,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68214806/convert-consecutive-columns-to-respective-rows-in-pandas-dataframe",
        "title": "Convert consecutive columns to respective rows in Pandas DataFrame",
        "answer_body": "<p>Use <code>pd.wide_to_long</code>:</p>\n<pre><code>pd.wide_to_long(df,['Key', 'Val'],['City', 'State', 'Name'],'No').reset_index()\n</code></pre>\n<p>Output:</p>\n<pre><code>       City    State      Name  No     Key Val\n0   Houston    Texas      Aria   1   test1  28\n1   Houston    Texas      Aria   2   test4  82\n2   Houston    Texas      Aria   3   test7   4\n3   Houston    Texas      Aria   4  test10  97\n4   Houston    Texas      Aria   5  test13   4\n5    Austin    Texas  Penelope   1   test2   4\n6    Austin    Texas  Penelope   2   test5  45\n7    Austin    Texas  Penelope   3   test8  76\n8    Austin    Texas  Penelope   4  test11  66\n9    Austin    Texas  Penelope   5  test14  10\n10   Hoover  Alabama      Niko   1   test3   7\n11   Hoover  Alabama      Niko   2   test6  76\n12   Hoover  Alabama      Niko   3   test9   9\n13   Hoover  Alabama      Niko   4  test12  10\n14   Hoover  Alabama      Niko   5  test15    \n</code></pre>\n<p>You are trying to simultaneously melt two columns.  pd.wide_to_long handles this situation.</p>\n",
        "question_body": "<p><strong>Trying to convert consecutive columns to rows in pandas.</strong> Ex: Consecutive column names are sequential numbers along with some strings i.e <code>Key1</code>,<code>Val1</code>,...., <code>KeyN</code>,<code>ValN</code> in <code>DataFrame</code>. You can use below code to generate the dataframe.</p>\n<pre><code>df = pd.DataFrame({'City': ['Houston', 'Austin', 'Hoover'],'State': ['Texas', 'Texas', 'Alabama'],'Name':['Aria', 'Penelope', 'Niko'],'Key1':[&quot;test1&quot;, &quot;test2&quot;, &quot;test3&quot;],'Val1':[28, 4, 7],'Key2':[&quot;test4&quot;, &quot;test5&quot;, &quot;test6&quot;],\n'Val2':[82, 45, 76],'Key3':[&quot;test7&quot;, &quot;test8&quot;, &quot;test9&quot;],'Val3':[4, 76, 9],'Key4':[&quot;test10&quot;, &quot;test11&quot;, &quot;test12&quot;],'Val4':[97, 66, 10],'Key5':[&quot;test13&quot;, &quot;test14&quot;, &quot;test15&quot;],'Val5':[4, 10, '']},columns=['City', 'State', 'Name', 'Key1', 'Val1', 'Key2', 'Val2', 'Key3', 'Val3', 'Key4', 'Val4', 'Key5', 'Val5'])\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/RMe79.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/RMe79.png\" alt=\"enter image description here\" /></a></p>\n<p>I tried <code>melt</code> function as below:</p>\n<pre><code>df.melt(id_vars=['City', 'State'], var_name='Column', value_name='Key')\n</code></pre>\n<p>But I got the below output:</p>\n<p><a href=\"https://i.stack.imgur.com/QtM1P.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/QtM1P.png\" alt=\"enter image description here\" /></a></p>\n<p>The problem is for every key, val column has different rows. The <strong>expected output</strong> is below:</p>\n<p><a href=\"https://i.stack.imgur.com/LOwYh.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/LOwYh.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "max"
        ],
        "owner": {
            "account_id": 12455877,
            "reputation": 141,
            "user_id": 9069109,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/--zw5lZFGmZ4/AAAAAAAAAAI/AAAAAAAAABE/zsx4UxsdDQA/photo.jpg?sz=128",
            "display_name": "Aniss Chohra",
            "link": "https://stackoverflow.com/users/9069109/aniss-chohra"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 68214798,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625161702,
        "creation_date": 1625161023,
        "question_id": 68214659,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68214659/pandas-idxmax-return-all-columns-with-maximum-value",
        "title": "pandas idxmax return all columns with maximum value",
        "answer_body": "<p>For each row, you can check if the entry is equal to the maximum of that row; this will form a boolean frame. Then you can <code>dot</code> product it with the column names to choose those columns' names that gave <code>True</code> for the rows:</p>\n<pre><code>is_max = df.eq(df.max(axis=1), axis=0)\nresult = is_max.dot(df.columns + &quot; &quot;)\n</code></pre>\n<p>where <code>axis=1</code> of <code>max</code> says take the maximum of each row and <code>axis=0</code> of <code>eq</code> says align the argument (i.e., <code>df.max(axis=1)</code>) to compare row-wise i.e., broadcast so),</p>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; is_max\n\n   column_1  column_2\n0     False      True\n1      True     False\n2      True      True\n\n&gt;&gt;&gt; result\n\n0             column_2\n1             column_1\n2    column_1 column_2\n</code></pre>\n",
        "question_body": "<p>I have a dataframe which looks like the following:</p>\n<pre><code>column_1    column_2\n2           3\n6           4\n5           5\n.\n.\n.\n</code></pre>\n<p>I would like to return for each row of the above dataframe the name(s) of the column(s) which have the maximum value in that row; for example for the first row I want to return 'column_2', and for the third row it will return both column names since they both have the same value.</p>\n<p>I have tried to achieve it by using Pandas's idxmax function; but this function returns only the first occurrence and ignores the rest of the possibilities; like in the example of the third row above, idxmax returns only 'column_1' instead of ['column_1', 'column_2'].</p>\n<p>Any idea how to solve this problem. Thanks in advance for your help.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 9744443,
            "reputation": 342,
            "user_id": 7224879,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/ee2QD.jpg?s=128&g=1",
            "display_name": "Bruno Ambrozio",
            "link": "https://stackoverflow.com/users/7224879/bruno-ambrozio"
        },
        "is_answered": true,
        "view_count": 16,
        "accepted_answer_id": 68214760,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625161469,
        "creation_date": 1625161196,
        "question_id": 68214700,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68214700/how-to-convert-the-pandas-string-values-of-a-column-into-a-column-of-a-list-of-s",
        "title": "How to convert the Pandas string values of a column into a column of a list of strings?",
        "answer_body": "<p>The most efficient way would be as follows:</p>\n<pre><code>df['Codes'] = df['Codes'].str.split()\n</code></pre>\n<p>Its usually a good idea to favor built in functions when using pandas. These will usually have very efficient implementations in cython.</p>\n<p>A more flexible, but slower solution would be to apply an arbitrary python function -</p>\n<pre><code>df['Codes'] = df['Codes'].apply(lambda x: x.split())\n</code></pre>\n",
        "question_body": "<p>I have following dataset:</p>\n<pre><code>    Name     Codes\n0    Tom  a1,a2,a3\n1   nick  a1,b2,b3\n2  krish  a1,b2,c3\n3   jack  a5,c3,b1\n</code></pre>\n<p>How do I convert <code>Codes</code> to be a list inside my data frame column?\nI need the following result:</p>\n<pre><code>    Name     Codes\n0    Tom  [a1,a2,a3]\n1   nick  [a1,b2,b3]\n2  krish  [a1,b2,c3]\n3   jack  [a5,c3,b1]\n</code></pre>\n<p>Thank you!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "algorithm",
            "dataframe",
            "pairwise"
        ],
        "owner": {
            "account_id": 21960123,
            "reputation": 45,
            "user_id": 16238148,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a87a2e4b04f8d0cd6d19f6e68eab5a4e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Eugene Zinder",
            "link": "https://stackoverflow.com/users/16238148/eugene-zinder"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 68213724,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625157900,
        "creation_date": 1625155883,
        "question_id": 68213612,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun",
        "title": "How to combine rows in a dataframe in a pairwise fashion while applying some function",
        "answer_body": "<p>You can use <code>df.rolling</code> after grouping by <code>ID</code>:</p>\n<pre><code>out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n</code></pre>\n<pre><code>&gt;&gt;&gt; out\n       Val1  Val2\nid0_1  10.5  19.5\nid1_1   3.0   3.0\nid1_2   1.5   2.5\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2:</p>\n<pre><code>ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n</code></pre>\n<p>I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is.</p>\n<p>Here is the resulting dataframe:</p>\n<pre><code>ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n</code></pre>\n<p>In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row.</p>\n<p>(id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows.</p>\n<p><strong>I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way?</strong></p>\n<p>Code:</p>\n<pre><code>df = pd.DataFrame(columns=['ID', 'Val1', 'Val2'], data=[['id0', 10, 20], ['id0', 11, 19], ['id1', 5, 5], ['id1', 1, 1], ['id1', 2, 4]])\n</code></pre>\n",
        "input_data_frames": [
            "ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n",
            "ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n"
        ],
        "output_codes": [
            "out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n"
        ],
        "ques_desc": "I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: ",
        "ans_desc": "You can use after grouping by : ",
        "formatted_input": {
            "qid": 68213612,
            "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun",
            "question": {
                "title": "How to combine rows in a dataframe in a pairwise fashion while applying some function",
                "ques_desc": "I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: "
            },
            "io": [
                "ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n",
                "ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n"
            ],
            "answer": {
                "ans_desc": "You can use after grouping by : ",
                "code": [
                    "out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "performance"
        ],
        "owner": {
            "account_id": 13126533,
            "reputation": 23,
            "user_id": 9482302,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "Murakma",
            "link": "https://stackoverflow.com/users/9482302/murakma"
        },
        "is_answered": true,
        "view_count": 26,
        "closed_date": 1625155778,
        "accepted_answer_id": 68213559,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625155680,
        "creation_date": 1625155320,
        "question_id": 68213482,
        "link": "https://stackoverflow.com/questions/68213482/efficiently-iterate-through-a-data-frame-to-identify-tags-in-over-20-mil-differe",
        "closed_reason": "Duplicate",
        "title": "Efficiently iterate through a data frame to identify tags in over 20 mil different titles",
        "answer_body": "<p>If you have this dataframe:</p>\n<pre class=\"lang-none prettyprint-override\"><code>                      title  tag1  tag2  tag3\n0  This is title tag1, tag2   NaN   NaN   NaN\n1  This is title tag3, tag2   NaN   NaN   NaN\n2        This is title tag3   NaN   NaN   NaN\n</code></pre>\n<p>Then you can do:</p>\n<pre class=\"lang-py prettyprint-override\"><code>for tag in df.columns[1:]:\n    df[tag] = df[&quot;title&quot;].str.contains(tag)\n\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>                      title   tag1   tag2   tag3\n0  This is title tag1, tag2   True   True  False\n1  This is title tag3, tag2  False   True   True\n2        This is title tag3  False  False   True\n</code></pre>\n",
        "question_body": "<p>we have a very large data frame with over 20 mil. instances. In this data frame we have a column named: &quot;titles&quot; which has a length of +/- 10 words. On the other hand we have a list of 7 tags - each of which as a column in the data frame. The challenge is to identify each tag in the title field and set a boolean to True if the tag is in the title.</p>\n<p>We have created a loop which works on a small data set, but it uses too much time and RAM on the large, original dataset. We therefore need a more efficient approach to the problem.</p>\n<p>Here is our code:</p>\n<pre><code>tags = ['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5', 'Tag6', 'Tag7', 'Tag8', 'Tag9']\ntitles = total[&quot;title&quot;]\ncheckpoint = 0\n\nfor i in tags:\n    total[i] = 0\n\nfor index, row in df.iterrows():\n    checkpoint += 1\n    title_list = row[&quot;title&quot;].split()\n    for i in title_list:\n        i = i.upper()\n        for x in tags:\n            if x in i:\n                df.at[index, x] = 1\n        else: continue\n    if checkpoint % 10000 == 0:\n        print(checkpoint)\n\n</code></pre>\n<p>Any help is highly appreciated!!</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22103378,
            "reputation": 13,
            "user_id": 16359484,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ddcf3d7b3c9ae13cf343bdef898f5482?s=128&d=identicon&r=PG&f=1",
            "display_name": "terribleatthis",
            "link": "https://stackoverflow.com/users/16359484/terribleatthis"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 68213324,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625154962,
        "creation_date": 1625154518,
        "question_id": 68213292,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68213292/how-can-i-pivot-one-column-of-unique-ids-to-show-all-matching-secondary-ids-in-a",
        "title": "How can I pivot one column of unique IDs to show all matching secondary IDs in adjacent columns?",
        "answer_body": "<p>Here's one way:</p>\n<pre><code>df = (\n    df.pivot_table(\n        index='Primary ID',\n        columns=df.groupby('Primary ID').cumcount().add(1),\n        values='Secondary ID'\n    ).add_prefix('Secondary').reset_index()\n)\n</code></pre>\n<p>Alternative:</p>\n<pre><code>df = df.assign(t=df.groupby('Primary ID').cumcount().add(\n    1)).set_index(['Primary ID',  't']).unstack(-1)\n</code></pre>\n<h4>OUTPUT:</h4>\n<pre><code>   Primary ID  Secondary1  Secondary2  Secondary3\n0           1    234234.0    435234.0     22233.0\n1           2    334342.0    543236.0    134623.0\n2           3   8475623.0   3928484.0         NaN\n3           4   3723429.0         NaN         NaN\n4           5   3945857.0  11112233.0   9878976.0\n</code></pre>\n",
        "question_body": "<p>I've got one column with Primary ID numbers, and each of these Primary ID numbers can have up to 3 Secondary ID numbers associated with it.  I want to pivot the secondary IDs so they all appear in up to 3 columns to the right of just one instance of each Primary ID.</p>\n<p>Currently it looks like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Primary ID</th>\n<th>Secondary ID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>234234</td>\n</tr>\n<tr>\n<td>1</td>\n<td>435234</td>\n</tr>\n<tr>\n<td>1</td>\n<td>22233</td>\n</tr>\n<tr>\n<td>2</td>\n<td>334342</td>\n</tr>\n<tr>\n<td>2</td>\n<td>543236</td>\n</tr>\n<tr>\n<td>2</td>\n<td>134623</td>\n</tr>\n<tr>\n<td>3</td>\n<td>8475623</td>\n</tr>\n<tr>\n<td>3</td>\n<td>3928484</td>\n</tr>\n<tr>\n<td>4</td>\n<td>3723429</td>\n</tr>\n<tr>\n<td>5</td>\n<td>3945857</td>\n</tr>\n<tr>\n<td>5</td>\n<td>11112233</td>\n</tr>\n<tr>\n<td>5</td>\n<td>9878976</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I want it to look like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Primary ID</th>\n<th>Secondary 1</th>\n<th>Secondary 2</th>\n<th>Secondary 3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>234234</td>\n<td>435234</td>\n<td>22233</td>\n</tr>\n<tr>\n<td>2</td>\n<td>334342</td>\n<td>543236</td>\n<td>134623</td>\n</tr>\n<tr>\n<td>3</td>\n<td>8475623</td>\n<td>3928484</td>\n<td>-</td>\n</tr>\n<tr>\n<td>4</td>\n<td>3723429</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>5</td>\n<td>3945857</td>\n<td>11112233</td>\n<td>9878976</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Not sure how to get the column headers there and probably where my issues are coming from when I try to use pivot or pivot table with pandas.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "fillna"
        ],
        "owner": {
            "account_id": 16501529,
            "reputation": 1760,
            "user_id": 11922765,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ed69b624cdb86e52caf0010e274df7b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mainland",
            "link": "https://stackoverflow.com/users/11922765/mainland"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68213002,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625154040,
        "creation_date": 1625153164,
        "question_id": 68212970,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68212970/python-dataframe-fill-nan-from-multiple-columns",
        "title": "Python Dataframe fill nan from multiple columns",
        "answer_body": "<p>Try via <code>bfill()</code>:</p>\n<pre><code>xdf['A']=xdf.bfill(1)['A']\n</code></pre>\n<p>output of <code>df</code>:</p>\n<pre><code>    A       B       C\n0   10.0    15.0    NaN\n1   20.0    NaN     NaN\n2   30.0    30.0    35.0\n3   40.0    NaN     40.0\n</code></pre>\n<p><strong>Update:</strong></p>\n<p>if there were additional columns (like D, E) not needed to fillna then select the subset of df and backword fill on axis 1:</p>\n<pre><code>xdf['A']=xdf[['A','B','C']].bfill(1)['A']\n</code></pre>\n",
        "question_body": "<p>I have a data frame with 3 columns. I want to fill <code>nan</code> in the first column with the second column. If there is also <code>nan</code> in the second, go to the third column.</p>\n<p>My code:</p>\n<pre><code>xdf = pd.DataFrame({'A':[10,20,np.nan,np.nan],'B':[15,np.nan,30,np.nan],'C':[np.nan,np.nan,35,40]})\n\n# fill nan in A\nxdf['A'].fillna(xdf[['B','C']],inplace=True)\n</code></pre>\n<p>Present output:</p>\n<pre><code>TypeError: &quot;value&quot; parameter must be a scalar, dict or Series, but you passed a &quot;DataFrame&quot;\n</code></pre>\n<p>Expected output:</p>\n<pre><code>xdf = \n      A     B     C\n0  10.0  15.0   NaN\n1  20.0   NaN   NaN\n2  30.0  30.0  35.0\n3  40.0   NaN  40.0\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 12789844,
            "reputation": 173,
            "user_id": 9254726,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9070bdb0341821aed239bfa229c8f43a?s=128&d=identicon&r=PG&f=1",
            "display_name": "5E4ME",
            "link": "https://stackoverflow.com/users/9254726/5e4me"
        },
        "is_answered": true,
        "view_count": 15,
        "accepted_answer_id": 68211988,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625149939,
        "creation_date": 1625149090,
        "question_id": 68211888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base",
        "title": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column",
        "answer_body": "<p>Let's try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> then build out a <code>dict</code>:</p>\n<pre><code>dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n</code></pre>\n<p><code>dfs</code>:</p>\n<pre><code>{'player1':       NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7,\n 'player2':       NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7,\n 'player3':       NAME  VAL1  VAL2  VAL3\n2  player3     3     6     7\n1  player3     2     6     8,\n 'player5':       NAME  VAL1  VAL2  VAL3\n2  player5     3     6     7}\n</code></pre>\n<p>Each player's DataFrame can then be accessed like:</p>\n<p><code>dfs['player1']</code>:</p>\n<pre><code>      NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7\n</code></pre>\n<hr />\n<p>Or as a <code>list</code>:</p>\n<pre><code>dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n</code></pre>\n<p><code>dfs</code>:</p>\n<pre><code>[      NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7,\n       NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7,\n       NAME  VAL1  VAL2  VAL3\n2  player3     3     6     7\n1  player3     2     6     8,\n       NAME  VAL1  VAL2  VAL3\n2  player5     3     6     7]\n</code></pre>\n<p>Each player's DataFrame can then be accessed like:</p>\n<p><code>dfs[1]</code>:</p>\n<pre><code>      NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7\n</code></pre>\n",
        "question_body": "<p>I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time.</p>\n<p>The starting dataframes look like this:</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n</code></pre>\n<p>And I would like to get to a series of frames looking like this</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n</code></pre>\n<p>My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible.</p>\n<p>I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.</p>\n",
        "input_data_frames": [
            "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n",
            "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n"
        ],
        "output_codes": [
            "dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n",
            "dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n"
        ],
        "ques_desc": "I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. ",
        "ans_desc": "Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : ",
        "formatted_input": {
            "qid": 68211888,
            "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base",
            "question": {
                "title": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column",
                "ques_desc": "I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. "
            },
            "io": [
                "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n",
                "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n"
            ],
            "answer": {
                "ans_desc": "Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : ",
                "code": [
                    "dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n",
                    "dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby",
            "series"
        ],
        "owner": {
            "account_id": 13931679,
            "reputation": 19,
            "user_id": 10059741,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/gf4t1.jpg?s=128&g=1",
            "display_name": "Philip O Brien",
            "link": "https://stackoverflow.com/users/10059741/philip-o-brien"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 68208587,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625145706,
        "creation_date": 1625134402,
        "question_id": 68208345,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68208345/populate-pandas-dataframe-with-group-by-calculations-made-in-pandas-series",
        "title": "Populate Pandas dataframe with group_by calculations made in Pandas series",
        "answer_body": "<p>You are on a good path. You can continue like this:</p>\n<pre><code>grp_by_series=grp_by_series.reset_index()\n\nres=df[['VehicleType', 'Colour']].merge(grp_by_series, how='left')\n\ndf['Frequency'] =  res[0]\n\nprint(df)\n</code></pre>\n<p>Output:</p>\n<pre><code>  VehicleType  Colour  Year  Frequency\n0       Truck   Green  2002          1\n1         Car   Green  2014          2\n2       Truck   Black  1975          1\n3         Car  Yellow  1987          1\n4         Car   Green  1987          2\n</code></pre>\n",
        "question_body": "<p>I have created a dataframe from a dictionary as follows:</p>\n<p><code>my_dict = {'VehicleType':['Truck','Car','Truck','Car','Car'],'Colour':['Green','Green','Black','Yellow','Green'],'Year':[2002,2014,1975,1987,1987],'Frequency': [0,0,0,0,0]}</code></p>\n<p><code>df = pd.DataFrame(my_dict)</code></p>\n<p>So my dataframe df currently looks like this:</p>\n<pre><code>  VehicleType  Colour  Year  Frequency\n0       Truck   Green  2002          0\n1         Car   Green  2014          0\n2       Truck   Black  1975          0\n3         Car  Yellow  1987          0\n4         Car   Green  1987          0\n</code></pre>\n<p>I'd like it to look like this:</p>\n<pre><code>  VehicleType  Colour  Year  Frequency\n0       Truck   Green  2002          1\n1         Car   Green  2014          2\n2       Truck   Black  1975          1\n3         Car  Yellow  1987          1\n4         Car   Green  1987          2\n</code></pre>\n<p>i.e., the Frequency column should represent the totals of VehicleType AND Colour combinations (but leaving out the Year column). So in row 4 for example, the 2 in the Frequency column tells you that there are a total of 2 rows with the combination of 'Car' and 'Green'.</p>\n<p>This is essentially a 'Count' with 'Group By' calculation, and Pandas provides a way to do the calculation as follows:</p>\n<p><code>grp_by_series = df.groupby(['VehicleType', 'Colour']).size()</code></p>\n<p><code>grp_by_series</code></p>\n<pre><code>VehicleType  Colour\nCar          Green     2\n             Yellow    1\nTruck        Black     1\n             Green     1\ndtype: int64\n</code></pre>\n<p>What I'd like to do next is to extract the calculated group_by values from the Panda series and put them into the Frequency column of the Pandas dataframe. I've tried various approaches but without success.</p>\n<p>The example I've given is hugely simplified - the dataframes I'm using are derived from genomic data and have hundreds of millions of rows, and will have several frequency columns based on various combinations of other columns, so ideally I need a solution which is fast and scales well.</p>\n<p>Thanks for any help!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 18556447,
            "reputation": 41,
            "user_id": 13521436,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2afe58c77bea8a32bc94328563ef41d2?s=128&d=identicon&r=PG&f=1",
            "display_name": "Muthu",
            "link": "https://stackoverflow.com/users/13521436/muthu"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 68211012,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625145684,
        "creation_date": 1625145061,
        "question_id": 68210857,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68210857/get-column-names-in-single-row-dataframe-pivot",
        "title": "Get column names in single row - DataFrame pivot",
        "answer_body": "<p>If you have an older version, you can use:</p>\n<pre><code>df1.columns.name = None  # alternative to rename_axis\ndf1 = df1.reset_index()\n</code></pre>\n<pre><code>&gt;&gt;&gt; df1\n   cola  colb  Average  Count  Total\n0     1  Val1      100     10   1000\n1     2  Val2       10      5    100\n</code></pre>\n",
        "question_body": "<p>I have a data frame which looks like this</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>cola</th>\n<th>colb</th>\n<th>MeasureName</th>\n<th>MeasureValue</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>1</td>\n<td>Val1</td>\n<td>Count</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>Val1</td>\n<td>Total</td>\n</tr>\n<tr>\n<td>2</td>\n<td>1</td>\n<td>Val1</td>\n<td>Average</td>\n</tr>\n<tr>\n<td>3</td>\n<td>2</td>\n<td>Val2</td>\n<td>Count</td>\n</tr>\n<tr>\n<td>4</td>\n<td>2</td>\n<td>Val2</td>\n<td>Total</td>\n</tr>\n<tr>\n<td>5</td>\n<td>2</td>\n<td>Val2</td>\n<td>Average</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>and when I pivot this table, it becomes like this</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>MeasureName</th>\n<th>Average</th>\n<th>Count</th>\n<th>Total</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>cola</strong></td>\n<td><strong>colb</strong></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>1</td>\n<td>Val1</td>\n<td>100</td>\n<td>10</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Val2</td>\n<td>10</td>\n<td>5</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>But what I wanted is to have column names in single header row like below. How can I achive this please?</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>cola</th>\n<th>colb</th>\n<th>Average</th>\n<th>Count</th>\n<th>Total</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>Val1</td>\n<td>100</td>\n<td>10</td>\n<td>1000</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Val2</td>\n<td>10</td>\n<td>5</td>\n<td>100</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>python code I use is below</p>\n<pre><code>d = {'cola':[1,1,1,2,2,2],'colb':['Val1','Val1','Val1','Val2','Val2','Val2'],'MeasureName':['Count','Total','Average','Count','Total','Average'],'MeasureValue':[10,1000,100,5,100,10]}\ndf1 = pd.DataFrame(d)\ndf1 = df1.pivot_table(index=[&quot;cola&quot;,&quot;colb&quot;],columns=&quot;MeasureName&quot;,values=&quot;MeasureValue&quot;, aggfunc='first')\nprint(df1)\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "append"
        ],
        "owner": {
            "account_id": 12075682,
            "reputation": 131,
            "user_id": 8829238,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-K2RnwHPfBCI/AAAAAAAAAAI/AAAAAAAABrM/_yW9nr1mPmY/photo.jpg?sz=128",
            "display_name": "Shraddha Avasthy",
            "link": "https://stackoverflow.com/users/8829238/shraddha-avasthy"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 68210635,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1625144198,
        "creation_date": 1625142000,
        "last_edit_date": 1625142020,
        "question_id": 68210073,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68210073/add-values-to-a-blank-column-from-2-other-columns-in-a-dataframe",
        "title": "Add values to a blank column from 2 other columns in a dataframe",
        "answer_body": "<p>you can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.combine_first.html\" rel=\"nofollow noreferrer\"><code>combine first</code></a></p>\n<pre><code>df['C'] = df.A.combine_first(df.B)\n</code></pre>\n<h5>OUTPUT:</h5>\n<pre><code>      A      B      C\n0    Cat    NaN    Cat\n1  Mouse  Mouse  Mouse\n2   Fish    NaN   Fish\n3    NaN    NaN    NaN\n4    NaN    Dog    Dog\n</code></pre>\n",
        "question_body": "<p>I want to create column C from column A and column B in my dataframe. I used append() but my column C does not pick values from one or the other of A and B for some reason. Can anyone help?</p>\n<pre><code>      A        B        C\n0     Cat     &quot;Blank&quot;   Cat     \n1     Mouse    Mouse    Mouse\n2     Fish    &quot;Blank&quot;   Fish\n3     &quot;Blank&quot; &quot;Blank&quot;   &quot;Blank&quot;\n4     &quot;Blank&quot;  Dog      Dog\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "csv"
        ],
        "owner": {
            "account_id": 17258259,
            "reputation": 235,
            "user_id": 12496869,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c07accea77bc7cb3a24f5822d39e3a6f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Sun",
            "link": "https://stackoverflow.com/users/12496869/sun"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 68210215,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625142582,
        "creation_date": 1625141830,
        "question_id": 68210024,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68210024/ignore-remove-some-lines-when-reading-csv-to-dataframe",
        "title": "Ignore/ remove some lines when reading csv to dataframe",
        "answer_body": "<p>To skip the first <code>n</code> rows, <code>skiprows=</code> parameter:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.read_csv(\n    &quot;movies.txt&quot;,\n    sep=r&quot;\\s*\\|\\s*&quot;,\n    comment=&quot;+&quot;,\n    usecols=range(1, 13),\n    skiprows=4,            # &lt;-- add skiprows here\n    engine=&quot;python&quot;,\n)\n</code></pre>\n",
        "question_body": "<p>i have a similar problem to this <a href=\"https://stackoverflow.com/a/68209482/12496869\">post</a>, I'm gonna keep working with the data of this poste. Supposed that i have a text file which looks like this.\n<a href=\"https://i.stack.imgur.com/S2VL6.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/S2VL6.png\" alt=\"enter image description here\" /></a></p>\n<p>How can i remove the rows <strong>highlighted in yellow</strong></p>\n<p>I used the code of @Andrej Kesely:</p>\n<pre><code>df = pd.read_csv(\n    &quot;movies.txt&quot;,\n    sep=r&quot;\\s*\\|\\s*&quot;,\n    comment=&quot;+&quot;,\n    usecols=range(1, 13),\n    engine=&quot;python&quot;,\n)\n</code></pre>\n<p>I got :</p>\n<pre><code>Empty DataFrame\nColumns: []    \nIndex: []  \n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 11172742,
            "reputation": 129,
            "user_id": 15920209,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cc103469b21c7e14061654750aefb315?s=128&d=identicon&r=PG&f=1",
            "display_name": "formanite",
            "link": "https://stackoverflow.com/users/15920209/formanite"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 68209755,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625141262,
        "creation_date": 1625140353,
        "question_id": 68209684,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68209684/error-bool-has-no-attribute-shift-shift-doesnt-work",
        "title": "error bool has no attribute shift, shift doesn&#39;t work",
        "answer_body": "<p>Try:</p>\n<pre><code>out=df[df['name'].eq('four').shift(-1,fill_value=False)]\n</code></pre>\n<p><strong>OR</strong>(If there is particular reason of using <code>groupby()</code>)</p>\n<pre><code>out=(df.groupby('in_id')['name']\n   .agg(lambda x: x[x.eq('four').shift(-1,fill_value=False)])\n   .reset_index())\n#you can also use apply() in place of agg() method\n</code></pre>\n<p>Now If you print <code>out</code> you will get your desired output</p>\n",
        "question_body": "<p>I have  the dataframe:</p>\n<pre><code>data = {'in_id': ['28076','28076','28076','28076'],\n'name': ['one', 'two', 'three', 'four'],  \n}\ndf = pd.DataFrame(data , columns = ['in_id','name'])\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/xEtHE.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/xEtHE.png\" alt=\"enter image description here\" /></a></p>\n<p>So, i try to run <code>df.groupby('in_id').apply(lambda x: x[(x.name=='four').shift(-1).fillna(False)])</code>\nbut i get an error 'bool' object has no attribute 'shift'</p>\n<p>How do i can to run it without getting an error?\nCan anyone see the problem</p>\n<p>Output dataframe</p>\n<p><a href=\"https://i.stack.imgur.com/XPg86.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/XPg86.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "max"
        ],
        "owner": {
            "account_id": 22086825,
            "reputation": 13,
            "user_id": 16345548,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJw7zCi2GCMXtlnPJIYIicVPcoCaAtfYmpuVYB_y=k-s128",
            "display_name": "Andrea Longoni",
            "link": "https://stackoverflow.com/users/16345548/andrea-longoni"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 68209340,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625140333,
        "creation_date": 1625137943,
        "question_id": 68209126,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68209126/how-to-extract-the-name-of-the-columns-corresponding-to-the-top-20-values-in-a",
        "title": "How to extract the name of the columns corresponding to the top 20% values in a dataframe with pandas",
        "answer_body": "<p>This works:</p>\n<pre><code>top20 = df.ge(df.quantile(0.8, axis=1), axis=0)\ntop_cols = top20.apply(lambda x: x.index[x], axis=1)\n</code></pre>\n<p>Example result:</p>\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame(data=np.random.randint(1, 5, (5, 10)),\n                      columns=[f&quot;i{i}&quot; for i in range(10)])\n&gt;&gt;&gt; df\n   i0  i1  i2  i3  i4  i5  i6  i7  i8  i9\n0   4   4   2   4   2   4   4   3   2   4\n1   2   3   4   2   2   3   1   1   4   1\n2   3   2   2   1   2   2   3   1   2   4\n3   3   1   1   3   3   4   2   2   1   3\n4   3   2   3   2   4   1   4   2   4   2\n\n&gt;&gt;&gt; top20 = df.ge(df.quantile(0.8, axis=1), axis=0)\n&gt;&gt;&gt; top20\n      i0     i1     i2     i3     i4     i5     i6     i7     i8     i9\n0   True   True  False   True  False   True   True  False  False   True\n1  False  False   True  False  False  False  False  False   True  False\n2   True  False  False  False  False  False   True  False  False   True\n3   True  False  False   True   True   True  False  False  False   True\n4  False  False  False  False   True  False   True  False   True  False\n\n&gt;&gt;&gt; top20.apply(lambda x: x.index[x], axis=1)\n0    Index(['i0', 'i1', 'i3', 'i5', 'i6', 'i9'], dt...\n1                  Index(['i2', 'i8'], dtype='object')\n2            Index(['i0', 'i6', 'i9'], dtype='object')\n3    Index(['i0', 'i3', 'i4', 'i5', 'i9'], dtype='o...\n4            Index(['i4', 'i6', 'i8'], dtype='object')\ndtype: object\n</code></pre>\n",
        "question_body": "<p>I have a dataframe of this type:</p>\n<pre><code>   i1  i3  i4  i9  i14  i16  i17  i18  i19  i20  i22  i26  i27  i28\n0   4   2   1   4    1    3    3    4    2    2    4    1    4    3\n</code></pre>\n<p>and I have to extract the name of the columns whose values are in the top 20% of all values in the row.</p>\n<p>The result would be as follows:</p>\n<pre><code>[i1,i9,i18,i22,i27]\n</code></pre>\n<p>the I need to itereate the same procedure over other rows of the same type.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "csv"
        ],
        "owner": {
            "account_id": 21977797,
            "reputation": 124,
            "user_id": 16253345,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/4227312227290363/picture?type=large",
            "display_name": "Ahmed Chater",
            "link": "https://stackoverflow.com/users/16253345/ahmed-chater"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 68209482,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625139530,
        "creation_date": 1625138439,
        "last_edit_date": 1625138581,
        "question_id": 68209241,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68209241/reading-csv-file-with-extra-line-in-pandas",
        "title": "Reading csv file with extra line in pandas",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.read_csv(\n    &quot;name_of_your_file.txt&quot;,\n    sep=r&quot;\\s*\\|\\s*&quot;,\n    comment=&quot;+&quot;,\n    usecols=range(1, 5),\n    engine=&quot;python&quot;,\n)\nprint(df)\n</code></pre>\n",
        "question_body": "<p>I'm trying to use pandas to manipulate a .txt file but I have extraline as shown in the picture below:\n<a href=\"https://i.stack.imgur.com/6lCNj.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/6lCNj.png\" alt=\"enter image description here\" /></a></p>\n<p>When i read the file</p>\n<pre><code>import pandas as pd\n\ndf=pd.read_csv('movies.txt',sep='|')\nprint(df)\n</code></pre>\n<p><strong>I got this as an output:</strong></p>\n<pre><code>                                                          +--------+--------------------------+------------+---------------+\nNaN                                                id       MovieNAme                  Year         Author                                                         NaN       \n\n+--------+--------------------------+----------... NaN      NaN                        NaN          NaN                                                            NaN       \n\nNaN                                                1234     once upon deadpool                 2017 Alicia                                                         NaN       \n\n+--------+--------------------------+----------... NaN      NaN                        NaN          NaN                                                            NaN       \n\nNaN                                                1244     avengers: endgame                  2014 John                                                           NaN       \n\n+--------+--------------------------+----------... NaN      NaN                        NaN          NaN                                                            NaN       \n\nNaN                                                1245     The bird King                      2017 Mark                                                           NaN       \n\n+--------+--------------------------+----------... NaN      NaN                        NaN          NaN                                                            NaN   \n</code></pre>\n<p>How can i fix this please and remove this line &quot;---------------------+----------&quot;</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "multi-index"
        ],
        "owner": {
            "account_id": 22100571,
            "reputation": 45,
            "user_id": 16357107,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/6c8b65f1c29d695a000264276248de33?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mop",
            "link": "https://stackoverflow.com/users/16357107/mop"
        },
        "is_answered": true,
        "view_count": 39,
        "closed_date": 1625180799,
        "accepted_answer_id": 68208717,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625138043,
        "creation_date": 1625135824,
        "last_edit_date": 1625137056,
        "question_id": 68208644,
        "link": "https://stackoverflow.com/questions/68208644/count-y-values-for-each-x-value-multiindex",
        "closed_reason": "Duplicate",
        "title": "Count Y values for each X value - MultiIndex",
        "answer_body": "<p>You can groupby and count the entries per group, since you want a series returned with the same length as the original, you can use <code>.transform()</code> which will do exactly that:</p>\n<pre><code>df['Count'] = df.groupby(['X'])['Modules'].transform('count')\n\n\n   X   Y Modules  Count\n0  1   1       A      3\n1  1   1       B      3\n2  1  45       C      3\n3  2  13       A      2\n4  2  12       B      2\n5  3  18       A      3\n6  3  16       B      3\n7  3  15       D      3\n</code></pre>\n",
        "question_body": "<p>I have this kind of df...</p>\n<pre><code>          Modules\nX     Y   \n1    1          A\n     2          B\n     45         C\n2    13         A\n     12         B\n3    18         A\n     16         B\n     15         D\n</code></pre>\n<p>I would like to count the number of Y values for each X value and add this as a column, like this:</p>\n<pre><code>          Modules  Count \nX     Y   \n1    1          A      3\n     2          B      3\n     45         C      3\n2    13         A      2\n     12         B      2\n3    18         A      3\n     16         B      3\n     15         D      3\n</code></pre>\n<p>For the moment I've tried :</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Count']=df.index.get_level_values(0).value_counts()\n</code></pre>\n<p>which gives me a list of NaN, then I tried :</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Count']=df.index.value_counts()\n</code></pre>\n<p>which obviously give me a list of 1.</p>\n<p>Can anyone please help me?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "transformation"
        ],
        "owner": {
            "account_id": 9049467,
            "reputation": 45,
            "user_id": 6740297,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-6RrKgT8gMyg/AAAAAAAAAAI/AAAAAAAAAyE/4MyU189Rs5k/photo.jpg?sz=128",
            "display_name": "Pythonidae",
            "link": "https://stackoverflow.com/users/6740297/pythonidae"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 68209128,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625137961,
        "creation_date": 1625137824,
        "question_id": 68209093,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68209093/transpose-rows-into-columns-and-create-new-columns-in-pandas",
        "title": "Transpose rows into columns and create new columns in pandas",
        "answer_body": "<p>Here's one way via <code>set_index</code> / <code>stack</code> / <code>unstack</code>:</p>\n<pre><code>df = df.set_index(df.columns[:-3].to_list()).stack().unstack(-2).reset_index()\n</code></pre>\n<p>OUTPUT:</p>\n<pre><code>print(df.head(10))\n\ncol_G  col_A  col_B col_C  col_D  col_E col_F level_6   a   b   c   d   e   f\n0          1     20     A     10  Alpha     X   col_H  97  96  59  71  98  19\n1          1     20     A     10  Alpha     X   col_I   0  55   8  26  70  85\n2          1     20     A     10  Alpha     X   col_J  90  87  16  70  68  39\n3          1     20     A     10  Alpha     Y   col_H  65  69   9  92  35  11\n4          1     20     A     10  Alpha     Y   col_I  28  47  56  83  43  81\n5          1     20     A     10  Alpha     Y   col_J   8  29  89  47  84  32\n6          1     20     A     10  Alpha     Z   col_H  97  12  95  17  34  47\n7          1     20     A     10  Alpha     Z   col_I  78  64  51  81  15  62\n8          1     20     A     10  Alpha     Z   col_J  47  83  58   3  90  11\n9          1     20     A     10   Beta     X   col_H  19   8  92  69  65  32\n</code></pre>\n",
        "question_body": "<p>Run the code to create the obfuscated data:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from itertools import product\n\nimport numpy as np\nimport pandas as pd\n\n\na = [1, 2, 3]\nb = [20, 40, 80, 160]\nc = ['A', 'B']\nd = [10, 20, 30, 40]\ne = ['Alpha', 'Beta']\nf = ['X', 'Y', 'Z']\ni = ['a', 'b', 'c', 'd', 'e', 'f']\n\ndf = pd.DataFrame(product(a, b, c, d, e, f, i),\n                  columns=['col_A', 'col_B', 'col_C', 'col_D', 'col_E', 'col_F', 'col_G'])\n\ndf['col_H'] = np.random.randint(0, 100, size=df.shape[0])\ndf['col_I'] = np.random.randint(0, 100, size=df.shape[0])\ndf['col_J'] = np.random.randint(0, 100, size=df.shape[0])\n\nprint(df)\n</code></pre>\n<p>I get the following table:</p>\n<pre><code>      col_A  col_B col_C  col_D  col_E col_F col_G  col_H  col_I  col_J\n0         1     20     A     10  Alpha     X     a     44     55     12\n1         1     20     A     10  Alpha     X     b     50     15     46\n2         1     20     A     10  Alpha     X     c     42     36     46\n3         1     20     A     10  Alpha     X     d     71     19     61\n4         1     20     A     10  Alpha     X     e     74      1      7\n5         1     20     A     10  Alpha     X     f     11     99      2\n...     ...    ...   ...    ...    ...   ...   ...    ...    ...    ...\n3450      3    160     B     40   Beta     Z     a     47     81     58\n3451      3    160     B     40   Beta     Z     b     63     82     26\n3452      3    160     B     40   Beta     Z     c     37     28     64\n3453      3    160     B     40   Beta     Z     d     54     60     91\n3454      3    160     B     40   Beta     Z     e     16     14     55\n3455      3    160     B     40   Beta     Z     f     40     18     97\n\n[3456 rows x 10 columns]\n</code></pre>\n<p>I want to transpose the last 4 columns, set the values of first column as headers, and add a new column based on the remaining 3 columns names.</p>\n<p>See below:</p>\n<pre><code>      col_A  col_B col_C  col_D  col_E col_F new_col   a   b   c   d   e   f\n0         1     20     A     10  Alpha     X   col_H  68  89   0  99  50  11\n1         1     20     A     10  Alpha     X   col_I  27  43  20  45  97  99\n2         1     20     A     10  Alpha     X   col_J  40  43  36  89  90   2\n...     ...    ...   ...    ...    ...   ...     ... ... ... ... ... ... ...\n1150      3    160     B     40   Beta     Z   col_H  59  50  95  31  32  68\n1151      3    160     B     40   Beta     Z   col_I  14  94  25  20  46  90\n1152      3    160     B     40   Beta     Z   col_J  87  32  34  99  91  90\n</code></pre>\n<p>I've tried doing it with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html\" rel=\"nofollow noreferrer\">melt</a>, <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html#pandas.DataFrame.pivot\" rel=\"nofollow noreferrer\">pivot</a>, and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html\" rel=\"nofollow noreferrer\">pivot_table</a>, but I can't figure it out.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "aggregate"
        ],
        "owner": {
            "account_id": 7315435,
            "reputation": 161,
            "user_id": 5573256,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/6db16c28999cee7d4e168d35a3736d6b?s=128&d=identicon&r=PG&f=1",
            "display_name": "Pet",
            "link": "https://stackoverflow.com/users/5573256/pet"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68208394,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625136454,
        "creation_date": 1625134063,
        "last_edit_date": 1625134369,
        "question_id": 68208254,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68208254/how-to-aggregate-mapping-table",
        "title": "How to aggregate mapping table?",
        "answer_body": "<p>IIUC:</p>\n<p>Try via <code>mask()</code> and <code>bfill()</code>:</p>\n<pre><code>df['id_new']=df['id_new'].mask(df['id_new'].isin(df['id_old'])).bfill()\n</code></pre>\n<p><strong>Explaination:</strong></p>\n<p>Checking If values of 'id_new' is in 'id_old' via <code>isin()</code></p>\n<p>So <code>isin()</code> method is giving us a boolean series so we are passing that series to <code>mask()</code> method so basically where where condition matches <code>mask()</code> method put <code>NaN</code> since we are chaining it on 'id_new' column so it will give <code>NaN</code> where the value in mask is True and where it is False it will give the values of 'id_new' column</p>\n<p>Finally backword filling values via <code>bfill()</code> method</p>\n",
        "question_body": "<p>I don't know how to put my questions into words, so I couldn't search for it specifically. I searched for some time but could not find a solution that fits my problem. I know I could create a custom solution by iterating over all the rows multiple times, but I'm hoping there is a more efficient way to solve the problem. Here is the dataframe I have:</p>\n<pre><code>id_old  id_new\nA1      A2\nA2      A3\nA4      A5\n</code></pre>\n<p>This is a mapping table and I want to change the first row because the ID has been updated again.</p>\n<p>I look for this result:</p>\n<pre><code>id_old  id_new\nA1      A3\nA2      A3\nA4      A5\n</code></pre>\n<p>Any help is welcome :-)</p>\n<p>Best</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "owner": {
            "account_id": 15085445,
            "reputation": 562,
            "user_id": 10886849,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f84345abcceabb9f273941ea74dc9e39?s=128&d=identicon&r=PG&f=1",
            "display_name": "Julio S.",
            "link": "https://stackoverflow.com/users/10886849/julio-s"
        },
        "is_answered": true,
        "view_count": 86,
        "accepted_answer_id": 61355712,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625135158,
        "creation_date": 1587517447,
        "question_id": 61355655,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61355655/pandas-how-to-sort-rows-of-a-column-using-a-dictionary-with-indexes",
        "title": "Pandas - how to sort rows of a column, using a dictionary with indexes",
        "answer_body": "<p>In two steps:</p>\n\n<p>First use <code>map</code> to create a new column with the values associated with the countries</p>\n\n<pre><code>df['country_value'] = df.countries.map(howToSortDict)\n</code></pre>\n\n<p>Then <code>sort</code> and <code>drop</code> the new column</p>\n\n<pre><code>df.sort_values('country_value').drop('country_value',axis=1)\n\n#    countries   amount\n#1   Russia      2000\n#2   China       3000\n#0   Brazil      1000\n</code></pre>\n",
        "question_body": "<p>I have to sort the values/rows of a column of a Dataframe according to a dictionary, which contains each value of the dataframe and its row index, but sorted differently.</p>\n\n<pre><code>howToSortDict = {'Brazil': 2, 'Russia': 0, 'China': 1} # row value and its index\n\nmyDict = {\n    \"countries\": [\"Brazil\", \"Russia\", \"China\"],\n    \"amount\": [1000, 2000, 3000]\n}\n\ndf = pd.DataFrame(myDict)\n</code></pre>\n\n<p>How they are sorted:</p>\n\n<pre><code>     countries  amount\n0    Brazil     1000\n1    Russia     2000\n2    China      3000\n</code></pre>\n\n<p>How I need them to be sorted (as a dataframe):</p>\n\n<pre><code>    countries   amount\n0   Russia      2000\n1   China       3000\n2   Brazil      1000\n</code></pre>\n\n<p>Any ideas of how to achieve this without looping over each row?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "account_id": 11962310,
            "reputation": 11,
            "user_id": 8753030,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/661c5cf01c959782ce763af7551d8ad5?s=128&d=identicon&r=PG&f=1",
            "display_name": "ChemBot",
            "link": "https://stackoverflow.com/users/8753030/chembot"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 68208055,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625133229,
        "creation_date": 1625133027,
        "question_id": 68208016,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68208016/how-do-i-convert-a-dataframe-to-a-dictionary-with-keys-that-are-lists",
        "title": "How do I convert a dataframe to a dictionary with keys that are lists?",
        "answer_body": "<p>Try via <code>groupby()</code> ,<code>agg()</code> and <code>to_dict()</code> method:</p>\n<pre><code>dct=df.groupby('uniprot_id')['GO_id'].agg(list).to_dict()\n</code></pre>\n<p>output of <code>dct</code>:</p>\n<pre><code>{'A0A009KHZ9': ['GO:0006097', 'GO:0006099', 'GO:0006099'],\n 'A0A009KJV3': ['GO:0006412', 'GO:0006417', 'GO:0006412'],\n 'A0A009KXK5': ['GO:0022900', 'GO:0006457'],\n 'A0A009LQ34': ['GO:0046690'],\n 'A0A009YU38': ['GO:0015074']}\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that looks like this:</p>\n<pre><code>\nuniprot_id  GO_id\n11  A0A009KHZ9  GO:0006097\n12  A0A009KHZ9  GO:0006099\n16  A0A009KHZ9  GO:0006099\n17  A0A009KJV3  GO:0006412\n20  A0A009KJV3  GO:0006417\n27  A0A009KJV3  GO:0006412\n28  A0A009KXK5  GO:0022900\n41  A0A009KXK5  GO:0006457\n43  A0A009LQ34  GO:0046690\n49  A0A009YU38  GO:0015074\n</code></pre>\n<p><code>print(len(df))</code> has over 50 million rows</p>\n<p>As you can see, uniprot_id has duplicates.</p>\n<p>I want to make a dictionary that has uniprot_id as the key and GO_id as a value. When there are multiple rows for a uniprot_id, I want to make a dictionary that has a list of GO_ids as a value. For example, <code>A0A009KHZ9</code> would look like this:</p>\n<pre><code>{'A0A009KHZ9': [GO:0006097, GO:0006099, GO:0006099]}\n</code></pre>\n<p>This is what I have tried:</p>\n<pre><code>results = {} \nfor k in df['uniprot_id']:\n  for v in df['GO_id']:\n    results.setdefault(k, []).append(v)\n\nresults\n</code></pre>\n<p>But this results in a dictionary that appends all values to each key (I tested this on a smaller dataframe).</p>\n<p>I also tried this:</p>\n<pre><code>results = {}                              \nfor k, v in df.set_index('uniprot_id'):                    \n    results.setdefault(k, []).append(v)   \n</code></pre>\n<p>But I get this error: <code>ValueError: too many values to unpack (expected 2)ValueError: too many values to unpack (expected 2)</code></p>\n<p>Can anyone help?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "account_id": 17123188,
            "reputation": 511,
            "user_id": 12390973,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/60b394bc8587cb83f435953c1778f95c?s=128&d=identicon&r=PG&f=1",
            "display_name": "Vesper",
            "link": "https://stackoverflow.com/users/12390973/vesper"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 68206146,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625126402,
        "creation_date": 1625124557,
        "question_id": 68206032,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68206032/how-to-add-elements-row-wise-in-a-dataframe-with-a-condition",
        "title": "how to add elements row wise in a DataFrame with a condition?",
        "answer_body": "<p>You can filter the desired columns by specifying the list of column names inside the <code>[]</code> brackets like so:</p>\n<pre><code>coal = df[dic1['coal']].sum(axis=1)\ngas = df[dic1['gas']].sum(axis=1)\n</code></pre>\n<p>Then you can combine the two outputs into one dataframe:</p>\n<pre><code>sol = pd.concat([coal, gas], axis=1)\n</code></pre>\n<p>finally we will change the names of the columns:</p>\n<pre><code>sol.columns = dic1.keys()\n</code></pre>\n<p>output:</p>\n<pre><code>print(sol)\n&gt;&gt;&gt;    coal  gas\n&gt;&gt;&gt; 0    42  237\n&gt;&gt;&gt; 1    50  333\n&gt;&gt;&gt; 2   102  142\n</code></pre>\n",
        "question_body": "<p>I have two dictionaries, one contains the generators list and the other contains their values.</p>\n<pre><code>dic1 = {\n    'coal': ['genCoal1', 'genCoal2', 'genCoal3'],\n    'gas': ['genGas1', 'genGas2', 'genGas3']\n}\ndic2 = {\n    'genCoal1': [12,23,34],\n    'genCoal2': [14,3,34],\n    'genCoal3': [16,24,34],\n    'genGas1': [132,23,34],\n    'genGas2': [13,257,34],\n    'genGas3': [92,53,74],\n}\n</code></pre>\n<p>I converted the second dictionary <code>dic2</code> into a pandas DataFrame so it looks like this:</p>\n<pre><code>     genCoal1  genCoal2  genCoal3  genGas1  genGas2  genGas3\n0        12        14        16      132       13       92\n1        23         3        24       23      257       53\n2        34        34        34       34       34       74\n</code></pre>\n<p>what I want to do is I want to add all the coal and gas generators values row wises, so the expected output should look like this:</p>\n<pre><code>    coal  gas\n0    42  237\n1    50  333\n2   102  142\n</code></pre>\n<p>my attempt:</p>\n<pre><code>import pandas as pd\ndic1 = {\n    'coal': ['genCoal1', 'genCoal2', 'genCoal3'],\n    'gas': ['genGas1', 'genGas2', 'genGas3']\n}\ndic2 = {\n    'genCoal1': [12,23,34],\n    'genCoal2': [14,3,34],\n    'genCoal3': [16,24,34],\n    'genGas1': [132,23,34],\n    'genGas2': [13,257,34],\n    'genGas3': [92,53,74],\n}\ndf = pd.DataFrame(dic2)\nfuelWise = {}\nfor i in dic1.keys():\n    for j in dic1[i]:\n        fuelWise[i] = df[j].sum(axis=1)\n\nprint(fuelWise)\n</code></pre>\n<p>But I am getting this error</p>\n<pre><code>ValueError: No axis named 1 for object type &lt;class 'pandas.core.series.Series'&gt;\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "max",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 18961033,
            "reputation": 321,
            "user_id": 13836545,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/pHmPb.jpg?s=128&g=1",
            "display_name": "Tabaraei",
            "link": "https://stackoverflow.com/users/13836545/tabaraei"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 68206278,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625125750,
        "creation_date": 1625125290,
        "question_id": 68206182,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68206182/pandas-first-n-maximum-values-groupby-pandas-dataframe",
        "title": "Pandas - first n maximum values groupby pandas dataframe",
        "answer_body": "<p><code>Sort</code> the values by column <code>b</code> then <code>group</code> the dataframe and aggregate using <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.head.html\" rel=\"nofollow noreferrer\"><code>head(n)</code></a> to select the first <code>n</code> rows of each group</p>\n<pre><code>df.sort_values('b', ascending=False).groupby('a').head(2)\n</code></pre>\n<hr />\n<pre><code>   a  b\n3  1  4\n0  1  3\n6  2  2\n7  2  2\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe as below:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.DataFrame({\n    'a': [1, 1, 1, 1, 1, 2, 2, 2, 2],\n    'b': [3, 2, 1, 4, 2, 1, 2, 2, 1]\n})\n</code></pre>\n<p>Which gives me</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; df\n    a   b\n0   1   3\n1   1   2\n2   1   1\n3   1   4\n4   1   2\n5   2   1\n6   2   2\n7   2   2\n8   2   1\n</code></pre>\n<p>I want to <strong>group the dataframe by column <code>a</code></strong>, and return <strong>first N maximum values</strong> from column <code>b</code> of each group, <strong>ordered descending by that maximum value</strong>.</p>\n<h2>What have I done?</h2>\n<p><strong>Let's consider I only want first two maximum values</strong>, I've done the following:</p>\n<ol>\n<li>Sort the dataframe by both columns in descending order</li>\n<li>Get first two values</li>\n<li>Since these values are order ascending, sort another time in descending order</li>\n</ol>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.sort_values(['a', 'b'], ascending=False)\ndf = df.groupby('a').nth([0,1])\ndf = df.sort_values(['a', 'b'], ascending=False).reset_index()\n</code></pre>\n<p>Which gives me</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; df\n    a   b\n0   2   2\n1   2   2\n2   1   4\n3   1   3\n</code></pre>\n<p><strong>The output Works fine</strong>, but it's not the efficient way to do this.</p>\n<p>I have also tried <code>nlargest</code> function, but it's not helping, because it drops other columns and returns only the <code>b</code> column.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.groupby('a')['b'].nlargest(2)\n</code></pre>\n<p>Which gives me</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; df\n0    2\n1    2\n2    4\n3    3\nName: b, dtype: int64\n</code></pre>\n<p><strong>What is the best way to do this?</strong></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "grouping"
        ],
        "owner": {
            "account_id": 20870870,
            "reputation": 43,
            "user_id": 15330859,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/eedf8d6f057e117d20bd7aad269fc2f4?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mag08",
            "link": "https://stackoverflow.com/users/15330859/mag08"
        },
        "is_answered": true,
        "view_count": 76,
        "accepted_answer_id": 68205038,
        "answer_count": 3,
        "score": -3,
        "last_activity_date": 1625124251,
        "creation_date": 1625110468,
        "last_edit_date": 1625122740,
        "question_id": 68203974,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68203974/check-multiple-conditions-based-on-grouping",
        "title": "Check multiple conditions based on grouping",
        "answer_body": "<ol>\n<li>find the target SiteLocation</li>\n<li>then fill the target SiteLocation with 'Found', other with 'Not Found'</li>\n</ol>\n<pre><code>cond1 = df['Boolean'] == 'true'\ncond2 = df['Active?'] == 'Pie-active'\nsite_list = set(df.loc[cond1, 'SiteLocation']) - set(df.loc[cond2, 'SiteLocation'])\ndf['Found?'] = np.where(df['SiteLocation'].isin(site_list), \n                        'Found', 'Not Found')\n</code></pre>\n",
        "question_body": "<p>Example sample table from a large table:</p>\n<pre><code>df = pd.DataFrame({'SiteLocation': ['1234 something street','1234 something street','1234 something street','1234 something street', '1234 something street','1234 something street', '567 other street', '567 other street', '567 other street', ],\n          'Boolean': ['true','false','false','false','false','false', 'false','true','false'],\n         'Active?': ['Cake-active','Pie-active','Cake-inactive','KeyLime-active', 'Vanilla Sundae-active', 'Pie-inactive', 'Cake-active','Cake-inactive','KeyLime-inactive']})\n</code></pre>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>SiteLocation</th>\n<th>Boolean</th>\n<th>Active?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1234 something street</td>\n<td>true</td>\n<td>Cake-active</td>\n</tr>\n<tr>\n<td>1234 something street</td>\n<td>false</td>\n<td>Pie-active</td>\n</tr>\n<tr>\n<td>1234 something street</td>\n<td>false</td>\n<td>Cake-inactive</td>\n</tr>\n<tr>\n<td>1234 something street</td>\n<td>false</td>\n<td>KeyLime-active</td>\n</tr>\n<tr>\n<td>1234 something street</td>\n<td>false</td>\n<td>Vanilla Sundae-active</td>\n</tr>\n<tr>\n<td>1234 something street</td>\n<td>false</td>\n<td>Pie-inactive</td>\n</tr>\n<tr>\n<td>567 other street</td>\n<td>false</td>\n<td>Cake-active</td>\n</tr>\n<tr>\n<td>567 other street</td>\n<td>true</td>\n<td>Cake-inactive</td>\n</tr>\n<tr>\n<td>567 other street</td>\n<td>false</td>\n<td>KeyLime-inactive</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>My goal: I\u2019d like to create a \u2018Found?\u2019 column that returns \u2018Found\u2019 if a SiteLocation has any <code>true</code> in column <code>Boolean</code> and should not have <code>Pie-Active</code> in column <code>Active?</code>.</p>\n<p>In this example, SiteLocation '567 other street' would get flagged as 'found'. See below for what I mean:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>SiteLocation</th>\n<th>Boolean</th>\n<th>Active?</th>\n<th>Found?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1234 something street</td>\n<td>true</td>\n<td>Cake-active</td>\n<td>Not Found</td>\n</tr>\n<tr>\n<td>1234 something street</td>\n<td>false</td>\n<td>Pie-active</td>\n<td>Not Found</td>\n</tr>\n<tr>\n<td>1234 something street</td>\n<td>false</td>\n<td>Cake-inactive</td>\n<td>Not Found</td>\n</tr>\n<tr>\n<td>1234 something street</td>\n<td>false</td>\n<td>KeyLime-active</td>\n<td>Not Found</td>\n</tr>\n<tr>\n<td>1234 something street</td>\n<td>false</td>\n<td>Vanilla Sundae-active</td>\n<td>Not Found</td>\n</tr>\n<tr>\n<td>1234 something street</td>\n<td>false</td>\n<td>Pie-inactive</td>\n<td>Not Found</td>\n</tr>\n<tr>\n<td>567 other street</td>\n<td>false</td>\n<td>Cake-active</td>\n<td>Found</td>\n</tr>\n<tr>\n<td>567 other street</td>\n<td>true</td>\n<td>Cake-inactive</td>\n<td>Found</td>\n</tr>\n<tr>\n<td>567 other street</td>\n<td>false</td>\n<td>KeyLime-inactive</td>\n<td>Found</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Although <code>1234 something street</code> has <code>true</code> in column <code>Boolean</code>, in column <code>Active?</code> it has <code>Pie-active</code> not satisfied the condition.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "conditional-statements",
            "slice"
        ],
        "owner": {
            "account_id": 15226115,
            "reputation": 45,
            "user_id": 10986708,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-r0gpH72qjqI/AAAAAAAAAAI/AAAAAAAAAD4/zanvZ9PfG7I/photo.jpg?sz=128",
            "display_name": "bgeaibreyi",
            "link": "https://stackoverflow.com/users/10986708/bgeaibreyi"
        },
        "is_answered": true,
        "view_count": 61,
        "accepted_answer_id": 68205200,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1625123516,
        "creation_date": 1625119511,
        "question_id": 68205087,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68205087/calculate-the-average-of-sections-of-a-column-with-condition-met-to-create-new-d",
        "title": "Calculate the average of sections of a column with condition met to create new dataframe",
        "answer_body": "<pre><code>df1 = df.groupby((df['B'].shift() != df['B']).cumsum()).mean().reset_index(drop=True)\ndf1 = df1[df1['B'] == 1].astype(int).reset_index(drop=True)\ndf1\n</code></pre>\n<p><strong>Output</strong></p>\n<pre><code>    A   B\n0   2   1\n1   3   1\n</code></pre>\n<p><strong>Explanation</strong></p>\n<p>We are checking if each row's value of B is not equal to next value using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html\" rel=\"nofollow noreferrer\">pd.shift</a>, if so then we are grouping those values and calculating its mean and assigning it to new dataframe <code>df1</code>.</p>\n<p>Since we have mean of groups of all consecutive 0s and 1s, so we are then filtering only values of <code>B==1</code>.</p>\n",
        "question_body": "<p>I have the below data table</p>\n<pre><code>A = [2, 3, 1, 2, 4, 1, 5, 3, 1, 7, 5]\nB = [0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\ndf = pd.DataFrame({'A':A, 'B':B})\n</code></pre>\n<p>I'd like to calculate the average of column A when consecutive rows see column B equal to 1. All rows where column B equal to 0 are neglected and subsequently create a new dataframe like below:</p>\n<p><a href=\"https://i.stack.imgur.com/LFtGy.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/LFtGy.png\" alt=\"Desired table\" /></a></p>\n<p>Thanks for your help!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "percentage"
        ],
        "owner": {
            "account_id": 19291329,
            "reputation": 33,
            "user_id": 14101264,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c9ffd44e442f23950c697c8c5bffdc6c?s=128&d=identicon&r=PG&f=1",
            "display_name": "johnnydoe",
            "link": "https://stackoverflow.com/users/14101264/johnnydoe"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68150825,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625118146,
        "creation_date": 1624792623,
        "last_edit_date": 1625118146,
        "question_id": 68150589,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68150589/percentage-decrease-based-on-column-value",
        "title": "Percentage decrease based on column value",
        "answer_body": "<p>You can create a function to calculate the score and apply it to every <code>timeSpent</code></p>\n<pre><code>def get_score(num):\n    if num &lt;= 2500: return 1\n    if num &gt;= 5500: return 0\n    x = 1\n    for _ in range((num - 2500) // 100):\n        x *= 0.8\n    return x\n\ndf = pd.DataFrame({'question': [a, b, c, d, e], 'timeSpent': [5354, 2344, 2555, 5200, 3567]})\ndf['Score'] = df.timeSpent.apply(lambda x: get_score(x))\n</code></pre>\n<p>Output:</p>\n<pre><code>  question  timeSpent     Score\n0        a       5354  0.001934\n1        b       2344  1.000000\n2        c       2555  1.000000\n3        d       5200  0.002418\n4        e       3567  0.107374\n</code></pre>\n",
        "question_body": "<p>My dataframe looks like this:</p>\n<pre><code>question   timeSpent\na          5354\nb          2344\nc          2555\nd          5200\ne          3567\n</code></pre>\n<p>I want to add an extra column, <code>Score</code>, which contains values between <code>0</code> and <code>1</code>. The greater the <code>timeSpent</code> (expressed in seconds) is, the closer to 0 the <code>Score</code>. If the time spent is smaller, then the <code>Score</code> approaches 1.</p>\n<p>Suppose the value is <code>1</code> if <code>timeSpent</code> is smaller or equal than <code>2500</code>. Then it drops by <code>20%</code> with every <code>100</code> seconds that have passed. If it hits or is greater than <code>5500</code>, it stays at <code>0</code>.</p>\n<p>So for <code>2600</code>, the score would be <code>0.8</code>, for <code>2700</code> the score would be <code>0.64</code> etc.</p>\n<p>I wrote if-else statements for every interval, but I think there must be a quicker way to do it.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 22061685,
            "reputation": 41,
            "user_id": 16324245,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3a04d2d2ea5d5402844e970fb4426307?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mark J.",
            "link": "https://stackoverflow.com/users/16324245/mark-j"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 68203969,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625111547,
        "creation_date": 1625110138,
        "last_edit_date": 1625111547,
        "question_id": 68203921,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68203921/combine-independent-words-into-a-sentence-recognize-the-first-start-word",
        "title": "Combine independent words into a sentence (recognize the first start word)",
        "answer_body": "<p>Just you need to split the sentence every time you see the &quot;&gt;&gt;&quot; element:</p>\n<pre class=\"lang-py prettyprint-override\"><code>sentences = []\nsentence = &quot;&quot;\nfor item in word:\n    if &quot;&gt;&gt;&quot; == item:\n        if sentence:\n            sentences.append(sentence)\n        sentence = &quot;&quot;\n        continue\n    sentence += item\n</code></pre>\n<p>That should do it, in order to integrate this solution to your dataframe you might need to create a method for this algorithm</p>\n",
        "question_body": "<p>Here is  dataframe and the '&gt;&gt;' is the beginning of one sentence. So, any suggestion for that?</p>\n<pre><code>import pandas as pd\ndata = {'start_time': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3],\n'end_time': [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4],\n'word':['&gt;&gt; I','AM','OK','&gt;&gt; HOW', 'ABOUT', 'YOU','&gt;&gt;OK']}\ndata\ndf = pd.DataFrame(data, columns = ['start_time', 'end_time','word'])\ndf\n</code></pre>\n<p>I need to keep &gt;&gt; as the beginning of the sentence, the result is</p>\n<pre><code>start_time end_time    word\n\n0.1         0.6       &gt;&gt; I AM OK\n\n0.7         1.2       &gt;&gt; HOW ABOUT YOU\n\n1.3         1.4       &gt;&gt;OK\n</code></pre>\n<p>I have no idea to get any sentence start with &gt;&gt;</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20889183,
            "reputation": 23,
            "user_id": 15345009,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-5bpg13bb0cs/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucl0EWjMK5fdjvBTSFWqmt2R7IqgAQ/s96-c/photo.jpg?sz=128",
            "display_name": "Clerami",
            "link": "https://stackoverflow.com/users/15345009/clerami"
        },
        "is_answered": true,
        "view_count": 69,
        "accepted_answer_id": 68203360,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1625107270,
        "creation_date": 1625066370,
        "last_edit_date": 1625066543,
        "question_id": 68197524,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68197524/how-to-make-highly-nested-dictionary-data-into-data-frames",
        "title": "How to make highly nested dictionary data into data frames",
        "answer_body": "<p>First of all, let's solve this problem.</p>\n<pre><code>raw = {\n    &quot;20211229&quot;: {\n        &quot;00101&quot;: [\n            &quot;\ube44\uace0101-1&quot;,\n            &quot;\ube44\uace0101-2&quot;,\n            &quot;\ube44\uace0101-3&quot;,\n            {0: [&quot; UT1213K&quot;, &quot;1&quot;, &quot;11.00&quot;, &quot;11&quot;, &quot;2&quot;]},\n        ]\n    },\n    &quot;20211230&quot;: {\n        &quot;00102&quot;: [\n            &quot;\ube44\uace0102-1&quot;,\n            &quot;\ube44\uace0102-2&quot;,\n            &quot;\ube44\uace0102-3&quot;,\n            {0: [&quot;B 001&quot;, &quot;2&quot;, &quot;22.00&quot;, &quot;44&quot;, &quot;5&quot;]},\n        ]\n    },\n    &quot;20211231&quot;: {\n        &quot;00103&quot;: [\n            &quot;\ube44\uace0103-1&quot;,\n            &quot;\ube44\uace0103-2&quot;,\n            &quot;\ube44\uace0103-3&quot;,\n            {\n                0: [&quot;B 004&quot;, &quot;10&quot;, &quot;66.00&quot;, &quot;660&quot;, &quot;66&quot;],\n                1: [&quot;B 005&quot;, &quot;20&quot;, &quot;77.00&quot;, &quot;1540&quot;, &quot;154&quot;],\n                2: [&quot;B 006&quot;, &quot;30&quot;, &quot;88.00&quot;, &quot;2640&quot;, &quot;264&quot;],\n                3: [&quot;B 007&quot;, &quot;40&quot;, &quot;99.00&quot;, &quot;3960&quot;, &quot;396&quot;],\n            },\n        ],\n        &quot;00104&quot;: [\n            &quot;\ube44\uace01&quot;,\n            &quot;\ube44\uace02&quot;,\n            &quot;&quot;,\n            {\n                0: [&quot;B 003&quot;, &quot;3&quot;, &quot;33.00&quot;, &quot;99&quot;, &quot;10&quot;],\n                1: [&quot;B 004&quot;, &quot;4&quot;, &quot;44.00&quot;, &quot;176&quot;, &quot;18&quot;],\n            },\n        ],\n    },\n}\n\nformatted_dict = []\nfor first_level_key, first_level_value in raw.items():\n    for second_level_key, second_level_value in first_level_value.items():\n        third_level_tmp_list = []\n        for third_level_value in second_level_value:\n            if isinstance(third_level_value, str):\n                third_level_tmp_list.append(third_level_value)\n        third_level_tmp_dict = dict(\n            zip([&quot;remark1&quot;, &quot;remark2&quot;, &quot;remark3&quot;], third_level_tmp_list)\n        )\n        for third_level_value in second_level_value:\n            if isinstance(third_level_value, dict):\n                for (\n                    fourth_level_key,\n                    fourth_level_value,\n                ) in third_level_value.items():\n                    new_record = {}\n                    new_record.update(\n                        {\n                            &quot;date&quot;: first_level_key,\n                            &quot;customer_code&quot;: second_level_key,\n                            &quot;item&quot;: fourth_level_key,\n                        }\n                    )\n                    new_record.update(\n                        dict(\n                            zip(\n                                [\n                                    &quot;item_code&quot;,\n                                    &quot;qty&quot;,\n                                    &quot;unit_price&quot;,\n                                    &quot;supply_price&quot;,\n                                    &quot;tax_amount&quot;,\n                                ],\n                                fourth_level_value,\n                            )\n                        )\n                    )\n                    new_record.update(third_level_tmp_dict)\n                    formatted_dict.append(new_record)\n\nprint(formatted_dict)\nresult = pd.DataFrame(formatted_dict).set_index(\n    [&quot;date&quot;, &quot;customer_code&quot;, &quot;remark1&quot;, &quot;remark2&quot;, &quot;remark3&quot;]\n)[[&quot;item&quot;, &quot;item_code&quot;, &quot;qty&quot;, &quot;unit_price&quot;, &quot;supply_price&quot;, &quot;tax_amount&quot;]]\npd.set_option(&quot;display.max_columns&quot;, 500)\npd.set_option(&quot;display.width&quot;, 1000)\nprint(result)\n</code></pre>\n<p>Second, I suggest you do not construct raw data like that. I suggest you make your raw data like this.</p>\n<pre><code>[\n    {\n        &quot;date&quot;: &quot;20211229&quot;,\n        &quot;customer_code&quot;: &quot;00101&quot;,\n        &quot;item&quot;: 0,\n        &quot;item_code&quot;: &quot; UT1213K&quot;,\n        &quot;qty&quot;: &quot;1&quot;,\n        &quot;unit_price&quot;: &quot;11.00&quot;,\n        &quot;supply_price&quot;: &quot;11&quot;,\n        &quot;tax_amount&quot;: &quot;2&quot;,\n        &quot;remark1&quot;: &quot;\ube44\uace0101-1&quot;,\n        &quot;remark2&quot;: &quot;\ube44\uace0101-2&quot;,\n        &quot;remark3&quot;: &quot;\ube44\uace0101-3&quot;,\n    },\n    {\n        &quot;date&quot;: &quot;20211230&quot;,\n        &quot;customer_code&quot;: &quot;00102&quot;,\n        &quot;item&quot;: 0,\n        &quot;item_code&quot;: &quot;B 001&quot;,\n        &quot;qty&quot;: &quot;2&quot;,\n        &quot;unit_price&quot;: &quot;22.00&quot;,\n        &quot;supply_price&quot;: &quot;44&quot;,\n        &quot;tax_amount&quot;: &quot;5&quot;,\n        &quot;remark1&quot;: &quot;\ube44\uace0102-1&quot;,\n        &quot;remark2&quot;: &quot;\ube44\uace0102-2&quot;,\n        &quot;remark3&quot;: &quot;\ube44\uace0102-3&quot;,\n    },\n    {\n        &quot;date&quot;: &quot;20211231&quot;,\n        &quot;customer_code&quot;: &quot;00103&quot;,\n        &quot;item&quot;: 0,\n        &quot;item_code&quot;: &quot;B 004&quot;,\n        &quot;qty&quot;: &quot;10&quot;,\n        &quot;unit_price&quot;: &quot;66.00&quot;,\n        &quot;supply_price&quot;: &quot;660&quot;,\n        &quot;tax_amount&quot;: &quot;66&quot;,\n        &quot;remark1&quot;: &quot;\ube44\uace0103-1&quot;,\n        &quot;remark2&quot;: &quot;\ube44\uace0103-2&quot;,\n        &quot;remark3&quot;: &quot;\ube44\uace0103-3&quot;,\n    },\n    {\n        &quot;date&quot;: &quot;20211231&quot;,\n        &quot;customer_code&quot;: &quot;00103&quot;,\n        &quot;item&quot;: 1,\n        &quot;item_code&quot;: &quot;B 005&quot;,\n        &quot;qty&quot;: &quot;20&quot;,\n        &quot;unit_price&quot;: &quot;77.00&quot;,\n        &quot;supply_price&quot;: &quot;1540&quot;,\n        &quot;tax_amount&quot;: &quot;154&quot;,\n        &quot;remark1&quot;: &quot;\ube44\uace0103-1&quot;,\n        &quot;remark2&quot;: &quot;\ube44\uace0103-2&quot;,\n        &quot;remark3&quot;: &quot;\ube44\uace0103-3&quot;,\n    },\n    {\n        &quot;date&quot;: &quot;20211231&quot;,\n        &quot;customer_code&quot;: &quot;00103&quot;,\n        &quot;item&quot;: 2,\n        &quot;item_code&quot;: &quot;B 006&quot;,\n        &quot;qty&quot;: &quot;30&quot;,\n        &quot;unit_price&quot;: &quot;88.00&quot;,\n        &quot;supply_price&quot;: &quot;2640&quot;,\n        &quot;tax_amount&quot;: &quot;264&quot;,\n        &quot;remark1&quot;: &quot;\ube44\uace0103-1&quot;,\n        &quot;remark2&quot;: &quot;\ube44\uace0103-2&quot;,\n        &quot;remark3&quot;: &quot;\ube44\uace0103-3&quot;,\n    },\n    {\n        &quot;date&quot;: &quot;20211231&quot;,\n        &quot;customer_code&quot;: &quot;00103&quot;,\n        &quot;item&quot;: 3,\n        &quot;item_code&quot;: &quot;B 007&quot;,\n        &quot;qty&quot;: &quot;40&quot;,\n        &quot;unit_price&quot;: &quot;99.00&quot;,\n        &quot;supply_price&quot;: &quot;3960&quot;,\n        &quot;tax_amount&quot;: &quot;396&quot;,\n        &quot;remark1&quot;: &quot;\ube44\uace0103-1&quot;,\n        &quot;remark2&quot;: &quot;\ube44\uace0103-2&quot;,\n        &quot;remark3&quot;: &quot;\ube44\uace0103-3&quot;,\n    },\n    {\n        &quot;date&quot;: &quot;20211231&quot;,\n        &quot;customer_code&quot;: &quot;00104&quot;,\n        &quot;item&quot;: 0,\n        &quot;item_code&quot;: &quot;B 003&quot;,\n        &quot;qty&quot;: &quot;3&quot;,\n        &quot;unit_price&quot;: &quot;33.00&quot;,\n        &quot;supply_price&quot;: &quot;99&quot;,\n        &quot;tax_amount&quot;: &quot;10&quot;,\n        &quot;remark1&quot;: &quot;\ube44\uace01&quot;,\n        &quot;remark2&quot;: &quot;\ube44\uace02&quot;,\n        &quot;remark3&quot;: &quot;&quot;,\n    },\n    {\n        &quot;date&quot;: &quot;20211231&quot;,\n        &quot;customer_code&quot;: &quot;00104&quot;,\n        &quot;item&quot;: 1,\n        &quot;item_code&quot;: &quot;B 004&quot;,\n        &quot;qty&quot;: &quot;4&quot;,\n        &quot;unit_price&quot;: &quot;44.00&quot;,\n        &quot;supply_price&quot;: &quot;176&quot;,\n        &quot;tax_amount&quot;: &quot;18&quot;,\n        &quot;remark1&quot;: &quot;\ube44\uace01&quot;,\n        &quot;remark2&quot;: &quot;\ube44\uace02&quot;,\n        &quot;remark3&quot;: &quot;&quot;,\n    },\n]\n</code></pre>\n<p>If your raw data like this, You could just get your result simply like this.</p>\n<pre><code>result = pd.DataFrame(raw).set_index(\n    [&quot;date&quot;, &quot;customer_code&quot;, &quot;remark1&quot;, &quot;remark2&quot;, &quot;remark3&quot;]\n)[[&quot;item&quot;, &quot;item_code&quot;, &quot;qty&quot;, &quot;unit_price&quot;, &quot;supply_price&quot;, &quot;tax_amount&quot;]]\n</code></pre>\n",
        "question_body": "<p>I've got answers in <a href=\"https://stackoverflow.com/questions/68188983/how-to-convert-nested-dict-data-to-data-frame-using-pandas/68189851?noredirect=1#comment120523143_68189851\">past question</a>, but when it has multiple top-level indexes (date) and different values for each index, there's a problem. so\nI changed the data structure based on the advice from the previous comment, but I'm not sure if it's correct. Also, it's hard to understand how to index.</p>\n<pre><code>{'20211229': {'00101': ['\ube44\uace0101-1',\n                    '\ube44\uace0101-2',\n                    '\ube44\uace0101-3',\n                    {0: [' UT1213K', '1', '11.00', '11', '2']}]},\n '20211230': {'00102': ['\ube44\uace0102-1',\n                    '\ube44\uace0102-2',\n                    '\ube44\uace0102-3',\n                    {0: ['B 001', '2', '22.00', '44', '5']}]},\n '20211231': {'00103': ['\ube44\uace0103-1',\n                    '\ube44\uace0103-2',\n                    '\ube44\uace0103-3',\n                    {0: ['B 004', '10', '66.00', '660', '66'],\n                     1: ['B 005', '20', '77.00', '1540', '154'],\n                     2: ['B 006', '30', '88.00', '2640', '264'],\n                     3: ['B 007', '40', '99.00', '3960', '396']}],\n          '00104': ['\ube44\uace01',\n                    '\ube44\uace02',\n                    '',\n                    {0: ['B 003', '3', '33.00', '99', '10'],\n                     1: ['B 004', '4', '44.00', '176', '18']}]}}\n</code></pre>\n<p>I want to index as below. I'd really appreciate your help.</p>\n<p><a href=\"https://i.stack.imgur.com/yveBO.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/yveBO.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20521922,
            "reputation": 23,
            "user_id": 15061312,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/022fdb32673cafb1d974cf89c9a676b6?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dev",
            "link": "https://stackoverflow.com/users/15061312/dev"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68203512,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625106806,
        "creation_date": 1625104852,
        "last_edit_date": 1625105123,
        "question_id": 68203385,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68203385/delimiter-to-specific-column-in-csv-using-dataframes-python",
        "title": "Delimiter to specific column in CSV using dataframes python",
        "answer_body": "<p>See comments in the code for details on the steps:</p>\n<pre><code>import io\nimport pandas as pd\n\n# data as a string\ntext = '''date     ctr     code          \n12-May   CN      1111/abc/12-e\n12-May   CN      1112/abc/wds/12-e\n12-May   CN      1113/abc/12-e'''\n\n# your original data frame\ndf = pd.read_csv(io.StringIO(text), sep=r'\\s+')\n\n# split code to new columns (0-based)\ndf2 = df.code.str.split(&quot;/&quot;, expand=True)\n\n# rename new columns from 0-base to 1-based\ndf2 = df2.rename(columns=lambda x: f&quot;code{int(x)+1}&quot;)\n\n# join with original dataframe\ndf2 = df.join(df2)\n\n# drop original code column\ndf2.drop(columns=['code'], inplace=True)\n\n# test\nprint(df2)\n</code></pre>\n<p>Outputs:</p>\n<pre><code>     date ctr code1 code2 code3 code4\n0  12-May  CN  1111   abc  12-e  None\n1  12-May  CN  1112   abc   wds  12-e\n2  12-May  CN  1113   abc  12-e  None\n</code></pre>\n",
        "question_body": "<p>below input table is in CSV file, I need to apply delimiter for code column based on &quot;/&quot; and split the values to other column and number of &quot;/&quot; will varies it is not constant and also output table is mention below I need to achieve that using data frames pandas</p>\n<p>input table:</p>\n<pre><code>date     ctr     code          \n12-May   CN      1111/abc/12-e\n12-May   CN      1112/abc/wds/12-e\n12-May   CN      1113/abc/12-e\n</code></pre>\n<p>output table:</p>\n<pre><code>date     ctr     code     Code1    code2  code3\n12-May   CN      1111     abc      12-e\n12-May   CN      1112     abc      wds    12-e\n12-May   CN      1113     abc      12-e\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 3604909,
            "reputation": 43,
            "user_id": 3007310,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2247041fd264fed86da1da713dc134d7?s=128&d=identicon&r=PG&f=1",
            "display_name": "Sasha18",
            "link": "https://stackoverflow.com/users/3007310/sasha18"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 68203525,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625106154,
        "creation_date": 1625104787,
        "question_id": 68203377,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68203377/flip-and-shift-multi-column-data-to-the-left-in-pandas",
        "title": "Flip and shift multi-column data to the left in Pandas",
        "answer_body": "<p>Try with <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.fliplr.html#numpy-fliplr\" rel=\"nofollow noreferrer\"><code>fliplr</code></a>:</p>\n<pre><code># Get numpy structure\nx = df.loc[:, 'Sup_1 ID':].to_numpy()\n# flip left to right\na = np.fliplr(x)\n# Overwrite not NaN values in x with not NaN in a\nx[~np.isnan(x)] = a[~np.isnan(a)]\n# Update DataFrame\ndf.loc[:, 'Sup_1 ID':] = x\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>   Emp_ID  Sup_1 ID  Sup_2 ID  Sup_3 ID  Sup_4 ID\n0     123     789.0     678.0     456.0     234.0\n1     234     789.0     678.0     456.0       NaN\n2     456     789.0     678.0       NaN       NaN\n3     678     789.0       NaN       NaN       NaN\n4     789       NaN       NaN       NaN       NaN\n</code></pre>\n<hr />\n<p>DataFrame Constructor and imports:</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Emp_ID': [123, 234, 456, 678, 789],\n    'Sup_1 ID': [234.0, 456.0, 678.0, 789.0, np.nan],\n    'Sup_2 ID': [456.0, 678.0, 789.0, np.nan, np.nan],\n    'Sup_3 ID': [678.0, 789.0, np.nan, np.nan, np.nan],\n    'Sup_4 ID': [789.0, np.nan, np.nan, np.nan, np.nan]\n})\n</code></pre>\n",
        "question_body": "<p>Here's an Employee - Supervisor mapping data.</p>\n<p>I'd like to flip and then shift whole column to left. Only data should be shifted to the left 1 time and the columns should be fixed. Could you tell me how can I do this?</p>\n<p>Input: Bottom - Up approach</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\">Emp_ID</th>\n<th style=\"text-align: center;\">Sup_1 ID</th>\n<th style=\"text-align: center;\">Sup_2 ID</th>\n<th style=\"text-align: center;\">Sup_3 ID</th>\n<th style=\"text-align: center;\">Sup_4 ID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">123</td>\n<td style=\"text-align: center;\">234</td>\n<td style=\"text-align: center;\">456</td>\n<td style=\"text-align: center;\">678</td>\n<td style=\"text-align: center;\">789</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">234</td>\n<td style=\"text-align: center;\">456</td>\n<td style=\"text-align: center;\">678</td>\n<td style=\"text-align: center;\">789</td>\n<td style=\"text-align: center;\">NaN</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">456</td>\n<td style=\"text-align: center;\">678</td>\n<td style=\"text-align: center;\">789</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">678</td>\n<td style=\"text-align: center;\">789</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">789</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Output: Top - Down approach</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\">Emp_ID</th>\n<th style=\"text-align: center;\">Sup_1 ID</th>\n<th style=\"text-align: center;\">Sup_2 ID</th>\n<th style=\"text-align: center;\">Sup_3 ID</th>\n<th style=\"text-align: center;\">Sup_4 ID</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">123</td>\n<td style=\"text-align: center;\">789</td>\n<td style=\"text-align: center;\">678</td>\n<td style=\"text-align: center;\">456</td>\n<td style=\"text-align: center;\">234</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">234</td>\n<td style=\"text-align: center;\">789</td>\n<td style=\"text-align: center;\">678</td>\n<td style=\"text-align: center;\">456</td>\n<td style=\"text-align: center;\">NaN</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">456</td>\n<td style=\"text-align: center;\">789</td>\n<td style=\"text-align: center;\">678</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">678</td>\n<td style=\"text-align: center;\">789</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">789</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n<td style=\"text-align: center;\">NaN</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Appreciate any kind of assistance</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 19590203,
            "reputation": 2249,
            "user_id": 14337399,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/6f262a09289e686e4045594edaeef8ef?s=128&d=identicon&r=PG&f=1",
            "display_name": "Juliette",
            "link": "https://stackoverflow.com/users/14337399/juliette"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68203353,
        "answer_count": 4,
        "score": -4,
        "last_activity_date": 1625104880,
        "creation_date": 1625103935,
        "question_id": 68203313,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68203313/finding-greatest-differential-in-pandas-dataframe-columns",
        "title": "finding greatest differential in pandas dataframe columns",
        "answer_body": "<p>Yes you can:</p>\n<pre class=\"lang-py prettyprint-override\"><code>differential = df['Price2'] - df['Price']\nticker = df.loc[differential.idxmax(), 'Ticker']\n</code></pre>\n<p>But seeing that you are working with stock prices, absolute price differential has little meaning. A $10 differential means more to a $136 stock (like Apple) than to a $3400 stock (like Amazon) and it's a rounding error on a $418k stock (Berkshire Hathaway). A better measure is to use differential percentage:</p>\n<pre class=\"lang-py prettyprint-override\"><code>differential = df['Price2'] / df['Price'] - 1\nticker = df.loc[differential.idxmax(), 'Ticker']\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe below:</p>\n<pre><code>df \n\n    Ticker       Price     Volume        Price2\n0        A  147.779999    51918.0  147.779999\n1      AAL   21.209999   229944.0   44.523753\n2      AAP  205.139999    32928.0   61.324705\n3     AAPL  136.919998  1175723.0  120.954594\n4     ABBV  112.599998   135235.0  120.259632\n...\n</code></pre>\n<p>I want to parse through the df and find the Ticker that has the greatest differential between Price2 and Price (Price2 minus Price). Whatever Ticker is selected I want the value of the row to be stored in a variable so I can access the specific columns.</p>\n<p>Is this possible? Any help would be much appreciated!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "indexing"
        ],
        "owner": {
            "account_id": 22034865,
            "reputation": 25,
            "user_id": 16301544,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/0b9d222575b2802c55b44c7feae9079f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Cristian",
            "link": "https://stackoverflow.com/users/16301544/cristian"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68202868,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625098240,
        "creation_date": 1625092202,
        "question_id": 68202268,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68202268/start-index-under-field-from-1-with-pandas-dataframe",
        "title": "Start index (under FIeld) from 1 with pandas DataFrame",
        "answer_body": "<p>I found a similar question here: <a href=\"https://stackoverflow.com/questions/32249960/in-python-pandas-start-row-index-from-1-instead-of-zero-without-creating-additi\">In Python pandas, start row index from 1 instead of zero without creating additional column</a></p>\n<p>For your question, it would be as simple as adding the following line:</p>\n<pre><code>df[&quot;Field&quot;] = np.arange(1, len(df) + 1)\n</code></pre>\n",
        "question_body": "<p>I would like to start the index from 1 undes the &quot;Field&quot; column</p>\n<pre><code>df = pd.DataFrame(list(zip(total_points, passing_percentage)),\n                 columns =['Pts Measured', '% pass'])\ndf = df.rename_axis('Field').reset_index()\ndf[&quot;Comments&quot;] = &quot;&quot;\ndf\n</code></pre>\n<p>Output:</p>\n<pre><code>  Field  Pts Measured   % pass  Comments\n0   0       92909       90.66   \n1   1       92830       91.85   \n2   2      130714       99.99   \n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21893135,
            "reputation": 19,
            "user_id": 16179815,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxI6y7X2uOXD8offMFw3wAtGN2IRnEM3cVPb3Cg=k-s128",
            "display_name": "Eng_GR",
            "link": "https://stackoverflow.com/users/16179815/eng-gr"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 68202582,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625096754,
        "creation_date": 1625094496,
        "question_id": 68202480,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68202480/python-group-by-two-columns-and-filter-to-get-specific-values",
        "title": "Python group by two columns and filter to get specific values",
        "answer_body": "<pre><code>cols = [&quot;Month&quot;, &quot;Type_failure&quot;]\ngrouped_df1 = df.groupby(cols).size()\ngrouped_df1.unstack()[['Roof fall']].stack()\n# to get as a new dataframe, use below code\n# df2 = pd.DataFrame(grouped_df1.unstack()[['Roof fall']].stack()).rename(columns={0: &quot;count&quot;})\n</code></pre>\n",
        "question_body": "<p>I have Data like this:</p>\n<p><a href=\"https://i.stack.imgur.com/Z9dlp.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Z9dlp.png\" alt=\"enter image description here\" /></a></p>\n<p>I <strong>grouped the data by</strong> ('<strong>Month</strong>' and '<strong>type_failure</strong>') to get the number of falls associated with each type_failure:</p>\n<pre><code>cols = [&quot;Month&quot;, &quot;Type_failure&quot;]\ngrouped_df1 = df.groupby(cols).size()\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/JaJVD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/JaJVD.png\" alt=\"enter image description here\" /></a></p>\n<p>I want to filter the grouped data to get the the <strong>'Roof fall' cases only</strong>. <strong>The expected output should be:</strong></p>\n<p><a href=\"https://i.stack.imgur.com/kcWAx.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/kcWAx.png\" alt=\"enter image description here\" /></a></p>\n<p>This is what I have done, but it did not work.</p>\n<pre><code>filtered_group = grouped_df1.filter(lambda x: x[x['Type_failure'] == 'Roof fall'])\nprint(filtered_group) \n</code></pre>\n<p>Any idea how to solve this problem.</p>\n<p>Thanks,</p>\n"
    },
    {
        "tags": [
            "python",
            "database",
            "sqlite",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 974718,
            "reputation": 11046,
            "user_id": 996366,
            "user_type": "registered",
            "accept_rate": 65,
            "profile_image": "https://i.stack.imgur.com/F8mcb.jpg?s=128&g=1",
            "display_name": "Eka",
            "link": "https://stackoverflow.com/users/996366/eka"
        },
        "is_answered": true,
        "view_count": 67523,
        "accepted_answer_id": 36029761,
        "answer_count": 7,
        "score": 75,
        "last_activity_date": 1625096188,
        "creation_date": 1458111686,
        "question_id": 36028759,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/36028759/how-to-open-and-convert-sqlite-database-to-pandas-dataframe",
        "title": "How to open and convert sqlite database to pandas dataframe",
        "answer_body": "<p>Despite sqlite being part of the Python Standard Library and is a nice and easy interface to SQLite databases, the Pandas tutorial <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#reading-tables\" rel=\"noreferrer\">states</a>:</p>\n<blockquote>\n<p>Note In order to use read_sql_table(), you must have the SQLAlchemy\noptional dependency installed.</p>\n</blockquote>\n<p>But Pandas still supports sqlite3 access if you want to avoid installing SQLAlchemy:</p>\n<pre><code>import sqlite3\nimport pandas as pd\n# Create your connection.\ncnx = sqlite3.connect('file.db')\n\ndf = pd.read_sql_query(&quot;SELECT * FROM table_name&quot;, cnx)\n</code></pre>\n<p>As stated <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sqlite-fallback\" rel=\"noreferrer\">here</a>, but you need to know the name of the used table in advance.</p>\n",
        "question_body": "<p>I have downloaded some datas as a sqlite database (data.db) and I want to open this database in python and then convert it into pandas dataframe.</p>\n\n<p>This is so far I have done</p>\n\n<pre><code>import sqlite3\nimport pandas    \ndat = sqlite3.connect('data.db') #connected to database with out error\npandas.DataFrame.from_records(dat, index=None, exclude=None, columns=None, coerce_float=False, nrows=None)\n</code></pre>\n\n<p>But its throwing this error</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py\", line 980, in from_records\n    coerce_float=coerce_float)\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py\", line 5353, in _to_arrays\n    if not len(data):\nTypeError: object of type 'sqlite3.Connection' has no len()\n</code></pre>\n\n<p>How to convert sqlite database to pandas dataframe</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "time-series",
            "runtime"
        ],
        "owner": {
            "account_id": 21959147,
            "reputation": 11,
            "user_id": 16237276,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJwe-LsiKTZaNoISvj8ZGlWSHtj9d6Swzdd0ZT7T=k-s128",
            "display_name": "Bag",
            "link": "https://stackoverflow.com/users/16237276/bag"
        },
        "is_answered": true,
        "view_count": 65,
        "accepted_answer_id": 68182904,
        "answer_count": 2,
        "score": -4,
        "last_activity_date": 1625088489,
        "creation_date": 1624970525,
        "last_edit_date": 1625088489,
        "question_id": 68178902,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68178902/loop-through-time-series-data-and-collect-specific-timeframe-window-keeping-the",
        "title": "Loop through time series data and collect specific timeframe window keeping the runtime O(N)",
        "answer_body": "<p>Does this do what you want:</p>\n<pre><code>fillData.set_index('time', drop=True, inplace=True)\ncondition = fillData.fill.eq(1)\nfillData['500 milli'] = (condition.rolling(pd.Timedelta('500ms'))\n                                  .agg(any)\n                                  .astype(int))\nfillData['6 minutes'] = (condition.rolling(pd.Timedelta('6m'))\n                                  .agg(any)\n                                  .astype(int))\nfillData['6 minutes'][fillData['500 milli'].eq(1)] = 0\nfillData.reset_index(drop=False, inplace=True)\n</code></pre>\n<p>I'm not sure how <code>fillData</code> is sorted. My assumption is that the sorting is ascending (in time). Otherwise you have to reverse it.</p>\n",
        "question_body": "<p>I am trying to loop through a time series data frame and for a specific time, I need to then go back 5 minutes and 10 minutes (need to make sure I also DO NOT over count the data because of multicollinearity) and check if a condition is met. Below is the code that I wrote, I would love for it to be in O(N) and not have to make two loops. I was thinking of saving the index somehow to save space but need help here.</p>\n<p>Thanks in advance</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "account_id": 6134038,
            "reputation": 143,
            "user_id": 4783448,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/807685225986833/picture?type=large",
            "display_name": "Stuart Smith",
            "link": "https://stackoverflow.com/users/4783448/stuart-smith"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 68201541,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625086968,
        "creation_date": 1625073643,
        "question_id": 68199111,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68199111/is-there-a-more-efficient-way-to-retrieve-rows-with-a-list-column-which-includes",
        "title": "Is there a more efficient way to retrieve rows with a list column which includes values in a list? ( either a subset, union, or superset )",
        "answer_body": "<p>I used a more lightweight dataframe:</p>\n<pre><code>&gt;&gt;&gt; df\n          id           grades\n0    smithsm     [1, 9, 2, 6]  # &lt;- 9\n1   mullenjb  [1, 5, 8, 4, 7]\n2    swainrl        [4, 2, 9]  # &lt;- 9\n3  rankinsns           [5, 2]\n4  carlsonrm  [7, 4, 6, 3, 2]  # &lt;- 3\n5     ragomv        [6, 1, 5]\n6    smithdl  [2, 9, 6, 7, 3]  # &lt;- 3 &amp; 9\n7  kappleraj        [9, 5, 8]  # &lt;- 9\n8   iresonss  [8, 6, 7, 5, 4]\n9  conklincc           [8, 6]\n</code></pre>\n<p>How to find grade list [3, 9]?</p>\n<p>Explode your column <code>grades</code> and find the grade is in the grade list.</p>\n<pre><code>&gt;&gt;&gt; df.loc[df['grades'].explode().isin([3, 9]).groupby(level=0).any()\n          id           grades\n0    smithsm     [1, 9, 2, 6]\n2    swainrl        [4, 2, 9]\n4  carlsonrm  [7, 4, 6, 3, 2]\n6    smithdl  [2, 9, 6, 7, 3]\n7  kappleraj        [9, 5, 8]\n</code></pre>\n<p>Same as:</p>\n<pre>\n>>> df.loc[df['grades'].explode() \\\n      <b>.apply(lambda x: x in [3, 9])</b> \\\n      .groupby(level=0).any()]`\n</pre>\n",
        "question_body": "<p>working with a pandas.dataframe, such that:</p>\n<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 685 entries, 7789285 to 8009947\nData columns (total 18 columns):\n #   Column            Non-Null Count  Dtype              \n---  ------            --------------  -----              \n 0   sourcedId         685 non-null    string             \n 1   status            685 non-null    string             \n 2   dateLastModified  685 non-null    datetime64[ns, UTC]\n 3   username          685 non-null    string             \n 4   userIds           685 non-null    object             \n 5   enabledUser       685 non-null    string             \n 6   givenName         685 non-null    string             \n 7   familyName        685 non-null    string             \n 8   middleName        685 non-null    string             \n 9   role              685 non-null    string             \n 10  identifier        685 non-null    string             \n 11  email             685 non-null    string             \n 12  sms               685 non-null    string             \n 13  phone             685 non-null    string             \n 14  agents            685 non-null    object             \n 15  orgs              685 non-null    object             \n 16  grades            685 non-null    object             \n 17  password          685 non-null    string             \ndtypes: datetime64[ns, UTC](1), object(4), string(13)\nmemory usage: 101.7+ KB\ndf.head()\n</code></pre>\n<p>The 'grades' column contains a list of integers as strings, i.e., ['9','10'].  I am able to filter for single values by</p>\n<pre><code>mask = df.grades.apply(lambda x: '10' in x)\n</code></pre>\n<p>In my test dataset, which is created from a list of list that I populate manually, I used integer values so below works just fine(?) <em>( assume for the sake of argument that the data is a list of integers rather than a list of strings )</em></p>\n<pre><code>gradeList = [9,10]\nmask = df.grades.apply(lambda x: any(map(lambda x,y: x==y,x gradeList)))\ndf[mask].head()\n</code></pre>\n<p>I'm relatively new to Python ( over the last five years I've accumulated what I would consider to be about six-to-eight months worth of Python experience, if that ) and completely new to Pandas.  I only have a tenative grasp of list comprehension and the map function.</p>\n<p>I had intended the above to allow me to retrieve any records for which a subset of <strong>gradesList</strong> was present in the <strong>grades</strong> column.  For a single integer in <strong>grade</strong>, this is accomplished via:</p>\n<pre><code>mask = df.grades.apply(lambda x: grade in x)\n</code></pre>\n<p>Instead of accomplishing my goal with the above nested lambdas and map, I've created something in which the order of terms in the query parameters ( <strong>gradesList</strong> ) is significant.  Below is the output from my testing script, which operates on test data included in the output. I'm trying not to assume an order for either list...</p>\n<pre><code>--------------------------------------------------------------------------------\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10 entries, 0 to 9\nData columns (total 5 columns):\n #   Column     Non-Null Count  Dtype\n---  ------     --------------  -----\n 0   id         10 non-null     object\n 1   email      10 non-null     object\n 2   fullName   10 non-null     object\n 3   jobTitles  10 non-null     object\n 4   grades     10 non-null     object\ndtypes: object(5)\nmemory usage: 528.0+ bytes\n--------------------------------------------------------------------------------\n          id                 email          fullName                                          jobTitles           grades\n0    smithsm    smithsm@aplace.com         Stu Smith  [developer, licensed pretend nurse, worthless ...  [9, 10, 11, 12]\n1   mullenjb   mullenjb@aplace.com      Jason Mullen               [printer guy, supervisor, senior it]         [11, 12]\n2    swainrl    swainrl@aplace.com        Ryan Swain                      [nap taker, goof-off, goober]          [9, 10]\n3  rankinsns  rankinsns@aplace.com  Nicholas Rankins                           [manual tesla autopilot]          [9, 10]\n4  carlsonrm  carlsonrm@aplace.com      Ryan Carlson                     [technician, snarky so-and-so]         [10, 11]\n5     ragomv     ragomv@aplace.com         Mike Rago                                  [nice guy, swole]             [10]\n6    smithdl    smithdl@aplace.com       David Smith                                         [old hand]              [9]\n7  kappleraj  kappleraj@aplace.com   Allison Kappler      [girl coder, definitely not prettier than me]             [11]\n8   iresonss   iresonss@aplace.com      Sandy Ireson                                      [hard worker]             [12]\n9  conklincc  conklincc@aplace.com     Caleb Conklin                              [millenial magnum pi]          [12, 9]\n--------------------------------------------------------------------------------\nquery for 'developer'\n        id               email   fullName                                          jobTitles           grades\n0  smithsm  smithsm@aplace.com  Stu Smith  [developer, licensed pretend nurse, worthless ...  [9, 10, 11, 12]\n--------------------------------------------------------------------------------\nquery for 11\n          id                 email         fullName                                          jobTitles           grades\n0    smithsm    smithsm@aplace.com        Stu Smith  [developer, licensed pretend nurse, worthless ...  [9, 10, 11, 12]\n1   mullenjb   mullenjb@aplace.com     Jason Mullen               [printer guy, supervisor, senior it]         [11, 12]\n4  carlsonrm  carlsonrm@aplace.com     Ryan Carlson                     [technician, snarky so-and-so]         [10, 11]\n7  kappleraj  kappleraj@aplace.com  Allison Kappler      [girl coder, definitely not prettier than me]             [11]\n--------------------------------------------------------------------------------\nquery for 10\n          id                 email          fullName                                          jobTitles           grades\n0    smithsm    smithsm@aplace.com         Stu Smith  [developer, licensed pretend nurse, worthless ...  [9, 10, 11, 12]\n2    swainrl    swainrl@aplace.com        Ryan Swain                      [nap taker, goof-off, goober]          [9, 10]\n3  rankinsns  rankinsns@aplace.com  Nicholas Rankins                                       [technician]          [9, 10]\n4  carlsonrm  carlsonrm@aplace.com      Ryan Carlson                     [technician, snarky so-and-so]         [10, 11]\n5     ragomv     ragomv@aplace.com         Mike Rago                                  [nice guy, swole]             [10]\n--------------------------------------------------------------------------------\nquery for 11,12\n          id                 email         fullName                                      jobTitles    grades\n1   mullenjb   mullenjb@aplace.com     Jason Mullen           [printer guy, supervisor, senior it]  [11, 12]\n7  kappleraj  kappleraj@aplace.com  Allison Kappler  [girl coder, definitely not prettier than me]      [11]\n--------------------------------------------------------------------------------\nquery for 10,11\n          id                 email      fullName                       jobTitles    grades\n4  carlsonrm  carlsonrm@aplace.com  Ryan Carlson  [technician, snarky so-and-so]  [10, 11]\n5     ragomv     ragomv@aplace.com     Mike Rago               [nice guy, swole]      [10]\n--------------------------------------------------------------------------------\nquery for 9,10\n          id                 email          fullName                                          jobTitles           grades\n0    smithsm    smithsm@aplace.com         Stu Smith  [developer, licensed pretend nurse, worthless ...  [9, 10, 11, 12]\n2    swainrl    swainrl@aplace.com        Ryan Swain                      [nap taker, goof-off, goober]          [9, 10]\n3  rankinsns  rankinsns@aplace.com  Nicholas Rankins                                       [technician]          [9, 10]\n6    smithdl    smithdl@aplace.com       David Smith                                         [old hand]              [9]\n--------------------------------------------------------------------------------\nquery for 10,9\n          id                 email       fullName                       jobTitles    grades\n4  carlsonrm  carlsonrm@aplace.com   Ryan Carlson  [technician, snarky so-and-so]  [10, 11]\n5     ragomv     ragomv@aplace.com      Mike Rago               [nice guy, swole]      [10]\n9  conklincc  conklincc@aplace.com  Caleb Conklin           [millenial magnum pi]   [12, 9]\n</code></pre>\n<p>Is anyone able to identify ( hopefully the core concept I'm missing ) or point me towards documentation that will help me unravel what's taking place?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22095204,
            "reputation": 13,
            "user_id": 16352621,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/7f74a658884df8ba42ce06afd41dabbc?s=128&d=identicon&r=PG&f=1",
            "display_name": "uma",
            "link": "https://stackoverflow.com/users/16352621/uma"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 68201283,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625085048,
        "creation_date": 1625081520,
        "question_id": 68200658,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68200658/filtering-pandas-dataset-based-on-multiple-conditions",
        "title": "Filtering Pandas dataset based on multiple conditions",
        "answer_body": "<p>With the clarified requirements in the comments, here is a working solution, that can be run and returns the expected result.</p>\n<p>It should be very efficient as well.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport io\n\ndf_txt = &quot;&quot;&quot;\nId  Calls   Distance  AirportSize\n0   HVN19   3.727263    2\n1   HVN19   3.727263    1\n2   HVN19   11.485452   2\n3   CCA839  2.094717    2\n4   CCA839  2.094717    1\n5   CCA839  6.622537    2\n6   CES219  1.751279    1\n7   CES219  5.436940    4\n8   CES219  6.950773    4\n9   ETH704  2.976954    4\n10  ETH704  3.844980    4\n11  ETH704  5.452634    4\n&quot;&quot;&quot;\n    \ndf1 = pd.read_fwf(io.StringIO(df_txt)).set_index('Id').rename(columns = {'Calls':'Callsign'})\n\n\n# Need to sort or to ensure following will always work\ndf1.sort_values(['Callsign','Distance'])\n# calc distance between each subsequent ap - fillna as 0 for closest ap\ndf1['dist_diff'] = df1.groupby('Callsign')['Distance'].diff().fillna(0)\ndf1['dist_diff'] = df1.groupby('Callsign')['dist_diff'].cumsum()\n\n# Keep only smallest qualifiying airport for each callsign, tiebreak with Distance\ndf1[df1['dist_diff']&lt;1].sort_values(['Callsign','AirportSize','Distance']).drop_duplicates('Callsign').drop('dist_diff',axis=1)\n</code></pre>\n<p>output</p>\n<pre><code>    Callsign    Distance    AirportSize\nId          \n4   CCA839  2.094717    1\n6   CES219  1.751279    1\n9   ETH704  2.976954    4\n1   HVN19   3.727263    1\n</code></pre>\n",
        "question_body": "<p>I'm using this dataframe:</p>\n<pre><code>  Callsign  Distance    AirportSize\n0   HVN19   3.727263    2\n1   HVN19   3.727263    1\n2   HVN19   11.485452   2\n3   CCA839  2.094717    2\n4   CCA839  2.094717    1\n5   CCA839  6.622537    2\n6   CES219  1.751279    1\n7   CES219  5.436940    4\n8   CES219  6.950773    4\n9   ETH704  2.976954    4\n10  ETH704  3.844980    4\n11  ETH704  5.452634    4\n</code></pre>\n<p>I'm trying to connect <strong>Callsign</strong> with the smallest value of <strong>AirportSize</strong> only if <strong>Distance</strong> between airports for the same <strong>Callsign</strong> is less than 1. Besides, I would like to keep only these lines and drop other rows within the same <strong>Callsign</strong>.</p>\n<p>I will give an example of the first three lines within the same <strong>Callsign</strong> (<em>HVN19</em>) to be more clear: we can instantly drop the 3rd row because of <strong>Distance</strong> difference between the third row and first two lines (11.485452 - 3.727263 &gt; 1). When it comes to the remaining two rows, we choose the second row because <strong>AirportSize</strong> is smaller compared to the first row (1 &lt; 2).</p>\n<p>The result should look like this:</p>\n<pre><code>  Callsign  Distance    AirportSize\n\n1   HVN19   3.727263    1\n2   CCA839  2.094717    1\n3   CES219  1.751279    1\n4   ETH704  2.976954    4\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "linear-interpolation"
        ],
        "owner": {
            "account_id": 21077069,
            "reputation": 361,
            "user_id": 15492238,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5bb9116f49e2fd92ba919ad7f1fd1141?s=128&d=identicon&r=PG&f=1",
            "display_name": "tj judge ",
            "link": "https://stackoverflow.com/users/15492238/tj-judge"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 67986413,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625084948,
        "creation_date": 1623759382,
        "last_edit_date": 1625084948,
        "question_id": 67986112,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67986112/linear-interpolation-to-find-y-values",
        "title": "Linear interpolation to find y values",
        "answer_body": "<p>The format of <code>df</code> seems weird (data points in columns, not rows).</p>\n<p>Below is not the cleanest solution at all:</p>\n<pre><code>import numpy as np\n\nlookup_df = df1.set_index([&quot;Name&quot;, &quot;Segment&quot;, &quot;Axis&quot;]).T\n\ndef find_interp(row):\n    try:\n        res = np.interp([row[&quot;x&quot;]], lookup_df[(row[&quot;Name&quot;], row[&quot;Segment&quot;], &quot;x&quot;)], lookup_df[(row[&quot;Name&quot;], row[&quot;Segment&quot;], &quot;y&quot;)])\n    except:\n        res = [np.nan]\n    return res[0]\n\n\n&gt;&gt;&gt; df2[&quot;y&quot;] = df2.apply(find_interp, axis=1)\n&gt;&gt;&gt; df2\n      Name  Segment    x     y\n0   Amazon        1  1.0  0.40\n1   Amazon        2  2.3  1.15\n2  Netflix        1  4.1   NaN\n3  Netflix        2  5.5   NaN\n</code></pre>\n",
        "question_body": "<p>I have a dataframe:</p>\n<pre><code>1  Amazon        1      x  0.0     1.0     2.0    3.0    4.0\n2  Amazon        1      y  0.0     0.4     0.8    1.2    1.6\n4  Amazon        2      x  0.0     2.0     4.0    6.0    8.0\n5  Amazon        2      y  0.0     1.0     2.0    3.0    4.0\n</code></pre>\n<p>df2:</p>\n<pre><code> Amazon   1       1\n Amazon   2       2.3\n Netflix  1       4.1\n Netflix  2       5.5\n</code></pre>\n<p>Given these two dataframes, I am trying to use linear interpolation to find the 'y values' for df2, using df1 breakpoints</p>\n<p>Expected output:</p>\n<pre><code>   Amazon   1       1    ...\n   Amazon   2       2.3  ...\n</code></pre>\n<p>The formula for Linear Interpolation is: y = y1 + ((x \u2013 x1) / (x2 \u2013 x1)) * (y2 \u2013 y1), where x is the known value, y is the unknown value, x1 and y1 are the coordinates that are below the known x value, and x2 and y2 are the coordinates that are above the x value.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "plotly",
            "bar-chart"
        ],
        "owner": {
            "account_id": 21021892,
            "reputation": 265,
            "user_id": 15448022,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-MzW4a3wzzuA/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclYe8-QKx-IsdFXk04ohEJX8Phq_g/s96-c/photo.jpg?sz=128",
            "display_name": "aj7amigo",
            "link": "https://stackoverflow.com/users/15448022/aj7amigo"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 68201123,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625084114,
        "creation_date": 1625077290,
        "question_id": 68199861,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68199861/resampling-pandas-dataframe-by-hour-and-plotting-a-stacked-bar-chart-using-plotl",
        "title": "Resampling Pandas DataFrame by hour and plotting a stacked bar chart using Plotly",
        "answer_body": "<p>You can do groupby with <code>pd.Grouper(key='ts', freq='1h')</code> to 'resample' the dataframe by hour. <code>size</code> will get you a frequency count of the MAC addresses:</p>\n<pre><code>import pandas as pd\nimport plotly.express as px\n\ndata = {'MAC Address': {1: 'af3d116c', 2: 'bffe479a', 3: 'c3a8fe37', 4: 'af3d116c', 5: 'bffe479a', 6: 'c3a8fe37', 7: 'af3d116c', 8: 'af3d116c', 9: 'bffe479a', 10: 'c3a8fe37', 11: 'af3d116c', 12: 'bffe479a', 13: 'bffe479a', 14: 'bffe479a', 15: 'c3a8fe37'}, 'ts': {1: '2021-05-05 21:58:45', 2: '2021-05-05 21:58:48', 3: '2021-05-05 21:58:52', 4: '2021-05-05 21:58:58', 5: '2021-05-05 21:59:16', 6: '2021-05-05 21:59:50', 7: '2021-05-05 22:08:32', 8: '2021-05-05 22:16:30', 9: '2021-05-05 22:31:37', 10: '2021-05-05 22:52:49', 11: '2021-05-05 23:22:02', 12: '2021-05-05 23:44:31', 13: '2021-05-05 23:45:12', 14: '2021-05-05 23:49:28', 15: '2021-05-05 23:52:47'}, 'Parameter1': {1: 20, 2: 22, 3: 21, 4: 27, 5: 23, 6: 28, 7: 30, 8: 27, 9: 20, 10: 32, 11: 41, 12: 37, 13: 29, 14: 34, 15: 47}, 'Parameter2': {1: 50, 2: 52, 3: 53, 4: 50, 5: 51, 6: 52, 7: 49, 8: 55, 9: 53, 10: 52, 11: 58, 12: 62, 13: 58, 14: 41, 15: 56}}\ndf = pd.DataFrame(data)\ndf['ts'] = pd.to_datetime(df['ts'])\n\nplot_df = df.groupby([pd.Grouper(key='ts', freq='1h'), 'MAC Address']).size().reset_index().rename(columns={0: &quot;count&quot;})\n</code></pre>\n<p>This will result in:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: right;\"></th>\n<th style=\"text-align: left;\">ts</th>\n<th style=\"text-align: left;\">MAC Address</th>\n<th style=\"text-align: right;\">count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: left;\">2021-05-05 21:00:00</td>\n<td style=\"text-align: left;\">af3d116c</td>\n<td style=\"text-align: right;\">2</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">1</td>\n<td style=\"text-align: left;\">2021-05-05 21:00:00</td>\n<td style=\"text-align: left;\">bffe479a</td>\n<td style=\"text-align: right;\">2</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">2</td>\n<td style=\"text-align: left;\">2021-05-05 21:00:00</td>\n<td style=\"text-align: left;\">c3a8fe37</td>\n<td style=\"text-align: right;\">2</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">3</td>\n<td style=\"text-align: left;\">2021-05-05 22:00:00</td>\n<td style=\"text-align: left;\">af3d116c</td>\n<td style=\"text-align: right;\">2</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">4</td>\n<td style=\"text-align: left;\">2021-05-05 22:00:00</td>\n<td style=\"text-align: left;\">bffe479a</td>\n<td style=\"text-align: right;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">5</td>\n<td style=\"text-align: left;\">2021-05-05 22:00:00</td>\n<td style=\"text-align: left;\">c3a8fe37</td>\n<td style=\"text-align: right;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">6</td>\n<td style=\"text-align: left;\">2021-05-05 23:00:00</td>\n<td style=\"text-align: left;\">af3d116c</td>\n<td style=\"text-align: right;\">1</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">7</td>\n<td style=\"text-align: left;\">2021-05-05 23:00:00</td>\n<td style=\"text-align: left;\">bffe479a</td>\n<td style=\"text-align: right;\">3</td>\n</tr>\n<tr>\n<td style=\"text-align: right;\">8</td>\n<td style=\"text-align: left;\">2021-05-05 23:00:00</td>\n<td style=\"text-align: left;\">c3a8fe37</td>\n<td style=\"text-align: right;\">1</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>You can then plot this as you wish. Eg:</p>\n<pre><code>fig = px.bar(plot_df, x=&quot;ts&quot;, y=&quot;count&quot;, color=&quot;MAC Address&quot;, title=&quot;MAC Addresses per hour&quot;)\nfig.show()\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/oiNoN.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/oiNoN.png\" alt=\"enter image description here\" /></a></p>\n",
        "question_body": "<p>I have a pandas dataframe as below</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">MAC Address</th>\n<th style=\"text-align: center;\">ts</th>\n<th style=\"text-align: center;\">Parameter1</th>\n<th style=\"text-align: center;\">Parameter2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">af3d116c</td>\n<td style=\"text-align: center;\">2021-05-05 21:58:45</td>\n<td style=\"text-align: center;\">20</td>\n<td style=\"text-align: center;\">50</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">bffe479a</td>\n<td style=\"text-align: center;\">2021-05-05 21:58:48</td>\n<td style=\"text-align: center;\">22</td>\n<td style=\"text-align: center;\">52</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">c3a8fe37</td>\n<td style=\"text-align: center;\">2021-05-05 21:58:52</td>\n<td style=\"text-align: center;\">21</td>\n<td style=\"text-align: center;\">53</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">af3d116c</td>\n<td style=\"text-align: center;\">2021-05-05 21:58:58</td>\n<td style=\"text-align: center;\">27</td>\n<td style=\"text-align: center;\">50</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">bffe479a</td>\n<td style=\"text-align: center;\">2021-05-05 21:59:16</td>\n<td style=\"text-align: center;\">23</td>\n<td style=\"text-align: center;\">51</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">c3a8fe37</td>\n<td style=\"text-align: center;\">2021-05-05 21:59:50</td>\n<td style=\"text-align: center;\">28</td>\n<td style=\"text-align: center;\">52</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">af3d116c</td>\n<td style=\"text-align: center;\">2021-05-05 22:08:32</td>\n<td style=\"text-align: center;\">30</td>\n<td style=\"text-align: center;\">49</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">af3d116c</td>\n<td style=\"text-align: center;\">2021-05-05 22:16:30</td>\n<td style=\"text-align: center;\">27</td>\n<td style=\"text-align: center;\">55</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">bffe479a</td>\n<td style=\"text-align: center;\">2021-05-05 22:31:37</td>\n<td style=\"text-align: center;\">20</td>\n<td style=\"text-align: center;\">53</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">c3a8fe37</td>\n<td style=\"text-align: center;\">2021-05-05 22:52:49</td>\n<td style=\"text-align: center;\">32</td>\n<td style=\"text-align: center;\">52</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">af3d116c</td>\n<td style=\"text-align: center;\">2021-05-05 23:22:02</td>\n<td style=\"text-align: center;\">41</td>\n<td style=\"text-align: center;\">58</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">bffe479a</td>\n<td style=\"text-align: center;\">2021-05-05 23:44:31</td>\n<td style=\"text-align: center;\">37</td>\n<td style=\"text-align: center;\">62</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">bffe479a</td>\n<td style=\"text-align: center;\">2021-05-05 23:45:12</td>\n<td style=\"text-align: center;\">29</td>\n<td style=\"text-align: center;\">58</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">bffe479a</td>\n<td style=\"text-align: center;\">2021-05-05 23:49:28</td>\n<td style=\"text-align: center;\">34</td>\n<td style=\"text-align: center;\">41</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">c3a8fe37</td>\n<td style=\"text-align: center;\">2021-05-05 23:52:47</td>\n<td style=\"text-align: center;\">47</td>\n<td style=\"text-align: center;\">56</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I would like to resample the data frame and finally plot stacked bar charts (preferably using plotly) representing the total number of rows recorded per hour and color coded based on MAC Address.</p>\n<p>Below is a representation of how I want it to be visualized. (Sorry, it is not using the data listed above, but gives an indication on how I want it to be. Each bar represents an hour eg: 22:00 till 23:00 separated by color representing MAC addresses.)</p>\n<p><a href=\"https://i.stack.imgur.com/ZKuAK.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ZKuAK.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 11259035,
            "reputation": 13,
            "user_id": 8257885,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/828b62428b7a85b2db9957c9ff1c47dd?s=128&d=identicon&r=PG&f=1",
            "display_name": "don_alberto",
            "link": "https://stackoverflow.com/users/8257885/don-alberto"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68200709,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625081762,
        "creation_date": 1625081533,
        "question_id": 68200661,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68200661/i-want-to-generate-a-new-column-in-a-pandas-dataframe-counting-edges-in-anoth",
        "title": "I want to generate a new column in a pandas dataframe, counting &quot;edges&quot; in another column",
        "answer_body": "<p>Use <code>cumsum</code></p>\n<pre><code>df['Y'] = (df.X == 'B').cumsum()\n\nOut[8]:\n    A  B  X  Y\n0   1  1  A  0\n1   2  2  B  1\n2   3  3  A  1\n3   4  6  K  1\n4   5  7  B  2\n5   6  8  L  2\n6   7  9  M  2\n7   8  1  N  2\n8   9  7  B  3\n9   1  6  A  3\n10  7  7  A  3\n</code></pre>\n",
        "question_body": "<p>i have a dataframe looking like this:</p>\n<pre><code>A B....X\n1 1    A\n2 2    B\n3 3    A\n4 6    K\n5 7    B\n6 8    L\n7 9    M\n8 1    N\n9 7    B\n1 6    A\n7 7    A\n</code></pre>\n<p>that is, some &quot;rising edges&quot; occur from time to time in the column X (in this example the edge is x==B)\nWhat I need is, a new column Y which increments every time a value of B occurs in X:</p>\n<pre><code>A B....X  Y\n1 1    A  0\n2 2    B  1\n3 3    A  1\n4 6    K  1\n5 7    B  2\n6 8    L  2\n7 9    M  2\n8 1    N  2\n9 7    B  3\n1 6    A  3\n7 7    A  3\n</code></pre>\n<p>In SQL I would use some trick like <code>sum(case when x=B then 1 else 0) over ... rows between first and previous</code>. How can I do it in Pandas?</p>\n",
        "input_data_frames": [
            "A B....X\n1 1    A\n2 2    B\n3 3    A\n4 6    K\n5 7    B\n6 8    L\n7 9    M\n8 1    N\n9 7    B\n1 6    A\n7 7    A\n",
            "A B....X  Y\n1 1    A  0\n2 2    B  1\n3 3    A  1\n4 6    K  1\n5 7    B  2\n6 8    L  2\n7 9    M  2\n8 1    N  2\n9 7    B  3\n1 6    A  3\n7 7    A  3\n"
        ],
        "output_codes": [
            "df['Y'] = (df.X == 'B').cumsum()\n\nOut[8]:\n    A  B  X  Y\n0   1  1  A  0\n1   2  2  B  1\n2   3  3  A  1\n3   4  6  K  1\n4   5  7  B  2\n5   6  8  L  2\n6   7  9  M  2\n7   8  1  N  2\n8   9  7  B  3\n9   1  6  A  3\n10  7  7  A  3\n"
        ],
        "ques_desc": "i have a dataframe looking like this: that is, some \"rising edges\" occur from time to time in the column X (in this example the edge is x==B) What I need is, a new column Y which increments every time a value of B occurs in X: In SQL I would use some trick like . How can I do it in Pandas? ",
        "ans_desc": "Use ",
        "formatted_input": {
            "qid": 68200661,
            "link": "https://stackoverflow.com/questions/68200661/i-want-to-generate-a-new-column-in-a-pandas-dataframe-counting-edges-in-anoth",
            "question": {
                "title": "I want to generate a new column in a pandas dataframe, counting &quot;edges&quot; in another column",
                "ques_desc": "i have a dataframe looking like this: that is, some \"rising edges\" occur from time to time in the column X (in this example the edge is x==B) What I need is, a new column Y which increments every time a value of B occurs in X: In SQL I would use some trick like . How can I do it in Pandas? "
            },
            "io": [
                "A B....X\n1 1    A\n2 2    B\n3 3    A\n4 6    K\n5 7    B\n6 8    L\n7 9    M\n8 1    N\n9 7    B\n1 6    A\n7 7    A\n",
                "A B....X  Y\n1 1    A  0\n2 2    B  1\n3 3    A  1\n4 6    K  1\n5 7    B  2\n6 8    L  2\n7 9    M  2\n8 1    N  2\n9 7    B  3\n1 6    A  3\n7 7    A  3\n"
            ],
            "answer": {
                "ans_desc": "Use ",
                "code": [
                    "df['Y'] = (df.X == 'B').cumsum()\n\nOut[8]:\n    A  B  X  Y\n0   1  1  A  0\n1   2  2  B  1\n2   3  3  A  1\n3   4  6  K  1\n4   5  7  B  2\n5   6  8  L  2\n6   7  9  M  2\n7   8  1  N  2\n8   9  7  B  3\n9   1  6  A  3\n10  7  7  A  3\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dictionary",
            "dataframe"
        ],
        "owner": {
            "account_id": 13387367,
            "reputation": 73,
            "user_id": 9660800,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "jh10",
            "link": "https://stackoverflow.com/users/9660800/jh10"
        },
        "is_answered": true,
        "view_count": 5904,
        "accepted_answer_id": 50632276,
        "answer_count": 2,
        "score": 7,
        "last_activity_date": 1625081187,
        "creation_date": 1527796344,
        "last_edit_date": 1625081187,
        "question_id": 50631799,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50631799/mapping-python-dictionary-with-multiple-keys-into-dataframe-with-multiple-column",
        "title": "Mapping Python dictionary with multiple keys into dataframe with multiple columns matching keys",
        "answer_body": "<p>You can create a <code>MultiIndex</code> from two series and then map. Data from @ALollz.</p>\n\n<pre><code>df['CountyType'] = df.set_index(['County', 'State']).index.map(dct.get)\n\nprint(df)\n\n  County  State CountyType\n0      A      1        One\n1      A      2       None\n2      B      1       None\n3      B      2        Two\n4      B      3      Three\n</code></pre>\n",
        "question_body": "<p>I have a dictionary that I would like to map onto a current dataframe and create a new column. I have keys in a tuple, which map onto two different columns in my dataframe.</p>\n<pre><code>dct = {('County', 'State'):'CountyType'}\ndf = pd.DataFrame(data=['County','State'])\n</code></pre>\n<p>I would like to create a new column, <code>CountyType</code>, using <code>dict</code> to map onto the two columns in <code>df</code>. However, doing the following gives me an error. How else could this be done?</p>\n<pre><code>df['CountyType'] = (list(zip(df.County,df.State)))\ndf = df.replace({'CountyType': county_type_dict)\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "series"
        ],
        "owner": {
            "account_id": 2634826,
            "reputation": 976,
            "user_id": 2279829,
            "user_type": "registered",
            "accept_rate": 22,
            "profile_image": "https://www.gravatar.com/avatar/07083c635fd3ea596331cb2f4f60c635?s=128&d=identicon&r=PG",
            "display_name": "JC23",
            "link": "https://stackoverflow.com/users/2279829/jc23"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 68200411,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625080658,
        "creation_date": 1625079764,
        "question_id": 68200351,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68200351/fastest-way-to-iterate-pandas-series-column",
        "title": "fastest way to iterate pandas series/column",
        "answer_body": "<p>If you finally want to add the newnames to <code>df</code>, you could do it directly by:</p>\n<pre><code>df['newnames'] = df['name'].str.replace(' ', '_')\n</code></pre>\n<p>If you just want to change <code>name</code> column to replace all spaces by <code>_</code>, you can also do it directly on the original column (overwrite it), as follows:</p>\n<pre><code>df['name'] = df['name'].str.replace(' ', '_')\n</code></pre>\n<p>In both ways, we are doing it using Pandas' vectorized operation which has been optimized for faster execution, instead of using looping which has not been optimized and is slow.</p>\n",
        "question_body": "<p>I'm more used to for loops but they can become slow in pandas once you get large sets of data. I keep finding iterrows, iter..., etc. examples but want to know if there's a faster way. What I currently have now is</p>\n<pre><code>newnames = []\nnames = df['name'].tolist()\nfor i in names:\n  i = i.replace(' ','_')\n  newnames.append(i)\n</code></pre>\n<p>and then I could add the newnames list to the df as a pandas column OR should I rewrite the existing df['name'] values in place? Not too familiar with pandas best practices so I welcome all feedback. Thanks</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21893135,
            "reputation": 19,
            "user_id": 16179815,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxI6y7X2uOXD8offMFw3wAtGN2IRnEM3cVPb3Cg=k-s128",
            "display_name": "Eng_GR",
            "link": "https://stackoverflow.com/users/16179815/eng-gr"
        },
        "is_answered": true,
        "view_count": 88,
        "accepted_answer_id": 68173533,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1625080483,
        "creation_date": 1624939381,
        "last_edit_date": 1624948840,
        "question_id": 68172477,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68172477/how-to-deal-with-pandas-dataframe-column-with-list-containing-string-values-get",
        "title": "How to deal with Pandas dataframe column with list containing string values, get unique words",
        "answer_body": "<p>First we deal with the annoyance that your 'dimensions' column is sometimes None, sometimes a list of one string element. So extract that element when it's non-null:</p>\n<pre><code>df['dimensions2'] = df['dimensions'].apply(lambda col: col[0] if col else None)\n</code></pre>\n<p>Next, get all alphabetic strings in each row, excluding measurements:</p>\n<pre><code>&gt;&gt;&gt; df['dimensions2'].str.findall(r'\\b([a-zA-Z]+)')\n0                 [long]\n1                   None\n2    [long, wide, thick]\n3     [high, long, wide]\n</code></pre>\n<p>Note we use <code>\\b</code> word-boundary (to exclude the 'ft' from '30ft'), and to avoid misinterpreting <code>\\b</code> as backslash we have to use r'' rawstring on the regex.</p>\n<p>This gives you a list. You wanted a set, to prevent duplicates occurring, so:</p>\n<pre><code> df['dimensions2'].str.findall(r'\\b([a-zA-Z]+)').apply(lambda l: set(l) if l else None)\n0                 {long}\n1                   None\n2    {thick, long, wide}\n3     {high, long, wide}\n</code></pre>\n",
        "question_body": "<p>I am trying to do some basic operations on a dataframe column (called <code>dimensions</code>) that contains a list. Do basic operations like <code>df['dimensions'].str.replace()</code> work when the dataframe column contains a list? It did not work for me. I also tried to replace the text in the column using <code>re.sub()</code> method and it did not work either.</p>\n<p>This is the last column in my dataframe:</p>\n<pre><code>**dimensions**\n\n[50' long]    \nNone    \n[70ft long, 19ft wide, 8ft thick]    \n[5' high, 30' long, 18' wide]\n</code></pre>\n<p><strong>This is what I have tried, but it did not work:</strong></p>\n<pre><code>def dimension_unique_words(dimensions):\nif dimensions != 'None':\n    for value in dimensions:\n        new_value = re.sub(r'[^\\w\\s]|ft|feet', ' ', value)\n        new_value = ''.join([i for i in new_value if not i.isdigit()])\n        return new_value\n\ndf['new_col'] = df['dimensions'].apply(dimension_unique_words)\n</code></pre>\n<p><strong>this is the output I got from my code:</strong></p>\n<pre><code>**new_col**\n\nNaN    \nNone    \nNaN    \nNone    \nNaN    \nNone\n</code></pre>\n<p>What I want to do is to replace the numbers and the units [ft, feet, ']  in the column called <code>dimensions</code> with a space and then apply the <code>df.unique()</code> on that column to get the unique values which are [long, wide, thick, high].</p>\n<p><em>The expected output would be:</em></p>\n<pre><code>**new_col**\n\n[long]    \nNone   \n[long, wide, thick]    \n[high, long, wide]\n</code></pre>\n<p><strong>...then I want to apply the <code>df.unique()</code> on the <code>new_col</code> to get [long, wide, thick, high]</strong></p>\n<p>How to do that?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20807836,
            "reputation": 13,
            "user_id": 15282410,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-ZdOqo7JynfU/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucnudAkGTI-A42ihZX1cf-6wjmAhvg/s96-c/photo.jpg?sz=128",
            "display_name": "Alexey Podoynitsyn",
            "link": "https://stackoverflow.com/users/15282410/alexey-podoynitsyn"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68199604,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625076067,
        "creation_date": 1625074714,
        "question_id": 68199329,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68199329/trying-to-shuffle-rows-in-panda-dataframe",
        "title": "Trying to shuffle rows in Panda DataFrame",
        "answer_body": "<p>Something like this where you just return the shuffled df, and use <code>pd.concat</code> on a list of these.</p>\n<pre><code>sales_to_do = pd.DataFrame({'id':[1,2], 'name':['bob','mike']})\n\ndef randomize(df):\n    return df.sample(frac=1)\n    \ndf_shuffled = pd.concat([randomize(sales_to_do) for x in range(15)])\n\ndf_shuffled.to_excel(r'C:\\Users\\Alex\\Desktop\\Output1.xlsx', index=False, header=True)\n</code></pre>\n",
        "question_body": "<p>Hopefully, someone can help, I'm trying to randomize the output 15 times, and save into excel, however, python is only giving me 1 output instead of 15</p>\n<pre><code>import pandas as pd\n\n# create a DataFrame\nsales_to_do = {'Task': ['Call with the client', 'Preparing for the calls', 'Training staff',\n                    'Daily tasks (Emails, questions, chasing)'],\n\n           'Type of task': ['Call (external - new lead)', 'Preparing communication with \nleads', 'Training',\n                            'Call (external - new lead)']}\n\ndf = pd.DataFrame(sales_to_do)\ndf_shuffled = df.sample(frac=1)\n\ndef randomize():\n    df = pd.DataFrame(sales_to_do)\n    df_shuffled = df.sample(frac=1)\n    print(df_shuffled)\n\n\nfor i in range(15):\n    randomize()\n\ndf_shuffled.to_excel(r'C:\\Users\\Alex\\Desktop\\Output1.xlsx', index=False, header=True)\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21734673,
            "reputation": 13,
            "user_id": 16042840,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e1df8d24da1e7c114a9b13f17b8bd628?s=128&d=identicon&r=PG&f=1",
            "display_name": "PolarVertex",
            "link": "https://stackoverflow.com/users/16042840/polarvertex"
        },
        "is_answered": true,
        "view_count": 36,
        "closed_date": 1625096816,
        "accepted_answer_id": 68198604,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625074072,
        "creation_date": 1625070399,
        "last_edit_date": 1625070471,
        "question_id": 68198478,
        "link": "https://stackoverflow.com/questions/68198478/convert-month-name-into-number-and-join-it-with-year-in-python-dataframe",
        "closed_reason": "Needs more focus",
        "title": "Convert month name into number and join it with year in python dataframe",
        "answer_body": "<p>You can use <code>pd.to_datetime</code> and concat like this</p>\n<pre><code>df['month_year'] = pd.to_datetime(df.Month, format='%B').dt.month.astype(str) +&quot;_&quot;+ df.Year\ndf['month_year']\n#output\n0    1_2019\n1    2_2021\n2    3_2021\nName: month_year, dtype: object\n</code></pre>\n<p><strong>Update from comment:</strong></p>\n<p>Not sure why you get float, but you can try to convert it explicitly to int by adding astype(int) before string</p>\n<pre><code>pd.to_datetime(df.Month , format='%B').dt.month.astype(int).astype(str) +&quot;_&quot;+ df.Year\n</code></pre>\n",
        "question_body": "<p>I have a DataFrame like this:</p>\n<pre><code>Month      Year\nJanuary    2019\nFebruary   2021\nMarch      2021\n</code></pre>\n<p>I want the output DataFrame like this:</p>\n<pre><code>Month      Year    month_year\nJanuary    2019    1_2019\nFebruary   2021    2_2021\nMarch      2021    3_2021\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 17592156,
            "reputation": 435,
            "user_id": 12763792,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/mIMZP.jpg?s=128&g=1",
            "display_name": "satoru",
            "link": "https://stackoverflow.com/users/12763792/satoru"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 68198374,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1625071846,
        "creation_date": 1625069523,
        "last_edit_date": 1625071484,
        "question_id": 68198298,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68198298/appending-to-new-df-if-items-in-rows-meet-conditions-between-columns",
        "title": "Appending to new df if items in rows meet conditions between columns",
        "answer_body": "<p>If I've understood correctly, try creating the conditions, then filtering <code>df</code> based on the condition:</p>\n<pre><code>c1 = df['color'].eq('green') &amp; df['correct'].eq('v')\nc2 = df['color'].eq('blue') &amp; df['correct'].eq('a')\nc3 = df['color'].eq('red') &amp; df['correct'].eq('r')\nm = c1 | c2 | c3\n</code></pre>\n<p>Then select values based on the full condition <code>m</code> using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html#pandas-dataframe-loc\" rel=\"nofollow noreferrer\"><code>loc</code></a>:</p>\n<pre><code>correct_df = df.loc[m]\nincorrect_df = df.loc[~m]\n</code></pre>\n<p>Optional Select specific columns as well:</p>\n<pre><code>correct_df = df.loc[m, ['word', 'rt']]\nincorrect_df = df.loc[~m, ['word', 'rt']]\n</code></pre>\n<hr />\n<p>Sample DataFrame:</p>\n<pre><code>df = pd.DataFrame({'color': ['green', 'blue', 'red'] * 2,\n                   'correct': ['v', 'r', 'v', 'a', 'a', 'r'],\n                   'word': list('abcdef'),\n                   'rt': range(1, 7)})\n</code></pre>\n<pre><code>   color correct word  rt\n0  green       v    a   1\n1   blue       r    b   2\n2    red       v    c   3\n3  green       a    d   4\n4   blue       a    e   5\n5    red       r    f   6\n</code></pre>\n<p><code>correct_df</code>:</p>\n<pre><code>correct_df = df.loc[m]\n\n   color correct word  rt\n0  green       v    a   1\n4   blue       a    e   5\n5    red       r    f   6\n\ncorrect_df = df.loc[m, ['word', 'rt']]\n\n  word  rt\n0    a   1\n4    e   5\n5    f   6\n</code></pre>\n<p><code>incorrect_df</code>:</p>\n<pre><code>incorrect_df = df.loc[~m]\n\n   color correct word  rt\n1   blue       r    b   2\n2    red       v    c   3\n3  green       a    d   4\n\nincorrect_df = df.loc[~m, ['word', 'rt']]\n\n  word  rt\n1    b   2\n2    c   3\n3    d   4\n</code></pre>\n",
        "question_body": "<p>I'm trying to organize a big dataset by &quot;correct&quot; and &quot;incorrect&quot; answers. The condition for correct answer is this:</p>\n<pre><code>df['color'] == 'green' and df['correct'] = 'v') or (df['color']=='blue' and df['correct']='a') or (df['color']=='red' and df['correct']=='r')\n</code></pre>\n<p>If the rows meet those conditions, I want to append them to a df_correct:</p>\n<pre><code>df_correct = pd.DataFrame(columns=['word','rt'])\n</code></pre>\n<p>If the conditions are not met, I need the rows to be appended to another dataframe.</p>\n<p>I thought of looping through the rows of the dataframe but this doesn't seem to be working:</p>\n<pre><code>df_correct = pd.DataFrame(columns=['word','rt'])\ndf_incorrect = pd.DataFrame(columns=['word','rt'])\n\nfor i in df:\n   if (df['color'] == 'green' and df['correct'] = 'v') or (df['color']=='blue' and df['correct']='a') or (df['color']=='red' and df['correct']=='r'):\n       df_correct.append(i)\n   else:\n       df_incorrect.append(i)\n</code></pre>\n<p>This is how part of the df looks:</p>\n<pre><code>,word,color,correct,rt\n\n923,ABUSO,red,r,993\n929,CHALECO, blue, a,1825\n935,ESTATUA, red, r,951\n941,ORQUESTRA, blue, a,1393\n1556,METAL, blue, a,1285\n1562,ABUSO,red,r,1272\n1568,CALLE, green, v,1210\n1574,CORTINA, blue, a,872\n1580,SILLA, blue, a,530\n1586,COBARDE, blue, a,510\n1592,TRISTEZA, green, v,786\n1598,P\u00c1RPADO, green, v,561\n1604,ACCIDENTE, blue, a,1329\n1853,VENTANA, red, r,1010\n1859,ABUSO,red,r,752\n1919,CANASTA, blue, a,628\n1925,TERRORISTA, green, v,589\n1931,BRAZO, red, r,970\n1937,TENEDOR, red, r,784\n1943,SILENCIO, red, r,614\n1949,TRISTEZA, green, v,564\n1955,POSTER, green, v,1314\n1961,MORGUE, green, v,703\n1967,LLUVIA, blue, a,898\n1973,VIOL\u00cdN, green, v,638\n1979,POBREZA, red, r,724\n1985,TRAICI\u00d3N, green, v,856\n1991,UTENSILLO, green, v,942\n1997,C\u00c1NCER, red, r,845\n2003,BANCO, green, v,927\n2009,DESASTRE, green, v,769\n2015,SILLA, blue, a,999\n2021,SOLEDAD, red, r,647\n2027,ESFERA, red, r,637\n2033,MISERIA, blue, a,823\n2039,ESTATUA, red, r,793\n2045,DEDO, red, r,566\n2051,VIOLACI\u00d3N, blue, a,840\n2057,HEBILLA, blue, a,818\n2063,ENOJO, red, r,731\n2069,DEMONIO, green, v,858\n2075,CABALLO, green, v,767\n2081,CORTINA, blue, a,763\n2087,ASIENTO, green, v,800\n2093,CRUEL, red, r,651\n2099,MIEDO, green, v,895\n2105,EDIFICIO, blue, a,726\n2111,PANFLETO, red, r,713\n2117,DEPRESI\u00d3N, red, r,598\n2123,MANTEL, green, v,578\n2129,PARTE, blue, a,580\n2135,C\u00c1RCEL, green, v,791\n2141,ORQUESTRA, blue, a,569\n2147,INFIERNO, red, r,519\n2153,MOMENTO, red, r,506\n2159,DIABLO, blue, a,680\n2165,CUCARACHA, green, v,627\n2171,BOMBA, red, r,483\n2177,GANGRENA, red, r,514\n2183,MUERTE, blue, a,895\n2189,AHOGADO, red, r,630\n2195,PARALISIS, blue, a,743\n2201,VENENO, green, v,646\n2207,CHALECO, blue, a,662\n2213,ODIO, green, v,792\n2219,FUNERAL, green, v,641\n2225,MONJA, green, v,728\n2231,L\u00c1PIZ, red, r,676\n2237,METAL, blue, a,590\n2243,PAPEL, green, v,614\n2249,CUADRADO, blue, a,795\n2255,ACCIDENTE, blue, a,665\n2261,CASA, red, r,662\n2267,DOLOR, blue, a,781\n2273,CHOQUE, green, v,596\n2366,HEBILLA, blue, a,1573\n2372,BANCO, green, v,732\n2378,VIOL\u00cdN, green, v,608\n2384,ACCIDENTE, blue, a,564\n2390,ASIENTO, green, v,687\n2396,SANGRIENTO, blue, a,820\n2402,GANGRENA, red, r,951\n2408,ENOJO, red, r,708\n2414,AHOGADO, red, r,786\n2420,TENEDOR, red, r,643\n2426,ESTATUA, red, r,427\n2432,DOLOR, blue, a,779\n2438,MENT\u00d3N, red, r,670\n2444,CHALECO, blue, a,450\n2450,NUBES, green, v,418\n2456,\u00cdTEM, red, r,627\n2462,DEPRESI\u00d3N, red, r,835\n2468,CORDERO, blue, a,770\n2474,INFIERNO, red, r,781\n2480,DIABLO, blue, a,542\n2486,SILENCIO, red, r,486\n2492,ORQUESTRA, blue, a,734\n2498,POSTER, green, v,902\n2504,PARALISIS, blue, a,1055\n2510,QUEMADURA, green, v,657\n2516,LLUVIA, blue, a,557\n2522,COBARDE, blue, a,485\n2528,CASA, red, r,550\n2534,TERRORISTA, green, v,483\n2540,ASALTO, blue, a,725\n2546,POBREZA, red, r,580\n2552,SILLA, blue, a,601\n2558,CORTINA, blue, a,467\n2564,FUNERAL, green, v,493\n2570,MANTECA, green, v,466\n2576,ABUSO,red,r,779\n2582,PARTE, blue, a,557\n2756,CANASTA, blue, a,477\n2762,BRAZO, red, r,567\n2768,ODIO, green, v,608\n2774,MIEDO, green, v,503\n2780,L\u00c1PIZ, red, r,561\n2786,EDIFICIO, blue, a,529\n2792,MATANZA, red, r,740\n2798,BOMBA, red, r,404\n2804,METAL, blue, a,601\n2810,P\u00c1RPADO, green, v,390\n2816,CABALLO, green, v,391\n2822,MOMENTO, red, r,1270\n2828,PIE, green, v,654\n2834,MANTEL, green, v,510\n2840,GRANJA, blue, a,477\n2846,TRAICI\u00d3N, green, v,544\n2852,CALLE, green, v,500\n2858,CHOQUE, green, v,510\n2864,PARAGUAS, blue, a,574\n2957,MISERIA, blue, a,1938\n2963,SOLEDAD, red, r,1426\n2969,LLUVIA, blue, a,1091\n2975,TENEDOR, red, r,1181\n2981,CORTINA, blue, a,1012\n2987,PESADILLA, red, r,865\n2993,PARALISIS, blue, a,793\n2999,ACCIDENTE, blue, a,776\n3005,DEDO, red, r,1669\n3011,MENT\u00d3N, red, r,665\n3017,TORTURA, blue, a,622\n3023,UTENSILLO, green, v,1353\n3029,CUADRADO, blue, a,1693\n3143,MANTECA, green, v,1028\n3149,P\u00c1RPADO, green, v,1257\n3155,CAD\u00c1VER, red, r,1192\n3161,PANFLETO, red, r,560\n3167,EDIFICIO, blue, a,1181\n3173,CORDERO, blue, a,485\n3179,PIE, green, v,1145\n3233,BANCO, green, v,989\n3239,BOMBA, red, r,935\n3371,ABUSO,red,r,981\n</code></pre>\n<p>Since all of them match the condition, they should go to the df_correct.</p>\n<p>What's the correct way to do this?\nThank you.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "matplotlib",
            "scikit-learn"
        ],
        "owner": {
            "account_id": 11400545,
            "reputation": 125,
            "user_id": 10280393,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3d3003d771f7c572207e625349fdefb2?s=128&d=identicon&r=PG&f=1",
            "display_name": "n.mathfreak",
            "link": "https://stackoverflow.com/users/10280393/n-mathfreak"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68198052,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625068448,
        "creation_date": 1625063904,
        "question_id": 68196875,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68196875/matplotlib-shape-mismatch-objects-cannot-be-broadcast-to-a-single-shape",
        "title": "Matplotlib: shape mismatch: objects cannot be broadcast to a single shape",
        "answer_body": "<p>The <code>rf</code> model is trained on <code>X</code> which is only a subset of <code>df</code>, so the feature importances should be plotted against <code>X.columns</code> (or <code>list_of_columns</code>) instead of <code>df.columns</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>plt.barh(X.columns, rf.feature_importances_)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/SNcgP.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/SNcgP.png\" alt=\"feature importance bar plot\" /></a></p>\n",
        "question_body": "<p>I have a dataframe that looks like this (it is obviously much bigger):</p>\n<pre><code>id     points isAvailable frequency   Score\nabc1   325    0           93          0.01\ndef2   467    1           80          0.59\nghi3   122    1           90          1 \njkl4   546    1           84          0\nmno5   355    0           93          0.99\n</code></pre>\n<p>I want to see how much the features <code>points</code>, <code>isAvailable</code> and <code>frequency</code> influence the <code>Score</code>. I want to use Random Forests like <a href=\"https://mljar.com/blog/feature-importance-in-random-forest/\" rel=\"nofollow noreferrer\">in this example</a>:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n#from sklearn.inspection import permutation_importance\n#import shap\nfrom matplotlib import pyplot as plt\n\nplt.rcParams.update({'figure.figsize': (12.0, 8.0)})\nplt.rcParams.update({'font.size': 14})\n\nlist_of_columns = ['points','isAvailable', 'frequency']\nX = df[list_of_columns]\ntarget_column = 'Score'\ny = df[target_column]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)\n\nrf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train, y_train)\nrf.feature_importances_ #the array below is the output \n&gt;&gt;&gt; array([0.44326132, 0.01666047, 0.        , 0.5400782 ])\n\nplt.barh(df.columns, rf.feature_importances_)\n</code></pre>\n<p>On the last line I get the following error: <code>ValueError: shape mismatch: objects cannot be broadcast to a single shape</code>. Should I have created those columns in the beginning? Is there a problem in the (bigger) data?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 13364661,
            "reputation": 880,
            "user_id": 9645391,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/H6ir7.jpg?s=128&g=1",
            "display_name": "Ali Bahaari",
            "link": "https://stackoverflow.com/users/9645391/ali-bahaari"
        },
        "is_answered": true,
        "view_count": 22,
        "closed_date": 1625023212,
        "accepted_answer_id": 68186111,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625067496,
        "creation_date": 1625003468,
        "question_id": 68186079,
        "link": "https://stackoverflow.com/questions/68186079/why-dataframe-isnt-accessible",
        "closed_reason": "Not suitable for this site",
        "title": "Why DataFrame isn&#39;t accessible?",
        "answer_body": "<p>Looks like <code>results</code> is a dictionary, of which the dataframe is one value, with the key <code>'df'</code>. So to print the <code>'peak'</code> column of the dataframe, try this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(results['df']['peak'])\n</code></pre>\n",
        "question_body": "<p>I use <code>findpeaks</code> module. Need to check my peaks and vallies.\nConsider the code below:</p>\n<pre class=\"lang-py prettyprint-override\"><code>results = fp.fit(X)\nprint(results)\n</code></pre>\n<p>When I run the code, it shows me a DataFrame as I upload it below:</p>\n<p><a href=\"https://i.stack.imgur.com/NDVyX.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NDVyX.png\" alt=\"enter image description here\" /></a></p>\n<p>But when I want to check or search in the columns, it gives me error.\nConsider the code below:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(results['peak'])\n</code></pre>\n<p>And it throws this error:</p>\n<p><em><strong>KeyError: 'peak'</strong></em></p>\n<p>For every columns, it throws this error.\nHow should I do it? I appreciate all answers.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22053903,
            "reputation": 27,
            "user_id": 16317755,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cdaba52a269332e8e7d62a5e3d09c74c?s=128&d=identicon&r=PG&f=1",
            "display_name": "koler",
            "link": "https://stackoverflow.com/users/16317755/koler"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68197157,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625064922,
        "creation_date": 1625055787,
        "last_edit_date": 1625059964,
        "question_id": 68194700,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68194700/how-to-recognize-order-of-values-in-column-in-python-pandas-data-frame",
        "title": "How to recognize order of values in column in Python Pandas Data Frame?",
        "answer_body": "<p>As the order does matter, using <code>set</code> is not possible, So we need to check each word one by one:</p>\n<pre><code># x[0] -&gt; ADRESAT, x[1] -&gt; NADAWCA\nintersection = lambda x: ' '.join([x1 for x1 in x[1].split()\n                             if x1.lower() in x[0].lower().split()])\n\ndifference = lambda x: ' '.join([x0 for x0 in x[0].split()\n                           if not x0.lower() in x[1].lower().split()])\n\ndf['col1'] = df[['ADRESAT', 'NADAWCA']].apply(intersection, axis='columns')\ndf['col2'] = df[['ADRESAT', 'NADAWCA']].apply(difference, axis='columns')\n</code></pre>\n<pre><code>&gt;&gt;&gt; df\n          ADRESAT     NADAWCA        col1 col2\n0       Kowal Jan   Jan Kowal   Jan Kowal\n1  Nowak Adam PHU  Adam Nowak  Adam Nowak  PHU\n</code></pre>\n",
        "question_body": "<p>I have Data Frame in Python Pandas like below:</p>\n<pre><code>import pandas as pd\nimport re\ndf = pd.DataFrame()\ndf[&quot;ADRESAT&quot;] = [&quot;Kowal Jan&quot;, &quot;Nowak Adam PHU&quot;]\ndf[&quot;NADAWCA&quot;] = [&quot;Jan Kowal&quot;, &quot;Adam Nowak&quot;]\n</code></pre>\n<p>And I had created 2 new columns:</p>\n<ul>\n<li><p>col1 - value from column &quot;NADAWCA&quot; which is in column &quot;ADRESAT&quot;</p>\n</li>\n<li><p>col2 - rest of values (values from column &quot;ADRESAT&quot; beyon values which are also in column &quot;NADAWCA&quot;)</p>\n<p>df[&quot;col2&quot;] = df.apply(lambda r: re.sub(r[&quot;NADAWCA&quot;], '', r[&quot;ADRESAT&quot;], flags = re.IGNORECASE).strip(), axis=1)\ndf[&quot;col1&quot;] = df[&quot;NADAWCA&quot;].str.title()</p>\n</li>\n</ul>\n<p>Nevertheless, as a result I have df like below. But as you can see in second row there is a mistake.</p>\n<ul>\n<li>In col1 is ok (value from column &quot;NADAWCA&quot; which are also in column &quot;ADRESAT&quot; but</li>\n<li>in col2 I need to have only PHU (means values from column &quot;ADRESAT&quot; beyond valyes which ares also in column &quot;NADAWCA&quot;)</li>\n</ul>\n<p><a href=\"https://i.stack.imgur.com/fnAt0.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/fnAt0.png\" alt=\"enter image description here\" /></a></p>\n<p>My question: How to modify my code so as to recognize that Adam Nowak and Nowak Adam is the same value ?</p>\n<p>I need result as below :</p>\n<p><a href=\"https://i.stack.imgur.com/bD4NA.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/bD4NA.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "account_id": 13933988,
            "reputation": 177,
            "user_id": 10061482,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/Aq7WA.jpg?s=128&g=1",
            "display_name": "pellerossa pelles",
            "link": "https://stackoverflow.com/users/10061482/pellerossa-pelles"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 67673991,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625064893,
        "creation_date": 1621866957,
        "last_edit_date": 1625064893,
        "question_id": 67673951,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67673951/choosing-the-last-element-of-a-column-composed-by-list-of-elements-pandas",
        "title": "choosing the last element of a column composed by list of elements pandas",
        "answer_body": "<p>Let us try <code>str</code> slice then <code>fillna</code>, also notice the last row should be 2 here</p>\n<pre><code>df['new'] = df['Column_1'].str[-1].fillna(df.Column_1)\ndf\nOut[118]: \n    ID Column_1  new_column  new\n0  142   [1, 2]         2.0  2.0\n1  144        3         3.0  3.0\n2  144        1         1.0  1.0\n3  142        1         1.0  1.0\n4  142   [1, 3]         3.0  3.0\n5  144   [1, 5]         5.0  5.0\n6  142        2         NaN  2.0\n</code></pre>\n",
        "question_body": "<p>I have a pandas Dataframe like this:</p>\n<pre><code> df = pd.DataFrame([\n    (142, [1,2]),\n    (144, 3),\n    (144, 1),\n    (142, 1),\n    (142, [1,3]),\n    (144, [1,5]),\n    (142, 2)\n], columns=['ID', 'Column_1'])\n</code></pre>\n<p>As you can see <code>Column_1</code> is column composed by single elements and list of elements.</p>\n<p>My goal is create a new column which takes the last elements where <code>Column_1</code> is a list and the single value where is just one value, like this:</p>\n<pre><code> df = pd.DataFrame([\n    (142, [1,2] ,2),\n    (144, 3 ,3),\n    (144, 1 ,1),\n    (142, 1 ,1),\n    (142, [1,3] , 3),\n    (144, [1,5] , 5),\n    (142, 2)\n], columns=['ID', 'Column_1' , 'new_column'])\n</code></pre>\n<p>Can anyone help me out on this?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 15407278,
            "reputation": 3,
            "user_id": 11115072,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-yJ82yct5ZuQ/AAAAAAAAAAI/AAAAAAAAutE/HC5g7eLCICM/photo.jpg?sz=128",
            "display_name": "Gabriel Caldas",
            "link": "https://stackoverflow.com/users/11115072/gabriel-caldas"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 67924629,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625064840,
        "creation_date": 1623340320,
        "last_edit_date": 1625064840,
        "question_id": 67924575,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67924575/how-to-change-the-value-of-a-column-items-using-pandas",
        "title": "How to change the value of a column items using pandas?",
        "answer_body": "<p>You can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Categorical.html\" rel=\"nofollow noreferrer\">pandas.Categorical</a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['column'] = pd.Categorical(df['column']).codes\n</code></pre>\n<p>You can also use the built in functionality for this too:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['column'] = df['column'].astype('category').cat.codes\n</code></pre>\n",
        "question_body": "<p>This is my fist question on stackoverflow.</p>\n<p>I'm implementing a Machine Learning classification algorithm and I want to generalize it for any input dataset that have their target class in the last column. For that, I want to modify all values of this column without needing to know the names of each column or rows using pandas in python.</p>\n<p>For example, let's suppose I load a dataset:</p>\n<pre><code>dataset = pd.read_csv('random_dataset.csv')\n</code></pre>\n<p>Let's say the last column has the following data:</p>\n<pre><code>0   dog\n1   dog\n2   cat\n3   dog\n4   cat\n</code></pre>\n<p>I want to change each &quot;dog&quot; appearence to 1 and each cat appearance to 0, so that the column would look:</p>\n<pre><code>0   1\n1   1\n2   0\n3   1\n4   0\n</code></pre>\n<p>I have found some ways of changing the values of specific cells using pandas, but for this case, what would be the best way to do that?</p>\n<p>I appreciate each answer.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 5000708,
            "reputation": 807,
            "user_id": 4019836,
            "user_type": "registered",
            "accept_rate": 100,
            "profile_image": "https://www.gravatar.com/avatar/adb23d8e2ee8a14ab37f44474447a9ff?s=128&d=identicon&r=PG&f=1",
            "display_name": "Drecker",
            "link": "https://stackoverflow.com/users/4019836/drecker"
        },
        "is_answered": true,
        "view_count": 23,
        "accepted_answer_id": 68197097,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625064831,
        "creation_date": 1625064112,
        "question_id": 68196924,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68196924/sort-groups-in-dataframegroupby-by-size",
        "title": "Sort groups in DataFrameGroupBy by size",
        "answer_body": "<p>You can sort the groups before iterating:</p>\n<pre class=\"lang-py prettyprint-override\"><code>gdf = df.groupby(&quot;label&quot;)\nfor key, grp in sorted(gdf, key=lambda k: len(k[1]), reverse=True):\n    print(key)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>y\nx\n</code></pre>\n",
        "question_body": "<p>I have a DataFrame such as the following one:</p>\n<pre><code>    label   value\n0   x       1\n1   y       5\n2   x       43\n3   y       22\n4   y       31\n</code></pre>\n<p>created by:</p>\n<pre><code>df = pd.DataFrame([['x', 1],['y', 5], ['x',43], ['y', 22], ['y', 31]], columns=['label', 'value'])\n</code></pre>\n<p>Now i want to group the data by label and iterate over the groups, which can be done as follows:</p>\n<pre><code>gdf = df.groupby('label')\nfor key, grp in gdf:\n  print(key)\n  # do stuff with grp\n</code></pre>\n<p>However, i want to iterate them in a specific order, the key being the size of the group.  In this case I would want to first process label <code>y</code> and its group as there are three records for the label and only after that label <code>x</code> as there are only two records.</p>\n<p>All I manage to find so far is either &quot;how to sort data within groups&quot; or applying <code>.size().sort_values</code> to the <code>gdf</code>, which would achieve the desired sorting, but it effectively discards the underlying data (i.e., if I would do that I would be able access only the sizes of the groups, but not the actual, original data in the group).</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "account_id": 18307729,
            "reputation": 5,
            "user_id": 13331235,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GihjT0Ko5UL5AA5qkK1LEGSudeowDRqGh9kh0Ki=k-s128",
            "display_name": "kamalesh waran",
            "link": "https://stackoverflow.com/users/13331235/kamalesh-waran"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 68195373,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625064587,
        "creation_date": 1625057748,
        "question_id": 68195205,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68195205/convert-nested-dictionary-to-dataframe",
        "title": "Convert Nested dictionary to Dataframe",
        "answer_body": "<p>Another way:</p>\n<pre><code>df=pd.DataFrame(sample_dict).T\n#Then:\ndf=pd.DataFrame(df.pop('Name').to_list()).add_prefix('Name.').join(df.add_prefix('.'))\n#Finally:\ndf=df.sort_index(axis=1)\ndf.columns=df.columns.str.split('.',expand=True)\n</code></pre>\n<p>Now if you print <code>df</code> you will get your expected output</p>\n",
        "question_body": "<p>Let's say I have a dictionary like below</p>\n<pre><code>sample_dict = {\n    0: {\n        'Name': {'First': 'Jack', 'Last': 'Daniels'},\n        'Age': 22,\n        'University': 'BHU',\n    },\n    1: {\n        'Name': {'First': 'Mark', 'Last': 'Zuckerberg'},\n        'Age': 21,\n        'University': 'JNU',\n    },\n    2: {\n        'Name': {'First': 'Tim', 'Last': 'Cook'},\n        'Age': 23,\n        'University': 'DU',\n    }\n} \n</code></pre>\n<p>I would like to convert this dictionary into Dataframe</p>\n<p><strong>Expected Output:</strong></p>\n<pre><code>        Name             Age       University\n  First      Last  \n  Jack       Daniels       22       BHU\n  Mark       Zuckerberg    21       JNU\n  Tim        Cook          23       DU\n</code></pre>\n<p>while performing the below code</p>\n<pre><code>df = pd.DataFrame(details).T\n</code></pre>\n<p>I'm getting the resultant output as:</p>\n<pre><code>                                      Name Age University\n0     {'First': 'Jack', 'Last': 'Daniels'}  22        BHU\n1  {'First': 'Mark', 'Last': 'Zuckerberg'}  21        JNU\n2         {'First': 'Tim', 'Last': 'Cook'}  23         DU\n</code></pre>\n<p>Ideas are welcome!!</p>\n<p>Thanks in Advance!!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 14059381,
            "reputation": 125,
            "user_id": 14824108,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/61f590e3d61e8abc27db397cfbddc221?s=128&d=identicon&r=PG&f=1",
            "display_name": "James Arten",
            "link": "https://stackoverflow.com/users/14824108/james-arten"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 68194200,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625060546,
        "creation_date": 1625053371,
        "question_id": 68194098,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68194098/saving-small-sub-dataframes-containing-all-values-associated-to-a-specific-key",
        "title": "Saving small sub-dataframes containing all values associated to a specific &#39;key&#39; string",
        "answer_body": "<p>Input data:</p>\n<pre><code>&gt;&gt;&gt; df\n   0       1\n0  A  0.4533\n1  B  0.2323\n2  A  1.2343\n3  A  1.2353\n4  B  4.3521\n5  C  3.2113\n6  C  2.1233\n</code></pre>\n<p>Use <code>groupby</code> before <code>min</code>:</p>\n<pre><code>out = df.groupby(0).min()\n</code></pre>\n<p>Output result:</p>\n<pre><code>&gt;&gt;&gt; out\n        1\n0\nA  0.4533\nB  0.2323\nC  2.1233\n</code></pre>\n<p><strong>Update</strong>:</p>\n<blockquote>\n<p>filter out all the values in the original dataset that are more than 20% different from the minimum</p>\n</blockquote>\n<pre><code>out = df[df.groupby(0)[1].apply(lambda x: x &lt;= x.min() * 1.2)]\n</code></pre>\n<pre><code>&gt;&gt;&gt; out\n   0       1\n0  A  0.4533\n1  B  0.2323\n6  C  2.1233\n</code></pre>\n",
        "question_body": "<p>I'd need a little suggestion on a procedure using pandas, I have a 2-columns dataset that looks like this:</p>\n<pre><code>A      0.4533\nB      0.2323\nA      1.2343\nA      1.2353\nB      4.3521\nC      3.2113\nC      2.1233\n..      ...\n</code></pre>\n<p>where first column contains strings and the second one floats. I would like to save the minimum value for each group of unique strings in order to have the associated minimum with <code>A, B, C</code>. Does anybody have any suggestions on that? It could help me also storing somehow all the values for each string they are associated.</p>\n<p>Many thanks,</p>\n<p>James</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "geometry",
            "data-analysis"
        ],
        "owner": {
            "account_id": 7246798,
            "reputation": 496,
            "user_id": 5528439,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/f4549a5e681e7a679ec971849facb941?s=128&d=identicon&r=PG&f=1",
            "display_name": "Simon",
            "link": "https://stackoverflow.com/users/5528439/simon"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 68186961,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625057202,
        "creation_date": 1625004240,
        "last_edit_date": 1625057202,
        "question_id": 68186190,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68186190/python-pandas-dataframe-find-for-each-entry-in-df-a-the-closest-timestamp-in",
        "title": "Python | Pandas Dataframe: find for each entry in df A the closest timestamp in df B",
        "answer_body": "<p>Matching rows to the closest values is called an <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.merge_asof.html\" rel=\"nofollow noreferrer\"><code>asof merge</code></a>, i.e. a \u201cleft join except that we match on nearest key rather than equal keys\u201d \u2212 both columns need to be sorted.</p>\n<pre><code>&gt;&gt;&gt; pd.merge_asof(df1, df2, on='timestamp', suffixes=('_a', '_b'), direction='nearest')\n   x_a  y_a  z_a  timestamp  x_b  y_b  z_b\n0    1    2    3       1.40    7    4    1\n1    4    5    6       1.73    8    5    2\n2    7    8    9       4.10    9    6    3\n</code></pre>\n<p>If you want to be able to subtract the 2 timestamp columns, you need the named differently. You can add suffixes before the merge:</p>\n<pre><code>&gt;&gt;&gt; df = pd.merge_asof(df1.add_suffix('_a'), df2.add_suffix('_b'), direction='nearest',\n...                    left_on='timestamp_a', right_on='timestamp_b')\n&gt;&gt;&gt; df['delta'] = df['timestamp_a'] - df['timestamp_b']\n&gt;&gt;&gt; df\n   x_a  y_a  z_a  timestamp_a  x_b  y_b  z_b  timestamp_b  delta\n0    1    2    3         1.40    7    4    1         1.70   -0.3\n1    4    5    6         1.73    8    5    2         1.73    0.0\n2    7    8    9         4.10    9    6    3         3.50    0.6\n</code></pre>\n",
        "question_body": "<p>I am new to python pandas. What I have are 2 pandas dataframes. Among other data both of them contain a timestamp column.</p>\n<p>Assume we have df A</p>\n<pre><code>x y z timestamp\n1 2 3 1.4\n4 5 6 1.73\n7 8 9 4.1\n</code></pre>\n<p>and df B is:</p>\n<pre><code>x y z timestamp\n7 4 1 1.7\n8 5 2 1.73\n9 6 3 3.5\n4 5 6 4.8\n</code></pre>\n<p>I would like to compute for each row in A the difference to the position in B which is closest to the timestamp in A. We can assume that the df are both sorted by timestamp. However these timestamps do not share the same start or end time but do certainly have some overlap.</p>\n<p>Furthermore the two data frames are not necessarily of same length.\nI have a brute force implementation in place which does exactly what I want and which I also can easily extend to potentially interpolate between timestamps -- something which I want to achieve in an improved version. However, my implementation is terribly slow.</p>\n<p>I am sure there is a more performant way of implementing the following:</p>\n<pre><code>idxA = 0\nidxB = 0\nendA = len(A)\nendB = len(B)\n\nwhile idxA &lt; endA and idxB &lt; endB:\n  currentA_ts = A['timestamp'][idxA]\n  currentB_ts = B['timestamp'][idxB]\n  if idxB &lt; endB-1:\n    nextB_ts = B['timestamp'][idxB+1]\n    if abs(currentB_ts - currentA_ts) &gt; abs(nextB_ts - currentA_ts):\n      idxB += 1\n\n  currentClosestB_row = B.iloc[idxB]\n  currentA_row = A.iloc[idxA]\n\n  B_location = currentClosestB_row[['x','y','z']]\n  A_location = currentA_row[['x', 'y', 'z']]\n\n  direction = get_direction_vector(B_location, A_location)\n\n  currentA_row['dir_x'] = direction[0]\n  currentA_row['dir_y'] = direction[1]\n  currentA_row['dir_z'] = direction[2]\n\n  out_df.append(currentA_row)\n\n  idxA += 1\n</code></pre>\n<p>I hope that code snippet clarifies what I try to achieve. But as mentioned above, this is terribly slow as the df A and B both have several 100k entries.</p>\n<p>I see two ways of improving the above code:</p>\n<ol>\n<li>The general structure of how I try to achieve the described goal.</li>\n<li>I can imagine that how I use python and pandas is not optimal. I am using pandas for the very first time, also python is not my main programming language - so please let me know in case you see something that can be improved.</li>\n</ol>\n<p>Any feedback on how to speed up that code is highly appreciated.</p>\n<p>Many thanks in advance.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-munging"
        ],
        "owner": {
            "account_id": 8033257,
            "reputation": 724,
            "user_id": 6057371,
            "user_type": "registered",
            "accept_rate": 31,
            "profile_image": "https://www.gravatar.com/avatar/b744e3dbff9c7fdb801570de75f6eff7?s=128&d=identicon&r=PG&f=1",
            "display_name": "okuoub",
            "link": "https://stackoverflow.com/users/6057371/okuoub"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68193597,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625051576,
        "creation_date": 1625051004,
        "question_id": 68193558,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values",
        "title": "pandas group many columns to one column where every cell is a list of values",
        "answer_body": "<p>Try:</p>\n<pre><code>#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n</code></pre>\n",
        "question_body": "<p>I have the dataframe</p>\n<pre><code>df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n</code></pre>\n<p>And I want to group all columns to a single list that will be the only columns, so I will get:</p>\n<pre><code>df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n</code></pre>\n<p>(Shape of df was change from (3,5) to (3,1))</p>\n<p>What is the best way to do this?</p>\n",
        "input_data_frames": [
            "df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n",
            "df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n"
        ],
        "output_codes": [
            "#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n"
        ],
        "ques_desc": "I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? ",
        "ans_desc": "Try: ",
        "formatted_input": {
            "qid": 68193558,
            "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values",
            "question": {
                "title": "pandas group many columns to one column where every cell is a list of values",
                "ques_desc": "I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? "
            },
            "io": [
                "df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n",
                "df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n"
            ],
            "answer": {
                "ans_desc": "Try: ",
                "code": [
                    "#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 21396861,
            "reputation": 13,
            "user_id": 15759786,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/002b02baee3603027b66ed2361da99e2?s=128&d=identicon&r=PG&f=1",
            "display_name": "big sad",
            "link": "https://stackoverflow.com/users/15759786/big-sad"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68193581,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625051129,
        "creation_date": 1625050872,
        "question_id": 68193521,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame",
        "title": "Concatenate values and column names in a data frame to create a new data frame",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = df.melt(&quot;Value&quot;, value_name=&quot;Col 1&quot;)\nx.Value += &quot;_&quot; + x.variable\nx = x.drop(columns=&quot;variable&quot;)\nprint(x)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>     Value Col 1\n0   a_col1    aa\n1   b_col1    ba\n2   c_col1    ca\n3   d_col1    da\n4   e_col1    ea\n5   a_col2    ab\n6   b_col2    bb\n7   c_col2    cb\n8   d_col2    db\n9   e_col2    eb\n10  a_col3    ac\n11  b_col3    bc\n12  c_col3    cc\n13  d_col3    dc\n14  e_col3    ec\n</code></pre>\n<hr />\n<p>Optionally, you can sort values afterwards:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = x.sort_values(by=&quot;Value&quot;).reset_index(drop=True)\nprint(x)\n\n     Value Col 1\n0   a_col1    aa\n1   a_col2    ab\n2   a_col3    ac\n3   b_col1    ba\n4   b_col2    bb\n5   b_col3    bc\n6   c_col1    ca\n7   c_col2    cb\n8   c_col3    cc\n9   d_col1    da\n10  d_col2    db\n11  d_col3    dc\n12  e_col1    ea\n13  e_col2    eb\n14  e_col3    ec\n</code></pre>\n",
        "question_body": "<p>I have the following data frame(<code>df1</code>):</p>\n<pre><code>  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n</code></pre>\n<p>I need to derive the data frame(<code>df2</code>) from <code>df1</code> such that column 1 of <code>d2</code> will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of <code>d2</code> will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate.   :</p>\n<pre><code>      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n</code></pre>\n<p>I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process?</p>\n<p>Below is the code I have used</p>\n<pre><code>d = {'Value': ['a','b','c','d','e'],'col1': ['aa','ba','ca','da','ea'], 'col2' : ['ab','bb','cb','db','eb'],'col3': ['ac','bc','cc','dc','ec']}\ndf1 = pd.DataFrame(data = d)\n\n# Repeat every value is Value column 3 times.\nX = df1['Value'].repeat(4).reset_index(drop=True)\n\n# Create separate series with Col 1, Col 2, Col 3 names.\nY = pd.Series(df1.columns[1:])\n\n# Repeated series Y to the length of data df1\nYY = pd.Series(np.tile(Y.values, len(df1)))\n\n# Create the first column by concatenating X and YY\nfirst_column_1 = X + &quot;_&quot; + YY\n\nZ = df1.set_index('Value')\nZZ = np.ravel(Z.values)\n\n#Create 2nd column from ZZ\nsecond_column = pd.Series(ZZ)\n\n#Create df2\ndf2 = pd.DataFrame([first_column, second_column]).T\n</code></pre>\n",
        "input_data_frames": [
            "  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n",
            "      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n"
        ],
        "output_codes": [
            "x = df.melt(\"Value\", value_name=\"Col 1\")\nx.Value += \"_\" + x.variable\nx = x.drop(columns=\"variable\")\nprint(x)\n",
            "x = x.sort_values(by=\"Value\").reset_index(drop=True)\nprint(x)\n\n     Value Col 1\n0   a_col1    aa\n1   a_col2    ab\n2   a_col3    ac\n3   b_col1    ba\n4   b_col2    bb\n5   b_col3    bc\n6   c_col1    ca\n7   c_col2    cb\n8   c_col3    cc\n9   d_col1    da\n10  d_col2    db\n11  d_col3    dc\n12  e_col1    ea\n13  e_col2    eb\n14  e_col3    ec\n"
        ],
        "ques_desc": "I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used ",
        "ans_desc": "Try: Prints: Optionally, you can sort values afterwards: ",
        "formatted_input": {
            "qid": 68193521,
            "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame",
            "question": {
                "title": "Concatenate values and column names in a data frame to create a new data frame",
                "ques_desc": "I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used "
            },
            "io": [
                "  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n",
                "      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n"
            ],
            "answer": {
                "ans_desc": "Try: Prints: Optionally, you can sort values afterwards: ",
                "code": [
                    "x = df.melt(\"Value\", value_name=\"Col 1\")\nx.Value += \"_\" + x.variable\nx = x.drop(columns=\"variable\")\nprint(x)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20213166,
            "reputation": 3,
            "user_id": 14824855,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/4285232638170336/picture?type=large",
            "display_name": "Jstns",
            "link": "https://stackoverflow.com/users/14824855/jstns"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 68193129,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625049179,
        "creation_date": 1625046003,
        "question_id": 68192281,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68192281/indexing-column-in-pandas-dataframe-returns-nan",
        "title": "Indexing column in Pandas Dataframe returns NaN",
        "answer_body": "<p>If the contents of your column <code>identifiers</code> is a real dict / json type, you can use the string accessor <code>str[]</code> to access the dict value by key, as follows:</p>\n<pre><code>testdf2['identifiers_test'] = testdf2['identifiers'].str['print_isbn_canonical']\n</code></pre>\n<p><strong>Demo</strong></p>\n<pre><code>data = {'identifiers': [{'print_isbn_canonical': '9780721682167', 'eis': '1234'}]}\ndf = pd.DataFrame(data)\n\ndf['isbn'] = df['identifiers'].str['print_isbn_canonical']\n\nprint(df)\n\n                                                identifiers           isbn\n0  {'print_isbn_canonical': '9780721682167', 'eis': '1234'}  9780721682167\n</code></pre>\n",
        "question_body": "<p>I am running into a problem with trying to index my dataframe. As shown in the attached picture, I have a column in the dataframe called 'Identifiers' that contains a lot of redundant information ({'print_isbn_canonical': '). I only want the ISBN that comes after.</p>\n<pre><code>    #Option 1 I tried\n    testdf2 = testdf2[testdf2['identifiers'].str[26:39]]\n    \n    #Option 2 I tried\n    testdf2['identifiers_test'] = testdf2['identifiers'].str.replace(&quot;{'print_isbn_canonical': '&quot;,&quot;&quot;)\n</code></pre>\n<p>Unfortunately both of these options turn the dataframe column into a colum <strong>only containing NaN values</strong></p>\n<p>Please help out! I cannot seem to find the solution and have tried several things. Thank you all in advance!</p>\n<p><a href=\"https://i.stack.imgur.com/ZNzGt.jpg\" rel=\"nofollow noreferrer\">Example image of the dataframe</a></p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "data-manipulation"
        ],
        "owner": {
            "account_id": 15542666,
            "reputation": 50,
            "user_id": 11212921,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/2288933594468912/picture?type=large",
            "display_name": "Denny Chen",
            "link": "https://stackoverflow.com/users/11212921/denny-chen"
        },
        "is_answered": true,
        "view_count": 21,
        "accepted_answer_id": 68264436,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625544994,
        "creation_date": 1625543922,
        "question_id": 68264352,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68264352/replace-a-subset-pandas-dataframe-with-rows-and-columns-via-a-numpy-array-in-pyt",
        "title": "Replace a subset pandas dataframe with rows and columns via a numpy array in Python",
        "answer_body": "<p>You can try via <code>loc</code> accessor:</p>\n<pre><code>df.loc[[1,3,5,7,9], [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;]]=replace_value\n#just like you grabbed values you can also assign that back like that\n</code></pre>\n<p>sample data:</p>\n<pre><code>df=pd.DataFrame(np.random.randn(25,3),columns=[&quot;A&quot;, &quot;B&quot;, &quot;C&quot;])\nreplace_value = np.array([[1, 2, 3], [4, 4, 4], [1, 6, 8], [1, 3, 6], [8, 0, 1]])\ndf.loc[[1,3,5,7,9], [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;]]=replace_value \n</code></pre>\n",
        "question_body": "<p>I would like to use a numpy array to replace a subset dataframe from a pandas dataframe.</p>\n<p>For example:\na pandas dataframe <code>df</code>.</p>\n<pre><code>df_subset = df.loc[[1,3,5,7,9], [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;]]\n</code></pre>\n<p>Here, the subset dataframe has a dimension of <code>(5, 3)</code>:</p>\n<p>And below is the numpy array example with the same shape as the subset dataframe I would like to replace to:</p>\n<pre><code>replace_value = np.array([[1, 2, 3], [4, 4, 4], [1, 6, 8], [1, 3, 6], [8, 0, 1]])\n</code></pre>\n<p>Is there any approach similar to:</p>\n<pre><code>df_subset.values = replace_value \n</code></pre>\n<p>What I hope is that the value I replaced will directly change the original values in <code>df</code>. Which means that if I subset <code>df</code> with the same indice and columns again, I will get the exact values as the numpy array I assigned as <code>replace_value</code> above.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22127805,
            "reputation": 13,
            "user_id": 16379843,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/af11554f22a743a1f798b553b8ec15e9?s=128&d=identicon&r=PG&f=1",
            "display_name": "qyx1996",
            "link": "https://stackoverflow.com/users/16379843/qyx1996"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 68250197,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625534659,
        "creation_date": 1625451695,
        "last_edit_date": 1625452585,
        "question_id": 68250141,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68250141/pandas-dataframe-how-to-sort-groups-by-the-earliest-time-of-a-group",
        "title": "Pandas dataframe: How to sort groups by the earliest time of a group",
        "answer_body": "<p>we can group by the eventid and get first(min) time as group value.<br />\nwill get data like this</p>\n<pre><code>            time\neventid \n1           9:10\n2           9:00\n3           9:40\n</code></pre>\n<p>then we merge to dataframe,and sort by the grouped time</p>\n<pre class=\"lang-py prettyprint-override\"><code>groups = df.groupby('eventid').min('time')\ndf = df.merge(groups,on='eventid',suffixes=('','_right'))\ndf = df.sort_values('time_right')\n</code></pre>\n<pre><code>    eventid time    time_right\n2   2       9:20    9:00\n3   2       9:00    9:00\n0   1       9:10    9:10\n1   1       9:30    9:10\n4   3       9:40    9:40\n5   3       9:50    9:40\n</code></pre>\n",
        "question_body": "<p>Is there a pandas way to solve the following problem? There is a dataframe with many columns including 'time' and 'eventid'. First I want to group the dataframe by 'eventid'. Each 'eventid' may linked with multiple 'time'. Then I need to sort the 'eventid' groups by the earliest 'time' of each group (order inside a group is not important). For example, my input is like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>eventid</th>\n<th>time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>9:10</td>\n</tr>\n<tr>\n<td>2</td>\n<td>9:20</td>\n</tr>\n<tr>\n<td>1</td>\n<td>9:30</td>\n</tr>\n<tr>\n<td>3</td>\n<td>9:40</td>\n</tr>\n<tr>\n<td>3</td>\n<td>9:50</td>\n</tr>\n<tr>\n<td>2</td>\n<td>9:00</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>my desired output is like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>eventid</th>\n<th>time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2</td>\n<td>9:00</td>\n</tr>\n<tr>\n<td>2</td>\n<td>9:20</td>\n</tr>\n<tr>\n<td>1</td>\n<td>9:30</td>\n</tr>\n<tr>\n<td>1</td>\n<td>9:10</td>\n</tr>\n<tr>\n<td>3</td>\n<td>9:50</td>\n</tr>\n<tr>\n<td>3</td>\n<td>9:40</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I have tried <code>df.sort_values(by=['time','eventid'])</code> and <code>df.groupby</code> but may be wrong for this situation. Another way is to sort by 'time' and then query and rewrite the dataframe. But it will take too much time since the original data is quite big.</p>\n<p>I couldn't find a similar question before. Thanks a lot for possible solutions.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "performance"
        ],
        "owner": {
            "account_id": 16634018,
            "reputation": 297,
            "user_id": 12021183,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/35b529ce45febdad8479b04886ad7259?s=128&d=identicon&r=PG&f=1",
            "display_name": "raven",
            "link": "https://stackoverflow.com/users/12021183/raven"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 68263348,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625533751,
        "creation_date": 1625532801,
        "last_edit_date": 1625533751,
        "question_id": 68263336,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68263336/subtracting-minimum-values-of-a-certain-pandas-dataframe-column-based-on-another",
        "title": "Subtracting minimum values of a certain pandas dataframe column based on another column",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df[&quot;ca&quot;] = df.groupby(&quot;id&quot;)[&quot;year&quot;].transform(lambda x: x - x.min())\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>   id  gender  year  ca\n0   3    male  1983   0\n1   3    male  1983   0\n2   3    male  1985   2\n3   3    male  1990   7\n4   6  female  1991   0\n5   6  female  1992   1\n</code></pre>\n",
        "question_body": "<p>I have a huge <code>pandas</code> DataFrame <code>df</code>, sorted by <code>id</code> and then <code>year</code>:</p>\n<pre><code>id        gender        year\n3         male          1983\n3         male          1983\n3         male          1985\n3         male          1990\n6         female        1991\n6         female        1992\n7         male          1980\n...\n592873    female        1989\n592873    female        1996\n593001    male          2001\n593428    female        2007\n593428    female        2009\n</code></pre>\n<p>My goal is to create another column <code>ca</code> which is computed by:</p>\n<ul>\n<li><code>year</code> - minimum <code>year</code> of that <code>id</code></li>\n</ul>\n<p>Hence, the first six rows of <code>df</code> should return:</p>\n<pre><code>id        gender        year        ca\n3         male          1983        0\n3         male          1983        0\n3         male          1985        2\n3         male          1990        7\n6         female        1991        0\n6         female        1992        1\n</code></pre>\n<p>(In other words, I'm searching for a Pythonic answer to <a href=\"https://stackoverflow.com/questions/24979757/subtract-minimum-of-the-column-based-on-other-column\">this question</a>.)</p>\n<hr />\n<p>One solution I could think of is to make a list and use a <code>for</code> loop:</p>\n<pre><code>ca_list = []\n\nfor i in range(len(df)):\n  if df['id'][i] != df['id'][i-1]:\n    num = df['year'][i]\n    ca_list.append(0)\n  else:\n    ca_list.append(df['year'][i] - num)\n\ndf['ca'] = ca_list\n</code></pre>\n<p>But I believe there is a more optimal way to devise this. Any insights are much appreciated.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 5124911,
            "reputation": 370,
            "user_id": 4107349,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d3066a3e418c462684a41e9307302a54?s=128&d=identicon&r=PG&f=1",
            "display_name": "Chris Dixon",
            "link": "https://stackoverflow.com/users/4107349/chris-dixon"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 68263097,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1625529350,
        "creation_date": 1625527864,
        "question_id": 68262980,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68262980/adding-row-to-pandas-dataframe-where-column-contains-additional-value-that-shoul",
        "title": "Adding row to pandas dataframe where column contains additional value that should be another row?",
        "answer_body": "<p>Please use the loc accessor to slice suitable columns and then append. You can sort values if needed.</p>\n<pre><code>df.loc[:,:'flavor'].append(df.loc[:, df.columns != 'flavor'].rename(columns={'another flavor':'flavor'}),ignore_index=True).dropna().sort_index().\n</code></pre>\n",
        "question_body": "<p>Simplified df where some rows contain additional entries in <code>another flavor</code> that should be another row:</p>\n<pre><code>   values more values   flavor another flavor\n0       6         foo  caramel      chocolate\n1       4         baz  vanilla            NaN\n\ndf = pd.DataFrame({&quot;values&quot;: [6, 4],&quot;more values&quot;: [&quot;foo&quot;,  &quot;baz&quot;],&quot;flavor&quot;: [&quot;caramel&quot;, &quot;vanilla&quot;],&quot;another flavor&quot;: [&quot;chocolate&quot;,  np.nan],})\n</code></pre>\n<p>We need to add another row containing values from other columns, populating <code>flavor</code> with values from <code>another flavor</code>. We'd then drop <code>another flavor</code> to get <code>desired_df</code>:</p>\n<pre><code>   values more values     flavor\n0       6         foo    caramel\n1       6         foo  chocolate\n2       4         baz    vanilla\n\ndesired_df = pd.DataFrame({&quot;values&quot;: [6, 6, 4],&quot;more values&quot;: [&quot;foo&quot;, &quot;foo&quot;, &quot;baz&quot;],&quot;flavor&quot;: [&quot;caramel&quot;, &quot;chocolate&quot;,  &quot;vanilla&quot;],})\n</code></pre>\n<p>What's a practical way to do this? Is there an expression for this I could search for as a keyword?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dynamic",
            "while-loop"
        ],
        "owner": {
            "account_id": 12369662,
            "reputation": 93,
            "user_id": 9020385,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2cc1859bbb5d09cbbe2bd28888aabb02?s=128&d=identicon&r=PG&f=1",
            "display_name": "Srinivas",
            "link": "https://stackoverflow.com/users/9020385/srinivas"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 68257196,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625516660,
        "creation_date": 1625488848,
        "last_edit_date": 1625489333,
        "question_id": 68256341,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68256341/how-to-declare-and-assign-python-dataframe-39column-name39-as-variable-of",
        "title": "How to declare and assign Python Dataframe &amp;#39;column name&amp;#39; as variable of two strings?",
        "answer_body": "<p>You can add a column to a <code>DataFrame</code> like you'd add a key/value to a dictionary, i.e. <code>df[&quot;your_key&quot;] = some_vals</code>. You can construct your key however you want.</p>\n<p>One wrinkle is that the values you assign should either be a single value or a sequence of values with the same number of elements as your <code>DataFrame</code> has rows. So in your case</p>\n<pre class=\"lang-py prettyprint-override\"><code>In [30]: from datetime import datetime\n\nIn [31]: import pandas\n\nIn [32]: df = pandas.DataFrame({&quot;symbol&quot;: [&quot;INFY&quot;, &quot;RELIANCE&quot;], &quot;SMA1010915&quot;: [1562.97, 2127], &quot;SMA100930&quot;: [15464, 2129],\n    ...: &quot;SMA100945&quot;: [1565.65, 2126.39]})\n\nIn [33]: time_str = datetime.now().strftime('%H%M')\n\nIn [34]: df[&quot;SMA10&quot; + time_str] = [2000, 3000] # List of length two since there are two rows\n\nIn [35]: df\nOut[35]:\n     symbol  SMA1010915  SMA100930  SMA100945  SMA100938\n0      INFY     1562.97      15464    1565.65       2000\n1  RELIANCE     2127.00       2129    2126.39       3000\n\nIn [36]: new_time_str = &quot;1230&quot;\n\nIn [40]: df[&quot;SMA10&quot; + new_time_str] = 100 # If single value, all rows get this\n\nIn [41]: df\nOut[41]:\n     symbol  SMA1010915  SMA100930  SMA100945  SMA100938  SMA101230\n0      INFY     1562.97      15464    1565.65       2000        100\n1  RELIANCE     2127.00       2129    2126.39       3000        100\n</code></pre>\n<p>If you want to assign values to individual rows, you can do that, too. I think it'd probably be easiest to index your <code>DataFrame</code> by the &quot;symbol&quot; column, so something like</p>\n<pre class=\"lang-py prettyprint-override\"><code>In [51]: by_symbol = df.set_index(&quot;symbol&quot;)\nIn [55]: by_symbol\nOut[55]:\n          SMA1010915  SMA100930  SMA100945\nsymbol\nINFY         1562.97      15464    1565.65\nRELIANCE     2127.00       2129    2126.39\n\nIn [56]: new_time_str = &quot;1111&quot;\n\nIn [57]: by_symbol.loc[&quot;INFY&quot;, &quot;SMA10&quot; + new_time_str] = 1234\n\nIn [58]: by_symbol\nOut[58]:\n          SMA1010915  SMA100930  SMA100945  SMA101111\nsymbol\nINFY         1562.97      15464    1565.65     1234.0\nRELIANCE     2127.00       2129    2126.39        NaN\n\n</code></pre>\n<p>Note that you could do this without indexing by symbol. In that case, you'd just do</p>\n<pre class=\"lang-py prettyprint-override\"><code>In [59]: df.loc[0, &quot;SMA10&quot; + new_time_str] = 1234\n\nIn [60]: df\nOut[60]:\n     symbol  SMA1010915  SMA100930  SMA100945  SMA101111\n0      INFY     1562.97      15464    1565.65     1234.0\n1  RELIANCE     2127.00       2129    2126.39        NaN\n</code></pre>\n",
        "question_body": "<p>I have a DataFrame of Stocks with columns as <code>'SMA100915'</code>, <code>'SMA500915'</code> and so on...\n<code>df['SMA100915']</code> is a column having Simple Moving Average value of the stock at 09:15 Hrs, likewise I'm collecting all the SMA's in different columns for analysis purpose using a <code>while</code> loop on <code>df</code>. This loop is scheduled to run every 15 mins from 09:15 Hrs</p>\n<p>Now the question is,</p>\n<ol>\n<li>How do I generate the name of the df column dynamically as <code>'SMA10' + 0915</code> and assign SMA value?</li>\n<li>Assign the SMA value to each row of the stock in df to each columns in df</li>\n</ol>\n<p><code>x = datetime.now()</code>  #assuming time now to be 09:15</p>\n<p><code>xtime = x.strftime(%H%M)</code></p>\n<p>I tried using <code>globals()[df.loc[i, '''SMA10'+ xtime']] = 13.84</code></p>\n<p>Sample Data:</p>\n<p><code>df()</code></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">'symbol'</th>\n<th style=\"text-align: left;\">'SMA100915'</th>\n<th style=\"text-align: left;\">'SMA100930'</th>\n<th style=\"text-align: left;\">'SMA100945'</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">INFY</td>\n<td style=\"text-align: left;\">1562.97</td>\n<td style=\"text-align: left;\">1564</td>\n<td style=\"text-align: left;\">1565.65</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">RELIANCE</td>\n<td style=\"text-align: left;\">2127</td>\n<td style=\"text-align: left;\">2129</td>\n<td style=\"text-align: left;\">2126.39</td>\n</tr>\n</tbody>\n</table>\n</div>"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 18963483,
            "reputation": 5,
            "user_id": 13838430,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-am1MHSb9sb0/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucmyF4HE1PkSd9gnDh0R4GtiWez_7w/photo.jpg?sz=128",
            "display_name": "Vivrd Prasanna",
            "link": "https://stackoverflow.com/users/13838430/vivrd-prasanna"
        },
        "is_answered": true,
        "view_count": 21,
        "closed_date": 1625514713,
        "accepted_answer_id": 68261421,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625514439,
        "creation_date": 1625514096,
        "question_id": 68261366,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68261366/right-way-to-reindex-a-dataframe",
        "closed_reason": "Duplicate",
        "title": "Right way to reindex a dataframe?",
        "answer_body": "<p>Use <code>reset_index</code>:</p>\n<pre><code>&gt;&gt;&gt; df.reset_index(drop=True)\n  column 1  column 2\n0        a         1\n1        b         2\n2        c         3\n</code></pre>\n",
        "question_body": "<p>I have a large dataset which I filtered by location. The end result is something like this:</p>\n<pre><code>   column 1  column 2\n 0        a         1\n 106      b         2\n 178      c         3\n</code></pre>\n<p>I guessed that the index values are skipping all over the place since the all the columns with the same locations aren't consecutive. To reset the indices, I did <code>df.reindex(index = np.arange(len(df)))</code>, and it worked... but broke everything else. The output is this:</p>\n<pre><code>   column 1  column 2\n 0        a         1\n 1      NAN       NAN\n 12     NAN       NAN\n</code></pre>\n<p>I don't have any idea why this is happening, and how I can fix this. Thanks for any help provided!</p>\n",
        "input_data_frames": [
            "   column 1  column 2\n 0        a         1\n 106      b         2\n 178      c         3\n",
            "   column 1  column 2\n 0        a         1\n 1      NAN       NAN\n 12     NAN       NAN\n"
        ],
        "output_codes": [
            ">>> df.reset_index(drop=True)\n  column 1  column 2\n0        a         1\n1        b         2\n2        c         3\n"
        ],
        "ques_desc": "I have a large dataset which I filtered by location. The end result is something like this: I guessed that the index values are skipping all over the place since the all the columns with the same locations aren't consecutive. To reset the indices, I did , and it worked... but broke everything else. The output is this: I don't have any idea why this is happening, and how I can fix this. Thanks for any help provided! ",
        "ans_desc": "Use : ",
        "formatted_input": {
            "qid": 68261366,
            "link": "https://stackoverflow.com/questions/68261366/right-way-to-reindex-a-dataframe",
            "question": {
                "title": "Right way to reindex a dataframe?",
                "ques_desc": "I have a large dataset which I filtered by location. The end result is something like this: I guessed that the index values are skipping all over the place since the all the columns with the same locations aren't consecutive. To reset the indices, I did , and it worked... but broke everything else. The output is this: I don't have any idea why this is happening, and how I can fix this. Thanks for any help provided! "
            },
            "io": [
                "   column 1  column 2\n 0        a         1\n 106      b         2\n 178      c         3\n",
                "   column 1  column 2\n 0        a         1\n 1      NAN       NAN\n 12     NAN       NAN\n"
            ],
            "answer": {
                "ans_desc": "Use : ",
                "code": [
                    ">>> df.reset_index(drop=True)\n  column 1  column 2\n0        a         1\n1        b         2\n2        c         3\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "whitespace",
            "delimiter"
        ],
        "owner": {
            "account_id": 20103670,
            "reputation": 171,
            "user_id": 14758388,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/iEU1x.jpg?s=128&g=1",
            "display_name": "Abhilash Singh Chauhan",
            "link": "https://stackoverflow.com/users/14758388/abhilash-singh-chauhan"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 68259408,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625513997,
        "creation_date": 1625500695,
        "last_edit_date": 1625507221,
        "question_id": 68258993,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68258993/creating-pandas-dataframe-from-textfile-of-long-term-climate-data",
        "title": "Creating Pandas Dataframe from textfile of long term Climate Data",
        "answer_body": "<ul>\n<li>This type of data is better handled by <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html\" rel=\"nofollow noreferrer\">read_fwf()</a></li>\n<li>to make <strong>infer</strong> work as wanted,  have given it 32 lines of fixed format data</li>\n<li>once all data is in dataframe,  cleanup by testing <strong>YEAR</strong> is numeric to exclude the blank lines and header lines at multiple points in data</li>\n<li>finally set expected data types on all columns</li>\n</ul>\n<pre><code>import requests\nimport pandas as pd\nimport numpy as np\nimport io\nfrom pathlib import Path\n\n# download sample data and save to file...\nurl = &quot;https://raw.githubusercontent.com/abhilashsinghimd/AASD_Geojson/main/25_may24_SD1.DAT&quot;\nres = requests.get(url)\nwith open(Path.cwd().joinpath(&quot;SO_example.DAT&quot;), &quot;w&quot;) as f: f.write(res.text)\n    \n# read file from your file system here...\nwith open(Path.cwd().joinpath(&quot;SO_example.DAT&quot;), &quot;r&quot;) as f: text = f.read()\n    \ndf = pd.read_fwf(\n    io.StringIO(\n        &quot;\\n&quot;.join(text.split(&quot;\\n&quot;)[6:7] + text.split(&quot;\\n&quot;)[8 : 8 + 31])\n        + &quot;\\n&quot;.join(text.split(&quot;\\n&quot;)[8+31:])\n    ),\n    infer_nrows=32,\n)\n\nprint(f&quot;expected row count:{(2019-1900)*31}&quot;)\n# exclude header rows littered through data\ndf = df.loc[~pd.to_numeric(df[&quot;YEAR&quot;], errors=&quot;coerce&quot;).isna()]\n# convert to expected datatypes\ndf = df.assign(**{c:df.loc[:,c].astype(&quot;int&quot; if c in [&quot;YEAR&quot;,&quot;DATE&quot;] else &quot;float&quot;) for c in df.columns})\n\npd.set_option(&quot;display.width&quot;,100)\nprint(df)\n\n</code></pre>\n<h3>output</h3>\n<pre><code>expected row count:3689\n      YEAR  DATE  JAN  FEB   MAR  APR   MAY   JUN   JUL   AUG   SEP  OCT  NOV  DEC\n0     1901     1  0.0  0.0   0.3  0.0   3.7   0.9  11.1   0.1   2.5  0.0  0.0  0.0\n1     1901     2  0.0  0.0  16.5  0.0  12.3   0.0  11.4   2.7   4.9  0.0  0.0  0.0\n2     1901     3  0.0  0.0   0.0  0.0   1.2   0.0   1.3   1.9   0.6  0.0  0.0  0.0\n3     1901     4  0.0  0.0   0.0  0.0   1.2   0.0   7.6  20.5   2.5  0.0  0.0  0.0\n4     1901     5  0.0  0.0   0.0  1.9   0.0   0.0  18.7  41.4   2.6  0.0  0.0  0.0\n...    ...   ...  ...  ...   ...  ...   ...   ...   ...   ...   ...  ...  ...  ...\n4156  2019    27  0.0  0.0   0.0  0.0   0.0   4.4  12.9   1.1  10.2  6.8  0.0  0.0\n4157  2019    28  0.0  0.0   0.0  0.1   0.0   6.0   7.3   0.1   0.3  9.8  0.0  0.0\n4158  2019    29  0.0  NaN   0.0  0.0   0.0   7.5   7.5   0.6   0.8  8.3  0.0  0.0\n4159  2019    30  0.0  NaN   0.0  0.0   0.0  10.2  10.0   3.9   2.0  2.3  0.0  0.0\n4160  2019    31  0.0  NaN   0.0  NaN   0.0   NaN  15.7  24.0   NaN  4.5  NaN  1.2\n\n[3689 rows x 14 columns]\n\n</code></pre>\n",
        "question_body": "<p>I have a Textfile (.DAT) file, in which there is daily climate data of a station,</p>\n<p><a href=\"https://github.com/abhilashsinghimd/AASD_Geojson/blob/main/25_may24_SD1.DAT?raw=true\" rel=\"nofollow noreferrer\">This is the URL of Dataset</a></p>\n<pre><code>daily_data_file=r&quot;..\\25_may24_SD.DAT&quot;\n\ndf = pd.read_csv(daily_data_file, skiprows=[5], delimiter=r&quot;\\s+&quot;, names=['YEAR', 'DATE', 'JAN', 'FEB','MAR','APR','MAY','JUN','JUL','AUG','SEP','OCT','NOV','DEC'])\n\n</code></pre>\n<p>It creates the data frame but</p>\n<p>as in case,\nsome month has 31 days,\nsome have 30\nand February has either 28 or 29</p>\n<p>but as the whitespace being omitted/delimited</p>\n<p>the last 3 columns at end of each month got shifted to left of dataframe\nas in here in the output leaving NaN values at the end.</p>\n<pre><code>In [4]: df\nOut [4]: \n\n         YEAR   DATE    JAN   FEB   MAR  APR    MAY  JUN    JUL     AUG  SEP    OCT  NOV    DEC\n0        YEAR   DATE    JAN  FEB    MAR  APR    MAY  JUN    JUL     AUG  SEP    OCT  NOV    DEC\n1        1901   1       0.0  0.0    0.3  0.0    3.7  0.9    11.1    0.1  2.5    0.0  0.0    0.0\n2        1901   2       0.0  0.0    16.5 0.0    12.3 0.0    11.4    2.7  4.9    0.0  0.0    0.0\n... ... ... ... ... ... ... ... ... ... ... ... ... ... ...\n3803     2019   27      0.0  0.0    0.0  0.0    0.0  4.4    12.9    1.1  10.2   6.8  0.0    0.0\n3804     2019   28      0.0  0.0    0.0  0.1    0.0  6.0    7.3     0.1  0.3    9.8  0.0    0.0\n3805     2019   29      0.0  0.0    0.0  0.0    7.5  7.5    0.6     0.8  8.3    0.0  0.0    NaN\n3806    2019    30      0.0  0.0    0.0  0.0    10.2 10.0   3.9     2.0  2.3    0.0  0.0    NaN\n3807    2019    31      0.0  0.0    0.0  15.7   24.0 4.5    1.2     NaN  NaN    NaN  NaN    NaN\n</code></pre>\n<p>How the text file should be delimited\nso that data stays in original form\ni.e NaN value on 29th, 30th and 31st of the each month in the respective column,</p>\n<p>instead of them shifting to the left of dataframe.</p>\n<p>The format of data in text file is like this.</p>\n<p><a href=\"https://i.stack.imgur.com/4nkmL.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/4nkmL.png\" alt=\"TextFile\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 16601273,
            "reputation": 165,
            "user_id": 11996885,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dc776500f5e018aae46f741b952c77e2?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ducttape19",
            "link": "https://stackoverflow.com/users/11996885/ducttape19"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68260966,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625511989,
        "creation_date": 1625510694,
        "last_edit_date": 1625510944,
        "question_id": 68260821,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68260821/python-merge-two-pandas-dataframes-into-one-and-update-the-row-if-the-index-ex",
        "title": "Python - Merge two Pandas dataframes into one and update the row if the index exists and add a row if it doesn&amp;#39;t",
        "answer_body": "<p>You can try to 'outer' merge the 2 dataframes and fill <code>NaN</code> (non-matching entries) with <code>False</code>, as follows:</p>\n<pre><code>df1.merge(df2, how='outer').fillna(False)\n</code></pre>\n<p>Result:</p>\n<pre><code>     Name  In_Frame_1  In_Frame_2\n0    Rick        True       False\n1   Morty        True       False\n2   Jerry        True        True\n3    Beth       False        True\n4  Summer       False        True\n</code></pre>\n<p>If you want to fill <code>NaN</code> (non-matching entries) with blank instead, you can use:</p>\n<pre><code>df1.merge(df2, how='outer').fillna('')\n</code></pre>\n<p>Result:</p>\n<pre><code>     Name In_Frame_1 In_Frame_2\n0    Rick       True           \n1   Morty       True           \n2   Jerry       True       True\n3    Beth                  True\n4  Summer                  True\n</code></pre>\n",
        "question_body": "<p>I am attempting to take multiple similar dataframes and combine them to form a single frame with all the data. Each frame has a 'Name' and an 'In_Frame_#' column with a true or false value.</p>\n<p>I would like to combine all frames into a single frame that has all the names, with no duplicates, and many columns labeled as 'In_Frame_#' with the corresponding True/False or blank value.</p>\n<p>When adding to the &quot;full&quot; dataframe, If the name exists then update that row to include the value for the new column derived from the incoming frame and if the name doesn't exist then add the name and include the value for the new column derived from the incoming frame.</p>\n<p>I have been playing with update(), merge() and join() functions but haven't cracked the solution. Any help or guidance would be greatly appreciated.</p>\n<p>The tables below may help visualize my current situation.</p>\n<p>Frame one looks like:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Name</th>\n<th>In_Frame_1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Rick</td>\n<td>True</td>\n</tr>\n<tr>\n<td>Morty</td>\n<td>True</td>\n</tr>\n<tr>\n<td>Jerry</td>\n<td>True</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Frame two looks like:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Name</th>\n<th>In_Frame_2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Beth</td>\n<td>True</td>\n</tr>\n<tr>\n<td>Summer</td>\n<td>True</td>\n</tr>\n<tr>\n<td>Jerry</td>\n<td>True</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>The desired output would look like:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Name</th>\n<th>In_Frame_1</th>\n<th>In_Frame_2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Rick</td>\n<td>True</td>\n<td>{blank}</td>\n</tr>\n<tr>\n<td>Morty</td>\n<td>True</td>\n<td>{blank}</td>\n</tr>\n<tr>\n<td>Jerry</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>Beth</td>\n<td>False</td>\n<td>True</td>\n</tr>\n<tr>\n<td>Summer</td>\n<td>False</td>\n<td>True</td>\n</tr>\n</tbody>\n</table>\n</div>"
    },
    {
        "tags": [
            "python",
            "excel",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20385914,
            "reputation": 115,
            "user_id": 14954932,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/1182b2f40a3ea5b66658f71449e1e144?s=128&d=identicon&r=PG&f=1",
            "display_name": "Numpy",
            "link": "https://stackoverflow.com/users/14954932/numpy"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 68260817,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625510987,
        "creation_date": 1625510429,
        "question_id": 68260778,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68260778/skipping-last-4-rows-and-the-first-16-rows-when-reading-excel-file-into-python-p",
        "title": "Skipping last 4 rows and the first 16 rows when reading excel file into Python Pandas",
        "answer_body": "<p>You can try to use also the <code>skipfooter</code> parameter, as follows:</p>\n<pre><code>df = pd.read_excel(file, engine='openpyxl', skiprows=16, skipfooter=4, usecols = &quot;B:F&quot;)\n</code></pre>\n<blockquote>\n<p><strong>skipfooter</strong>   int, default 0<br />\nRows at the end to skip (0-indexed).</p>\n</blockquote>\n",
        "question_body": "<p>I know how to skip the first 16 rows of a excel file when reading into Pandas like</p>\n<pre><code>df = pd.read_excel(file, engine='openpyxl', skiprows=16, usecols = &quot;B:F&quot;)\n</code></pre>\n<p>But how can I skip the last 4 rows and the first 16 rows?</p>\n<p>Any suggestions?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "account_id": 1814446,
            "reputation": 52,
            "user_id": 1649843,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9e6f212946d18b7094650501880616b3?s=128&d=identicon&r=PG&f=1",
            "display_name": "user144700",
            "link": "https://stackoverflow.com/users/1649843/user144700"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68259654,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625504104,
        "creation_date": 1625503961,
        "question_id": 68259621,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68259621/creating-dictionary-from-dataframe",
        "title": "creating dictionary from dataframe",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html\" rel=\"nofollow noreferrer\"><code>groupby aggregate</code></a> + <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.to_dict.html\" rel=\"nofollow noreferrer\"><code>Series.to_dict</code></a>:</p>\n<pre><code>d = df.groupby('a')['b'].agg(list).to_dict()\n</code></pre>\n<pre><code>{1: [10], 2: [20, 30], 3: [40], 5: [50]}\n</code></pre>\n<p>Or conditionally aggregate into a <code>list</code> based on number of elements in the series to get exact output:</p>\n<pre><code>d = df.groupby('a')['b'].agg(lambda s: list(s) if len(s) &gt; 1 else s).to_dict()\n</code></pre>\n<pre><code>{1: 10, 2: [20, 30], 3: 40, 5: 50}\n</code></pre>\n",
        "question_body": "<p>I have below dataframe</p>\n<pre><code>a = [1,2,2,3,5]\nb = [10,20,30,40,50]\ndf = pd.DataFrame(list(zip(a,b)), columns=['a','b'])\n</code></pre>\n<p>I want to generate dictionary like below</p>\n<pre><code>{1:10, 2:[20,30], 3:40, 5:50}\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "boolean",
            "filtering"
        ],
        "owner": {
            "account_id": 8341947,
            "reputation": 5661,
            "user_id": 6267003,
            "user_type": "registered",
            "accept_rate": 83,
            "profile_image": "https://www.gravatar.com/avatar/4f58a4a7bf465d30bfa4a3869164b9ba?s=128&d=identicon&r=PG&f=1",
            "display_name": "obabs",
            "link": "https://stackoverflow.com/users/6267003/obabs"
        },
        "is_answered": true,
        "view_count": 1132728,
        "protected_date": 1562191958,
        "accepted_answer_id": 36922103,
        "answer_count": 10,
        "score": 539,
        "last_activity_date": 1625499526,
        "creation_date": 1461865590,
        "last_edit_date": 1615499575,
        "question_id": 36921951,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o",
        "title": "Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()",
        "answer_body": "<p>The <code>or</code> and <code>and</code> python statements require <code>truth</code>-values. For <code>pandas</code> these are considered ambiguous so you should use \"bitwise\" <code>|</code> (or) or <code>&amp;</code> (and) operations:</p>\n\n<pre><code>result = result[(result['var']&gt;0.25) | (result['var']&lt;-0.25)]\n</code></pre>\n\n<p>These are overloaded for these kind of datastructures to yield the element-wise <code>or</code> (or <code>and</code>).</p>\n\n<hr>\n\n<p>Just to add some more explanation to this statement:</p>\n\n<p>The exception is thrown when you want to get the <code>bool</code> of a <code>pandas.Series</code>:</p>\n\n<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; x = pd.Series([1])\n&gt;&gt;&gt; bool(x)\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n\n<p>What you hit was a place where the operator <strong>implicitly</strong> converted the operands to <code>bool</code> (you used <code>or</code> but it also happens for <code>and</code>, <code>if</code> and <code>while</code>):</p>\n\n<pre><code>&gt;&gt;&gt; x or x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n&gt;&gt;&gt; x and x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n&gt;&gt;&gt; if x:\n...     print('fun')\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n&gt;&gt;&gt; while x:\n...     print('fun')\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n\n<p>Besides these 4 statements there are several python functions that hide some <code>bool</code> calls (like <code>any</code>, <code>all</code>, <code>filter</code>, ...) these are normally not problematic with <code>pandas.Series</code> but for completeness I wanted to mention these.</p>\n\n<hr>\n\n<p>In your case the exception isn't really helpful, because it doesn't mention the <strong>right alternatives</strong>. For <code>and</code> and <code>or</code> you can use (if you want element-wise comparisons):</p>\n\n<ul>\n<li><p><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_or.html\"><code>numpy.logical_or</code></a>:</p>\n\n<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.logical_or(x, y)\n</code></pre>\n\n<p>or simply the <code>|</code> operator:</p>\n\n<pre><code>&gt;&gt;&gt; x | y\n</code></pre></li>\n<li><p><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_and.html\"><code>numpy.logical_and</code></a>:</p>\n\n<pre><code>&gt;&gt;&gt; np.logical_and(x, y)\n</code></pre>\n\n<p>or simply the <code>&amp;</code> operator:</p>\n\n<pre><code>&gt;&gt;&gt; x &amp; y\n</code></pre></li>\n</ul>\n\n<p>If you're using the operators then make sure you set your parenthesis correctly because of <a href=\"https://docs.python.org/reference/expressions.html#operator-precedence\">the operator precedence</a>.</p>\n\n<p>There are <a href=\"https://docs.scipy.org/doc/numpy/reference/routines.logic.html\">several logical numpy functions</a> which <em>should</em> work on <code>pandas.Series</code>.</p>\n\n<hr>\n\n<p>The alternatives mentioned in the Exception are more suited if you encountered it when doing <code>if</code> or <code>while</code>. I'll shortly explain each of these:</p>\n\n<ul>\n<li><p>If you want to check if your Series is <strong>empty</strong>:</p>\n\n<pre><code>&gt;&gt;&gt; x = pd.Series([])\n&gt;&gt;&gt; x.empty\nTrue\n&gt;&gt;&gt; x = pd.Series([1])\n&gt;&gt;&gt; x.empty\nFalse\n</code></pre>\n\n<p>Python normally interprets the <code>len</code>gth of containers (like <code>list</code>, <code>tuple</code>, ...) as truth-value if it has no explicit boolean interpretation. So if you want the python-like check, you could do: <code>if x.size</code> or <code>if not x.empty</code> instead of <code>if x</code>.</p></li>\n<li><p>If your <code>Series</code> contains <strong>one and only one</strong> boolean value:</p>\n\n<pre><code>&gt;&gt;&gt; x = pd.Series([100])\n&gt;&gt;&gt; (x &gt; 50).bool()\nTrue\n&gt;&gt;&gt; (x &lt; 50).bool()\nFalse\n</code></pre></li>\n<li><p>If you want to check the <strong>first and only item</strong> of your Series (like <code>.bool()</code> but works even for not boolean contents):</p>\n\n<pre><code>&gt;&gt;&gt; x = pd.Series([100])\n&gt;&gt;&gt; x.item()\n100\n</code></pre></li>\n<li><p>If you want to check if <strong>all</strong> or <strong>any</strong> item is not-zero, not-empty or not-False:</p>\n\n<pre><code>&gt;&gt;&gt; x = pd.Series([0, 1, 2])\n&gt;&gt;&gt; x.all()   # because one element is zero\nFalse\n&gt;&gt;&gt; x.any()   # because one (or more) elements are non-zero\nTrue\n</code></pre></li>\n</ul>\n",
        "question_body": "<p>Having issue filtering my result dataframe with an <code>or</code> condition. I want my result <code>df</code> to extract all column <code>var</code> values that are above 0.25 and below -0.25.</p>\n<p>This logic below gives me an ambiguous truth value however it work when I split this filtering in two separate operations. What is happening here? not sure where to use the suggested <code>a.empty(), a.bool(), a.item(),a.any() or a.all()</code>.</p>\n<pre><code>result = result[(result['var'] &gt; 0.25) or (result['var'] &lt; -0.25)]\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "conditional-statements"
        ],
        "owner": {
            "account_id": 16495657,
            "reputation": 351,
            "user_id": 11918314,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e0527f96ed53dc456ed90a908d5904aa?s=128&d=identicon&r=PG&f=1",
            "display_name": "wjie08",
            "link": "https://stackoverflow.com/users/11918314/wjie08"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68258405,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625499046,
        "creation_date": 1625497324,
        "question_id": 68258281,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68258281/creating-a-new-column-based-on-conditions-of-first-and-last-value-of-groupby-pyt",
        "title": "Creating a new column based on conditions of first and last value of groupby Python",
        "answer_body": "<pre><code>df2 = df.groupby('email', as_index=False).nth([0,-1])\n</code></pre>\n<p>You can try:</p>\n<pre><code>d={'mgr:mgr':'hired as mgr','emp:mgr':'promoted to mgr','emp:emp':'hired as emp','mgr:emp':'status change'}\n#created a dict for mapping\n</code></pre>\n<p>Finally:</p>\n<pre><code>df2.loc[:,'status']=df2.groupby('email')['level'].transform(':'.join).map(d)\n</code></pre>\n<p>output of <code>df2</code>:</p>\n<pre><code>    date        email           level   status\n0   01/01/2000  john@abc.com    mgr     hired as mgr\n2   10/01/2001  john@abc.com    mgr     hired as mgr\n3   14/02/2000  kimdo@abc.com   emp     promoted to mgr\n4   19/10/2001  kimdo@abc.com   mgr     promoted to mgr\n5   12/05/2000  waint@abc.com   emp     hired as emp\n7   14/04/2001  waint@abc.com   emp     hired as emp\n8   22/05/2000  neds@abc.com    mgr     status change\n10  12/06/2001  neds@abc.com    emp     status change\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe and would like to create a new column with values based on conditions from the first and last row of a groupby. The conditions required are</p>\n<p>mgr to mgr = hired as mgr</p>\n<p>emp to mgr = promoted to mgr</p>\n<p>emp to emp = hired as emp</p>\n<p>mgr to emp = status change</p>\n<pre><code>date        email          level \n01/01/2000  john@abc.com   mgr\n05/06/2000  john@abc.com   mgr     \n10/01/2001  john@abc.com   mgr     \n14/02/2000  kimdo@abc.com  emp     \n19/10/2001  kimdo@abc.com  mgr     \n12/05/2000  waint@abc.com  emp  \n08/08/2000  waint@abc.com  emp  \n14/04/2001  waint@abc.com  emp     \n22/05/2000  neds@abc.com   mgr\n08/11/2000  neds@abc.com   mgr     \n12/06/2001  neds@abc.com   emp\n</code></pre>\n<p>Would like to achieve the results below</p>\n<pre><code>date        email          level   status\n01/01/2000  john@abc.com   mgr     hired as mgr\n10/01/2001  john@abc.com   mgr     hired as mgr\n14/02/2000  kimdo@abc.com  emp     promoted to mgr\n19/10/2001  kimdo@abc.com  mgr     promoted to mgr\n12/05/2000  waint@abc.com  emp     hired as emp\n14/04/2001  waint@abc.com  emp     hired as emp\n22/05/2000  neds@abc.com   mgr     status change\n12/06/2001  neds@abc.com   emp     status change\n</code></pre>\n<p>So far, am able to select the first and last rows of the dataframe based on a groupyby, but am not entirely sure how to apply the conditions to get the new 'status' column. Appreciate any form of help, thank you.</p>\n<pre><code>df2 = df.groupby('email', as_index=False).nth([0,-1])\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "account_id": 21595285,
            "reputation": 17,
            "user_id": 15924357,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a70f8cf5314596d6c471047d9efd8d87?s=128&d=identicon&r=PG&f=1",
            "display_name": "an10b3",
            "link": "https://stackoverflow.com/users/15924357/an10b3"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 68257044,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1625493928,
        "creation_date": 1625490830,
        "question_id": 68256776,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68256776/extract-and-subset-dictionary-from-pandas-dataframe-in-correct-format-python",
        "title": "extract and subset dictionary from pandas dataframe in correct format - python",
        "answer_body": "<p>We can use <code>findall</code> to extract the values corresponding to the key <code>q</code> in each list of dict like strings, then using <code>to_dict</code> convert the series to dictionary</p>\n<pre><code>df_dic.set_index('b')['a'].str.findall(r&quot;'q': '(.*?)'&quot;).to_dict()\n</code></pre>\n<hr />\n<pre><code>{'s1': ['male', 'female'], 's2': ['employed', 'unemployed']}\n</code></pre>\n",
        "question_body": "<p>I have dictionary information stored in a column in a pandas dataframe. The dataframe is set to string type. I need to extract the dictionary and subset it to create a new dictionary.</p>\n<p>here is a command that works just fine if using a <em>normal</em> dictionary: to select specific fields from pairs to create a new dictionary</p>\n<pre><code># initial dictionary \ndic = {'s1': [{'q': 'male','id':'1'},{'q':'female','id':'2'}], 's2' : [{'q':'employed'},{'q':'unemployed'}]}\n\n# dic output \n{'s1': [{'q': 'male', 'id': '1'}, {'q': 'female', 'id': '2'}],\n 's2': [{'q': 'employed'}, {'q': 'unemployed'}]}\n\n# subset dictionary to first item in pair \nnew_dic = {k: [x.get(&quot;q&quot;) for x in v] for k, v in dic.items()}\n\n# new_dic output \n{'s1': ['male', 'female'], 's2': ['employed', 'unemployed']}\n</code></pre>\n<p>However I need to extract and produce the same output as above from a dictionary that is stored inside a dataframe, rather than the original dict. The df looks like this:</p>\n<pre><code>df_dic = pd.DataFrame({'a': [([{'q': 'male','id':'1'},{'q':'female','id':'2'}]), ([{'q': 'employed','id':'1'},{'q':'unemployed','id':'2'}]) ], 'b': ['s1', 's2']}).applymap(str)\n\n# create dictionary from dataframe \ndic_from_df = columns = dict(zip(df_dic['b'],df_dic['a']))\n\n# output \n{'s1': &quot;[{'q': 'male', 'id': '1'}, {'q': 'female', 'id': '2'}]&quot;,\n 's2': &quot;[{'q': 'employed', 'id': '1'}, {'q': 'unemployed', 'id': '2'}]&quot;}\n</code></pre>\n<p>Here i extract the information from the dataframe in to a dictionary which works fine (dic_from_df) - and the expected output is exactly the same as the output in new_dic. The dictionary stored inside the df is in a string format (i cannot help this - this is just the way it is due to some intermediate processing).</p>\n<p>So when i run the same command ({k: [x.get(&quot;q&quot;) for x in v] for k, v in dic.items()}) on dic_from_df, i get the error ''str' object has no attribute 'get''...I am really stuck! Im not sure if its because it is a string, or the apostrophes (i can not strip them either..) so any comments are super welcome. thanks so much</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 8353497,
            "reputation": 166,
            "user_id": 6274697,
            "user_type": "registered",
            "accept_rate": 92,
            "profile_image": "https://www.gravatar.com/avatar/29fe62ad6f1c1ce75085dbecc0efb192?s=128&d=identicon&r=PG&f=1",
            "display_name": "gabor aron",
            "link": "https://stackoverflow.com/users/6274697/gabor-aron"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 68245555,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625490521,
        "creation_date": 1625407338,
        "question_id": 68245481,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68245481/how-to-merge-two-excel-columns-in-python-using-colab",
        "title": "how to merge two excel columns in python using colab",
        "answer_body": "<p>Try via <code>vstack()</code>:</p>\n<pre><code>out=pd.DataFrame(np.vstack((ds1.columns[2:4].values,ds2[ds2.columns[2:4]].values)))\n</code></pre>\n<p><strong>OR</strong></p>\n<p>via <code>concatenate()</code>:</p>\n<pre><code>out=pd.DataFrame(np.concatenate((ds1.columns[2:4].values,ds2[ds2.columns[2:4]].values)))\n</code></pre>\n<p><strong>OR</strong></p>\n<pre><code>out=ds1[ds1.columns[2:4]].append(ds2[ds2.columns[2:4]]).T.agg(sorted,key=pd.isnull).dropna().T\n</code></pre>\n<p><strong>OR</strong></p>\n<p>You can also rename the name of columns of any 1 dataset so that both subset of df's has same name then use <code>concat()</code> or <code>append()</code> them</p>\n",
        "question_body": "<p>I'm working on a data-oriented project, we have some cancer measurements, and want to classify with K-means algorithms.</p>\n<p>Now I have two basic example datasets, with two-two columns, but the K-means algorithms need only 2 columns, so I decided to concatenate the columns, but how can I do it?</p>\n<p>For example fst dataset looks like this:</p>\n<pre><code>0   2713.9  566.42\n1   2718.9  566.42\n2   2723.3  566.25\n3   2729.5  565.99\n4   2735.9  565.83\n</code></pre>\n<p>the snd one looks like this:</p>\n<pre><code>0   6571.5  959.12\n1   6571.6  959.13\n2   6571.7  959.12\n3   6571.7  959.16\n4   6571.7  959.15\n</code></pre>\n<p>And I want something like this (without the row number of course):</p>\n<pre><code>0   2713.9  566.42\n1   2718.9  566.42\n2   2723.3  566.25\n3   2729.5  565.99\n4   2735.9  565.83\n0   6571.5  959.12\n1   6571.6  959.13\n2   6571.7  959.12\n3   6571.7  959.16\n4   6571.7  959.15\n</code></pre>\n<p>I tried with this:</p>\n<pre><code>X = ds1[ds1.columns[2:4]].append(ds2[ds2.columns[2:4]])\nX\n</code></pre>\n<p>and got this:</p>\n<pre><code>0   2713.9  566.42  NaN     NaN\n1   2718.9  566.42  NaN     NaN\n2   2723.3  566.25  NaN     NaN\n3   2729.5  565.99  NaN     NaN\n4   2735.9  565.83  NaN     NaN\n...     ...     ...     ...     ...\n44  NaN     NaN     6571.8  959.01\n45  NaN     NaN     6571.7  959.00\n46  NaN     NaN     6571.7  958.98\n47  NaN     NaN     6571.5  959.00\n48  NaN     NaN     6571.4  959.01\n</code></pre>\n<p>Also got this with this code:</p>\n<pre><code>X = pd.concat([ds1[ds1.columns[2:4]], ds2[ds2.columns[2:4]]],  axis=0, join='outer', ignore_index=False)\n</code></pre>\n<p>How can I do this? Is there any method for this, or I have to transform the data in Excel?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "python-2.7",
            "numpy"
        ],
        "owner": {
            "account_id": 22123152,
            "reputation": 3,
            "user_id": 16376014,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GiKtVuqMebnhAxat8d_G6C0dHJBJd5czBZbWZaySIA=k-s128",
            "display_name": "saurabh bulandani",
            "link": "https://stackoverflow.com/users/16376014/saurabh-bulandani"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 68243414,
        "answer_count": 1,
        "score": -2,
        "last_activity_date": 1625487932,
        "creation_date": 1625389413,
        "last_edit_date": 1625487932,
        "question_id": 68243139,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68243139/extract-a-column-data-with-input-command-in-a-csv-file-in-python",
        "title": "Extract a column data with Input command in a csv file in python",
        "answer_body": "<pre><code>import pandas as pd\ndf = pd.read_csv(&quot;example.csv&quot;)\nx1 = input(&quot;write the column name?&quot;)\n</code></pre>\n<p>if x1 in df.columns This code will work correctly:</p>\n<pre><code>value_mean = df[x1].mean()\nvalue_std = df[x1].std()\n</code></pre>\n<p>You can use the above code for any number of inputs.\nYou can also use loops to get high input and operation:</p>\n<pre><code>for i in range(2):\n    x = input(&quot;column name?&quot;)\n    mean = df[x].mean()\n    std = df[x].std()\n</code></pre>\n",
        "question_body": "<p>I need to extract column data with user defined column name through Input command. user should enter the column name and accordingly that particular column data is displayed or processed for further calculation.</p>\n<p>Here is my code.</p>\n<pre><code># Importing required libraries\nfrom scipy.stats import norm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sb\n  \n# Read the data into a series\ndf=pd.read_csv(&quot;Modified.csv&quot;)\n\n#Input Column Name\ncol1=str(input(&quot;Enter Column Name (Case sensitive) :  &quot;))\ncol2=str(input(&quot;Enter Column Name (Case sensitive) :  &quot;))\n\n# Creating a series of data \nx=df[['col1']]\ny=df[['col2']]\n\n#Calculate mean and Standard deviation.\n\nmean_x = np.mean(x)\nsd_x = np.std(x)\n\nmean_y = np.mean(y)\nsd_y = np.std(y)\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20385914,
            "reputation": 115,
            "user_id": 14954932,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/1182b2f40a3ea5b66658f71449e1e144?s=128&d=identicon&r=PG&f=1",
            "display_name": "Numpy",
            "link": "https://stackoverflow.com/users/14954932/numpy"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68255169,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625484570,
        "creation_date": 1625483478,
        "question_id": 68255137,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68255137/replace-value-in-a-dataframe-column-if-value-starts-with-a-specific-character",
        "title": "Replace value in a dataframe column if value starts with a specific character",
        "answer_body": "<p>Try:</p>\n<pre><code>m=df['ISIN'].str.startswith('_')\n#your condition\n</code></pre>\n<p>Finally:</p>\n<pre><code>df.loc[m,'ISIN']='Cash'\n</code></pre>\n<p>OR</p>\n<p>via numpy's <code>where()</code> method</p>\n<pre><code>#import numpy as np\ndf['ISIN']=np.where(m,'Cash',df['ISIN'])\n</code></pre>\n<p>OR</p>\n<p>via <code>mask()</code> method:</p>\n<pre><code>df['ISIN']=df['ISIN'].mask(m,'Cash')\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe:</p>\n<pre><code>df = pd.DataFrame({'ISIN': ['_A1kT23', '4523', '_Currency', 'NI49O33'], 'Name': ['Example A', 'Name Xy', 'Example B', 'Test123'], 'Debt_Equity': [-65.56, 0.55, 0, 37], 'EV_Sales': [9.28, 0.53, 11.3, 45], 'Bestand': ['', '4523', 'B333', '']})\n</code></pre>\n<p>I would like to change the value of ISIN to &quot;Cash&quot; if it starts with a &quot;_&quot; so that I get the the new dataframe:</p>\n<pre><code>df = pd.DataFrame({'ISIN': ['Cash', '4523', 'Cash', 'NI49O33'], 'Name': ['Example A', 'Name Xy', 'Example B', 'Test123'], 'Debt_Equity': [-65.56, 0.55, 0, 37], 'EV_Sales': [9.28, 0.53, 11.3, 45], 'Bestand': ['', '4523', 'B333', '']})\n</code></pre>\n<p>I tried it by coding</p>\n<pre><code>df['ISIN'] = df['ISIN'].replace({'_':'Cash'})\n</code></pre>\n<p>but this works only if the value is exactly '_' and not if it starts with that specific character.\nCan anyone help out please?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 4823544,
            "reputation": 624,
            "user_id": 3893580,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/02bbf5b1f688a462c3a076210ea9efb3?s=128&d=identicon&r=PG&f=1",
            "display_name": "Caerus",
            "link": "https://stackoverflow.com/users/3893580/caerus"
        },
        "is_answered": true,
        "view_count": 708,
        "accepted_answer_id": 53662762,
        "answer_count": 2,
        "score": 6,
        "last_activity_date": 1625482960,
        "creation_date": 1544152663,
        "last_edit_date": 1561690552,
        "question_id": 53662717,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53662717/groupby-two-columns-ignoring-order-of-pairs",
        "title": "Groupby two columns ignoring order of pairs",
        "answer_body": "<p><code>sort</code> the first two columns (you can do this in-place, or create a copy and do the same thing; I've done the former), then <code>groupby</code> and <code>agg</code>:</p>\n\n<pre><code>df[['start', 'stop']] = np.sort(df[['start', 'stop']], axis=1)\n\n(df.groupby(['start','stop'])\n   .duration\n   .agg(['count', 'mean'])\n   .reset_index()\n   .values\n   .tolist())\n# [['A', 'B', 2, 1.5], ['C', 'D', 2, 1.0]]\n</code></pre>\n",
        "question_body": "<p>Suppose we have a dataframe that looks like this:</p>\n\n<pre><code>    start   stop   duration\n0   A       B      1\n1   B       A      2\n2   C       D      2\n3   D       C      0\n</code></pre>\n\n<p>What's the best way to construct a list of: i) start/stop pairs; ii) count of start/stop pairs; iii) avg duration of start/stop pairs? In this case, order should not matter: <code>(A,B)=(B,A)</code>.</p>\n\n<p>Desired output: <code>[[start,stop,count,avg duration]]</code></p>\n\n<p>In this example: <code>[[A,B,2,1.5],[C,D,2,1]]</code></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 17258259,
            "reputation": 235,
            "user_id": 12496869,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c07accea77bc7cb3a24f5822d39e3a6f?s=128&d=identicon&r=PG&f=1",
            "display_name": "Sun",
            "link": "https://stackoverflow.com/users/12496869/sun"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 68254672,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1625482716,
        "creation_date": 1625481124,
        "question_id": 68254602,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68254602/replace-value-in-the-dataframe-with-valure-from-dictionary-using-pandas",
        "title": "replace value in the dataframe with valure from dictionary using pandas",
        "answer_body": "<p>Use <code>.replace()</code>.</p>\n<pre><code>df['corrected_name'] = df['movieName'].replace(cor_dict)\n</code></pre>\n",
        "question_body": "<p>I have the dataframe below:</p>\n<pre><code>          movieName  Year\n            Iron Man  2008\n  X-Men Dark Phoenix  2014\n      Doctor Strange  2007\n          Spider-Man  2014\n   x-Men First class  2006\n     Captain America  2011\n</code></pre>\n<p>And i have this dictionary</p>\n<pre><code>cor_dict = {'Spider-Man':'The amazing Spider-Man', 'X-Men Dark Phoenix': 'X-Men Days of future',\n'x-Men First class':'x-Men:The last stand' }\n</code></pre>\n<p>I want to replace the name in the df with the value of dictionary Like this:</p>\n<pre><code> 'Spider-Man'=&gt;'The amazing Spider-Man', \n     X-Men Dark Phoenix' =&gt; 'X-Men Days of future'\n    'x-Men First class'=&gt;'x-Men:The last stand\n</code></pre>\n<p>to get :</p>\n<pre><code>            movieName          corrected_name  Year\n            Iron Man                Iron Man  2008\n  X-Men Dark Phoenix    X-Men Days of future  2014\n      Doctor Strange          Doctor Strange  2007\n          Spider-Man  The amazing Spider-Man  2014\n   x-Men First class    x-Men:The last stand  2006\n     Captain America         Captain America  2011\n</code></pre>\n<p><strong>NB: They don't have the same order</strong></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "for-loop",
            "counter"
        ],
        "owner": {
            "account_id": 22129406,
            "reputation": 13,
            "user_id": 16381169,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f6de7bb6e1108906745b427255978fa3?s=128&d=identicon&r=PG&f=1",
            "display_name": "Skuller",
            "link": "https://stackoverflow.com/users/16381169/skuller"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68254307,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625482151,
        "creation_date": 1625478790,
        "last_edit_date": 1625482151,
        "question_id": 68254048,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68254048/counter-for-consecutive-negative-values-in-a-data-frame",
        "title": "Counter for consecutive negative values in a Data-frame",
        "answer_body": "<p>I am pretty unsure how to write the &quot;Nan&quot; (not very great myself), but here is a code that seems to do what you asked for:</p>\n<pre><code>df = pd.DataFrame()\ndf[&quot;data&quot;] = [-1, -2, 4, 12, -22, -12, -7, -5, -22, 2, 2]\ndef generateOutput(df):\n    a = [0]\n    for i in range(len(df) - 1):\n        if df[&quot;data&quot;][i] &lt; 0:\n            a.append(a[-1] + 1)\n        else:\n            a.append(0)\n    df[&quot;output&quot;] = a\n    return df\n\nprint(df)\ndf = generateOutput(df)\nprint(df)\n</code></pre>\n<p>And here is my output when launched the program</p>\n<pre><code>    data\n0     -1\n1     -2\n2      4\n3     12\n4    -22\n5    -12\n6     -7\n7     -5\n8    -22\n9      2\n10     2\n</code></pre>\n<pre><code>    data  output\n0     -1       0\n1     -2       1\n2      4       2\n3     12       0\n4    -22       0\n5    -12       1\n6     -7       2\n7     -5       3\n8    -22       4\n9      2       5\n10     2       0\n</code></pre>\n",
        "question_body": "<p>I need to implement a counter, which does the counting as shown in the below OUTPUT. It checks the past values of &quot;data&quot; column for negative values.</p>\n<pre><code>    data    output\n0   -1      Nan        //  since there are no past values for data: count=NaN \n1   -2       1         //-1, so count= 1\n2    4       2         //-2,-1   count=2\n3    12      0         //         count=0\n4   -22      0         //         count=0    \n5   -12      1         //-22      count=1          \n6   -7       2         // -22,-12   count=2     \n7   -5       3         // -7,-22,-12    count=3\n8   -33      4         // -5,7,-22,-12    count=4\n9    2       5         // -33,-5,7,-22,-12    count=5\n10   2       1         //        count=0\n</code></pre>\n<h3>MY CODE</h3>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport talib\nimport numpy as np     \n\ndf=pd.DataFrame()\ndf[&quot;data&quot;]=[-1,-2,4,12,-22,-12,-7,-5,-33,2,2]\nprint(df)\n\n\nc=0\nfor y in [0,len(ff)-1] : \n    for z in [1,10]:\n        if (ff[&quot;data&quot;].shift(-z)).any()&lt;=0:c=c+1\n        else:c\n        if (ff[&quot;data&quot;].shift(-z)).any()&gt;0:break\n    count[&quot;dd&quot;]=c\n</code></pre>\n<h3>OUTPUT needed:</h3>\n<p><a href=\"https://i.stack.imgur.com/pwlJa.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/pwlJa.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21885148,
            "reputation": 23,
            "user_id": 16172873,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f59cd8a15488cdcebe6715889c8fb150?s=128&d=identicon&r=PG&f=1",
            "display_name": "hahilas",
            "link": "https://stackoverflow.com/users/16172873/hahilas"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68254407,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1625481711,
        "creation_date": 1625479867,
        "last_edit_date": 1625479962,
        "question_id": 68254321,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68254321/how-do-i-convert-a-list-of-dictionaries-formatted-this-way-to-a-dataframe-lookin",
        "title": "How do I convert a list of dictionaries formatted this way to a dataframe looking like as shown below? (without hard-coding)",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>lst = [\n    {&quot;Data&quot;: [{&quot;VarCharValue&quot;: &quot;date&quot;}, {&quot;VarCharValue&quot;: &quot;clientid&quot;}]},\n    {&quot;Data&quot;: [{&quot;VarCharValue&quot;: &quot;20200203&quot;}, {&quot;VarCharValue&quot;: &quot;123457&quot;}]},\n    {&quot;Data&quot;: [{&quot;VarCharValue&quot;: &quot;20200202&quot;}, {&quot;VarCharValue&quot;: &quot;123456&quot;}]},\n]\n\ndf = pd.DataFrame(\n    [[v[&quot;VarCharValue&quot;] for v in d[&quot;Data&quot;]] for d in lst[1:]],\n    columns=[v[&quot;VarCharValue&quot;] for v in lst[0][&quot;Data&quot;]],\n)\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>       date clientid\n0  20200203   123457\n1  20200202   123456\n</code></pre>\n",
        "question_body": "<p>I am trying to convert data originally formatted as a list of dictionaries shown as below:</p>\n<pre><code>[{'Data': [{'VarCharValue': 'date'}, {'VarCharValue': 'clientid'}]},\n {'Data': [{'VarCharValue': '20200203'}, {'VarCharValue': '123457'}]},\n {'Data': [{'VarCharValue': '20200202'}, {'VarCharValue': '123456'}]}]\n</code></pre>\n<p>To a\n<a href=\"https://i.stack.imgur.com/NBxvG.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NBxvG.png\" alt=\"enter image description here\" /></a></p>\n<p>whereby the 'date' and 'clientid' are column headers.</p>\n<p>Would greatly appreciate if someone could help me out with this!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21977797,
            "reputation": 124,
            "user_id": 16253345,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/4227312227290363/picture?type=large",
            "display_name": "Ahmed Chater",
            "link": "https://stackoverflow.com/users/16253345/ahmed-chater"
        },
        "is_answered": true,
        "view_count": 52,
        "accepted_answer_id": 68230825,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1625480616,
        "creation_date": 1625253420,
        "last_edit_date": 1625479459,
        "question_id": 68230315,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68230315/pandas-replace-blank-field-only-with-quotnaquot-in-a-specific-column-mixed",
        "title": "Pandas: Replace blank field only with &amp;quot;Na&amp;quot; in a specific column mixed with float objects and blank strings",
        "answer_body": "<p>As your <code>id</code> column is mixed with float objects and blank fields, and assume you don't want to change the float objects to strings, you can use <code>.replace()</code> with regex, as follows:</p>\n<pre><code>df['id'] = df['id'].replace(r'^\\s*$', 'Na', regex=True)\n</code></pre>\n<p><strong>Explanation:</strong></p>\n<p>Regex <code>^\\s*$</code> matches for zero or more white space(s) <code>\\s</code> in the whole strings.  Thus, it matches empty string (zero white space), one space character, two space characters, etc.  It replaces with only one <code>Na</code> no matter how many white spaces matched (e.g. won't replace with <code>NaNa</code> even with 2 space characters).</p>\n<p><code>^</code>   Start of string anchor (together with <code>$</code> to signify matching for the whole string)</p>\n<p><code>\\s</code>  White space</p>\n<p><code>*</code>   Zero or more repetition of the character preceding it (<code>\\s</code>).</p>\n<p><code>$</code>   End of string anchor</p>\n<p><strong>Result:</strong></p>\n<pre><code>print(df)\n\n    id      cars rent sale\n0  123       Kia         2\n1  345       Bmw    1    4\n2   Na  Mercedes         1\n3  345      Ford    1     \n4   Na      Audi    2    1\n</code></pre>\n",
        "question_body": "<p><strong>I have this dataframe:</strong></p>\n<pre><code>    id      cars  rent  sale\n0  123       Kia           2\n1  345       Bmw     1     4\n2         Mercedes         1\n3  345      Ford     1     \n4           Audi     2     1\n</code></pre>\n<p>I want to fill the blank field only in <strong>the column id with &quot;Na&quot;</strong> and leave the blank fiekd in the others columns(rent/Sale)\nAny suggestions please?</p>\n<p><strong>Expected output:</strong></p>\n<pre><code>   id      cars  rent  sale\n0  123       Kia           2\n1  345       Bmw     1     4\n2   Na  Mercedes           1\n3  345      Ford     1     \n4   Na      Audi     2     1\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "performance"
        ],
        "owner": {
            "account_id": 22130338,
            "reputation": 3,
            "user_id": 16381970,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/831bd77f548a13dfbe8f788d2df40d24?s=128&d=identicon&r=PG&f=1",
            "display_name": "borris",
            "link": "https://stackoverflow.com/users/16381970/borris"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68253839,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625478436,
        "creation_date": 1625475194,
        "last_edit_date": 1625475813,
        "question_id": 68253277,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68253277/how-to-count-the-match-number-between-two-dataframe-fast",
        "title": "how to count the match number between two dataframe fast?",
        "answer_body": "<p>You can use <strong>broadcasting</strong> instead of concatenating the columns:</p>\n<pre><code>def win_prb_func(df1, p_name):\n    df1['match_num'] += np.sum(df1[p_name].values[:, np.newaxis] == df1[open_ball_name_ls].values, 1)\n    return df1\n</code></pre>\n<p>Since <code>df1[p_name].values</code> will return an 1-D array, you have to convert it into the column vector by adding a new axis. It only takes me <code>0.004</code> second.</p>\n",
        "question_body": "<p>I'm writing some programs on calculate the match item number between two dataframes.</p>\n<p>for example,</p>\n<pre><code>A is the dataframe as : A = pd.DataFrame({'pick_num1':[1, 2, 3], 'pick_num2':[2, 3, 4], 'pick_num3':[4, 5, 6]})\n\n\nB is the answer I want to match, like:\nB = pd.DataFrame({'ans_num1':[1, 2, 3], 'ans_num2':[2, 3, 4], 'ans_num3':[4, 5, 6], 'ans_num4':[7, 8, 1], 'ans_num5':[9, 1, 9]})\n\n\n</code></pre>\n<pre><code>DataFrame A\n   pick_num1  pick_num2  pick_num3  match_num\n0          1          2          4          2\n1          2          3          5          2\n2          3          4          6          2\n</code></pre>\n<pre><code>DataFrame B\n\n   ans_num1  ans_num2  ans_num3  ans_num4  ans_num5\n0         1         2         4         7         9\n1         2         3         5         8         1\n2         3         4         6         1         9\n</code></pre>\n<p>and I want to append a new column of ['match_num'] at the end of A.</p>\n<p>Now I have tried to write a mapping function to compare and calculate, and I found the speed is not that fast while the dataframe is huge, the functions are below:</p>\n<pre><code>def win_prb_func(df1, p_name):\n    df1['match_num'] += np.sum(pd.concat([df1[p_name]]*5, axis=1).values==df1[open_ball_name_ls].values, 1)\n    return df1\n\ndef compute_win_prb(df1):\n    return list(map(lambda p_name: win_prb_func(df1, p_name), pick_name_ls))\n\ndf1 = pd.concat([A, B], axis=1)\ndf1['win prb.'] = 0\nresult_df = compute_win_prb(df1)\n</code></pre>\n<p>where <code>pick_name_ls</code> is ['pick_num1', 'pick_num2', 'pick_num3'], and <code>open_ball_name_ls</code> is ['ans_num1', 'ans_num2', 'ans_num3', 'ans_num4', 'ans_num5'].</p>\n<p>I'm wondering is it possible to make the computation more fast or smart than I did?</p>\n<p>now the performance would is: 0.015626192092895508 seconds</p>\n<p>Thank you for helping me!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21977797,
            "reputation": 124,
            "user_id": 16253345,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/4227312227290363/picture?type=large",
            "display_name": "Ahmed Chater",
            "link": "https://stackoverflow.com/users/16253345/ahmed-chater"
        },
        "is_answered": true,
        "view_count": 29,
        "closed_date": 1625506910,
        "accepted_answer_id": 68253509,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625477647,
        "creation_date": 1625475500,
        "last_edit_date": 1625477611,
        "question_id": 68253348,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68253348/replace-zero-with-index-value-using-pandas",
        "closed_reason": "Needs more focus",
        "title": "replace zero with index value using pandas",
        "answer_body": "<ul>\n<li>Create a new column for what you want using <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\" rel=\"nofollow noreferrer\"><code>pandas.DataFrame.apply</code></a>:\n<pre class=\"lang-py prettyprint-override\"><code>df['new_id'] = df.apply(lambda row: row.id or row.name, axis=1)\n</code></pre>\n</li>\n<li>Use the new column to compare with another.</li>\n<li>Drop the new column using <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\" rel=\"nofollow noreferrer\"><code>pandas.DataFrame.drop</code></a> if you don't need it anymore.\n<pre class=\"lang-py prettyprint-override\"><code>df.drop(columns=['new_id'], inplace=True)\n</code></pre>\n</li>\n</ul>\n",
        "question_body": "<p>I have a dataframe df1:</p>\n<pre><code>    ref   Name   id  Score\n4  8400   John    0     12\n8  3840  Peter  414      0\n15 7400  David  612     64\n24 5200  Karen    0      0\n</code></pre>\n<p>I want to replace 0 in the <strong>id column</strong> with value of <strong>their index(4,24)</strong> of the same row So it will become:</p>\n<pre><code>   ref    Name   id   Score\n  8400   John  4       12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen  24      0\n</code></pre>\n<p>Then I want to compare it with another df based on id value, So Aafter that i need to remove again the value of the index putted in id. In order to go back to the initial value of <strong>the column id</strong></p>\n<p><strong>the final output</strong> will be:</p>\n<pre><code>     ref    Name   id   Score   best_id\n      8400   John   0     12    8734\n      3840  Peter  414    0    7364\n      7400  David  612    64    4367\n      5200  Karen   0     0     3467\n</code></pre>\n<p>UPDATE: Here is the <strong>df ref</strong></p>\n<pre><code>    Name      ID\n    John     8734\n    Peter   7364\n    David    4367\n    Karen    3467\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-cleaning"
        ],
        "owner": {
            "account_id": 19241890,
            "reputation": 45,
            "user_id": 14061717,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1961942067270739/picture?type=large",
            "display_name": "Micha\u0142 Gosk",
            "link": "https://stackoverflow.com/users/14061717/micha%c5%82-gosk"
        },
        "is_answered": true,
        "view_count": 62,
        "accepted_answer_id": 63289635,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1625449169,
        "creation_date": 1596737578,
        "last_edit_date": 1596738486,
        "question_id": 63289454,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63289454/pandas-convert-list-of-list-to-columns-names-and-append-values",
        "title": "Pandas convert list of list to columns names and append values",
        "answer_body": "<p>Try using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.explode.html\" rel=\"nofollow noreferrer\"><code>explode</code></a> and reshaping the dataframe.</p>\n<pre><code>#pandas 1.3.0 update\ndf_new = example.explode(['col1', 'col2'])\n#df_new = example.apply(pd.Series.explode)    \ndf_new.set_index('col1', append=True).unstack()\n</code></pre>\n<p>Output:</p>\n<pre><code>col1    key1    key2    key3    key4    key5\n0     value1  value2  value3     NaN     NaN\n1     value1     NaN     NaN  value4     NaN\n2     value1     NaN  value3  value4  value5\n</code></pre>\n",
        "question_body": "<p>I have to columns in pandas dataframe, one with keys second with values, where both are list of lists.</p>\n<p>Like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd \nexample = pd.DataFrame( {'col1': [['key1','key2','key3'],['key1','key4'],['key1', 'key3', 'key4','key5']], 'col2': [['value1','value2','value3'], ['value1','value4'], ['value1', 'value3', 'value4','value5']]  }) \nprint(example)\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>    col1    col2\n0   [key1, key2, key3]  [value1, value2, value3]\n1   [key1, key4]    [value1, value4]\n2   [key1, key3, key4, key5]    [value1, value3, value4, value5]\n</code></pre>\n<p>First i want to convert all possible keys to columns, the append values to them.\nFinal result should look like this</p>\n<pre><code>    key1      key2    key3     key4    key5\n0   value1    value2  value3   NaN     NaN\n1   value1    NaN     NaN      value4  NaN\n2   value1    NaN     value3   value4  value5\n        \n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 166334,
            "reputation": 33199,
            "user_id": 390388,
            "user_type": "registered",
            "accept_rate": 95,
            "profile_image": "https://i.stack.imgur.com/NqksL.jpg?s=128&g=1",
            "display_name": "John",
            "link": "https://stackoverflow.com/users/390388/john"
        },
        "is_answered": true,
        "view_count": 2372488,
        "protected_date": 1507096824,
        "accepted_answer_id": 13485766,
        "answer_count": 17,
        "score": 1680,
        "last_activity_date": 1625448706,
        "creation_date": 1353047200,
        "last_edit_date": 1622848512,
        "question_id": 13411544,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/13411544/delete-a-column-from-a-pandas-dataframe",
        "title": "Delete a column from a Pandas DataFrame",
        "answer_body": "<p>As you've guessed, the right syntax is </p>\n\n<pre><code>del df['column_name']\n</code></pre>\n\n<p>It's difficult to make <code>del df.column_name</code> work simply as the result of syntactic limitations in Python. <code>del df[name]</code> gets translated to <code>df.__delitem__(name)</code> under the covers by Python.</p>\n",
        "question_body": "<p>When deleting a column in a DataFrame I use:</p>\n<pre><code>del df['column_name']\n</code></pre>\n<p>And this works great. Why can't I use the following?</p>\n<pre><code>del df.column_name\n</code></pre>\n<p>Since it is possible to access the column/Series as <code>df.column_name</code>, I expected this to work.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 22053207,
            "reputation": 49,
            "user_id": 16317158,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyBUnbLv2PI4_Agtm14oXQyk_ozZyrNZkI9w4DE=k-s128",
            "display_name": "Jewel_R",
            "link": "https://stackoverflow.com/users/16317158/jewel-r"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 68246754,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625428343,
        "creation_date": 1625090523,
        "last_edit_date": 1625091335,
        "question_id": 68202057,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68202057/merging-multiple-dataframe-into-one-with-each-dataframe-as-a-header-name-contain",
        "title": "Merging multiple dataframe into one with each dataframe as a header name containing many columns in it and creating a 3D dataframe",
        "answer_body": "<p>You could use the <code>keys</code> in Pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> command (using the correct <code>range</code> with f-string to create a relevant nomenclature or use your already defined <code>list1</code>):</p>\n<blockquote>\n<p><strong>keys</strong> sequence, default None</p>\n<p>If multiple levels passed, should contain tuples. Construct hierarchical index using the passed keys as the outermost level.</p>\n</blockquote>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport numpy as np\n\n# setup\nnp.random.seed(12345)\nall_df_list = []\nfor i in range(3):\n    d = {\n        'time': (pd.timedelta_range(start='00:01:00', periods=5, freq='1s')\n                    + pd.Timestamp(&quot;00:00:00&quot;)).strftime(&quot;%M:%S&quot;),\n        'a': np.random.rand(5),\n        'b': np.random.rand(5),\n        'c': np.random.rand(5),\n    }\n    all_df_list.append(pd.DataFrame(d).round(2))\n\n# code\ndfc = pd.concat(all_df_list, axis=1,\n        keys=[f'df{i}' for i in range(1,4)]) # use the correct 'range' or your already defined 'list1'\n\ndfc = dfc.set_index(dfc.df1.time)\ndfc = dfc.drop('time', axis=1, level=1)\nprint(dfc)\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>        df1               df2               df3\n          a     b     c     a     b     c     a     b     c\ntime\n01:00  0.93  0.60  0.75  0.66  0.64  0.73  0.03  0.53  0.82\n01:01  0.32  0.96  0.96  0.81  0.72  0.99  0.80  0.60  0.50\n01:02  0.18  0.65  0.01  0.87  0.47  0.68  0.90  0.05  0.81\n01:03  0.20  0.75  0.11  0.96  0.33  0.79  0.02  0.90  0.10\n01:04  0.57  0.65  0.30  0.72  0.44  0.17  0.49  0.73  0.22\n</code></pre>\n<p>Extracting columns <code>a</code> and <code>b</code> from <code>df2</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>In [190]: dfc.df2[['a','b']]\nOut[190]:\n          a     b\ntime\n01:00  0.66  0.64\n01:01  0.81  0.72\n01:02  0.87  0.47\n01:03  0.96  0.33\n01:04  0.72  0.44\n</code></pre>\n",
        "question_body": "<p>I have multiple dataframes df1, df2 ,df3 etc to df10. The dataframe has 135 columns. each look like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>time</th>\n<th>a</th>\n<th>b</th>\n<th>c</th>\n<th>d</th>\n<th>e</th>\n<th>f</th>\n<th>g</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>2</td>\n<td>3</td>\n<td>4</td>\n<td>5</td>\n<td>6</td>\n<td>7</td>\n<td>8</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I wanted to arrange them in one data frame and stack them together side by side but having their df name as the header. Meaning one heading df1 having all those columns names( time,a,b...) and their value under it and so on.Seeing this example here  <a href=\"https://stackoverflow.com/questions/24290495/constructing-3d-pandas-dataframe?fbclid=IwAR1UUSLW-4MKggL32kheccenziMhbUhUjqkAgGJHtn-TPFfrWXFqSg7j3Bc\">Constructing 3D Pandas DataFrame</a>\nI tried following codes</p>\n<pre><code>   list1=['df1', 'df2', 'df3', 'df4', 'df5','df6', 'df7', 'df8', 'df9', \n   'df10']\n   list2=[]\n   for df in list1:\n    for i in range(135):\n        list2.append(df)\n   A=np.array(list2)\n   B = np.array([df1.columns]*10)\n   C=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10], axis=1)\n   C=C.values.tolist()\n   C=np.array(C)\n   df = pd.DataFrame(data=C.T, columns=pd.MultiIndex.from_tuples(zip(A,B)))\n   print(df)\n</code></pre>\n<p>But each time I am having an error\n<code>TypeError: unhashable type: 'numpy.ndarray'</code>\nI have a column <strong>time</strong>: where the time are in hhmm format. 01:00,01:01 so on. I tried dropping the column from the data frames but getting same error. How could I fix this? Can anyone help?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "apply"
        ],
        "owner": {
            "account_id": 467153,
            "reputation": 7491,
            "user_id": 872328,
            "user_type": "registered",
            "accept_rate": 60,
            "profile_image": "https://www.gravatar.com/avatar/5c1a9d104a5a415d3cf8d33be3abdd0e?s=128&d=identicon&r=PG",
            "display_name": "PaulMest",
            "link": "https://stackoverflow.com/users/872328/paulmest"
        },
        "is_answered": true,
        "view_count": 107901,
        "accepted_answer_id": 35208597,
        "answer_count": 10,
        "score": 154,
        "last_activity_date": 1625423833,
        "creation_date": 1399757270,
        "last_edit_date": 1587296457,
        "question_id": 23586510,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/23586510/return-multiple-columns-from-pandas-apply",
        "title": "Return multiple columns from pandas apply()",
        "answer_body": "<p>You can return a Series from the applied function that contains the new data, preventing the need to iterate three times.  Passing <code>axis=1</code> to the apply function applies the function <code>sizes</code> to each row of the dataframe, returning a series to add to a new dataframe.  This series, s, contains the new values, as well as the original data.</p>\n<pre><code>def sizes(s):\n    s['size_kb'] = locale.format(&quot;%.1f&quot;, s['size'] / 1024.0, grouping=True) + ' KB'\n    s['size_mb'] = locale.format(&quot;%.1f&quot;, s['size'] / 1024.0 ** 2, grouping=True) + ' MB'\n    s['size_gb'] = locale.format(&quot;%.1f&quot;, s['size'] / 1024.0 ** 3, grouping=True) + ' GB'\n    return s\n\ndf_test = df_test.append(rows_list)\ndf_test = df_test.apply(sizes, axis=1)\n</code></pre>\n",
        "question_body": "<p>I have a pandas DataFrame, <code>df_test</code>.  It contains a column 'size' which represents size in bytes.  I've calculated KB, MB, and GB using the following code:</p>\n\n<pre><code>df_test = pd.DataFrame([\n    {'dir': '/Users/uname1', 'size': 994933},\n    {'dir': '/Users/uname2', 'size': 109338711},\n])\n\ndf_test['size_kb'] = df_test['size'].astype(int).apply(lambda x: locale.format(\"%.1f\", x / 1024.0, grouping=True) + ' KB')\ndf_test['size_mb'] = df_test['size'].astype(int).apply(lambda x: locale.format(\"%.1f\", x / 1024.0 ** 2, grouping=True) + ' MB')\ndf_test['size_gb'] = df_test['size'].astype(int).apply(lambda x: locale.format(\"%.1f\", x / 1024.0 ** 3, grouping=True) + ' GB')\n\ndf_test\n\n\n             dir       size       size_kb   size_mb size_gb\n0  /Users/uname1     994933      971.6 KB    0.9 MB  0.0 GB\n1  /Users/uname2  109338711  106,776.1 KB  104.3 MB  0.1 GB\n\n[2 rows x 5 columns]\n</code></pre>\n\n<p>I've run this over 120,000 rows and time it takes about 2.97 seconds per column * 3 = ~9 seconds according to %timeit.</p>\n\n<p>Is there anyway I can make this faster?  For example, can I instead of returning one column at a time from apply and running it 3 times, can I return all three columns in one pass to insert back into the original dataframe?</p>\n\n<p>The other questions I've found all want to <strong>take multiple values and return a single value</strong>. I want to <strong>take a single value and return multiple columns</strong>.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "datetime"
        ],
        "owner": {
            "account_id": 16501529,
            "reputation": 1760,
            "user_id": 11922765,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ed69b624cdb86e52caf0010e274df7b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mainland",
            "link": "https://stackoverflow.com/users/11922765/mainland"
        },
        "is_answered": true,
        "view_count": 48,
        "accepted_answer_id": 68246871,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625417280,
        "creation_date": 1625414532,
        "last_edit_date": 1625414915,
        "question_id": 68246558,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68246558/python-dataframe-convert-hhmmss-object-into-datetime-format",
        "title": "Python Dataframe Convert hh:mm:ss object into datetime format",
        "answer_body": "<p><code>autofmt_xdate()</code> will not work if the values are of type <code>str</code>, instead change the type to <code>datetime</code> and manipulate the xaxis through a <code>Locator</code> and a <code>Formatter</code>:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt, dates as mdates\n\nnp.random.seed(15)\ndr = pd.date_range('2019-10-01 00:00:00', '2019-10-01 23:59', freq='1T')\ndf = pd.DataFrame({'HH:MM': dr.strftime('%H:%M'),\n                   'y': np.random.random(len(dr)) * 10},\n                  index=dr)\n\ndf['HH:MM'] = pd.to_datetime(df['HH:MM'])\nax = df.plot(kind='scatter', x='HH:MM', y='y', rot=45)\nax.xaxis.set_major_locator(mdates.HourLocator(interval=2))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\nplt.tight_layout()\nplt.show()\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/WipTP.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/WipTP.png\" alt=\"plot 2\" /></a></p>\n<hr />\n<p>Sample Data and imports:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt, dates as mdates\n\nnp.random.seed(15)\ndr = pd.date_range('2019-10-01 00:00:00', '2019-10-01 23:59', freq='1T')\ndf = pd.DataFrame({'HH:MM': dr.strftime('%H:%M'),\n                   'y': np.random.random(len(dr)) * 10},\n                  index=dr)\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>                     HH:MM         y\n2019-10-01 00:00:00  00:00  8.488177\n2019-10-01 00:01:00  00:01  1.788959\n2019-10-01 00:02:00  00:02  0.543632\n2019-10-01 00:03:00  00:03  3.615384\n2019-10-01 00:04:00  00:04  2.754009\n</code></pre>\n<p>Convert 'HH:MM' <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html#pandas-to-datetime\" rel=\"nofollow noreferrer\"><code>to_datetime</code></a> then plot:</p>\n<pre><code>df['HH:MM'] = pd.to_datetime(df['HH:MM'])\nax = df.plot(kind='scatter', x='HH:MM', y='y', rot=45)\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/19ub2.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/19ub2.png\" alt=\"plot 1\" /></a></p>\n<p>To adjust the number of ticks and format in 'HH:MM' format set the <a href=\"https://matplotlib.org/stable/api/dates_api.html#matplotlib.dates.DateLocator\" rel=\"nofollow noreferrer\">Date Locator</a> and the <a href=\"https://matplotlib.org/stable/api/dates_api.html#matplotlib.dates.DateFormatter\" rel=\"nofollow noreferrer\">Date Formatter</a>:</p>\n<pre><code>ax.xaxis.set_major_locator(mdates.HourLocator(interval=2))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n</code></pre>\n<p>Adjust the type of locator or interval to increase or decrease the number of ticks.</p>\n",
        "question_body": "<p>I am trying to convert <code>HH:MM</code> into the datetime format. It converts but it adds an unwanted year <code>1900</code>. I don't know why?</p>\n<p>My code:</p>\n<pre><code>df['HH:MM'] = \ndatetime\n2019-10-01 08:19:40    08:19:40\n2019-10-01 08:20:15    08:20:15\n2019-10-01 08:21:29    08:21:29\n2019-10-01 08:22:39    08:22:39\n2019-10-01 08:29:07    08:29:07\nName: HH:MM, Length: 5, dtype: object\n\ndf['HH:MM'] = pd.to_datetime(cdf['HH:MM'], format = '%H:%M:%S', errors='ignore')\n</code></pre>\n<p>Present output</p>\n<pre><code>df['HH:MM'] =\ndatetime\n2019-10-01 08:19:40   1900-01-01 08:19:40\n2019-10-01 08:20:15   1900-01-01 08:20:15\n2019-10-01 08:21:29   1900-01-01 08:21:29\n2019-10-01 08:22:39   1900-01-01 08:22:39\n2019-10-01 08:29:07   1900-01-01 08:29:07\nName: HH:MM, Length: 5, dtype: datetime64[ns]\n</code></pre>\n<p>Why I need this?</p>\n<p>I am plotting <code>HH:MM</code> on the x-axis and value on the y-axis. The x-axis ticks look crazy and we cannot read even after I used <code>plt.gcf().autofmt_xdate()</code>.\n<a href=\"https://i.stack.imgur.com/RQFwv.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/RQFwv.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "sum"
        ],
        "owner": {
            "account_id": 18896632,
            "reputation": 29,
            "user_id": 13785648,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/917285ebf87146bb99224dd185fd2784?s=128&d=identicon&r=PG&f=1",
            "display_name": "Prince_persia22",
            "link": "https://stackoverflow.com/users/13785648/prince-persia22"
        },
        "is_answered": true,
        "view_count": 28,
        "closed_date": 1625412434,
        "accepted_answer_id": 68246214,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625412766,
        "creation_date": 1625412249,
        "question_id": 68246198,
        "link": "https://stackoverflow.com/questions/68246198/how-do-i-sum-up-when-complying-to-two-conditions-and-then-put-the-summed-data-in",
        "closed_reason": "Duplicate",
        "title": "How do I sum up when complying to two conditions and then put the summed data in a new data frame?",
        "answer_body": "<p>you can use <code>pivot_table()</code>:</p>\n<pre><code>out=(df.pivot_table('Duration','Date','Category',fill_value=0,aggfunc='sum')\n       .rename_axis(columns=None)\n       .reset_index())\n</code></pre>\n<p><strong>OR</strong></p>\n<p>you can use <code>pd.crosstab()</code>:</p>\n<pre><code>out=(pd.crosstab(df['Date'],df['Category'],df['Duration'],aggfunc='sum')\n       .fillna(0)\n       .rename_axis(columns=None)\n       .reset_index())\n</code></pre>\n<p>output of <code>out</code>:</p>\n<pre><code>      Date      Entertainment   Sleeping    Working\n0   01/01/2021  2.2             0.0         1.4\n1   02/01/2021  0.0             7.9         4.0\n2   04/01/2021  0.0             6.2         0.0\n</code></pre>\n",
        "question_body": "<p>I have a data frame with dates, categories, and time durations. I want to sum the time durations if the entries have the same date and the same category.</p>\n<p>Input:</p>\n<pre><code>Date        Duration   Category\n01/01/2021  0.1        Entertainment\n01/01/2021  1.4        Working\n01/01/2021  2.1        Entertainment\n02/01/2021  7.9        Sleeping\n02/01/2021  1.2        Working\n02/01/2021  2.8        Working\n04/01/2021  6.2        Sleeping\n</code></pre>\n<p>Output:</p>\n<pre><code>Date        Entertainment   Working   Sleeping\n01/01/2021  2.2             1.4       0\n02/01/2021  0               4.0       7.9\n03/01/2021  0               0         0\n04/01/2021  0               0         6.2\n</code></pre>\n<p>I have more categories so if you can allow it to easily add new categories. The code I have doesn't work at all so please help me out thanks.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 18602351,
            "reputation": 31,
            "user_id": 13556873,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-OfmJ1UZQe7Y/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucnj6YuMZr_seZd0aeKqwIICuctkIA/photo.jpg?sz=128",
            "display_name": "ubant",
            "link": "https://stackoverflow.com/users/13556873/ubant"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 68245341,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625412255,
        "creation_date": 1625406220,
        "question_id": 68245308,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68245308/how-to-reverse-all-values-in-a-dataframe-column",
        "title": "How to reverse all values in a dataframe column?",
        "answer_body": "<p>Just simply substract 1 from your series:</p>\n<pre><code>s=pd.Series([1.0, 0.122, 0.95, 0.324546])\ns=(1-s)\n#here s is your Series\n#If needed the difference as positive number use abs() method\ns=(s-1).abs()\n</code></pre>\n<p>output:</p>\n<pre><code>0    0.000000\n1    0.878000\n2    0.050000\n3    0.675454\n</code></pre>\n<p><strong>Or</strong></p>\n<p>If you have df with int/float values:</p>\n<pre><code>df=pd.DataFrame({0: {0: 1.0, 1: 0.122, 2: 0.95, 3: 0.324546},\n 1: {0: 1.0, 1: 0.122, 2: 0.95, 3: 0.324546},\n 2: {0: 1.0, 1: 0.122, 2: 0.95, 3: 0.324546}})\n\ndf=1-df\n#If needed the difference as positive number use abs() method\ndf.sub(1).abs()\n</code></pre>\n<p>output of <code>df</code>:</p>\n<pre><code>      0           1         2\n0   0.000000    0.000000    0.000000\n1   0.878000    0.878000    0.878000\n2   0.050000    0.050000    0.050000\n3   0.675454    0.675454    0.675454\n</code></pre>\n",
        "question_body": "<p>I wasn't sure how to form this question, so the problem isn't really what it sounds for.\nLet's say I have a column with floats, ranging from 0.000000 to 1.000000\nI want to reverse those values so for example:</p>\n<pre><code>1.000000 == 0.000000\n0.122000 == 0.888000\n0.950000 == 0.050000\n0.324546 == 0.675454\n</code></pre>\n<p>How can I do it?\nI tried</p>\n<pre><code>normalized_df = normalized_df[headers[-1]].apply(lambda n: (1.000000 - n))\n</code></pre>\n<p>But I got an error with this block of code:</p>\n<pre><code>vals = normalized_df.values.tolist()\nfor e in vals:\n    del e[:3]\nresults = dict(zip(countries, vals))\n</code></pre>\n<p>An error:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;...&quot;, line ..., in &lt;module&gt;\n    del e[:3]\nTypeError: 'float' object does not support item deletion\n</code></pre>\n<p>This error normally doesn't happen without this code at the beginning of my question</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 13484607,
            "reputation": 149,
            "user_id": 9728755,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3853a85a63dbaa8a8e715613059fa645?s=128&d=identicon&r=PG&f=1",
            "display_name": "Gaurav",
            "link": "https://stackoverflow.com/users/9728755/gaurav"
        },
        "is_answered": true,
        "view_count": 63,
        "accepted_answer_id": 68183788,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625048183,
        "creation_date": 1624989383,
        "last_edit_date": 1624989622,
        "question_id": 68183550,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68183550/compare-size-of-values-in-columns-which-are-in-kbs-mbs-and-gbs-in-pandas",
        "title": "Compare size of values in columns which are in kbs, mbs and gbs in pandas",
        "answer_body": "<p>You can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.replace.html\" rel=\"nofollow noreferrer\"><code>.replace()</code></a> to translate the <code>size</code> column with <code>K</code>, <code>M</code>, <code>G</code>, etc. to their corresponding values scaled up by the magnitude symbols, as follows:</p>\n<p><code>K</code> converted to <code>e+03</code> in scientific notation</p>\n<p><code>M</code> converted to <code>e+06</code> in scientific notation</p>\n<p><code>G</code> converted to <code>e+09</code> in scientific notation</p>\n<p><strong>(supports <code>integer</code> as well as <code>float</code> numbers in <em>any number of decimal places</em>)</strong></p>\n<p>Then, convert the text in scientific notation to float type, followed by casting to integer for final required format, as follows:</p>\n<pre><code>size_val = df['size'].replace({' ': '', 'K': 'e+03', 'M': 'e+06', 'G': 'e+09'}, regex=True).astype(float).astype(int)\n</code></pre>\n<p>Then, use <code>df.loc</code> to filter the rows with size ratio of current row and previous row (with getting values of previous row by <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.shift.html\" rel=\"nofollow noreferrer\"><code>.shift()</code></a>):</p>\n<pre><code>df.loc[(size_val / size_val.shift()) &lt; 0.5]\n</code></pre>\n<p><strong>Result:</strong></p>\n<pre><code>   size  number    key      date\n2  30 K   12345  Hello  20181003\n6  70 M   12345  Hello  20181006\n</code></pre>\n<p><em><strong>Translated values of <code>size</code></strong></em> (in <code>size_val</code>) <em><strong>are the actual values</strong></em> (translated from texts to integers) <em><strong>scaled up by the magnitude symbols:</strong></em></p>\n<pre><code>print(size_val)\n\n\n0       120000\n1       119000\n2        30000\n3        90000\n4       150000\n5    180000000\n6     70000000\nName: size, dtype: int32\n</code></pre>\n",
        "question_body": "<p>I want to compare each row with its previous row size, lets say if first row have 6kb size and 2nd row has 2kb size. if second row of dataframe has 50% less size than the previous one then that row should be printed.</p>\n<p>following is my dataframe,</p>\n<pre><code>    size     number     key      date\n0  120 K    12345     Hello     20181002\n1  119 K    12345     No        20181001\n2  30 K     12345     Hello     20181003\n3  90 K     12345     No        20181003\n4  150 K    12345     Hello     20181004\n5  180 M    12345     No        20181005\n6  70 M     12345     Hello     20181006\n</code></pre>\n<p>in above dataframe 2nd row compare with 1st and the difference in not less than 50% then it will ignore, but 3rd row size is less than 50% of 2nd row so it will print 3rd row\nsame for 6th row will be print as it is less than 50% of size.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-science",
            "backend"
        ],
        "owner": {
            "account_id": 15470092,
            "reputation": 174,
            "user_id": 11160421,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/4vhSk.png?s=128&g=1",
            "display_name": "Suchi",
            "link": "https://stackoverflow.com/users/11160421/suchi"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 68192801,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625047962,
        "creation_date": 1625047684,
        "question_id": 68192722,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68192722/python-pandas-convert-selective-columns-into-rows",
        "title": "Python Pandas convert selective columns into rows",
        "answer_body": "<p>Let us try pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.wide_to_long.html\" rel=\"nofollow noreferrer\"><code>wide_to_long</code></a></p>\n<pre><code>pd.wide_to_long(df, i='Items', j='year', \n                stubnames=['Price', 'Sales'], \n                suffix=r'\\d+', sep=' in ').sort_index()\n</code></pre>\n<hr />\n<pre><code>              Price Sales\nItems year              \nA     2018    100   5000\n      2019    120   6000\n      2020    135   6500\nB     2018    110   2000\n      2019    130   4000\n      2020    150   4500\nC     2018    150   1000\n      2019    110   3000\n      2020    175   3000\n</code></pre>\n",
        "question_body": "<p>My dataset has some information about price and sales for different years. The problem is each year is actually a different column header for price and for sales as well. For example the CSV looks like</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Items</th>\n<th>Price in 2018</th>\n<th>Price in 2019</th>\n<th>Price in 2020</th>\n<th>Sales in 2018</th>\n<th>Sales in 2019</th>\n<th>Sales in 2020</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>100</td>\n<td>120</td>\n<td>135</td>\n<td>5000</td>\n<td>6000</td>\n<td>6500</td>\n</tr>\n<tr>\n<td>B</td>\n<td>110</td>\n<td>130</td>\n<td>150</td>\n<td>2000</td>\n<td>4000</td>\n<td>4500</td>\n</tr>\n<tr>\n<td>C</td>\n<td>150</td>\n<td>110</td>\n<td>175</td>\n<td>1000</td>\n<td>3000</td>\n<td>3000</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I want to show it something like this</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Items</th>\n<th>Year</th>\n<th>Price</th>\n<th>Sales</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>A</td>\n<td>2018</td>\n<td>100</td>\n<td>5000</td>\n</tr>\n<tr>\n<td>A</td>\n<td>2019</td>\n<td>120</td>\n<td>6000</td>\n</tr>\n<tr>\n<td>A</td>\n<td>2020</td>\n<td>135</td>\n<td>6500</td>\n</tr>\n<tr>\n<td>B</td>\n<td>2018</td>\n<td>110</td>\n<td>2000</td>\n</tr>\n<tr>\n<td>B</td>\n<td>2019</td>\n<td>130</td>\n<td>4000</td>\n</tr>\n<tr>\n<td>B</td>\n<td>2020</td>\n<td>150</td>\n<td>4500</td>\n</tr>\n<tr>\n<td>C</td>\n<td>2018</td>\n<td>150</td>\n<td>1000</td>\n</tr>\n<tr>\n<td>C</td>\n<td>2019</td>\n<td>110</td>\n<td>3000</td>\n</tr>\n<tr>\n<td>C</td>\n<td>2020</td>\n<td>175</td>\n<td>3000</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I used melt function from Pandas like this\ndf.melt(id_vars = ['Items'], var_name=&quot;Year&quot;, value_name=&quot;Price&quot;)</p>\n<p>But I'm struggling in getting separate columns for Price and Sales as it gives Price and Sales in one column. Thanks</p>\n"
    },
    {
        "tags": [
            "python",
            "regex",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 19564453,
            "reputation": 33,
            "user_id": 14316994,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/122723d07e96e4eae5c2bbf32ae05771?s=128&d=identicon&r=PG&f=1",
            "display_name": "LuisGan",
            "link": "https://stackoverflow.com/users/14316994/luisgan"
        },
        "is_answered": true,
        "view_count": 60,
        "accepted_answer_id": 67891819,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1625047573,
        "creation_date": 1623172372,
        "last_edit_date": 1623220743,
        "question_id": 67891653,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67891653/python-pandas-df-best-way-to-replace-m-and-k-in-currency-amount-to-change-to",
        "title": "Python Pandas df, best way to replace $, M and K in currency amount to change to int",
        "answer_body": "<h3>Updated Solution:</h3>\n<p><strong>New solution: Using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.replace.html\" rel=\"nofollow noreferrer\"><code>.replace()</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.astype.html\" rel=\"nofollow noreferrer\"><code>astype()</code></a> only.<br />\n<em>Without relying on <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.eval.html\" rel=\"nofollow noreferrer\"><code>pd.eval</code></a> for formula evaluation:</em></strong></p>\n<p><strong>You can translate <code>M</code>, <code>K</code> to the corresponding magnitudes in exponential format:</strong></p>\n<p><code>K</code> converted to <code>e+03</code> in scientific notation</p>\n<p><code>M</code> converted to <code>e+06</code> in scientific notation</p>\n<p><strong>(supports <code>integer</code> as well as <code>float</code> numbers in <em>any number of decimal places</em>)</strong></p>\n<p>Then, convert the text in scientific notation to float type, followed by casting to integer for final required format, as follows:</p>\n<pre><code>df['Value'] = df['Value'].replace({'\u20ac': '', ' ': '', 'M': 'e+06', 'K': 'e+03'}, regex=True).astype(float).astype(int)\n</code></pre>\n<p><strong>Input data:</strong></p>\n<pre><code>         Value\n0        \u20ac8.5M\n1           \u20ac0\n2        \u20ac9.5M\n3          \u20ac2M\n4         \u20ac21M\n16534    \u20ac1.8M\n16535    \u20ac1.1M\n16536    \u20ac550K\n16537    \u20ac650K\n16538    \u20ac1.1M\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>print(df)\n\n          Value\n0       8500000\n1             0\n2       9500000\n3       2000000\n4      21000000\n16534   1800000\n16535   1100000\n16536    550000\n16537    650000\n16538   1100000\n</code></pre>\n<h3>Old Solution:</h3>\n<p>You can convert <code>M</code>, <code>K</code> to formula and then use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.eval.html\" rel=\"nofollow noreferrer\"><code>pd.eval</code></a> to evaluate the numeric values.</p>\n<p><code>K</code> converted to formula <code>* 1000</code></p>\n<p><code>M</code> converted to formula <code>* 1000000</code></p>\n<p>In this way, we can support the base values with any number of decimal points (with or without decimal point and how long the fractional part could be).  We can just get the correct results from the formulas for all lengths of fractional parts after decimal points.</p>\n<pre><code>df['Value'] = df['Value'].str.replace('\u20ac', '')\ndf['Value'] = df['Value'].str.replace('M', ' * 1000000')\ndf['Value'] = df['Value'].str.replace('K', ' * 1000')\ndf['Value'] = df['Value'].map(pd.eval).astype(int)\n</code></pre>\n<hr />\n<p>Or simplified code in one line, thanks to @MustafaAyd\u0131n's suggestion:</p>\n<pre><code>df['Value'] = df['Value'].replace({&quot;\u20ac&quot;: &quot;&quot;, &quot;M&quot;: &quot;*1E6&quot;, &quot;K&quot;: &quot;*1E3&quot;}, regex=True).map(pd.eval).astype(int)\n</code></pre>\n<p>Result:</p>\n<pre><code>print(df)\n\n\n          Value\n0       8500000\n1             0\n2       9500000\n3       2000000\n4      21000000\n16534   1800000\n16535   1100000\n16536    550000\n16537    650000\n16538   1100000\n</code></pre>\n<p>With the input sample data as follows:</p>\n<pre><code>         Value\n0        \u20ac8.5M\n1           \u20ac0\n2        \u20ac9.5M\n3          \u20ac2M\n4         \u20ac21M\n16534    \u20ac1.8M\n16535    \u20ac1.1M\n16536    \u20ac550K\n16537    \u20ac650K\n16538    \u20ac1.1M\n</code></pre>\n<p>Before the last step, we got:</p>\n<pre><code>               Value\n0      8.5 * 1000000\n1                  0\n2      9.5 * 1000000\n3        2 * 1000000\n4       21 * 1000000\n16534  1.8 * 1000000\n16535  1.1 * 1000000\n16536     550 * 1000\n16537     650 * 1000\n16538  1.1 * 1000000\n</code></pre>\n<p>Then we feed it to <code>pd.eval</code> for it to evaluate and convert to numeric value (in float) where we can further cast it to integer.</p>\n",
        "question_body": "<p>I'm doing a personal project to practice pandas, and Beautiful soup, I scraped this info and have it in a pandas df like this:</p>\n<pre><code>0        \u20ac8.5M\n1           \u20ac0\n2        \u20ac9.5M\n3          \u20ac2M\n4         \u20ac21M\n         ...  \n16534    \u20ac1.8M\n16535    \u20ac1.1M\n16536    \u20ac550K\n16537    \u20ac650K\n16538    \u20ac1.1M\nName: Value, Length: 16539, dtype: object\n0        \u20ac67K\n1          \u20ac0\n2        \u20ac15K\n3        \u20ac11K\n4        \u20ac13K\n         ... \n16534     \u20ac3K\n16535     \u20ac2K\n16536     \u20ac2K\n16537     \u20ac7K\n16538     \u20ac3K\nName: Wage, Length: 16539, dtype: object  \n</code></pre>\n<p>So in order to analyse this info I want to clean and transform this data into integers, what I could come up with was this:</p>\n<pre><code>df['Wage'] = df['Wage'].apply(lambda x: re.sub('\u20ac','',x))\ndf['Wage'] = df['Wage'].apply(lambda x: re.sub('K','000',x))\n\ndf['Value'] = df['Value'].apply(lambda x: re.sub('\u20ac','',x))\ndf['Value'] = df['Value'].apply(lambda x : re.sub('M','00000',x) if (('M' in x) and ('.' in x))else x)\ndf['Value'] = df['Value'].apply(lambda x : re.sub('[.]','',x))\ndf['Value'] = df['Value'].apply(lambda x : re.sub('M','000000',x))\ndf['Value'] = df['Value'].apply(lambda x : re.sub('K','000',x))\n\ndf['Wage'] = df['Wage'].astype(int)\ndf['Value'] = df['Value'].astype(int)\n</code></pre>\n<p>I replaced first the money sign, then check for dots so I can replace the M for 5 zeros, then the remaining M's for 6 zeros, then the K's for 3 zeros and then I do the type change into int.\nBut I feel like this is not a good way of doing this, What do you think? What would be a better way of doing this? I tried creating a function but couldn't do it.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-science",
            "data-munging"
        ],
        "owner": {
            "account_id": 8033257,
            "reputation": 724,
            "user_id": 6057371,
            "user_type": "registered",
            "accept_rate": 31,
            "profile_image": "https://www.gravatar.com/avatar/b744e3dbff9c7fdb801570de75f6eff7?s=128&d=identicon&r=PG&f=1",
            "display_name": "okuoub",
            "link": "https://stackoverflow.com/users/6057371/okuoub"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68192246,
        "answer_count": 2,
        "score": -2,
        "last_activity_date": 1625045932,
        "creation_date": 1625044782,
        "last_edit_date": 1625045454,
        "question_id": 68191944,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68191944/pandas-how-to-divide-columns-of-rows-within-groupby-based-on-condition",
        "title": "Pandas how to divide columns of rows within groupby based on condition",
        "answer_body": "<p>We can <code>pivot</code> the dataframe to reshape then using <code>eval</code> calculate <code>target / end</code>, then <code>merge</code> the given <code>df</code> with the evaluated column on <code>C1, c10</code></p>\n<pre><code>c = ['C1', 'c10']\ndf.merge(df.pivot(c, 'val_type', 'val').eval('target/end').rename('new'), on=c)\n</code></pre>\n<hr />\n<pre><code>   C1  c10  val val_type       new\n0   1    3    5   target  0.625000\n1   1    3    8      end  0.625000\n2   1    3    9    other  0.625000\n3   2    8   12      end  0.166667\n4   2    8    2   target  0.166667\n5   2    8    9    other  0.166667\n</code></pre>\n",
        "question_body": "<p>I have the dataframe</p>\n<pre><code>C1    c10 val val_type\n1      3   5   target\n1      3   8   end\n1      3   9   other\n2      8   1   end\n2      8   2   target\n2      8   9   other\n</code></pre>\n<p>The values of C1, C10 creates groups of 3.\nWithin these groups I want to create a new column that is target/end.\nSo the output will be:</p>\n<pre><code>C1    c10 val val_type   new \n1      3   5   target    0.652\n1      3   8   end       0.652\n1      3   9   other     0.652\n2      8   12  end       0.166\n2      8   2   target    0.166\n2      8   9   other     0.166\n</code></pre>\n<p>What is the best way to do so?</p>\n<p>edit: Ignore other</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 18386352,
            "reputation": 3,
            "user_id": 13393300,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-_j9hHKT7zpU/AAAAAAAAAAI/AAAAAAAAAkU/AAKWJJMN93fmDuMMWaAqx9LO5DpWB5XiZw/photo.jpg?sz=128",
            "display_name": "Shubham Batra",
            "link": "https://stackoverflow.com/users/13393300/shubham-batra"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 68190205,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625037918,
        "creation_date": 1625035621,
        "question_id": 68189696,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68189696/is-there-an-easy-way-to-collapse-multiple-rows-with-the-same-unique-identifier-i",
        "title": "Is there an easy way to collapse multiple rows with the same unique identifier into one row using numpy/ pandas?",
        "answer_body": "<p>I think you can do <code>groupby</code> with <code>cumcount</code> and apply <code>pivot_table</code></p>\n<pre><code>df=pd.DataFrame({'Loan ID':['a','a','a','b','b','c','c','c'],'Borrower':['co','co','main','main','co','main','co','co'],'age':[23,24,27,27,45,34,33,23]})\ndf['temp']=df.groupby([df['Loan ID'],df['Borrower']]).cumcount().astype(str)\ndf['temp']=np.where(df['Borrower']=='main','',df['temp'])\ndf_new=df.pivot_table(index='Loan ID',columns=df['Borrower']+df['temp'],values='age',aggfunc=sum)\ndf_new.columns=[i+ ' Age' for i in df_new.columns]\ndf_new=df_new.reset_index()\n</code></pre>\n",
        "question_body": "<p>I have a dataframe for loans, which looks like this:</p>\n<p><a href=\"https://i.stack.imgur.com/hMkNE.png\" rel=\"nofollow noreferrer\">Loan Dataframe</a></p>\n<p>My goal is to have only one row per loan ID, instead of multiple rows. I want to have separate columns for the age of co-borrowers and main borrower. I know that the maximum number of co-borrowers, so I know the number of columns to create.</p>\n<p><a href=\"https://i.stack.imgur.com/ADz9G.png\" rel=\"nofollow noreferrer\">Desired data frame</a></p>\n<p>I wrote a script to achieve this, however, it takes about 6 minutes to run on a dataframe with 30K rows. Is there a faster way to do this? Below is a snippet of my code:</p>\n<pre><code>loan_id = []\nidx = 0\ncol_count = 0\nidx_col = 0\n\n# first, sort the dataframe to make sure same loan numbers are together\nco_ap.sort_values(by = ['Loan No.'], inplace = True)\n\nfor loan in laon['Loan ID'].items():\n    if loan[1] not in loan_id:\n        loan_id.append(loan[1])\n        col_count = 0\n        idx_col = idx \n    if loan['Borroer'][idx] != 'Main':\n        col_count += 1\n# update desired column\n        loan['ST_Age_coap_' + str(col_count)][idx_col] = loan['Age'][idx]\n    else:\n        loan['ST_Age_main'][idx_col] = loan['Age'][idx]\n\n# if the idx_col != idx, that means we are operating on a row, which we eventually have to drop,\n# input a dummy value in any column, which will act as an identifier later on to know which rows to drop\n    if idx_col != idx:\n        loan['ST_Age_main'][idx] = -1\n        \n    idx += 1\n\n# drop rows not required    \nco_ap = co_ap[co_ap['ST_Age_main'] != -1]   \n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "scikit-learn"
        ],
        "owner": {
            "account_id": 11400545,
            "reputation": 125,
            "user_id": 10280393,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3d3003d771f7c572207e625349fdefb2?s=128&d=identicon&r=PG&f=1",
            "display_name": "n.mathfreak",
            "link": "https://stackoverflow.com/users/10280393/n-mathfreak"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 68189889,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625037334,
        "creation_date": 1625033868,
        "last_edit_date": 1625035049,
        "question_id": 68189339,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68189339/how-do-i-load-a-dataframe-in-python-sklearn",
        "title": "How do I load a dataframe in Python sklearn?",
        "answer_body": "<p>Ok, so some clarifications first:\nin your example, it is unclear what the load_boston() function does. they just import it. whatever that function returns has an attribute called &quot;data&quot;.</p>\n<p>They use this line:</p>\n<pre><code>X = pd.DataFrame(boston.data, columns=boston.feature_names)\n</code></pre>\n<p>to create a dataframe. Your situation is different because you have a dataframe already and dataframes don't have an attribute &quot;.data&quot;. Hence, the error you're getting: &quot;DataFrame' object has no attribute 'data'.</p>\n<p>What you need is simply</p>\n<pre><code>X = df\ny = df['score']\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)\n</code></pre>\n<p>or if you need only some of the columns from you dataframe:</p>\n<pre><code># set data\nlist_of_columns = ['id','value']\nX = df[list_of_columns]\n# set target\ntarget_column = 'score'\ny = df[target_column]\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)\n</code></pre>\n",
        "question_body": "<p>I did some computations in an IPython Notebook and ended up with a dataframe <code>df</code> which isn't saved anywhere yet. In the same IPython Notebook, I want to work with this dataframe using sklearn.</p>\n<p>df is a dataframe with 4 columns: id (string), value(int), rated(bool), score(float). I am trying to determine what influences the score the most just like in this <a href=\"https://mljar.com/blog/feature-importance-in-random-forest/\" rel=\"nofollow noreferrer\">example</a>. There they load a standard dataset, but instead I want to use my own dataframe in the notebook.</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom matplotlib import pyplot as plt\n\nplt.rcParams.update({'figure.figsize': (12.0, 8.0)})\nplt.rcParams.update({'font.size': 14})\n\ndataset = df\nX = pd.DataFrame(dataset.data, columns=dataset.feature_names)\ny = dataset.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)\n</code></pre>\n<p>But I get the AttributeError that the <code>'DataFrame' object has no attribute 'data'</code></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dummy-variable"
        ],
        "owner": {
            "account_id": 16609329,
            "reputation": 17,
            "user_id": 12002845,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mAz7r3IOVvqdeg-wtqrpw9qPrhNP_4wBn9eyDYmPA=k-s128",
            "display_name": "Sukanto Mukherjee",
            "link": "https://stackoverflow.com/users/12002845/sukanto-mukherjee"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68189258,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1625034993,
        "creation_date": 1625032988,
        "question_id": 68189186,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68189186/create-binary-categorical-variables-in-pandas-combine-4-categories-into-2",
        "title": "Create binary categorical variables in pandas - combine 4 categories into 2",
        "answer_body": "<p>Just go with</p>\n<pre><code>df['Status_open'] = 0\ndf['Status_closed'] = 0\ndf.loc[(df['Status'] == 'Open') | (df['Status'] == 'Pending'), 'Status_open'] = 1\ndf.loc[(df['Status'] == 'Closed') | (df['Status'] == 'Solved'), 'Status_closed'] = 1\n</code></pre>\n",
        "question_body": "<p>I have the following pandas dataframe, where the column 'Status' consists of 4 categorical values - 'Open', 'Closed', 'Solved' and 'Pending'.</p>\n<pre><code>0   250635                      Comcast Cable Internet Speeds  22-04-15   \n1   223441       Payment disappear - service got disconnected  04-08-15   \n2   242732                                  Speed and Service  18-04-15   \n3   277946  Comcast Imposed a New Usage Cap of 300GB that ...  05-07-15   \n4   307175         Comcast not working and no service to boot  26-05-15   \n\n  Date_month_year         Time        Received Via      City     State  \\\n0       22-Apr-15   3:53:50 PM  Customer Care Call  Abingdon  Maryland   \n1       04-Aug-15  10:22:56 AM            Internet   Acworth   Georgia   \n2       18-Apr-15   9:55:47 AM            Internet   Acworth   Georgia   \n3       05-Jul-15  11:59:35 AM            Internet   Acworth   Georgia   \n4       26-May-15   1:25:26 PM            Internet   Acworth   Georgia   \n\n   Zip code  Status Filing on Behalf of Someone  \n0     21009  Closed                          No  \n1     30102  Closed                          No  \n2     30101  Closed                         Yes  \n3     30101    Open                         Yes  \n4     30101  Solved                          No  \n</code></pre>\n<p>I would like to combine the 'Open' and 'Pending' categories as 'Open' column and 'Closed' and 'Solved' as 'Closed' column with 0 and 1 binaries. If I use <code>pd.get_dummies(df, columns=['Status'])</code> I get the following output with 4 new columns for the 4 values but I only want 2, as mentioned earlier. I couldn't find any previous thread here on this so please suggest any possible method. Thank you.</p>\n<pre><code>0          22-Apr-15   3:53:50 PM  Customer Care Call    Abingdon  Maryland   \n1          04-Aug-15  10:22:56 AM            Internet     Acworth   Georgia   \n2          18-Apr-15   9:55:47 AM            Internet     Acworth   Georgia   \n3          05-Jul-15  11:59:35 AM            Internet     Acworth   Georgia   \n4          26-May-15   1:25:26 PM            Internet     Acworth   Georgia   \n             ...          ...                 ...         ...       ...   \n2219       04-Feb-15   9:13:18 AM  Customer Care Call  Youngstown   Florida   \n2220       06-Feb-15   1:24:39 PM  Customer Care Call   Ypsilanti  Michigan   \n2221       06-Sep-15   5:28:41 PM            Internet   Ypsilanti  Michigan   \n2222       23-Jun-15  11:13:30 PM  Customer Care Call   Ypsilanti  Michigan   \n2223       24-Jun-15  10:28:33 PM  Customer Care Call   Ypsilanti  Michigan   \n\n      Zip code Filing on Behalf of Someone  Status_Closed  Status_Open  \\\n0        21009                          No              1            0   \n1        30102                          No              1            0   \n2        30101                         Yes              1            0   \n3        30101                         Yes              0            1   \n4        30101                          No              0            0   \n       ...                         ...            ...          ...   \n2219     32466                          No              1            0   \n2220     48197                          No              0            0   \n2221     48197                          No              0            0   \n2222     48197                          No              0            0   \n2223     48198                         Yes              0            1   \n\n      Status_Pending  Status_Solved  \n0                  0              0  \n1                  0              0  \n2                  0              0  \n3                  0              0  \n4                  0              1  \n             ...            ...  \n2219               0              0  \n2220               0              1  \n2221               0              1  \n2222               0              1  \n2223               0              0  \n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 11461970,
            "reputation": 327,
            "user_id": 8401374,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://i.stack.imgur.com/NlGnH.jpg?s=128&g=1",
            "display_name": "Shaida Muhammad",
            "link": "https://stackoverflow.com/users/8401374/shaida-muhammad"
        },
        "is_answered": true,
        "view_count": 20,
        "closed_date": 1625032569,
        "accepted_answer_id": 68189145,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625032701,
        "creation_date": 1625032412,
        "question_id": 68189093,
        "link": "https://stackoverflow.com/questions/68189093/pandas-create-new-dataframe-based-on-unique-value-in-a-column-of-existing-datafr",
        "closed_reason": "Duplicate",
        "title": "Pandas create new dataframe based on unique value in a column of existing dataframe efficiently",
        "answer_body": "<p>The easiest way would be to use groupby -</p>\n<p>And populate the first occurrences of the column values</p>\n<h1>Group By</h1>\n<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; \n&gt;&gt;&gt; d = {\n...   'Main':['v1','v2','v1','v2','v5','v2']\n...   ,'Col1':[1,0,1,1,1,1]\n...   ,'Col2':[0,1,1,0,0,0]\n...   ,'Col3':[0,1,0,1,0,0]\n... }\n&gt;&gt;&gt; \n&gt;&gt;&gt; df = pd.DataFrame(d)\n&gt;&gt;&gt; \n&gt;&gt;&gt; df.groupby('Main').agg('first')\n      Col1  Col2  Col3\nMain                  \nv1       1     0     0\nv2       0     1     1\nv5       1     0     0\n&gt;&gt;&gt; df.groupby('Main').agg('first').reset_index()\n  Main  Col1  Col2  Col3\n0   v1     1     0     0\n1   v2     0     1     1\n2   v5     1     0     0\n\n</code></pre>\n<h1>Drop Duplicates</h1>\n<pre><code>&gt;&gt;&gt; df.drop_duplicates(subset='Main')\n  Main  Col1  Col2  Col3\n0   v1     1     0     0\n1   v2     0     1     1\n4   v5     1     0     0\n</code></pre>\n",
        "question_body": "<p>I have a Dataframe <strong>(df)</strong> that looks like this.</p>\n<pre><code>   Main     Col_1    Col_2     Col_3\n0     v1     1        0         0\n1     v2     0        1         1\n2     v1     1        1         0\n3     v2     1        0         1\n4     v5     1        0         0\n5     v2     1        0         0\n</code></pre>\n<p>I'm creating a new Dataframe based on unique values in <strong>Main</strong> column. i.e. Iterating through every row and when encounter a new value in <strong>Main</strong> column, add that row to new DataFrame.</p>\n<p>New DataFrame <strong>(new_df)</strong> should look like this.</p>\n<pre><code>   Main     Col_1    Col_2     Col_3\n0     v1     1        0         0\n1     v2     0        1         1\n2     v5     1        0         0\n</code></pre>\n<p>My current approach is iterating through every row and ...</p>\n<pre><code>unique_message_list = []\nnew_df_list = []\n\nfor index, row in df.iterrows():\n    if row['Main'] not in unique_message_list:\n        unique_message_list.append(row['Main'])\n        new_df_list.append(row.tolist())\n\nnew_df = pd.DataFrame(new_df_list, columns=['Main', 'Col_1', 'Col_2', 'Col_3'])\n</code></pre>\n<p>But <strong>df</strong> has 1 Million rows so it takes time to process it with iterating. How to solve it efficiently?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 20337129,
            "reputation": 21,
            "user_id": 14917539,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/0e9c8349a2ec958d537752833bdbc6fe?s=128&d=identicon&r=PG&f=1",
            "display_name": "AviG88",
            "link": "https://stackoverflow.com/users/14917539/avig88"
        },
        "is_answered": true,
        "view_count": 54,
        "accepted_answer_id": 68188581,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625032466,
        "creation_date": 1625027175,
        "last_edit_date": 1625032466,
        "question_id": 68188439,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68188439/how-to-populate-value-of-a-column-in-a-pandas-df-from-another-df",
        "title": "How to populate value of a column in a pandas df from another df?",
        "answer_body": "<p>You can try doing this:</p>\n<pre><code>dfNew=df1.merge(df2,on='Name',how='outer',suffixes=('','_y'))\ndfNew['Group']=dfNew['Group'].fillna(dfNew['Group_y'])\ndfNew['Class']=dfNew['Class'].fillna(dfNew['Class_y'])\ndfNew=dfNew.drop(dfNew.filter(like='_').columns,1)\n\nOut[35]: \n  Name   Group   Class\n0  abc  Group1  ClassA\n1  def  Group2  ClassB\n2  ghi  Group3  ClassC\n3  jkl  Group4  ClassD\n</code></pre>\n",
        "question_body": "<p>I have a dataframe:</p>\n<pre><code>dict1 = {'Name': ['abc', 'def', 'ghi' , 'jkl'], 'Group': ['Group1', 'Group2', np.nan, np.nan], 'Class' : ['ClassA', 'ClassB' , np.nan, np.nan]}\ndf1 = pd.DataFrame(dict1)\ndf1:\n    Class   Group Name\n0  ClassA  Group1  abc\n1  ClassB  Group2  def\n2     NaN     NaN  ghi\n3     NaN     NaN  jkl\n</code></pre>\n<p>Here I want to replace the NaN values by looking up the name and its corresponding group and class values from another df:</p>\n<pre><code>dict2 = {'Name': ['ghi', 'jkl'], 'Group': ['Group3', 'Group4'], 'Class':['ClassC', 'ClassD']}\ndf2 = pd.DataFrame(dict2)\ndf2:\n    Class   Group Name\n0  ClassC  Group3  ghi\n1  ClassD  Group4  jkl\n</code></pre>\n<p>So basically I want to replace the df1 values by looking at the mapping in df2. So my df1 should look like:</p>\n<pre><code>df1:\n    Class   Group Name\n0  ClassA  Group1  abc\n1  ClassB  Group2  def\n2  ClassC  Group3  ghi\n3  ClassD  Group4  jkl\n</code></pre>\n<p>I tried doing df.loc but I am not sure how to make it work in this case.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 262063,
            "reputation": 3117,
            "user_id": 546678,
            "user_type": "registered",
            "accept_rate": 37,
            "profile_image": "https://www.gravatar.com/avatar/b4acec9c036ebceb61a6f41386ce65e3?s=128&d=identicon&r=PG",
            "display_name": "mommomonthewind",
            "link": "https://stackoverflow.com/users/546678/mommomonthewind"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 68188530,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625030947,
        "creation_date": 1625027331,
        "last_edit_date": 1625030947,
        "question_id": 68188455,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68188455/pandas-group-the-continuous-rows-with-same-values-into-one-group",
        "title": "pandas: group the continuous rows with same values into one group",
        "answer_body": "<p>IIUC, here's one way:</p>\n<pre><code>df['order_id'] = df.customer_id.ne(df.customer_id.shift()).cumsum()\n</code></pre>\n<p>OUTPUT:</p>\n<pre><code>   item_id customer_id  order_id\n0        1           A         1\n1        2           A         1\n2        1           B         2\n3        3           C         3\n4        4           C         3\n5        1           A         4\n6        5           A         4\n</code></pre>\n",
        "question_body": "<p>Assuming that I have a pandas dataframe of purchase, with no invoice ID like that</p>\n<pre><code>item_id customer_id\n1 A\n2 A\n1 B\n3 C\n4 C\n1 A\n5 A\n</code></pre>\n<p>So, my assumption is, if multiple items are bought by a customer in continuous orders, they belong to one group. So I would like to create an order_id column as:</p>\n<pre><code>item_id customer_id order_id\n1 A 1\n2 A 1\n1 B 2\n3 C 3\n4 C 3\n1 A 4\n5 A 4\n</code></pre>\n<p>The order_id shall be created automatically and incremental. How should I do that with pandas?</p>\n<p>Many thanks</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "indexing"
        ],
        "owner": {
            "account_id": 18716239,
            "reputation": 55,
            "user_id": 13645178,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/88f03c066943c38882b72ea361ab95d5?s=128&d=identicon&r=PG&f=1",
            "display_name": "encer",
            "link": "https://stackoverflow.com/users/13645178/encer"
        },
        "is_answered": true,
        "view_count": 21,
        "accepted_answer_id": 68187321,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625015855,
        "creation_date": 1625014413,
        "last_edit_date": 1625015855,
        "question_id": 68187247,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68187247/pandas-dataframes-how-to-delete-rows-in-a-dataframe-based-on-a-sequential-compa",
        "title": "Pandas DataFrames: How to delete rows in a dataframe based on a sequential comparison of their index values",
        "answer_body": "<p>In your case try</p>\n<pre><code>out = df[df.index.to_series().diff().ne(1)]\n</code></pre>\n",
        "question_body": "<p>I have a data frame named <code>red_data_frame</code>. Each row has a unique index value spanning from <code>1 to 54075</code>. What I need to do is to iterate through each value and delete the row if it is immediately sequential. For example:</p>\n<pre><code>1\n721\n722\n...\n</code></pre>\n<p>Delete <code>722</code></p>\n<pre><code>...\n1442\n1443\n...\n</code></pre>\n<p>Delete <code>1443</code> etc...</p>\n<p>The new dataframe should remove all rows that meet this condition.</p>\n<p><a href=\"https://i.stack.imgur.com/fxptC.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/fxptC.png\" alt=\"Snippet of red_data_frame\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 14248342,
            "reputation": 910,
            "user_id": 10292638,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5f3502307576b14f6c3d19924030dce7?s=128&d=identicon&r=PG&f=1",
            "display_name": "AlvaroMartinez",
            "link": "https://stackoverflow.com/users/10292638/alvaromartinez"
        },
        "is_answered": true,
        "view_count": 34,
        "closed_date": 1625059115,
        "accepted_answer_id": 68186205,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625006360,
        "creation_date": 1625003099,
        "question_id": 68186025,
        "link": "https://stackoverflow.com/questions/68186025/convert-columns-from-a-dictionary-of-dataframes-to-strings",
        "closed_reason": "Duplicate",
        "title": "Convert columns from a dictionary of dataframes to strings",
        "answer_body": "<p>The missing part, was to rename each dataframe inside the loop and change <code>astype()</code> argument from <code>str</code> to <code>&quot;string&quot;</code>:</p>\n<pre><code>for key in data:\n     data[key]=data[key].astype(&quot;string&quot;)\n</code></pre>\n<p>This returns string columns:</p>\n<pre><code>#&lt;class 'pandas.core.frame.DataFrame'&gt;\n#RangeIndex: 2 entries, 0 to 1\n#Data columns (total 2 columns):\n# #   Column  Non-Null Count  Dtype \n#---  ------  --------------  ----- \n# 0   col1    2 non-null      string\n# 1   col2    2 non-null      string\n#dtypes: object(2)\n#memory usage: 160.0+ bytes\n</code></pre>\n",
        "question_body": "<p>I am trying to convert all columns in all dataframes contained in a dictionary structure, from object to string, in order to apply a cleaning function over strings.</p>\n<pre><code># each key represents a dataframe:\n# iterate over each dataframe and convert all columns to strings\n\nimport pandas as pd\n\n\ndata = {'dataframe_1':pd.DataFrame({'col1': ['John', 'Ashley'], 'col2': ['+10', '-1']}), 'dataframe_2':pd.DataFrame({'col3': ['Italy', 'Brazil', 'Japan'], 'col4': ['Milan', 'Rio do Jaineiro', 'Tokio'], 'percentage':['+95%', '\u22640%', '80%+']})}\n\nfor key in data:\n     data[key].astype(str)\n</code></pre>\n<p>However, after applying the above code and given <code>data['dataframe_1'].info()</code>, <code>object</code> columns remain <code>object</code> instead of <code>string</code>:</p>\n<pre><code>#&lt;class 'pandas.core.frame.DataFrame'&gt;\n#RangeIndex: 2 entries, 0 to 1\n#Data columns (total 2 columns):\n# #   Column  Non-Null Count  Dtype \n#---  ------  --------------  ----- \n# 0   col1    2 non-null      object\n# 1   col2    2 non-null      object\n#dtypes: object(2)\n#memory usage: 160.0+ bytes\n</code></pre>\n<p>What am I missing?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 17129566,
            "reputation": 31,
            "user_id": 12395961,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b8c002884c8576bb3bbae36b5eb1016b?s=128&d=identicon&r=PG",
            "display_name": "FrancescoL",
            "link": "https://stackoverflow.com/users/12395961/francescol"
        },
        "is_answered": true,
        "view_count": 34,
        "closed_date": 1625015356,
        "accepted_answer_id": 68185533,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625000134,
        "creation_date": 1624999492,
        "last_edit_date": 1624999829,
        "question_id": 68185425,
        "link": "https://stackoverflow.com/questions/68185425/how-to-apply-a-filter-on-a-pandas-df-based-on-column-values",
        "closed_reason": "Needs details or clarity",
        "title": "How to apply a filter on a pandas DF based on column values",
        "answer_body": "<p>If the results needs to be Series:</p>\n<pre class=\"lang-py prettyprint-override\"><code>for _, row in df.iterrows():\n    print(row[row &gt;= 0])\n    print()\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>a    0\nc    2\ne    4\nName: 0, dtype: int64\n\na    0\nb    1\nd    3\nName: 1, dtype: int64\n\na    0\ne    4\nName: 2, dtype: int64\n</code></pre>\n<hr />\n<p>If DataFrames:</p>\n<pre class=\"lang-py prettyprint-override\"><code>for _, row in df.iterrows():\n    print(row[row &gt;= 0].to_frame().T)\n    print()\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>   a  c  e\n0  0  2  4\n\n   a  b  d\n1  0  1  3\n\n   a  e\n2  0  4\n</code></pre>\n",
        "question_body": "<p>I have a DF like the one below:</p>\n<pre><code>a b  c d  e\n0 -1 2 -3 4\n0 1 -2 3 -4\n0 -1 -2 -3 4\n</code></pre>\n<p>and i need to select from each row only columns with positive values.</p>\n<p>So the first row should return</p>\n<pre><code>a c e\n0 2 4\n</code></pre>\n<p>the second row</p>\n<pre><code>a b. d\n0 1 3\n</code></pre>\n<p>the third</p>\n<pre><code>a e \n0 4\n</code></pre>\n<p>anyone can help with this?</p>\n<p>Thanks in advance</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "xml",
            "dataframe",
            "matplotlib"
        ],
        "owner": {
            "account_id": 17176282,
            "reputation": 49,
            "user_id": 12432400,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/tx9Sm.png?s=128&g=1",
            "display_name": "Marsh Rangel",
            "link": "https://stackoverflow.com/users/12432400/marsh-rangel"
        },
        "is_answered": true,
        "view_count": 99,
        "accepted_answer_id": 67796316,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624998995,
        "creation_date": 1622574383,
        "last_edit_date": 1624998995,
        "question_id": 67794684,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67794684/merge-list-of-results-into-a-single-variable-with-python",
        "title": "Merge list of results into a single variable with Python",
        "answer_body": "<p>Simply parse to needed <code>&lt;Signal&gt;</code> nodes which can be handled in list/dict comprehension passed into <code>pandas.DataFrame</code> constructor:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import xml.etree.ElementTree as ET\nimport pandas as pd\n\nfile_xml = ET.parse(&quot;Input.xml&quot;)\n\ndata = [\n    {&quot;Name&quot;: signal.attrib[&quot;Name&quot;],\n     &quot;Value&quot;: signal.attrib[&quot;Value&quot;]\n    } for signal in file_xml.findall(&quot;.//Signal&quot;)\n]\n\nsignals_df = pd.DataFrame(data)\n\nsignals_df\n#            Name             Value\n# 0        Status             4 Run\n# 1   PhysicalRes       0 0,1 1,2 2\n# 2        Status             4 Run\n# 3        Status             1 Off\n# 4     GlblClkYr    0 2000,21 2021\n# 5        BrkTot        8191 Fault\n# 6           ACU  0 FrontRequester\n# 7           ACU           7 Radio\n# 8           ACU  0 FrontRequester\n# 9           ACU         4 Granted\n# 10   GlblClkDay           1 1-3 3\n\nsignals_df.groupby([&quot;Name&quot;]).count()\n#              Value\n# Name\n# ACU              4\n# BrkTot           1\n# GlblClkDay       1\n# GlblClkYr        1\n# PhysicalRes      1\n# Status           3\n</code></pre>\n<p>Should you need all attributes of <code>&lt;Signal&gt;</code>, simply return the <code>attrib</code> dictionary:</p>\n<pre class=\"lang-py prettyprint-override\"><code>data = [signal.attrib for signal in file_xml.findall(&quot;.//Signal&quot;)]\n\nsignals_df = pd.DataFrame(data)\n\nsignals_df\n#        Error    Hexval         Name             Value\n# 0   {x:Null}     0,1,2       Status             4 Run\n# 1   {x:Null}     0,1,2  PhysicalRes       0 0,1 1,2 2\n# 2   {x:Null}      0,15       Status             4 Run\n# 3   {x:Null}      0,15       Status             1 Off\n# 4   {x:Null}      0,15    GlblClkYr    0 2000,21 2021\n# 5   {x:Null}      1FFF       BrkTot        8191 Fault\n# 6   {x:Null}  {x:Null}          ACU  0 FrontRequester\n# 7   {x:Null}  {x:Null}          ACU           7 Radio\n# 8   {x:Null}  {x:Null}          ACU  0 FrontRequester\n# 9   {x:Null}  {x:Null}          ACU         4 Granted\n# 10  {x:Null}         1   GlblClkDay           1 1-3 3\n</code></pre>\n<p>And in forthcoming <a href=\"https://pandas.pydata.org/pandas-docs/dev/whatsnew/v1.3.0.html#contributors\" rel=\"nofollow noreferrer\">Pandas v1.3</a>, there is now a direct handler for <a href=\"https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.read_xml.html\" rel=\"nofollow noreferrer\"><code>read_xml</code></a> which by default returns all attributes and child elements:</p>\n<pre class=\"lang-py prettyprint-override\"><code>signals_df = pd.read_xml(&quot;Input.xml&quot;, xpath=&quot;.//Signal&quot;)\n</code></pre>\n<hr />\n<p>Once all data is compiled, run plotting as needed:</p>\n<pre><code>import xml.etree.ElementTree as ET\nimport pandas as pd\nimport matplotlib.pyplot as plt\n...\n\nsignals_df.groupby([&quot;Name&quot;]).count().plot(kind='bar', rot=0)\n\nplt.show()\nplt.clf()\nplt.close()\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/dzO0n.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/dzO0n.png\" alt=\"Plot Output\" /></a></p>\n",
        "question_body": "<p>The development that I have to do in Python consists of taking an xml file with the tree built from the sig. shape:</p>\n<p><img src=\"https://i.stack.imgur.com/WbpDj.png\" alt=\"diagram\" /></p>\n<p>Xml file example:<a href=\"https://github.com/MR/blob/master/example.xml\" rel=\"nofollow noreferrer\">I put here xml file becuase of big it is </a></p>\n<p>The last tag (Signal) as seen in the image above in green is from which I have to extract the values \u200b\u200bof the Name and Value attributes; this can be repeated at the same level two or more times with different values \u200b\u200bin these attributes.</p>\n<p>The functions that read the xml would be the following:</p>\n<pre><code>    import xml.etree.ElementTree as ET\n    import pandas as pd\n    \n    file_xml = ET.parse('ejemplo.xml')\n    print(&quot;File: &quot;, rootXML)\n    rootXML= file_xml.getroot()\n    \n    def fragmentXML(rootXML):\n       for child1 in root:\n          for child2 in child1:\n             for child3 in child2:\n                for child4 in child3:\n                    for child5 in child4:\n                       for child6 in child5:\n                           for child7 in child6:\n                               levelChild7(child7)\n\n             transformData(values)\n</code></pre>\n<p>When taking the data with Pandas, putting it into a dataframe and grouping the data, I group them one by one and not all at once; this is the function I am using to group them and paint them in a graph with Matplotlib:</p>\n<pre><code>     def transformData(data_final):\n        df_dataXML.groupby('Name')['Name'].count().plot(kind='bar')\n        plot.show()\n</code></pre>\n<p>This is the console result of this last function, which brings me all the data separately and obviously the graphs too:</p>\n<p><img src=\"https://i.stack.imgur.com/LquoW.png\" alt=\"graph 1\" /></p>\n<p><strong>The problem is that I do not know if there is any way to join the attributes (Name and Value) of Signal to pass them to Pandas in the Dataframe and I graph everything together with names and values \u200b\u200bin the graph. And I did not graph each name and value separately as in the last image.</strong></p>\n<p>I have tried with lists, tuples and dictionaries but I cannot unite the values, as if the label were a single entity and thus it was painted, separately.</p>\n<p>Next I share a failed attempt with lists by changing the levelChild7 and transformData function:</p>\n<pre><code>    def levelChild7(child):\n       nameSignal = []\n       valueSignal = []\n       if child7.tag == 'chid7_e':\n          for child8 in child7:\n             for child9 in child8:\n             print(child9.tag)\n       elif child7.tag == 'chid7_f':\n          for child8 in child7:\n             for child9 in child8:\n                print(child9.tag)\n       elif child7.tag == 'chid7_p':\n          for child8 in child7:\n             for child9 in child8:\n                print(child9.tag)\n       else:\n          for child8 in child7:\n             nameSignal.append(Signal.attrib['Name'])\n             prevValueSignal = Signal.attrib['Value']\n             splitValueSignal = prevValueSignal.split(' ')\n             valueSignal1st = splitValueSignal[0]\n             valueSignal.append(int(valueSignal1st))\n             values = nameSignal+valueSignal\n             #print(values) \n\n             transformData(values)\n\n    def transformData(data_final):\n       df_dataXML = pd.DataFrame(data_final)\n       print(df_dataXML)\n</code></pre>\n<p>The result is this below, which is the Name and Value attribute as something that I cannot find how to group, since they are separated by rows and by each tag that it finds in the XML document:</p>\n<p><img src=\"https://i.stack.imgur.com/sfW0B.png\" alt=\"result\" /></p>\n<p><strong>Could someone guide me to know how to regroup the values \u200b\u200bof Name and Values \u200b\u200band put them in a single variable? Or tell me what is missing from my code?\nThanks in advance.</strong></p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 17987931,
            "reputation": 457,
            "user_id": 13072752,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gjbqi-6VSgS1r0gUchXvUfs1lXkH8VId1VwRDhd=k-s128",
            "display_name": "Tiago Emanuel Pratas",
            "link": "https://stackoverflow.com/users/13072752/tiago-emanuel-pratas"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 68184756,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624995912,
        "creation_date": 1624995160,
        "question_id": 68184632,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68184632/look-for-name-in-column-and-get-their-index",
        "title": "Look for name in column and get their index",
        "answer_body": "<p><code>.get_loc</code> gives you the integer index for an index.</p>\n<p><code>'pair_stock'</code> isn't the index.</p>\n<p>One option you have is to make it the index, which I think is actually what you want.</p>\n<p>Another option (to get the index value for a row with that label) is akin to this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.loc[df['pair_stock']==&quot;MMM-MO&quot;].index.values\n</code></pre>\n<p>That gives you an array. You can grab just the first item, but if you know it's unique, maybe it should just be your index.</p>\n",
        "question_body": "<p>I have a dataframe with several names. I want to look for the name on the pair_stock column and get the index value of that name.</p>\n<p><a href=\"https://i.stack.imgur.com/TL1nu.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/TL1nu.png\" alt=\"enter image description here\" /></a></p>\n<p>So if i do :</p>\n<pre><code>df['pair_stock'].get_loc(&quot;MMM-MO&quot;)\n</code></pre>\n<p>I want to get a 0</p>\n<p>So if i do :</p>\n<pre><code>df['pair_stock'].get_loc(&quot;WU-ZBH&quot;)\n</code></pre>\n<p>But it shows these error</p>\n<p>i want to get 5539</p>\n<p>But it shows these error:</p>\n<p><a href=\"https://i.stack.imgur.com/UqcHc.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/UqcHc.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "date"
        ],
        "owner": {
            "account_id": 13484607,
            "reputation": 149,
            "user_id": 9728755,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3853a85a63dbaa8a8e715613059fa645?s=128&d=identicon&r=PG&f=1",
            "display_name": "Gaurav",
            "link": "https://stackoverflow.com/users/9728755/gaurav"
        },
        "is_answered": true,
        "view_count": 74,
        "closed_date": 1625041613,
        "accepted_answer_id": 68183637,
        "answer_count": 1,
        "score": -2,
        "last_activity_date": 1624990748,
        "creation_date": 1624987955,
        "last_edit_date": 1624988838,
        "question_id": 68183270,
        "link": "https://stackoverflow.com/questions/68183270/get-missing-dates-from-datafrane-pandas",
        "closed_reason": "Needs details or clarity",
        "title": "Get missing dates from datafrane pandas",
        "answer_body": "<p>Let's try:</p>\n<pre><code>df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\nmidx = pd.MultiIndex.from_frame(\n    df.groupby('key')['date'].agg(['min', 'max'])\n        .apply(lambda x: pd.date_range(x['min'], x['max']), axis=1)\n        .explode()\n        .reset_index(),\n    names=['key', 'date']\n)\n\nnew_df = (df.set_index(['key', 'date'])\n          .reindex(midx)\n          .loc[lambda df_: df_['size'].isna()]\n          .index\n          .to_frame(index=False)\n          .rename(columns={0: 'date'}))\n</code></pre>\n<p><code>new_df</code>:</p>\n<pre><code>     key       date\n0  Hello 2018-10-05\n1     No 2018-10-02\n2     No 2018-10-04\n</code></pre>\n<p>Optional convert dates back to strings:</p>\n<pre><code>new_df['date'] = new_df['date'].dt.strftime('%Y%m%d')\n</code></pre>\n<pre><code>     key      date\n0  Hello  20181005\n1     No  20181002\n2     No  20181004\n</code></pre>\n<hr />\n<p>DataFrame:</p>\n<pre><code>df = pd.DataFrame({\n    'size': ['153.2 K', '153.2 K', '153.2 K', '153.2 K', '153.2 K', '153.2 K',\n             '153.2 K'],\n    'number': [12345, 12345, 12345, 12345, 12345, 12345, 12345],\n    'key': ['Hello', 'No', 'Hello', 'No', 'Hello', 'No', 'Hello'],\n    'date': [20181002, 20181001, 20181003, 20181003, 20181004, 20181005,\n             20181006]\n})\n</code></pre>\n<p>Explanations:</p>\n<ol>\n<li><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html\" rel=\"nofollow noreferrer\"><code>Groupby aggregate</code></a> <code>min</code> and <code>max</code> values per <code>key</code> group:</li>\n</ol>\n<pre><code>df.groupby('key')['date'].agg(['min', 'max'])\n             min        max\nkey                        \nHello 2018-10-02 2018-10-06\nNo    2018-10-01 2018-10-05\n</code></pre>\n<ol start=\"2\">\n<li>Turn this into a <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html\" rel=\"nofollow noreferrer\"><code>Date Range</code></a>:</li>\n</ol>\n<pre><code>df.groupby('key')['date'].agg(['min', 'max']) \\\n        .apply(lambda x: pd.date_range(x['min'], x['max']), axis=1)\n</code></pre>\n<pre class=\"lang-none prettyprint-override\"><code>key\nHello    DatetimeIndex(['2018-10-02', '2018-10-03', '20...\nNo       DatetimeIndex(['2018-10-01', '2018-10-02', '20...\ndtype: object\n</code></pre>\n<ol start=\"3\">\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.explode.html\" rel=\"nofollow noreferrer\"><code>explode</code></a> into multiple rows:</li>\n</ol>\n<pre><code>df.groupby('key')['date'].agg(['min', 'max']) \\\n        .apply(lambda x: pd.date_range(x['min'], x['max']), axis=1) \\\n        .explode()\n \nkey\nHello   2018-10-02\nHello   2018-10-03\nHello   2018-10-04\nHello   2018-10-05\nHello   2018-10-06\nNo      2018-10-01\nNo      2018-10-02\nNo      2018-10-03\nNo      2018-10-04\nNo      2018-10-05\ndtype: datetime64[ns]\n</code></pre>\n<ol start=\"4\">\n<li>Turn this frame into a <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.MultiIndex.from_frame.html\" rel=\"nofollow noreferrer\"><code>MultiIndex.from_frame</code></a>:</li>\n</ol>\n<pre><code>midx = pd.MultiIndex.from_frame(\n    df.groupby('key')['date'].agg(['min', 'max'])\n        .apply(lambda x: pd.date_range(x['min'], x['max']), axis=1)\n        .explode()\n        .reset_index(),\n    names=['key', 'date']\n)\n\nMultiIndex([('Hello', '2018-10-02'),\n            ('Hello', '2018-10-03'),\n            ('Hello', '2018-10-04'),\n            ('Hello', '2018-10-05'),\n            ('Hello', '2018-10-06'),\n            (   'No', '2018-10-01'),\n            (   'No', '2018-10-02'),\n            (   'No', '2018-10-03'),\n            (   'No', '2018-10-04'),\n            (   'No', '2018-10-05')],\n           names=['key', 'date'])\n</code></pre>\n<p>The rest is borrowed from <a href=\"https://stackoverflow.com/a/68182031/15497888\">this excellent answer</a> by @ScottBoston</p>\n<ol start=\"5\">\n<li><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html\" rel=\"nofollow noreferrer\"><code>set_index</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html\" rel=\"nofollow noreferrer\"><code>reindex</code></a> with the Multi-Index:</li>\n</ol>\n<pre><code>df.set_index(['key', 'date'])\\\n          .reindex(midx)\n\n                     size   number\nkey   date                           \nHello 2018-10-02  153.2 K  12345.0\n      2018-10-03  153.2 K  12345.0\n      2018-10-04  153.2 K  12345.0\n      2018-10-05      NaN      NaN\n      2018-10-06  153.2 K  12345.0\nNo    2018-10-01  153.2 K  12345.0\n      2018-10-02      NaN      NaN\n      2018-10-03  153.2 K  12345.0\n      2018-10-04      NaN      NaN\n      2018-10-05  153.2 K  12345.0\n</code></pre>\n<ol start=\"6\">\n<li>Keep <code>NaN</code> rows with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>loc</code></a>:</li>\n</ol>\n<pre><code>df.set_index(['key', 'date']) \\\n          .reindex(midx) \\\n          .loc[lambda df_: df_['size'].isna()]\n \n                 size  number\nkey   date                      \nHello 2018-10-05  NaN     NaN\nNo    2018-10-02  NaN     NaN\n      2018-10-04  NaN     NaN\n</code></pre>\n<ol start=\"7\">\n<li>Turn the remaining index into a <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Index.to_frame.html\" rel=\"nofollow noreferrer\"><code>to_frame</code></a>:</li>\n</ol>\n<pre><code>(df.set_index(['key', 'date'])\n .reindex(midx)\n .loc[lambda df_: df_['size'].isna()]\n .index\n .to_frame(index=False))\n\n     key       date\n0  Hello 2018-10-05\n1     No 2018-10-02\n2     No 2018-10-04\n</code></pre>\n",
        "question_body": "<p>I want to get missing dates for each keys in columns. my dataframe as follows,</p>\n<pre><code>     size     number     key      date\n0  153.2 K    12345     Hello     20181002\n1  153.2 K    12345     No        20181001\n2  153.2 K    12345     Hello     20181003\n3  153.2 K    12345     No        20181003\n4  153.2 K    12345     Hello     20181004\n5  153.2 K    12345     No        20181005\n6  153.2 K    12345     Hello     20181006\n</code></pre>\n<p>and i want to get following result</p>\n<pre><code> key        date\n  No        20181002\n  No        20181004\n  Hello     20181005\n</code></pre>\n<p>for key=&quot;No&quot; there are no date 20181002, 20181004. what i am trying to say is for each key there is different date range and the output should be missing dates for each key in there specific date range</p>\n<p>following code i have used but it takes dates of both keys and takes start date as 20181001 and end date as 20181006</p>\n<pre><code>import pandas as pd\n\ndf = pd.read_csv('4002.csv')\nprint(df.head(1))\n\ndates = pd.date_range(*pd.to_datetime(df['date'], format='%Y%m%d',errors='coerce').agg(['min', 'max']), freq='D').strftime('%Y%m%d').astype(int)\ndf1=pd.DataFrame(index=pd.Index(df['key'].unique(), name='key'),columns=dates.difference(df['date'])).reset_index().melt('key').drop(columns=['value'])\nprint(df1)\ndf1.to_csv('4002output.csv', index=False)\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "rows"
        ],
        "owner": {
            "account_id": 22082485,
            "reputation": 13,
            "user_id": 16341857,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/94641832cc8c964eaa7dbb708aa6ec43?s=128&d=identicon&r=PG&f=1",
            "display_name": "Allyx",
            "link": "https://stackoverflow.com/users/16341857/allyx"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68178024,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624989746,
        "creation_date": 1624965908,
        "last_edit_date": 1624989746,
        "question_id": 68177757,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68177757/pandas-deleting-rows-with-string-a-in-column-x-and-string-b-in-column-y",
        "title": "Pandas - Deleting rows with string a in column x and string b in column y",
        "answer_body": "<p>The problem with your code (<code>df[~df.\u2026)]</code>) is that it creates a <strong>temporary</strong> output,\nbut the original DataFrame remains unchanged.</p>\n<p>To get your expected result:</p>\n<ul>\n<li>run <em>drop</em> passing indices of all rows which you can select by comparison\nwith any fixed values,</li>\n<li>then, to filter out <em>NaN</em> in <em>D</em> column, run <em>dropna</em>, as in your code,\nbut this should be a <strong>chained</strong> statement,</li>\n<li>save the result back under <em>df</em>.</li>\n</ul>\n<p>Example:</p>\n<pre><code>df = df.drop(df.query(&quot;A == 'out' or B == 'winter' or D == 0&quot;).index).dropna(subset = ['D'])\n</code></pre>\n<p>Then when you print <em>df</em> you will get:</p>\n<pre><code>    A       B  C    D\n5  in  summer  y  2.6\n</code></pre>\n<h1>Another option</h1>\n<p>Run:</p>\n<pre><code>df = df.drop(df.query(&quot;A == 'out' or B == 'winter' or D == 0 or D != D&quot;).index)\n</code></pre>\n<p>This code relies on the fact that <em>NaN</em> is <strong>not</strong> equal to another <em>NaN</em>,\nso you can do it without explicit call to <em>dropna</em>.</p>\n<h1>Yet another option</h1>\n<p>Just <em>query</em> for the <strong>wanted</strong> rows and save the result back under <em>df</em>:</p>\n<pre><code>df = df.query(&quot;A != 'out' and B != 'winter' and D != 0 and D == D&quot;)\n</code></pre>\n<p>To see any &quot;partial set&quot; of rows to be dropped, you can run <em>query</em> with\nthe respective partial query, e.g.:</p>\n<pre><code>df.query(&quot;A == 'out'&quot;)\n</code></pre>\n<p>will show rows with the &quot;unwanted&quot; value in <em>A</em> column.</p>\n<p>If you want to print rows with &quot;unwanted&quot; values in e.g. 2 columns, extend the query\naccordingly:</p>\n<pre><code>df.query(&quot;A == 'out' or B == 'winter'&quot;)\n</code></pre>\n",
        "question_body": "<p>I'm looking for a method to delete rows that contain certain different strings in different columns at the same time.</p>\n<p>I have a dataset like this:</p>\n<pre class=\"lang-python prettyprint-override\"><code>    A       B      C       D\n0  in    summer    x      NaN\n1  in    summer    y      0.0\n2  out   summer    g      3.2\n3  out   winter    h      4.4\n4  in    winter    e      0.0\n5  in    summer    y      2.6\n</code></pre>\n<p>I want to get rid of all entries that in A contain out, in B contain winter, and get rid of all NaN and 0.0 values in D</p>\n<p>To get rid of the NaNs I used:</p>\n<pre><code>df.dropna(subset = ['D'])\n</code></pre>\n<p>and this to clean out winter from B</p>\n<pre><code>df[~df.B.str.contains('winter')] \ndf[~df.A.str.contains('out')] --&gt; the above for winter will be undone\n</code></pre>\n<p>But when trying to use the upper command again to get rid of out in A, the first command acts like it never happened and the winter entries are all back, except that I sorted out the out in A. And how do I get rid of the 0.0 float values in D as well?</p>\n<p>Desired output:</p>\n<pre class=\"lang-python prettyprint-override\"><code>    A       B      C       D\n5  in    summer    y      2.6\n</code></pre>\n<p>Sorry, I'm super inexperienced in Python but I need to do this for a project.</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 19965788,
            "reputation": 143,
            "user_id": 14632409,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/6b59ada5cbd7c398f0f0dd398db38872?s=128&d=identicon&r=PG&f=1",
            "display_name": "Shivika Patel",
            "link": "https://stackoverflow.com/users/14632409/shivika-patel"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 68181545,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624980958,
        "creation_date": 1624979457,
        "question_id": 68181399,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68181399/how-to-give-sequence-number-of-list-items-and-create-dataframe",
        "title": "How to give sequence number of list items and create dataframe?",
        "answer_body": "<p>You can use:</p>\n<pre><code>lst = ['file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt', 'file6.txt', 'file7.txt', 'file8.txt', 'file9.txt', 'file10.txt', 'file11.txt', 'file12.txt', 'file13.txt', 'file14.txt', 'file15.txt']\n\ndf = pd.DataFrame({'filename': lst, 'sequence': [x // 2 + 1 for x in range(len(lst))]})\n</code></pre>\n<p><strong>Result:</strong></p>\n<pre><code>print(df)\n\n      filename  sequence\n0    file1.txt         1\n1    file2.txt         1\n2    file3.txt         2\n3    file4.txt         2\n4    file5.txt         3\n5    file6.txt         3\n6    file7.txt         4\n7    file8.txt         4\n8    file9.txt         5\n9   file10.txt         5\n10  file11.txt         6\n11  file12.txt         6\n12  file13.txt         7\n13  file14.txt         7\n14  file15.txt         8\n</code></pre>\n",
        "question_body": "<p>I have a list and I want to give sequence number to every item and create a dataframe after that. Please help me how can I do this?</p>\n<p>Example:</p>\n<pre><code>lst = ['file1.txt', 'file2.txt', 'file3.txt', 'file4.txt', 'file5.txt', 'file6.txt', 'file7.txt', 'file8.txt', 'file9.txt', 'file10.txt', 'file11.txt', 'file12.txt', 'file13.txt', 'file14.txt', 'file15.txt']\n</code></pre>\n<p>I have to give sequence number like this-</p>\n<pre><code>df:\n\nfilename       sequence\nfile1.txt       1\nfile2.txt       1\nfile3.txt       2\nfile4.txt       2\nfile5.txt       3\nfile6.txt       3\nfile7.txt       4\nfile8.txt       4\nfile9.txt       5\nfile10.txt      5\nfile11.txt      6\nfile12.txt      6\nfile13.txt      7\nfile14.txt      7\nfile15.txt      8\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "account_id": 22053111,
            "reputation": 11,
            "user_id": 16317070,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/12bcbde1380ff5897a98f133fd7eb54e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Anthony Knighton",
            "link": "https://stackoverflow.com/users/16317070/anthony-knighton"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 68181723,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624980687,
        "creation_date": 1624646211,
        "question_id": 68135746,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68135746/creating-pandas-dataframe-from-dictionary",
        "title": "Creating pandas dataframe from dictionary",
        "answer_body": "<p>I found that the solution was to pass a tuple that contains a slice when indexing the values assigned to the <code>W</code> key. This was necessary because each value assigned to the <code>W</code> key was an array at each time stamp. These arrays are made of 6 elements, each one corresponding to some parameters <code>W1</code>-<code>W6</code>. Slicing was necessary in order to extract the values from the array that corresponded to those parameters. Here is some code that works.</p>\n<pre><code># Data to be formatted into pandas data frame: PDYN, DST, BYIMF, BZIMF, W1, W2, W3, W4, W5, W6.\n\n# Import the necessary modules\nimport numpy as np\nimport datetime as dtm\nimport pandas as pd\nimport json\nimport spacepy\nimport spacepy.time as spt\nimport spacepy.omni as spo\n#import ipdb;ipdb.set_trace()\n\n# Initial time: 2003-10-29T06:00:00 (DST = -10nT)\n# Final time: 2003-10-30T17:00:00 (DST = -97nT)\n\n# Extract the data during time interval using spacepy\nstart_time = dtm.datetime(2003, 10, 29, 6)                      # Initial time\nend_time = dtm.datetime(2003, 10, 30, 17)                       # Final time\ndt = dtm.timedelta(hours = 1)                                   # Time delta\nticks = spt.tickrange(start_time, end_time, dt, 'UTC')          # Range for time ticks\ntime_data = spo.get_omni(ticks)                                 # Create data dictionary\n\n# Equate the index with the hour\ndatetime_series = pd.Series(pd.date_range(start = start_time, end = end_time, freq = 'h'))\n# Create time dependent data frame using Pandas\nd ={'Date, Time': datetime_series, 'PDYN': time_data['Pdyn'], 'DST': time_data['Dst'],'BYIMF': time_data['ByIMF'], 'BZIMF': time_data['BzIMF'], 'W1': time_data['W'][:,0], 'W2': time_data['W'][:,1], 'W3': time_data['W'][:,2], 'W4': time_data['W'][:,3], 'W5': time_data['W'][:,4], 'W6': time_data['W'][:,5]}\ndf = pd.DataFrame(d)\nprint(df)\n</code></pre>\n",
        "question_body": "<p>I have written the python following code:</p>\n<pre><code># Data to be formatted into pandas data frame: PDYN, DST, BYIMF, BZIMF, W1, W2, W3, W4, W5, W6.\n\n# Import the necessary modules\nimport numpy as np\nimport datetime as dtm\nimport pandas as pd\nimport spacepy\nimport spacepy.time as spt\nimport spacepy.omni as spo\n\n# Initial time: 2003-10-29T06:00:00 (DST = -10nT)\n# Final time: 2003-10-30T17:00:00 (DST = -97nT)\n\n# Extract the data during time interval using spacepy\n\nstart_time = dtm.datetime(2003, 10, 29, 6)                  # Initial time\nend_time = dtm.datetime(2003, 10, 30, 17)               # Final time\ndt = dtm.timedelta(hours = 1)                               # Time delta\nticks = spt.tickrange(start_time, end_time, dt, 'UTC')      # Range for time ticks\ntime_data = spo.get_omni(ticks)                                 # Create data dictionary\n\n# Create data frame using Pandas\nd = {'Time Stamp': time_data['ticks'], 'PDYN': time_data['Pdyn'], 'DST': time_data['Dst'], 'BYIMF': time_data['ByIMF'], 'BZIMF': time_data['BzIMF'], 'W1': time_data['W'][0], 'W2': time_data['W'][1], 'W3': time_data['W'][2], 'W4': time_data['W'][3], 'W5': time_data['W'][4], 'W6': time_data['W'][5]}\ndf = pd.DataFrame(data = d)\ndf\n</code></pre>\n<p>I have extracted data from spacepy, and I am trying to create a pandas data frame representing the parameters that I need. As you can see, the variable <code>time_data</code> is a dictionary. When I go to create the data frame, I continue to get the following error:</p>\n<pre><code>ValueError: arrays must all be same length.\n</code></pre>\n<p>In formatting the dictionary, the key <code>W</code> corresponds to an array that is composed of 6 other arrays which correspond to the parameters <code>W1</code>-<code>W6</code>, respectively. For these, I am trying to index the <code>W</code> array inside the dictionary. Is there a better way to do this? In trying to diagnose the problem, I just wanted to see if it would produce a data frame for at least one of the parameters. With this, I had</p>\n<pre><code># Import the necessary modules\nimport numpy as np\nimport datetime as dtm\nimport pandas as pd\nimport spacepy\nimport spacepy.time as spt\nimport spacepy.omni as spo\n\n# Initial time: 2003-10-29T06:00:00 (DST = -10nT)\n# Final time: 2003-10-30T17:00:00 (DST = -97nT)\n\n# Extract the data during time interval using spacepy\n\nstart_time = dtm.datetime(2003, 10, 29, 6)                  # Initial time\nend_time = dtm.datetime(2003, 10, 30, 17)               # Final time\ndt = dtm.timedelta(hours = 1)                               # Time delta\nticks = spt.tickrange(start_time, end_time, dt, 'UTC')      # Range for time ticks\ntime_data = spo.get_omni(ticks)                                 # Create data dictionary\n\n# Create data frame using Pandas\nd = {'Time Stamp': time_data['ticks']}\ndf = pd.DataFrame(data = d)\ndf\n</code></pre>\n<p>With this I get two errors:</p>\n<pre><code>ValueError: maximum supported dimension for an ndarray is 32, found 33\n</code></pre>\n<p>and</p>\n<pre><code>ValueError: Shape of passed values is (1, 36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), indices imply (36, 1)\n</code></pre>\n<p>Does anyone have any solutions? Thank you.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "numpy",
            "dataframe",
            "merge"
        ],
        "owner": {
            "account_id": 6321039,
            "reputation": 280076,
            "user_id": 4909087,
            "user_type": "registered",
            "accept_rate": 97,
            "profile_image": "https://i.stack.imgur.com/LmD3e.png?s=128&g=1",
            "display_name": "cs95",
            "link": "https://stackoverflow.com/users/4909087/cs95"
        },
        "is_answered": true,
        "view_count": 16123,
        "accepted_answer_id": 53699013,
        "answer_count": 3,
        "score": 56,
        "last_activity_date": 1624978983,
        "creation_date": 1544411559,
        "last_edit_date": 1548275692,
        "question_id": 53699012,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53699012/performant-cartesian-product-cross-join-with-pandas",
        "title": "Performant cartesian product (CROSS JOIN) with pandas",
        "answer_body": "<p>Let's start by establishing a benchmark. The easiest method for solving this is using a temporary &quot;key&quot; column:</p>\n<pre><code># pandas &lt;= 1.1.X\ndef cartesian_product_basic(left, right):\n    return (\n       left.assign(key=1).merge(right.assign(key=1), on='key').drop('key', 1))\n\ncartesian_product_basic(left, right)\n</code></pre>\n\n<pre><code># pandas &gt;= 1.2 (est)\nleft.merge(right, how=&quot;cross&quot;)\n</code></pre>\n\n<pre><code>  col1_x  col2_x col1_y  col2_y\n0      A       1      X      20\n1      A       1      Y      30\n2      A       1      Z      50\n3      B       2      X      20\n4      B       2      Y      30\n5      B       2      Z      50\n6      C       3      X      20\n7      C       3      Y      30\n8      C       3      Z      50\n</code></pre>\n<p>How this works is that both DataFrames are assigned a temporary &quot;key&quot; column with the same value (say, 1). <code>merge</code> then performs a many-to-many JOIN on &quot;key&quot;.</p>\n<p>While the many-to-many JOIN trick works for reasonably sized DataFrames, you will see relatively lower performance on larger data.</p>\n<p>A faster implementation will require NumPy. Here are some famous <a href=\"https://stackoverflow.com/a/11146645/4909087\">NumPy implementations of 1D cartesian product</a>. We can build on some of these performant solutions to get our desired output. My favourite, however, is @senderle's first implementation.</p>\n<pre><code>def cartesian_product(*arrays):\n    la = len(arrays)\n    dtype = np.result_type(*arrays)\n    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n    for i, a in enumerate(np.ix_(*arrays)):\n        arr[...,i] = a\n    return arr.reshape(-1, la)  \n</code></pre>\n<h3>Generalizing: CROSS JOIN on Unique <em>or</em> Non-Unique Indexed DataFrames</h3>\n<blockquote>\n<p><strong>Disclaimer</strong><br />\nThese solutions are optimised for DataFrames with non-mixed scalar dtypes. If dealing with mixed dtypes, use at your\nown risk!</p>\n</blockquote>\n<p>This trick will work on any kind of DataFrame. We compute the cartesian product of the DataFrames' numeric indices using the aforementioned <code>cartesian_product</code>, use this to reindex the DataFrames, and</p>\n<pre><code>def cartesian_product_generalized(left, right):\n    la, lb = len(left), len(right)\n    idx = cartesian_product(np.ogrid[:la], np.ogrid[:lb])\n    return pd.DataFrame(\n        np.column_stack([left.values[idx[:,0]], right.values[idx[:,1]]]))\n\ncartesian_product_generalized(left, right)\n\n   0  1  2   3\n0  A  1  X  20\n1  A  1  Y  30\n2  A  1  Z  50\n3  B  2  X  20\n4  B  2  Y  30\n5  B  2  Z  50\n6  C  3  X  20\n7  C  3  Y  30\n8  C  3  Z  50\n\nnp.array_equal(cartesian_product_generalized(left, right),\n               cartesian_product_basic(left, right))\nTrue\n</code></pre>\n<p>And, along similar lines,</p>\n<pre><code>left2 = left.copy()\nleft2.index = ['s1', 's2', 's1']\n\nright2 = right.copy()\nright2.index = ['x', 'y', 'y']\n    \n\nleft2\n   col1  col2\ns1    A     1\ns2    B     2\ns1    C     3\n\nright2\n  col1  col2\nx    X    20\ny    Y    30\ny    Z    50\n\nnp.array_equal(cartesian_product_generalized(left, right),\n               cartesian_product_basic(left2, right2))\nTrue\n</code></pre>\n<p>This solution can generalise to multiple DataFrames. For example,</p>\n<pre><code>def cartesian_product_multi(*dfs):\n    idx = cartesian_product(*[np.ogrid[:len(df)] for df in dfs])\n    return pd.DataFrame(\n        np.column_stack([df.values[idx[:,i]] for i,df in enumerate(dfs)]))\n\ncartesian_product_multi(*[left, right, left]).head()\n\n   0  1  2   3  4  5\n0  A  1  X  20  A  1\n1  A  1  X  20  B  2\n2  A  1  X  20  C  3\n3  A  1  X  20  D  4\n4  A  1  Y  30  A  1\n</code></pre>\n<h3>Further Simplification</h3>\n<p>A simpler solution not involving @senderle's <code>cartesian_product</code> is possible when dealing with <em>just two</em> DataFrames. Using <code>np.broadcast_arrays</code>, we can achieve almost the same level of performance.</p>\n<pre><code>def cartesian_product_simplified(left, right):\n    la, lb = len(left), len(right)\n    ia2, ib2 = np.broadcast_arrays(*np.ogrid[:la,:lb])\n\n    return pd.DataFrame(\n        np.column_stack([left.values[ia2.ravel()], right.values[ib2.ravel()]]))\n\nnp.array_equal(cartesian_product_simplified(left, right),\n               cartesian_product_basic(left2, right2))\nTrue\n</code></pre>\n<hr />\n<h3>Performance Comparison</h3>\n<p>Benchmarking these solutions on some contrived DataFrames with unique indices, we have</p>\n<p><a href=\"https://i.stack.imgur.com/S92LX.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/S92LX.png\" alt=\"enter image description here\" /></a></p>\n<p>Do note that timings may vary based on your setup, data, and choice of <code>cartesian_product</code> helper function as applicable.</p>\n<p><strong>Performance Benchmarking Code</strong><br />\nThis is the timing script. All functions called here are defined above.</p>\n<pre><code>from timeit import timeit\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nres = pd.DataFrame(\n       index=['cartesian_product_basic', 'cartesian_product_generalized', \n              'cartesian_product_multi', 'cartesian_product_simplified'],\n       columns=[1, 10, 50, 100, 200, 300, 400, 500, 600, 800, 1000, 2000],\n       dtype=float\n)\n\nfor f in res.index: \n    for c in res.columns:\n        # print(f,c)\n        left2 = pd.concat([left] * c, ignore_index=True)\n        right2 = pd.concat([right] * c, ignore_index=True)\n        stmt = '{}(left2, right2)'.format(f)\n        setp = 'from __main__ import left2, right2, {}'.format(f)\n        res.at[f, c] = timeit(stmt, setp, number=5)\n\nax = res.div(res.min()).T.plot(loglog=True) \nax.set_xlabel(&quot;N&quot;); \nax.set_ylabel(&quot;time (relative)&quot;);\n\nplt.show()\n</code></pre>\n<hr />\n<hr />\n<h1>Continue Reading</h1>\n<p>Jump to other topics in Pandas Merging 101 to continue learning:</p>\n<ul>\n<li><p><a href=\"https://stackoverflow.com/a/53645883/4909087\">Merging basics - basic types of joins</a></p>\n</li>\n<li><p><a href=\"https://stackoverflow.com/a/65167356/4909087\">Index-based joins</a></p>\n</li>\n<li><p><a href=\"https://stackoverflow.com/a/65167327/4909087\">Generalizing to multiple DataFrames</a></p>\n</li>\n<li><p><a href=\"https://stackoverflow.com/a/53699013/4909087\">Cross join</a> <sup>*</sup></p>\n</li>\n</ul>\n<p><sub>* you are here </sub></p>\n",
        "question_body": "<blockquote>\n  <p>The contents of this post were originally meant to be a part of\n  <a href=\"https://stackoverflow.com/questions/53645882/pandas-merging-101\">Pandas Merging 101</a>,\n  but due to the nature and size of the content required to fully do\n  justice to this topic, it has been moved to its own QnA.</p>\n</blockquote>\n\n<p>Given two simple DataFrames; </p>\n\n<pre><code>left = pd.DataFrame({'col1' : ['A', 'B', 'C'], 'col2' : [1, 2, 3]})\nright = pd.DataFrame({'col1' : ['X', 'Y', 'Z'], 'col2' : [20, 30, 50]})\n\nleft\n\n  col1  col2\n0    A     1\n1    B     2\n2    C     3\n\nright\n\n  col1  col2\n0    X    20\n1    Y    30\n2    Z    50\n</code></pre>\n\n<p>The cross product of these frames can be computed, and will look something like:</p>\n\n<pre><code>A       1      X      20\nA       1      Y      30\nA       1      Z      50\nB       2      X      20\nB       2      Y      30\nB       2      Z      50\nC       3      X      20\nC       3      Y      30\nC       3      Z      50\n</code></pre>\n\n<p>What is the most performant method of computing this result?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 3042085,
            "reputation": 443,
            "user_id": 2578823,
            "user_type": "registered",
            "accept_rate": 88,
            "profile_image": "https://www.gravatar.com/avatar/2c58fc8987f992bd9bc815c2bd15c057?s=128&d=identicon&r=PG",
            "display_name": "jjdblast",
            "link": "https://stackoverflow.com/users/2578823/jjdblast"
        },
        "is_answered": true,
        "view_count": 33,
        "closed_date": 1624977603,
        "accepted_answer_id": 68180927,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624977646,
        "creation_date": 1624977287,
        "question_id": 68180837,
        "link": "https://stackoverflow.com/questions/68180837/pandas-merge-two-series-to-a-dataframe-jointly",
        "closed_reason": "Duplicate",
        "title": "Pandas merge two series to a dataframe jointly",
        "answer_body": "<p>One way:</p>\n<pre><code>df = S1.to_frame(name='S1').merge(S2.to_frame(name='S2'), how='cross')\n</code></pre>\n<p>OUTPUT:</p>\n<pre><code>  S1  S2\n0  A   1\n1  A   2\n2  A   3\n3  B   1\n4  B   2\n5  B   3\n</code></pre>\n",
        "question_body": "<p>I want merge two series effectively, <code>S1 = pd.Series([&quot;A&quot;, &quot;B&quot;])</code> and <code>S2 = pd.Series([1, 2, 3])</code>.</p>\n<p>Result dataframe is like below:</p>\n<pre><code>    S1  S2\n 0  A   1\n 1  A   2\n 2  A   3\n 3  B   1\n 4  B   2\n 5  B   3\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "database",
            "dataframe",
            "csv"
        ],
        "owner": {
            "account_id": 22075223,
            "reputation": 11,
            "user_id": 16335653,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GhmtGswAGZLTAkY2QKC3egC_leDqmzLHA6VKxyZXA=k-s128",
            "display_name": "Sean Roudnitsky",
            "link": "https://stackoverflow.com/users/16335653/sean-roudnitsky"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 68179714,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624975074,
        "creation_date": 1624973248,
        "question_id": 68179652,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68179652/pandas-matching-values-across-different-csvs-and-then-appending-a-column-to-or",
        "title": "Pandas - matching values across different CSVs and then appending a column to original file",
        "answer_body": "<p>You are looking for merge,</p>\n<p>Docs here : <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html\" rel=\"nofollow noreferrer\">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html</a></p>\n<p>You can merge the data frames like this,</p>\n<pre><code>data1.merge(data2, on=['inc_key'], how='left')\n</code></pre>\n<p>If you are okay with data loss if the inc_key can't be found in data2, go with inner join.</p>\n<p>You can also select just the columns you need from data2 and join like this,</p>\n<pre><code>data1.merge(data2[list_of_columns + ['inc_key']], on=['inc_key'], how='left')\n</code></pre>\n",
        "question_body": "<p>Primitive programmer here. I have been tasked with cleaning medical data which is stored in csv format.</p>\n<p>(please keep in mind while you read this that I am just a beginner programmer so your patience is appreciated)</p>\n<p>I have a file, we'll call it data1, which looks like this:\n<a href=\"https://i.stack.imgur.com/VQPC3.png\" rel=\"nofollow noreferrer\">data1</a>. It has ~17,000 rows/patients</p>\n<p>inc_key refers to a unique patient ID.</p>\n<p>I have another file, which we'll call data2, which is identical in format except with different information stored in it, however it contains MILLIONS of rows/patients.</p>\n<p>My goal is, for each row/patient in data1, I need to find the matching patient (inc_key value) in data2, and then append (add columns to the end of that patient) the corresponding information to the same patient in data1.</p>\n<p>In other words, I need to merge these two files, except the inc_key values need to match.</p>\n<p>I am using the pandas module, can anyone help me with this?</p>\n<p>Thank you in advance to anyone who helps, it is sincerely appreciated since I am only a beginner programmer.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime",
            "merge"
        ],
        "owner": {
            "account_id": 22036818,
            "reputation": 25,
            "user_id": 16303180,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxmRjS_p0GtNCtVKhl5scPV68BUbaa5fsXwOOid=k-s128",
            "display_name": "Hoi Lun Sin ",
            "link": "https://stackoverflow.com/users/16303180/hoi-lun-sin"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 68160711,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624971495,
        "creation_date": 1624869228,
        "question_id": 68160108,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68160108/merge-three-dataframe-between-datetime-and-adding-corresponding-columns",
        "title": "Merge three dataframe between datetime and adding corresponding columns",
        "answer_body": "<p>You can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html\" rel=\"nofollow noreferrer\"><code>.merge()</code></a> and filter with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.between.html\" rel=\"nofollow noreferrer\"><code>.between()</code></a>, as follows:</p>\n<pre><code>df1_3 = df1.merge(df3, on='Node')\ndf1_3_filtered = df1_3[df1_3['Timestamp'].between(df1_3['Start Time'], df1_3['End Time'])]\n\ndf2_3 = df2.merge(df3, on='Node')\ndf2_3_filtered = df2_3[df2_3['Timestamp'].between(df2_3['Start Time'], df2_3['End Time'])]\n\ndf_out = df1_3_filtered.append(df2_3_filtered)[['Node', 'JobID', 'Timestamp']]\ndf_out = df3.merge(df_out, how='left')\n</code></pre>\n<p><strong>Result:</strong></p>\n<pre><code>print(df_out)\n\n\n  Node Timestamp    JobID\n0    A  00:05:42  12345.0\n1    A  00:09:50  34567.0\n2    A  00:11:27      NaN\n3    B  00:04:48  13579.0\n4    B  00:09:59      NaN\n5    B  00:10:32  14680.0\n</code></pre>\n<h2>Edit</h2>\n<p>If you have multiple dataframes with same structure as <code>df1</code> and <code>df2</code> and want to merge with <code>df3</code>, you can do:</p>\n<p><em><strong>Simply put all your dataframes into the list <code>List_dfs</code> below:</strong></em></p>\n<pre><code>List_dfs = [df1, df2]              # put all your dataframes of same structure here\n</code></pre>\n<p>Then, run the codes below. You will get the merged and filtered results of all these dataframes in <code>df_out</code>:</p>\n<pre><code>df_all_filtered = pd.DataFrame()   # init. df for acculumating filtered results\nfor df in List_dfs:\n    dfx_3 = df.merge(df3, on='Node')\n    dfx_3_filtered = dfx_3[dfx_3['Timestamp'].between(dfx_3['Start Time'], dfx_3['End Time'])]\n    df_all_filtered = df_all_filtered.append(dfx_3_filtered)   # append filtered result\n\ndf_out = df_all_filtered[['Node', 'JobID', 'Timestamp']]\ndf_out = df3.merge(df_out, how='left')\n</code></pre>\n",
        "question_body": "<p>Given two dataframes df1, df2 and df3, how to join them such that df3 timestamps is in between start and end in dataframe df1 and df2.</p>\n<p>I have to merge the Job ID to df3 based whether the df3'Timestamp' is in df1 or df2 'Start time' and 'End Time', and also match the Node(No.</p>\n<p>df1(1230rows*3 columns)</p>\n<pre><code>Node      Start Time      End Time      JobID\nA         00:03:50        00:05:45      12345\nA         00:06:10        00:07:39      56789\nA         00:08:30        00:10:45      34567\n.\n.\n.\n</code></pre>\n<p>df2(1130rows*3 columns)</p>\n<pre><code>Node      Start Time      End Time      JobID\nB         00:02:30        00:07:35      13579\nB         00:08:56        00:09:39      24680\nB         00:10:32        00:13:47      14680\n.\n.\n.\n</code></pre>\n<p>df3(4002rows*3 columns)</p>\n<pre><code>Node      Timestamp     \nA         00:05:42       \nA         00:09:50       \nA         00:11:27       \nB         00:04:48\nB         00:09:59\nB         00:10:32\n.\n.\n.\n.\n</code></pre>\n<p>Expected Output:\ndf3(4002rows*3 columns)</p>\n<pre><code>No.       Timestamp       Job ID\nA         00:05:42        12345              \nA         00:09:50        34567       \nA         00:11:27        NaN\nB         00:04:48        13579\nB         00:09:59        NaN\nB         00:10:32        14680\n.\n.\n.\n.\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 7571911,
            "reputation": 136,
            "user_id": 5745505,
            "user_type": "registered",
            "accept_rate": 93,
            "profile_image": "https://i.stack.imgur.com/29iKP.gif?s=128&g=1",
            "display_name": "JHk1821",
            "link": "https://stackoverflow.com/users/5745505/jhk1821"
        },
        "is_answered": true,
        "view_count": 91,
        "accepted_answer_id": 68176993,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624970482,
        "creation_date": 1624451791,
        "question_id": 68099960,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68099960/why-does-using-merge-function-in-two-different-dataframes-results-me-more-rows",
        "title": "Why does using merge function in two different dataframes results me more rows?",
        "answer_body": "<p>You can use an alternate way, without merge function:</p>\n<pre><code>dwt_lst = []\nfor imo in df1.imo.values:\n   dwt = df2[df2.imo == imo].dwt.values[0]\n   dwt_lst.append(dwt)\ndf1['dwt'] = dwt_lst\n</code></pre>\n",
        "question_body": "<p>I have two dataframes of the shape: (4000,3) (2000,3) , with the bellow info and cols:</p>\n<p><strong>df1:</strong></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">imo</th>\n<th style=\"text-align: center;\">speed</th>\n<th style=\"text-align: center;\">length</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">4</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: center;\">2</td>\n<td style=\"text-align: center;\">4</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">2</td>\n<td style=\"text-align: center;\">10</td>\n<td style=\"text-align: center;\">10</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">2</td>\n<td style=\"text-align: center;\">12</td>\n<td style=\"text-align: center;\">10</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>df2:</strong></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">imo</th>\n<th style=\"text-align: center;\">dwt</th>\n<th style=\"text-align: center;\">name</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: center;\">52</td>\n<td style=\"text-align: center;\">test1</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">2</td>\n<td style=\"text-align: center;\">62</td>\n<td style=\"text-align: center;\">test2</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">3</td>\n<td style=\"text-align: center;\">785</td>\n<td style=\"text-align: center;\">test3</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">4</td>\n<td style=\"text-align: center;\">353</td>\n<td style=\"text-align: center;\">test4</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>i would like to add column <em>dwt</em> of <em><strong>df2</strong></em> to <em><strong>df1</strong></em> based on the same <em>imo</em>.</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">imo</th>\n<th style=\"text-align: center;\">speed</th>\n<th style=\"text-align: center;\">length</th>\n<th style=\"text-align: center;\">dwt</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">4</td>\n<td style=\"text-align: center;\">52</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">1</td>\n<td style=\"text-align: center;\">2</td>\n<td style=\"text-align: center;\">4</td>\n<td style=\"text-align: center;\">52</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">2</td>\n<td style=\"text-align: center;\">10</td>\n<td style=\"text-align: center;\">10</td>\n<td style=\"text-align: center;\">62</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">2</td>\n<td style=\"text-align: center;\">12</td>\n<td style=\"text-align: center;\">10</td>\n<td style=\"text-align: center;\">62</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>but when i am trying to do <code>pd.merge(df1,df2, on = 'imo', how = 'inner')</code> , the result is much more rows than the original shape of <strong>df1</strong> how is that possible?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 10513730,
            "reputation": 585,
            "user_id": 7748514,
            "user_type": "registered",
            "accept_rate": 80,
            "profile_image": "https://i.stack.imgur.com/Dy0VE.png?s=128&g=1",
            "display_name": "Liquidity",
            "link": "https://stackoverflow.com/users/7748514/liquidity"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 68172996,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624970160,
        "creation_date": 1624943714,
        "question_id": 68172967,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68172967/python-and-pandas-use-bin-counts-from-one-df-to-get-similarly-binned-counts-fro",
        "title": "python and pandas: use bin counts from one df to get similarly binned counts from another df with no shared columns",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html\" rel=\"nofollow noreferrer\"><code>cut</code></a> with <code>bins</code> from index by <code>Series</code> called <code>s</code>:</p>\n<pre><code>s = df1['numbers'].value_counts(bins=[0, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1])\n\ndf2['new'] = pd.cut(df2['numbers'], bins=s.index)\nprint (df2)\n    numbers nonshared_column            new\n0      0.10              cat  (-0.001, 0.1]\n1      0.11              dog     (0.1, 0.2]\n2      0.20              cat     (0.1, 0.2]\n3      0.30              dog     (0.2, 0.3]\n4      0.33             fish     (0.3, 0.4]\n5      0.60              cat     (0.5, 0.6]\n6      0.66              dog     (0.6, 0.7]\n7      0.70              dog     (0.6, 0.7]\n8      0.90             fish     (0.8, 0.9]\n9      0.10              cat  (-0.001, 0.1]\n10     0.11              dog     (0.1, 0.2]\n11     0.20              cat     (0.1, 0.2]\n12     0.30              dog     (0.2, 0.3]\n13     0.33             fish     (0.3, 0.4]\n14     0.60              cat     (0.5, 0.6]\n15     0.66              dog     (0.6, 0.7]\n16     0.70              dog     (0.6, 0.7]\n17     0.90             fish     (0.8, 0.9]\n18     0.10              cat  (-0.001, 0.1]\n19     0.11              dog     (0.1, 0.2]\n20     0.20              cat     (0.1, 0.2]\n21     0.30              dog     (0.2, 0.3]\n22     0.33             fish     (0.3, 0.4]\n23     0.60              cat     (0.5, 0.6]\n24     0.66              dog     (0.6, 0.7]\n25     0.70              dog     (0.6, 0.7]\n26     0.98             fish     (0.9, 1.0]\n</code></pre>\n<p>Last if need count per all 3 columns:</p>\n<pre><code>df3 = df2.groupby(['numbers','nonshared_column','new'], observed=True).size().reset_index(name='count')\nprint (df3)\n   numbers nonshared_column            new  count\n0     0.10              cat  (-0.001, 0.1]      3\n1     0.11              dog     (0.1, 0.2]      3\n2     0.20              cat     (0.1, 0.2]      3\n3     0.30              dog     (0.2, 0.3]      3\n4     0.33             fish     (0.3, 0.4]      3\n5     0.60              cat     (0.5, 0.6]      3\n6     0.66              dog     (0.6, 0.7]      3\n7     0.70              dog     (0.6, 0.7]      3\n8     0.90             fish     (0.8, 0.9]      2\n9     0.98             fish     (0.9, 1.0]      1\n</code></pre>\n<p>EDIT:</p>\n<p>If need same counts like <code>s</code> first use <code>sample</code> for random order of rows and then <code>head()</code> with map by <code>s</code> for filter by counts:</p>\n<pre><code>df2 = df2.sample(frac=1).groupby('new', group_keys=False).apply(lambda x: x.head(s[x.name])).sort_index()\nprint (df2)\n    numbers nonshared_column            new\n2      0.20              cat     (0.1, 0.2]\n3      0.30              dog     (0.2, 0.3]\n4      0.33             fish     (0.3, 0.4]\n9      0.10              cat  (-0.001, 0.1]\n11     0.20              cat     (0.1, 0.2]\n14     0.60              cat     (0.5, 0.6]\n15     0.66              dog     (0.6, 0.7]\n17     0.90             fish     (0.8, 0.9]\n24     0.66              dog     (0.6, 0.7]\n26     0.98             fish     (0.9, 1.0]\n</code></pre>\n",
        "question_body": "<p>Given df1, I know how to get binned value counts using <code>.value_counts()</code>:</p>\n<pre><code>df1 = pd.DataFrame({'numbers': [0.1, 0.11, 0.2, 0.3, 0.33, 0.6, 0.66, 0.7, 0.9, 1],\n'another_column': ['blue', 'blue', 'blue', 'red', 'green', 'purple', 'blue', 'blue', 'blue', 'orange']})\n\ndf1['numbers'].value_counts(bins=[0, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1])\n</code></pre>\n<p>result:</p>\n<pre><code>(0.6, 0.7]       2\n(0.1, 0.2]       2\n(0.9, 1.0]       1\n(0.8, 0.9]       1\n(0.5, 0.6]       1\n(0.3, 0.4]       1\n(0.2, 0.3]       1\n(-0.001, 0.1]    1\n(0.7, 0.8]       0\n(0.4, 0.5]       0\nName: numbers, dtype: int64\n</code></pre>\n<p>Given another df much larger than df1 (example below):</p>\n<pre><code>df2 = pd.DataFrame({'numbers': [0.1, 0.11, 0.2, 0.3, 0.33, 0.6, 0.66, 0.7, 0.9, 0.1, 0.11, 0.2, 0.3, 0.33, 0.6, 0.66, 0.7, 0.9, 0.1, 0.11, 0.2, 0.3, 0.33, 0.6, 0.66, 0.7, 0.98],\n'nonshared_column': ['cat', 'dog', 'cat', 'dog', 'fish', 'cat', 'dog', 'dog', 'fish', 'cat', 'dog', 'cat', 'dog', 'fish', 'cat', 'dog', 'dog', 'fish', 'cat', 'dog', 'cat', 'dog', 'fish', 'cat', 'dog', 'dog', 'fish']})\n</code></pre>\n<p>I want to take the bins from df1 to filter df2, so the output df is a subset of df2 that matches bins from df1,</p>\n<p>so the output df will have 1 row with 'numbers' values between 0-0.1, 2 rows with 'numbers' values between 0.1-0.2 ... all the way to 1 row with 'numbers' values between 0.9-1. The output df rows should include all the columns from df2 (the <code>nonshared_column</code> in this example, as well as the <code>numbers</code> column).</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20912304,
            "reputation": 25,
            "user_id": 15650961,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/608bba875916c673ae482bb19acbb17f?s=128&d=identicon&r=PG",
            "display_name": "Tobias",
            "link": "https://stackoverflow.com/users/15650961/tobias"
        },
        "is_answered": true,
        "view_count": 12,
        "accepted_answer_id": 68177702,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624965639,
        "creation_date": 1624964393,
        "question_id": 68177430,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68177430/using-output-of-a-function-as-input-for-the-same-function-for-each-new-iteration",
        "title": "Using output of a function as input for the same function for each new iteration",
        "answer_body": "<p>The easy way would be to fall back to a for loop, instead of a list comprehension:</p>\n<pre><code>df_titles_list = []\nindex_article = 0\nfor element in list_news_january:\n  index_article = func_convert_updates(element, index_article)\n\ndf_titles = pd.concat(df_titles_list, ignore_index=True)\n</code></pre>\n",
        "question_body": "<p>so I am converting multiple docx files to a dataframe file. The code works for one document and this leads to the following structure:</p>\n<pre><code>data = {'Index Title': ['Index first title', 'Index second title'], 'Title': ['title first article, 'title second article'], 'Sources': ['source of first article', 'source of second article']}\ndf = pd.DataFrame(data=data)\n</code></pre>\n<p>The structure is the result from a function:</p>\n<pre><code>def func_convert_updates(filename):\n    path = os.chdir('C:/Users/docxfiles')\n    regex = '\\xc2\\xb7'\n    with open(filename, &quot;rb&quot;) as docx_file:\n        result = mammoth.convert_to_html(docx_file)\n        text = result.value # The raw text\n        text2=re.sub(u'[|\u2022\u25cf]', &quot; &quot;, text, count= 0) \n        with open('output.txt', 'w', encoding='utf-8') as text_file:\n            text_file.write(text2)\n\n    #followed by many lines of code, omitted here, to create a dataframe\n\n    x = 0\n    \n    index = [idx for idx, s in enumerate(article_titles) if '&lt;h2&gt;' in s][0]\n    for i, row in enumerate(article_titles):       \n        if i &lt; index:\n            x = x+1 \n            index_article.append(x)\n        else:\n            if ('empty' in row) or ('&lt;h2&gt;' in row):\n                x = x+1 \n                index_article.append(x)\n            else:\n                index_article.append(x)\n\n    return df_titles, index_article\n</code></pre>\n<p>And then I want to analyse multiple docx files so therefore I wrote the following code:</p>\n<pre><code>\nlist_january= [f for f in listdir('C:/Users/january2021') if isfile(join('C:/Users/january2021', f))]\ndf_titles = pd.concat([func_convert_updates(element) for element in list_news_january], ignore_index=True)\n</code></pre>\n<p>Now the problem is as follows. Each document has multiple articles (so several article titles). I want to  create a dateframe based on multiple documents where each article gets a number. With the following code, I create an index number for each article of each document:</p>\n<p>This works fine, as each article gets a number assigned to it. But the problem is that for each new document, it starts again at x = 0. Somehow, I need to get to get the index_article as output and using this value as input for each new iteration. How can I solve this?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "time-series"
        ],
        "owner": {
            "account_id": 17492579,
            "reputation": 195,
            "user_id": 12684429,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/05f5fbe7177f2a22b70e8e10066b0547?s=128&d=identicon&r=PG&f=1",
            "display_name": "spcol",
            "link": "https://stackoverflow.com/users/12684429/spcol"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 68176576,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624961044,
        "creation_date": 1624960717,
        "question_id": 68176493,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68176493/repeat-script-with-different-string-python",
        "title": "Repeat script with different string - Python",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unstack.html\" rel=\"nofollow noreferrer\"><code>Series.unstack</code></a> by first and third level:</p>\n<pre><code>df1 = pd.melt(df, 'Date', var_name='country', value_name='val')\ndf1['Date']= pd.to_datetime(df1['Date']) \ndf2 = (df1.groupby(['country', df1.Date.dt.year,df1.Date.dt.month])['val']\n           .mean()\n           .unstack([0,2])\n           .div(1745)\n           .round())\n\nprint (df2)\ncountry Norway      Qatar     \nDate        1    12    1    12\nDate                          \n2004       0.0  NaN   0.0  NaN\n2021       NaN  0.0   NaN  0.0\n</code></pre>\n<p>So possible select columns by names:</p>\n<pre><code>Qatar = df2[['Qatar']]\nprint (Qatar)\ncountry Qatar     \nDate       1    12\nDate              \n2004      0.0  NaN\n2021      NaN  0.0\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with a number of countries;</p>\n<pre><code>Date         Qatar  Norway . . . \n \n01/01/2004   43      33\n.\n.\n.\n31/12/2021  32       32 . . .\n</code></pre>\n<p>I need each column to be its' own dataframe and in a pivot table which is fine and I am using the following;</p>\n<pre><code>Qatar = df[['Date', 'Qatar']]\nQatar['Date']= pd.to_datetime(Qatar['Date']) \nQatar = Qatar.groupby([Qatar.Date.dt.year,Qatar.Date.dt.month]).mean()\nQatar = Qatar.unstack()\nQatar = Qatar/1745\nQatar = Qatar.round()\nCell('Exports_Country','D3').df = Qatar\n</code></pre>\n<p>I have 40+ countries, is there any way to write the above for the list of countries without doing it manually??</p>\n<p>I had tried this;</p>\n<pre><code>df1 = pd.melt(df, 'Date', var_name='country', value_name='val')\ndf1['Date']= pd.to_datetime(df1['Date']) \ndf2 = (df1.groupby(['country', df1.Date.dt.year,df1.Date.dt.month])['val']\n           .mean()\n           .unstack()\n           .div(1745)\n           .round())\n</code></pre>\n<p>but unfortunately the raw data it is pulling it changes quite a bit so the code needs to be dynamic enough to be referenced by the name of the country.</p>\n<p>Any help appreciated!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "transform"
        ],
        "owner": {
            "account_id": 16282669,
            "reputation": 53,
            "user_id": 11758143,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-i1RX7W3AE3w/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdy29x3sCxhJttI_EEVWTUIR35vaw/photo.jpg?sz=128",
            "display_name": "Fhtsm",
            "link": "https://stackoverflow.com/users/11758143/fhtsm"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68175203,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1624958807,
        "creation_date": 1624955259,
        "question_id": 68175142,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68175142/pandas-dataframe-wide-to-long-based-on-first-column-values",
        "title": "Pandas DataFrame Wide to Long based on first column values",
        "answer_body": "<p>You can use <code>df.stack</code>:</p>\n<pre><code>&gt;&gt;&gt; df.set_index('ABC') \\\n      .stack() \\\n      .reset_index(level='ABC') \\\n      .reset_index(drop=True)\n\n  ABC  0\n0   A  1\n1   A  2\n2   A  3\n3   B  4\n4   B  5\n5   B  6\n6   C  7\n7   C  8\n8   C  9\n</code></pre>\n<p>or use <code>df.melt</code> as suggested by @MustafaAyd\u0131n:</p>\n<pre><code>&gt;&gt;&gt; df.melt('ABC') \\\n      .sort_values('ABC') \\\n      .drop(columns='variable') \\\n      .reset_index(drop=True)\n\n  ABC value\n0   A     1\n1   A     2\n2   A     3\n3   B     4\n4   B     5\n5   B     6\n6   C     7\n7   C     8\n8   C     9\n</code></pre>\n",
        "question_body": "<p>I would like to transform a Pandas DataFrame of the following wide format</p>\n<pre><code>df = pd.DataFrame([['A', '1', '2', '3'], ['B', '4', '5', '6'], ['C', '7', '8', '9']], columns=['ABC', 'def', 'ghi', 'jkl'])\n\ndf =\n   ABC  def  ghi  jkl\n0    A    1    2    3\n1    B    4    5    6\n2    C    7    8    9\n</code></pre>\n<p>into a long format, where the values from the first column still correspond to the values in the lower-case columns. The column names cannot be used as stub names. The names of the new columns are irrelevant and could be renamed later.</p>\n<p>The output should look something like this:</p>\n<pre><code>df =\n   0  1\n0  A  1\n1  A  2\n2  A  3\n3  B  4\n4  B  5\n5  B  6\n6  C  7\n7  C  8\n8  C  9\n</code></pre>\n<p>I am not sure how to best and efficiently do this. Can this be done with wide_to_long()? Then I would not know how to deal with stub names. The best would be an efficient one-liner that can be used on a large table.</p>\n<p>Many thanks!!</p>\n"
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe",
            "data-structures"
        ],
        "owner": {
            "account_id": 11953345,
            "reputation": 23,
            "user_id": 9079043,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/974eaeafce0b16f35dce0911a26ade93?s=128&d=identicon&r=PG&f=1",
            "display_name": "Isha",
            "link": "https://stackoverflow.com/users/9079043/isha"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68174753,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624955956,
        "creation_date": 1624953161,
        "last_edit_date": 1624953363,
        "question_id": 68174614,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
        "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
        "answer_body": "<p>The problem here is, when you call apply on <code>axis=1</code>, pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; s = pd.Series([1, 2, 3.2])\ns\n0    1.0\n1    2.0\n2    3.2\ndtype: float64\n</code></pre>\n<p>As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to :</p>\n<pre><code>df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n</code></pre>\n<p>There's already an issue <a href=\"https://github.com/pandas-dev/pandas/issues/23230\" rel=\"nofollow noreferrer\">DataFrame.apply unintuitively changes int to float because of another column</a> on github for this upcasting behavior of pandas <code>apply</code>.\nSo, one possible option for you is as I have mentioned in the comment, to call <code>to_json</code> on the entire dataframe as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt;df.to_json(orient='index')\n'{&quot;0&quot;:{&quot;A&quot;:2,&quot;B&quot;:6,&quot;C&quot;:5,&quot;D&quot;:8.0},&quot;1&quot;:{&quot;A&quot;:6,&quot;B&quot;:11,&quot;C&quot;:2,&quot;D&quot;:3.6},&quot;2&quot;:{&quot;A&quot;:1,&quot;B&quot;:5,&quot;C&quot;:7,&quot;D&quot;:5.2}}'\n</code></pre>\n<p>A working solution for you may be using python's <code>json</code> module alongwith <code>DataFrame.to_json()</code>, but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['total'] = list(map(json.dumps, [*json.loads(df.to_json(orient='index')).values()]))\n</code></pre>\n<p><strong>OUTPUT:</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>   A   B  C    D                                total\n0  2   6  5  8.0   {&quot;A&quot;: 2, &quot;B&quot;: 6, &quot;C&quot;: 5, &quot;D&quot;: 8.0}\n1  6  11  2  3.6  {&quot;A&quot;: 6, &quot;B&quot;: 11, &quot;C&quot;: 2, &quot;D&quot;: 3.6}\n2  1   5  7  5.2   {&quot;A&quot;: 1, &quot;B&quot;: 5, &quot;C&quot;: 7, &quot;D&quot;: 5.2}\n</code></pre>\n",
        "question_body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "input_data_frames": [
            "A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n",
            "{\"A\":2.0, \"B\":6.0, \"C\":5.0, \"D\":8.0}\n{\"A\":6.0, \"B\":11.0, \"C\":2.0, \"D\":3.6}\n{\"A\":1.0, \"B\":5.0, \"C\":7.0, \"D\":5.2}\n"
        ],
        "output_codes": [
            ">>> s = pd.Series([1, 2, 3.2])\ns\n0    1.0\n1    2.0\n2    3.2\ndtype: float64\n",
            "df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n"
        ],
        "ques_desc": "I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? ",
        "ans_desc": "The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: ",
        "formatted_input": {
            "qid": 68174614,
            "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
            "question": {
                "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
                "ques_desc": "I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? "
            },
            "io": [
                "A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n",
                "{\"A\":2.0, \"B\":6.0, \"C\":5.0, \"D\":8.0}\n{\"A\":6.0, \"B\":11.0, \"C\":2.0, \"D\":3.6}\n{\"A\":1.0, \"B\":5.0, \"C\":7.0, \"D\":5.2}\n"
            ],
            "answer": {
                "ans_desc": "The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: ",
                "code": [
                    "df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 1795830,
            "reputation": 337,
            "user_id": 1635253,
            "user_type": "registered",
            "accept_rate": 0,
            "profile_image": "https://www.gravatar.com/avatar/59376176b2ccacb43b2200334a1d41bc?s=128&d=identicon&r=PG&f=1",
            "display_name": "thenoirlatte",
            "link": "https://stackoverflow.com/users/1635253/thenoirlatte"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 68175079,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624955036,
        "creation_date": 1624954118,
        "question_id": 68174847,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174847/compare-list-in-a-dataframe-with-another-list-and-if-not-found-then-save-it-in",
        "title": "Compare list in a dataframe with another list, and if not found then save it in another column",
        "answer_body": "<p>The following can solve your problem in one line:</p>\n<pre><code>df['token_not_found_in_vocab'] = df['tokenized_sentenced'].apply(lambda x: list(set(x).difference(vocab_list)))\n</code></pre>\n",
        "question_body": "<p>I want to ask, for example I have a list of vocabulary and a dataframe. The dataframe contains tokenized sentences.</p>\n<pre><code>vocab_list = ['aaa',....,'zzz']\n</code></pre>\n<p>Dataframe</p>\n<pre><code>tokenized_sentenced\n========\n[lorem , ipsum]\n[it , is, a, long, established, fact ]\n[various, versions, have, evolved]\n[the, generated, lorem, ipsum]\n</code></pre>\n<p>How to store the list of the token that is not found in the vocabulary list into a new column in the dataframe. The result should be like this:</p>\n<pre><code>   tokenized_sentenced                        token_not_found_in_vocab\n    =========================================|===========================\n    [lorem , ipsum]                          |[lorem, ipsum]\n    [it , is, a, long, established, fact ]   |[]\n    [various, versions, have, evolved, toq]  |[toq]\n    [the, generated, lorem, ipsum]           |[lorem, ipsum]\n</code></pre>\n<p>i tried this:</p>\n<pre><code>for i in range(0,1005):\n  for j in range(0, len(df['tokenized_sentenced'][i])-1):\n    if (df['tokenized_sentenced'][i][j] not in vocab_list):\n      \n      df['token_not_found_in_vocab'][i].append(df['tokenized_sentenced'][i][j])\n</code></pre>\n<p>but i got error:</p>\n<pre><code>AttributeError: 'str' object has no attribute 'append'\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 19291329,
            "reputation": 33,
            "user_id": 14101264,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c9ffd44e442f23950c697c8c5bffdc6c?s=128&d=identicon&r=PG&f=1",
            "display_name": "johnnydoe",
            "link": "https://stackoverflow.com/users/14101264/johnnydoe"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68173507,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624952405,
        "creation_date": 1624947287,
        "question_id": 68173496,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68173496/checking-if-the-cell-values-in-a-dataframe-are-strings",
        "title": "Checking if the cell values in a dataframe are strings",
        "answer_body": "<p>If there are <code>None</code> like <code>Nonetype</code>s:</p>\n<p>If testing not <code>NaN</code> or <code>None</code>s use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.notna.html\" rel=\"nofollow noreferrer\"><code>Series.notna</code></a>:</p>\n<pre><code>df = pd.DataFrame({&quot;ID&quot;: list(&quot;abcd&quot;), &quot;Image&quot;: [None, &quot;ushfkf.jpg&quot;, &quot;ihfskjd.jpg&quot;,None]})\n\ndf['hasimage1'] = df['Image'].apply(lambda x: isinstance(x, str))\ndf['hasimage2'] = df['Image'].notna()\n\nprint (df)\n  ID        Image  hasimage1  hasimage2\n0  a         None      False      False\n1  b   ushfkf.jpg       True       True\n2  c  ihfskjd.jpg       True       True\n3  d         None      False      False\n</code></pre>\n<p>EDIT:</p>\n<p>If <code>None</code>s are strings:</p>\n<p>Test by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.endswith.html\" rel=\"nofollow noreferrer\"><code>Series.str.endswith</code></a>:</p>\n<pre><code>df = pd.DataFrame({&quot;ID&quot;: list(&quot;abcd&quot;), &quot;Image&quot;: [&quot;None&quot;, &quot;ushfkf.jpg&quot;, &quot;ihfskjd.jpg&quot;,&quot;None&quot;]})\n\ndf['hasimage1'] = df['Image'] != 'None'\ndf[&quot;hasimage2&quot;] = df[&quot;Image&quot;].str.endswith(&quot;.jpg&quot;, na=False)\n\nprint (df)\n  ID        Image  hasimage1  hasimage2\n0  a         None      False      False\n1  b   ushfkf.jpg       True       True\n2  c  ihfskjd.jpg       True       True\n3  d         None      False      False\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe:</p>\n<pre><code>ID   Image   \na    None\nb    ushfkf.jpg\nc    ihfskjd.jpg\nd    None\n</code></pre>\n<p>The .jpg's values are of string type. I want to check whether the row contains an Image. I tried:</p>\n<pre><code>df['hasimage'] = np.where(df['Image']==None, True, False)\n</code></pre>\n<p>But I only get an extra column of <code>False</code>s. How can I simply check if the cell has a string in it, without complicating it with None?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "account_id": 5179700,
            "reputation": 23,
            "user_id": 4149213,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/vBcjv.jpg?s=128&g=1",
            "display_name": "openwater",
            "link": "https://stackoverflow.com/users/4149213/openwater"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 68174309,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624951870,
        "creation_date": 1624950821,
        "last_edit_date": 1624951870,
        "question_id": 68174113,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
        "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
        "answer_body": "<p>A simpler solution would be to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.GroupBy.apply.html\" rel=\"nofollow noreferrer\"><code>apply</code></a> a custom function on each group. In this case, we can define a function <code>reclass</code> that obtains the correct bins and ids and then uses <code>pd.cut</code>:</p>\n<pre><code>def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n</code></pre>\n<p>Result:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "input_data_frames": [
            "bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n",
            "ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n"
        ],
        "output_codes": [
            "def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n",
            "    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n"
        ],
        "ques_desc": "I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? ",
        "ans_desc": "A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: ",
        "formatted_input": {
            "qid": 68174113,
            "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
            "question": {
                "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
                "ques_desc": "I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? "
            },
            "io": [
                "bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n",
                "ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n"
            ],
            "answer": {
                "ans_desc": "A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: ",
                "code": [
                    "def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "unpivot",
            "suffix"
        ],
        "owner": {
            "account_id": 19146977,
            "reputation": 43,
            "user_id": 15448810,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/bca8fbf2e31fc03c5222eae3ac0d65e8?s=128&d=identicon&r=PG&f=1",
            "display_name": "Jamie",
            "link": "https://stackoverflow.com/users/15448810/jamie"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 68173393,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1624947803,
        "creation_date": 1624946097,
        "question_id": 68173284,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68173284/remove-suffix-of-the-column-names-and-unpivot",
        "title": "Remove suffix of the column names and unpivot",
        "answer_body": "<p>Concatenation with list comprehension</p>\n<pre><code>&gt;&gt;&gt; pd.concat([df[[&quot;Year_&quot; + c, &quot;Item_&quot; + c, &quot;$&quot;]].rename({&quot;Year_&quot; + c : &quot;Year&quot;, &quot;Item_&quot; + c : &quot;Item&quot;}, axis=1) for c in (&quot;x&quot;, &quot;y&quot;)]).reset_index(drop=True)\n   Year Item  $\n0  1993    A  3\n1  1994    B  4\n2  1995    C  5\n3  1996    D  6\n4  2000    E  3\n5  2001    F  4\n6  2002    G  5\n7  2003    H  6\n</code></pre>\n<p>Or, via <code>pd.wide_to_long(...)</code></p>\n<pre><code>&gt;&gt;&gt; pd.wide_to_long(df, [&quot;Year&quot;, &quot;Item&quot;], i=[&quot;$&quot;], j=&quot;Var&quot;, sep=&quot;_&quot;, suffix=&quot;\\w+&quot;).reset_index()\n   $ Var  Year Item\n0  3   x  1993    A\n1  4   x  1994    B\n2  5   x  1995    C\n3  6   x  1996    D\n4  3   y  2000    E\n5  4   y  2001    F\n6  5   y  2002    G\n7  6   y  2003    H\n</code></pre>\n",
        "question_body": "<p>I'd like to unpivot the following table with column names &quot;Year&quot;, &quot;Item&quot;, and &quot;$&quot;. My workaround is to separate the table into two dataframes and remove the suffixes, then concatenate the two columns vertically. Are there any other easier ways to approach this?</p>\n<p>Example Dataframe:</p>\n<pre><code>data = {'Year_x': [1993, 1994, 1995, 1996], \n       'Year_y': [2000, 2001, 2002, 2003],\n       'Item_x':['A','B','C','D'],\n       'Item_y':['E','F','G','H'],\n       '$':[3,4,5,6]}\n\npd.DataFrame.from_dict(data)\n</code></pre>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">Year_x</th>\n<th style=\"text-align: left;\">Year_y</th>\n<th style=\"text-align: left;\">Item_x</th>\n<th style=\"text-align: left;\">Item_y</th>\n<th style=\"text-align: center;\">$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">1993</td>\n<td style=\"text-align: left;\">2000</td>\n<td style=\"text-align: left;\">A</td>\n<td style=\"text-align: left;\">E</td>\n<td style=\"text-align: center;\">3</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">1994</td>\n<td style=\"text-align: left;\">2001</td>\n<td style=\"text-align: left;\">B</td>\n<td style=\"text-align: left;\">F</td>\n<td style=\"text-align: center;\">4</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">1995</td>\n<td style=\"text-align: left;\">2002</td>\n<td style=\"text-align: left;\">C</td>\n<td style=\"text-align: left;\">G</td>\n<td style=\"text-align: center;\">5</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">1996</td>\n<td style=\"text-align: left;\">2003</td>\n<td style=\"text-align: left;\">D</td>\n<td style=\"text-align: left;\">H</td>\n<td style=\"text-align: center;\">6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>What I want to achieve:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">Year</th>\n<th style=\"text-align: left;\">Item</th>\n<th style=\"text-align: center;\">$</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">1993</td>\n<td style=\"text-align: left;\">A</td>\n<td style=\"text-align: center;\">3</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">1994</td>\n<td style=\"text-align: left;\">B</td>\n<td style=\"text-align: center;\">4</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">1995</td>\n<td style=\"text-align: left;\">C</td>\n<td style=\"text-align: center;\">5</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">1995</td>\n<td style=\"text-align: left;\">D</td>\n<td style=\"text-align: center;\">6</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">2000</td>\n<td style=\"text-align: left;\">E</td>\n<td style=\"text-align: center;\">3</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">2001</td>\n<td style=\"text-align: left;\">F</td>\n<td style=\"text-align: center;\">4</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">2002</td>\n<td style=\"text-align: left;\">G</td>\n<td style=\"text-align: center;\">5</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">2003</td>\n<td style=\"text-align: left;\">H</td>\n<td style=\"text-align: center;\">6</td>\n</tr>\n</tbody>\n</table>\n</div>"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 11400545,
            "reputation": 125,
            "user_id": 10280393,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3d3003d771f7c572207e625349fdefb2?s=128&d=identicon&r=PG&f=1",
            "display_name": "n.mathfreak",
            "link": "https://stackoverflow.com/users/10280393/n-mathfreak"
        },
        "is_answered": true,
        "view_count": 18,
        "accepted_answer_id": 68173218,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624945913,
        "creation_date": 1624945467,
        "question_id": 68173195,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68173195/building-a-dataframe-based-on-various-columns-from-other-dataframes",
        "title": "Building a dataframe based on various columns from other dataframes",
        "answer_body": "<p>Chain multiple <code>merge</code> and then set <code>Score</code> to last column:</p>\n<pre><code>df = df1.merge(df2, on='ID').merge(df3, on='ID')\ndf['Score'] = df.pop('Score')\n</code></pre>\n<p>Or if there is <code>list of DataFrames</code> use:</p>\n<pre><code>from functools import reduce\n\ndfs = [df1, df2, df3]\ndf = reduce(lambda df1,df2: pd.merge(df1,df2,on='ID'), dfs)\ncols = df.columns.drop(['Score']).tolist() + ['Score']\ndf = df[cols]\n</code></pre>\n",
        "question_body": "<p>I have various dataframes that look like this:</p>\n<pre><code>df1\n    ID    Number   Score   Time   Result\n    a     45       0.3     2535   0.9\n    b     46       0.5     345    0.8\n    c     34       0.94    346    0.6\n    d     36       1       356    0.7\n\ndf2 \nID Image Video\na   0     0\nb   0     0\nc   1     0\nd   0     1\ne   1     0\nf   1     0\n\ndf3\nID Length\na   35\nb   57\nc   75\nd   57\ne   85\nf   48\n</code></pre>\n<p>How can I merge them to look like:</p>\n<pre><code>ID   Time   Image   Video   Length  Score\na    2535   0        0       35      0.3\nb    345    0        0       57      0.5\nc    346    1        0       75      0.94\nd    356    0        1       57      1\n</code></pre>\n<p>My idea is to use <code>pd.merge</code> (<code>on=&quot;ID&quot;</code>) (this will only give me the rows from a to d, right?) and then delete the unnecessary columns. But how do I move Score at the end? Is there any other approach?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 7500169,
            "reputation": 2520,
            "user_id": 5698888,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/f59433488f998fb6ae08ce079dbc9b05?s=128&d=identicon&r=PG&f=1",
            "display_name": "chessosapiens",
            "link": "https://stackoverflow.com/users/5698888/chessosapiens"
        },
        "is_answered": true,
        "view_count": 54,
        "accepted_answer_id": 68163861,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1624944996,
        "creation_date": 1624885076,
        "question_id": 68163767,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68163767/group-by-a-pandas-dataframe-by-15-min-time-intervals-but-for-the-whole-day",
        "title": "Group by a pandas dataframe by 15 min time intervals but for the whole day",
        "answer_body": "<p>Add <code>00:00:00</code> to minimal <code>time</code> and <code>23:45:00</code> to maximal, so in ouput are all expected values:</p>\n<pre><code>s = df['time'].agg(['min','max']).dt.normalize().copy()\ns['max'] = s['max'] + pd.DateOffset(hours=23, minutes=45)\n\ndf = df.append(s.to_frame().assign(Q = 0), ignore_index=True)   \nprint (df)\n                  time   Q\n0  2019-12-07 09:13:00  10\n1  2019-12-07 09:33:00   1\n2  2019-12-07 09:41:00   1\n3  2019-12-07 10:03:00   6\n4  2019-12-07 10:15:00   5\n5  2019-12-07 10:37:00   3\n6  2019-12-07 10:48:00  15\n7  2019-12-07 11:05:00   3\n8  2019-12-07 11:16:00   8\n9  2019-12-07 11:34:00   5\n10 2019-12-07 11:48:00  10\n11 2019-12-07 12:01:00   6\n12 2019-12-07 12:18:00   7\n13 2019-12-07 00:00:00   0\n14 2019-12-07 23:45:00   0\n</code></pre>\n<p>And then use your solution, e.g.:</p>\n<pre><code>df.groupby(pd.Grouper(key=&quot;time&quot;, freq=&quot;15Min&quot;))['Q'].sum()\n</code></pre>\n<p>If need processing each dates separately - first use your solution and then add misisng <code>Datetimes</code> by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reindex.html\" rel=\"nofollow noreferrer\"><code>Series.reindex</code></a>:</p>\n<pre><code>print (df)\n                  time   Q\n0  2019-12-07 09:13:00  10\n1  2019-12-07 09:33:00   1\n2  2019-12-07 09:41:00   1\n3  2019-12-07 10:03:00   6\n4  2019-12-07 10:15:00   5\n5  2019-12-07 10:37:00   3\n6  2019-12-07 10:48:00  15\n7  2019-12-07 11:05:00   3\n8  2019-12-09 11:16:00   8\n9  2019-12-09 11:34:00   5\n10 2019-12-09 11:48:00  10\n11 2019-12-09 12:01:00   6\n12 2019-12-09 12:18:00   7\n\n\ndates = [y for x in df.time.dt.normalize().drop_duplicates() \n           for y in pd.date_range(x, x + pd.DateOffset(hours=23, minutes=45), freq='15T')]\nprint (dates[:2])\n[Timestamp('2019-12-07 00:00:00', freq='15T'), Timestamp('2019-12-07 00:15:00', freq='15T')]\n</code></pre>\n<hr />\n<pre><code>df = df.groupby(df.time.dt.floor('15T'))[&quot;Q&quot;].sum().reindex(dates, fill_value=0)\nprint (df)\ntime\n2019-12-07 00:00:00    0\n2019-12-07 00:15:00    0\n2019-12-07 00:30:00    0\n2019-12-07 00:45:00    0\n2019-12-07 01:00:00    0\n                      ..\n2019-12-09 22:45:00    0\n2019-12-09 23:00:00    0\n2019-12-09 23:15:00    0\n2019-12-09 23:30:00    0\n2019-12-09 23:45:00    0\nName: Q, Length: 192, dtype: int64\n</code></pre>\n",
        "question_body": "<p>I have the follwing dataframe i'd like to group by 15 minutes bin and sum the column Q, but i would like to have these bins for the whole day.</p>\n<pre><code> time                   Q\n 2019-12-07 09:13:00   10 \n 2019-12-07 09:33:00    1 \n 2019-12-07 09:41:00    1 \n 2019-12-07 10:03:00    6 \n 2019-12-07 10:15:00    5 \n 2019-12-07 10:37:00    3 \n 2019-12-07 10:48:00   15 \n 2019-12-07 11:05:00    3 \n 2019-12-07 11:16:00    8 \n 2019-12-07 11:34:00    5 \n 2019-12-07 11:48:00   10 \n 2019-12-07 12:01:00    6 \n 2019-12-07 12:18:00    7 \n</code></pre>\n<p>so I would like to have for examples the beans like this:</p>\n<pre><code>time                  SUM(Q)\n 2019-12-07 00:00:00               \n 2019-12-07 00:15:00\n 2019-12-07 00:30:00\n 2019-12-07 00:45:00\n 2019-12-07 01:00:00\n               .\n               .\n               .\n2019-12-07 23:00:00\n2019-12-07 23:15:00\n2019-12-07 23:30:00\n2019-12-07 23:45:00\n</code></pre>\n<p>I have tried</p>\n<pre><code> df.groupby(df.time.dt.floor('15T'))[&quot;Q&quot;].sum() \n</code></pre>\n<p>and</p>\n<pre><code> df.groupby(pd.Grouper(key=&quot;time&quot;, freq=&quot;15Min&quot;))['Q'].sum()\n</code></pre>\n<p>but they both  group by only for available times in the column not from the start of the day (00:00:00 or 00:15:00) to the end of day ( 23:45:00)</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 17176942,
            "reputation": 19,
            "user_id": 12432950,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/aa6f85355f364b18d953f303999d6623?s=128&d=identicon&r=PG&f=1",
            "display_name": "Lauren N.",
            "link": "https://stackoverflow.com/users/12432950/lauren-n"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 68171355,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624927945,
        "creation_date": 1624925980,
        "last_edit_date": 1624926810,
        "question_id": 68171235,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68171235/adding-column-to-df-that-calculates-count-of-different-column-using-groupby",
        "title": "adding column to df that calculates count of different column using groupby",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html#pandas-core-groupby-dataframegroupby-transform\" rel=\"nofollow noreferrer\"><code>groupby transform('count')</code></a>:</p>\n<pre><code>df['count'] = df.groupby('mother_ID')['hatchling_masses_g'].transform('count')\n</code></pre>\n<hr />\n<p>Notice the difference between <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.count.html\" rel=\"nofollow noreferrer\"><code>groupby count</code></a> and <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html#pandas-core-groupby-dataframegroupby-transform\" rel=\"nofollow noreferrer\"><code>groupby tranform</code></a> with <code>'count'</code>.</p>\n<p>Sample Data:</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\nnp.random.seed(5)\ndf = pd.DataFrame({\n    'mother_ID': np.random.choice(['a', 'b'], 10),\n    'hatchling_masses_g': np.random.randint(1, 100, 10)\n})\n</code></pre>\n<pre><code>  mother_ID  hatchling_masses_g\n0         b                  63\n1         a                  28\n2         b                  31\n3         b                  81\n4         a                   8\n5         a                  77\n6         a                  16\n7         b                  54\n8         a                  81\n9         a                  28\n</code></pre>\n<hr />\n<h3><code>groupby.count</code></h3>\n<pre><code>counts = df.groupby('mother_ID')['hatchling_masses_g'].count()\n</code></pre>\n<pre><code>mother_ID\na    6\nb    4\nName: hatchling_masses_g, dtype: int64\n</code></pre>\n<p>Notice how there are only 2 rows. When assigning back to the DataFrame there are 10 rows which means that pandas doesn't know how to align the data back. Which results in <code>NaN</code>s indicating missing data:</p>\n<pre><code>df['count'] = counts\n</code></pre>\n<pre><code>  mother_ID  hatchling_masses_g  count\n0         b                  63    NaN\n1         a                  28    NaN\n2         b                  31    NaN\n3         b                  81    NaN\n4         a                   8    NaN\n5         a                  77    NaN\n6         a                  16    NaN\n7         b                  54    NaN\n8         a                  81    NaN\n9         a                  28    NaN\n</code></pre>\n<p>It's trying to find 'a' and 'b' in the index and since it cannot it fills with only <code>NaN</code> values.</p>\n<hr />\n<h3><code>groupby.tranform('count')</code></h3>\n<p><code>transform</code>, on the other hand, will populate the entire group with the count:</p>\n<pre><code>counts = df.groupby('mother_ID')['hatchling_masses_g'].transform('count')\n</code></pre>\n<p><code>counts</code>:</p>\n<pre><code>0    4\n1    6\n2    4\n3    4\n4    6\n5    6\n6    6\n7    4\n8    6\n9    6\nName: hatchling_masses_g, dtype: int64\n</code></pre>\n<p>Notice 10 rows were created (one for every row in the DataFrame):</p>\n<p>This assigns back to the dataframe nicely (since the indexes align):</p>\n<pre><code>df['count'] = counts\n</code></pre>\n<pre><code>  mother_ID  hatchling_masses_g  count\n0         b                  63      4\n1         a                  28      6\n2         b                  31      4\n3         b                  81      4\n4         a                   8      6\n5         a                  77      6\n6         a                  16      6\n7         b                  54      4\n8         a                  81      6\n9         a                  28      6\n</code></pre>\n<hr />\n<p>If needed counts can be done via <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.count.html\" rel=\"nofollow noreferrer\"><code>groupby count</code></a>, then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html#pandas-dataframe-join\" rel=\"nofollow noreferrer\"><code>join</code></a> back to the DataFrame on the group key:</p>\n<pre><code>counts = df.groupby('mother_ID')['hatchling_masses_g'].count().rename('count')\ndf = df.join(counts, on='mother_ID')\n</code></pre>\n<p><code>counts</code>:</p>\n<pre><code>mother_ID\na    6\nb    4\nName: count, dtype: int64\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>  mother_ID  hatchling_masses_g  count\n0         b                  63      4\n1         a                  28      6\n2         b                  31      4\n3         b                  81      4\n4         a                   8      6\n5         a                  77      6\n6         a                  16      6\n7         b                  54      4\n8         a                  81      6\n9         a                  28      6\n</code></pre>\n",
        "question_body": "<p>I'm trying to create a new column in a df. I want the new column to equal the count of the number rows of each unique <code>'mother_ID</code>, which is a different column in the df.</p>\n<p>This is what I'm currently doing. It makes the new column but the new column is filled with 'NaN's.</p>\n<pre><code>df.columns = ['mother_ID', 'date_born', 'mother_mass_g', 'hatchling_masses_g'] \ndf.to_numpy()\n\n</code></pre>\n<p>This is how the original df appears when I print it:</p>\n<p><a href=\"https://i.stack.imgur.com/vseOh.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/vseOh.png\" alt=\"how df appears when I print it\" /></a></p>\n<pre><code>\ncount = df.groupby('mother_ID').hatchling_masses_g.count() \ndf['count']= count\n\n</code></pre>\n<p>Pic below shows what I get when I print new df, although if I simply <code>print(count)</code> I get the correct counts for each <code>mother_ID</code> . Does anyone know what I'm doing wrong?</p>\n<p><a href=\"https://i.stack.imgur.com/8HYZM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/8HYZM.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary",
            "filter"
        ],
        "owner": {
            "account_id": 16282669,
            "reputation": 53,
            "user_id": 11758143,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-i1RX7W3AE3w/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdy29x3sCxhJttI_EEVWTUIR35vaw/photo.jpg?sz=128",
            "display_name": "Fhtsm",
            "link": "https://stackoverflow.com/users/11758143/fhtsm"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 68170580,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624918723,
        "creation_date": 1624893923,
        "last_edit_date": 1624895363,
        "question_id": 68166048,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68166048/using-a-dictionary-to-filter-pandas-dataframe-where-the-keys-correspond-to-the",
        "title": "Using a Dictionary to filter Pandas DataFrame, where the Keys correspond to the Column Names",
        "answer_body": "<p>You can try:</p>\n<pre><code>df[(df[list(dict_example)] == list(dict_example.values())).all(axis=1)]\n</code></pre>\n",
        "question_body": "<p>I am trying to use the values of a dictionary to 'filter' a Pandas DataFrame, where the dictionary's keys correspond to the columns in the DataFrame.</p>\n<p>Example:</p>\n<pre><code>In [5]: df                                                                                                                                                     \nOut[5]: \n  Col1 Col2 Col3\n0    A    L    Y\n1    B    M    Y\n2    B    N    Z                                                                                                   \n\nIn [7]: dict_example                                                                                                                                           \nOut[7]: {'Col1': 'B', 'Col2': 'M', 'Col3': 'Y'}\n</code></pre>\n<p>The result I would want, based on the values in dict_example, would simply be the second row,</p>\n<pre><code>Out[5]: \n  Col1 Col2 Col3\n1    B    M    Y\n</code></pre>\n<p>I have tried something like this:</p>\n<pre><code>In [8]: df[df.isin(dict_example.values())]                                                                                                                     \nOut[8]: \n  Col1 Col2 Col3\n0  NaN  NaN    Y\n1    B    M    Y\n2    B  NaN  NaN\n</code></pre>\n<p>But it is giving me back the whole DataFrame, instead of just the row, where all values from the dictionary correspond to the values in the DataFrame.</p>\n<p>I could of course do something like this:</p>\n<pre><code>In [9]: df[(df['Col1']=='B')&amp;(df['Col2']=='M')&amp;(df['Col3']=='Y')]                                                                                              \nOut[9]: \n  Col1 Col2 Col3\n1    B    M    Y\n</code></pre>\n<p>But this gets increasingly difficult, the more dimensions you have to filter for.</p>\n<p>I would be very grateful for any input!</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 13060858,
            "reputation": 775,
            "user_id": 9581909,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/20fd65781a68386f33593b0b6593cc70?s=128&d=identicon&r=PG&f=1",
            "display_name": "Grendel",
            "link": "https://stackoverflow.com/users/9581909/grendel"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 65203503,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624915932,
        "creation_date": 1607445411,
        "last_edit_date": 1624915932,
        "question_id": 65203030,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65203030/get-all-possible-pairs-within-a-cell-in-df-python",
        "title": "Get all possible pairs within a cell in df python",
        "answer_body": "<p>Use <code>itertools.combinations</code>. The <code>COL3</code> column contains list as string which requires <code>literal_eval</code> to convert to <code>list</code>.</p>\n<pre><code>from itertools import combinations\nfrom ast import literal_eval\n\ndef all_combinations(x):\n    return [list(combinations(x, i)) for i in range(1, 3)]\n\ndf['COL3'] = df.COL3.map(literal_eval)\ndf['COL3'] = df.COL3.map(all_combinations)\n</code></pre>\n",
        "question_body": "<p>I have a df such as :</p>\n<pre><code>COL1 COL2 COL3\nG1   1    ['B_-__Canis_lupus']\nG1   2    ['A_-__Felis_cattus','O_+__Felis_cattus','D_-__Felis_sylvestris]\nG2   1    ['Q_-__Mus_musculus','S_-__Mus_griseus','P_-__Mus_rattus']\n</code></pre>\n<p>and I would like from that to create 1 new column :</p>\n<p><strong>COL4</strong> which is all the pairwise possible combination of <strong>COL3</strong> contain (without against itself) and in a form a list of list within cells</p>\n<p>Here I should then get :</p>\n<pre><code>COL1 COL2 COL3 COL4\nG1   1    ['B_-__Canis_lupus'] NA \nG1   2    ['A_-__Felis_cattus','O_+__Felis_cattus','D_-__Felis_sylvestris'] [['A_-__Felis_cattus','O_+__Felis_cattus'],['A_-__Felis_cattus','D_-__Felis_sylvestris'];['O_+__Felis_cattus','D_-__Felis_sylvestris']] \nG2   1    ['Q_-__Mus_musculus','S_-__Mus_griseus','P_-__Mus_rattus'] [['Q_-__Mus_musculus','S_-__Mus_griseus'],['Q_-__Mus_musculus','P_-__Mus_rattus'],['S_-__Mus_griseus','P_-__Mus_rattus']]\n</code></pre>\n<p>does someone have an idea?</p>\n<p>here are the data in dic format :</p>\n<pre><code>   data= {'COL1': {0: 'G1', 1: 'G1', 2: 'G2'}, 'COL2': {0: 1, 1: 2, 2: 1}, 'COL3': {0: &quot;['B_-__Canis_lupus']&quot;, 1: &quot;['A_-__Felis_cattus','O_+__Felis_cattus','D_-__Felis_sylvestris']&quot;, 2: &quot;['Q_-__Mus_musculus','S_-__Mus_griseus','P_-__Mus_rattus']&quot;}}\n</code></pre>\n<p>I use :</p>\n<pre><code>import pandas as pd \ndf=pd.read_csv(&quot;test.tab&quot;,sep=&quot;;&quot;)\n</code></pre>\n<p>or</p>\n<pre><code>pd.DataFrame.from_dict(data)\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "plotly",
            "histogram"
        ],
        "owner": {
            "account_id": 312542,
            "reputation": 1105,
            "user_id": 626664,
            "user_type": "registered",
            "accept_rate": 47,
            "profile_image": "https://www.gravatar.com/avatar/8dcc04f9e7c6315b7e896c560d1a805e?s=128&d=identicon&r=PG",
            "display_name": "Droid-Bird",
            "link": "https://stackoverflow.com/users/626664/droid-bird"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 67082011,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624913601,
        "creation_date": 1618311219,
        "last_edit_date": 1624913601,
        "question_id": 67073478,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67073478/how-to-build-a-histogram-from-a-pandas-dataframe-where-each-observation-is-a-lis",
        "title": "How to build a histogram from a pandas dataframe where each observation is a list?",
        "answer_body": "<p>If you reshape your data, this would be a perfect case for <a href=\"https://plotly.com/python/histograms/\" rel=\"nofollow noreferrer\">px.histogram</a>. And from there you can opt between several outputs like <code>sum, average, count</code> through the <code>histfunc</code> method:</p>\n<pre><code>fig = px.histogram(df, x = 'Area_code', y = 'Values', histfunc='sum')\nfig.show()\n</code></pre>\n<p>You haven't specified what kind of output you're aiming for, but I'll leave it up to you to change the argument for <code>histfunc</code> and see which option suits your needs best.</p>\n<p><a href=\"https://i.stack.imgur.com/zbf7w.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/zbf7w.png\" alt=\"enter image description here\" /></a></p>\n<p>I'm often inclined to urge users to rethink their entire data process, but I'm just going to assume that there are good reasons why you're stuck with what seems like a pretty weird setup in your dataframe. The snippet below contains a complete data munginge process to reshape your data from your setup, to a so-called <a href=\"https://stackoverflow.com/questions/62012194/plotly-how-to-make-a-line-plot-from-a-pandas-dataframe-with-a-long-or-wide-form/62012200#62012200\">long</a> format:</p>\n<pre><code>   Area_code  Values\n0   New_York     999\n1   New_York      54\n2   New_York     231\n3   New_York      43\n4   New_York     177\n5   New_York     313\n6   New_York     212\n7   New_York     279\n8   New_York     199\n9   New_York     267\n10    Dallas     915\n11    Dallas     183\n12    Dallas    2326\n13    Dallas     316\n14    Dallas     206\n15    Dallas      31\n16    Dallas     317\n17    Dallas      26\n18    Dallas      31\n19    Dallas      56\n20    Dallas     316\n21       XXX     560\n22       YYY     884\n23       YYY      13\n24       ZZZ     203\n</code></pre>\n<p>And this is a perfect format for many of the great functionalites of <a href=\"https://plotly.com/python/plotly-express/\" rel=\"nofollow noreferrer\">plotly.express</a>.</p>\n<h3>Complete code:</h3>\n<pre><code>import plotly.graph_objects as go\nimport plotly.express as px\nimport pandas as pd\n\n# data input\ndf = pd.DataFrame({'Area_code': {0: 'New_York', 1: 'Dallas', 2: 'XXX', 3: 'YYY', 4: 'ZZZ'},\n                 'Values': {0: [999, 54, 231, 43, 177, 313, 212, 279, 199, 267],\n                  1: [915, 183, 2326, 316, 206, 31, 317, 26, 31, 56, 316],\n                  2: [560],\n                  3: [884, 13],\n                  4: [203, 1066, 453, 266, 160, 109, 45, 627, 83, 685, 120, 410, 151, 33, 618, 164, 496]}})\n\n# data munging\nareas = []\nvalue = []\nfor i, row in df.iterrows():\n#     print(row['Values'])\n        for j, val in enumerate(row['Values']):\n            areas.append(row['Area_code'])\n            value.append(val)\ndf = pd.DataFrame({'Area_code': areas,\n                   'Values': value})\n\n# plotly\nfig = px.histogram(df, x = 'Area_code', y = 'Values', histfunc='sum')\nfig.show()\n</code></pre>\n",
        "question_body": "<p>I have a dataframe as follows. The values are in a cell, a list of elements. I want to visualize distribution of the values from the &quot;Values&quot; column using histogram&quot;S&quot; stacked in rows OR separated by colours (Area_code).</p>\n<p>How can I get the values and construct histogram&quot;S&quot; in plotly? Any other idea also welcome. Thank you.</p>\n<pre><code>    Area_code   Values\n0   New_York    [999, 54, 231, 43, 177, 313, 212, 279, 199, 267]\n1   Dallas  [915, 183, 2326, 316, 206, 31, 317, 26, 31, 56, 316]\n2   XXX     [560]\n3   YYY     [884, 13]\n4   ZZZ     [203, 1066, 453, 266, 160, 109, 45, 627, 83, 685, 120, 410, 151, 33, 618, 164, 496]\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "max"
        ],
        "owner": {
            "account_id": 22021729,
            "reputation": 25,
            "user_id": 16290458,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/f9b510b73ec5376033dc2e5a37618ba2?s=128&d=identicon&r=PG&f=1",
            "display_name": "Elvis",
            "link": "https://stackoverflow.com/users/16290458/elvis"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68168462,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624906798,
        "creation_date": 1624904183,
        "last_edit_date": 1624904930,
        "question_id": 68168284,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68168284/fillter-the-largest-n-values-of-a-column-using-another-column-as-index-in-pandas",
        "title": "Fillter the largest n values of a column using another column as index in Pandas - Python",
        "answer_body": "<p>Option without using <code>.apply()</code> and lambda function.</p>\n<h3>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>.loc</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>.groupby()</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.SeriesGroupBy.nlargest.html\" rel=\"nofollow noreferrer\"><code>.nlargest()</code></a>:</h3>\n<p>(with all vectorized operations for faster execution):</p>\n<pre><code>df.loc[df.groupby('N')['ey'].nlargest(2).reset_index(0).index]\n</code></pre>\n<p><strong>Result:</strong></p>\n<pre><code>    N     Ret  upside_tri      ey\n2   1 -0.0045     -1.0086  0.0272\n3   1  0.3458     -7.1714  0.0000\n6   2  0.2798     -0.4965  0.1343\n7   2  0.2273      0.0770  0.0368\n9   3 -0.0526      1.8242  0.2686\n11  3  0.2182     -1.0752 -0.0331\n12  4  0.0201      4.6152  0.2242\n13  4  0.0527     -0.3465  0.1953\n17  5 -0.0319      0.9984  0.2314\n18  5  0.0800      1.9057  0.2101\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with 4 columns. I would like to get the largest 40 values of one column (column ey) for each data input in column N, while keeping the data of the other 2 columns (ie: I wouldnt like to delete or lose the data of the other two columns).</p>\n<p>The question is very similar to this post: <a href=\"https://stackoverflow.com/questions/25071937/filter-pandas-dataframe-based-on-max-values-in-a-column\">Filter pandas Dataframe based on max values in a column</a>. The difference is that I am looking for the 40 largest values, and not the largest value, and also that my dataframe has more columns.</p>\n<p>The table below is an example of the table I have, while it only has 4 rows in each column N dataset (actual dataframe has many more rows for each N).</p>\n<pre><code>N   Ret upside_tri  ey\n1   -0.1478 -14.4097 -0.3702\n1   0.7571  -9.4190 -0.4609\n1   -0.0045 -1.0086 0.0272\n1   0.3458  -7.1714 0.0000\n2   -0.1218 -9.7807 -1.5318\n2   0.2283  14.7490 -0.2328\n2   0.2798  -0.4965 0.1343\n2   0.2273  0.0770  0.0368\n3   0.0904  0.1881  -0.2433\n3   -0.0526 1.8242  0.2686\n3   0.0822  4.9049  -0.0416\n3   0.2182  -1.0752 -0.0331\n4   0.0201  4.6152  0.2242\n4   0.0527  -0.3465 0.1953\n4   0.1169  -1.2500 -0.0266\n4   -0.1854 2.7845  0.0947\n5   0.0192  -0.4258 0.1783\n5   -0.0319 0.9984  0.2314\n5   0.0800  1.9057  0.2101\n5   -0.0447 -0.5313 0.0865\n</code></pre>\n<p>Assuming I would like the 2 largest values of ey for each N (my actual wish is to have the 40 largest values), the desired outcome would become something like this:</p>\n<pre><code>N   Return  upside  ey\n1   -0.0045 -1.0086 0.0272\n1   0.3458  -7.1714 0.0000\n2   0.2798  -0.4965 0.1343\n2   0.2273  0.0770  0.0368\n3   -0.0526 1.8242  0.2686\n3   0.2182  -1.0752 -0.0331\n4   0.0201  4.6152  0.2242\n4   0.0527  -0.3465 0.1953\n5   -0.0319 0.9984  0.2314\n5   0.0800  1.9057  0.2101\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "csv",
            "data-manipulation"
        ],
        "owner": {
            "account_id": 22053207,
            "reputation": 49,
            "user_id": 16317158,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyBUnbLv2PI4_Agtm14oXQyk_ozZyrNZkI9w4DE=k-s128",
            "display_name": "Jewel_R",
            "link": "https://stackoverflow.com/users/16317158/jewel-r"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 68157946,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1624905202,
        "creation_date": 1624856697,
        "last_edit_date": 1624857205,
        "question_id": 68157820,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68157820/extracting-multiple-csv-file-information-from-a-path-in-pc-with-python-and-manip",
        "title": "Extracting multiple csv file information from a path in pc with python and manipulate it at the same time",
        "answer_body": "<p>Use <code>glob</code> for get all files from your folder to list, add new columns and write back:</p>\n<pre><code>import glob, os\n\nfiles = glob.glob('D:/sevenday/*.csv')\n\nfor fp in files:\n    name = os.path.basename(fp)\n    df = pd.read_csv(fp)\n    df['name'] = name\n    df.to_csv(fp, index=False)\n</code></pre>\n",
        "question_body": "<p>I have multiple (say more than 20/30) CSV files named as <strong>'ABCD.csv','EFGH.csv','IJKL.csv','MNOP.csv' etc</strong> in my folder path <strong>D:\\sevenday</strong> . I want to add a column named <strong>name</strong> in each CSV file and add their respective names inside the CSV file itself. Let us say, <strong>ABCD</strong> CSV file will have a column named <strong>name</strong> and it will have <strong>ABCD</strong> as a name in that column. How could I extract that string information from the file's name in python? Is there a way?\nThere are ways if files are few. But I don't know how to extract many files like that at once as writing each time one by one is tedious.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 1795830,
            "reputation": 337,
            "user_id": 1635253,
            "user_type": "registered",
            "accept_rate": 0,
            "profile_image": "https://www.gravatar.com/avatar/59376176b2ccacb43b2200334a1d41bc?s=128&d=identicon&r=PG&f=1",
            "display_name": "thenoirlatte",
            "link": "https://stackoverflow.com/users/1635253/thenoirlatte"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 68167715,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624902638,
        "creation_date": 1624899983,
        "last_edit_date": 1624902283,
        "question_id": 68167438,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68167438/error-when-try-to-replace-bracket-character-or-in-a-dataframe",
        "title": "Error when try to replace bracket character &quot;(&quot; or &quot;)&quot; in a dataframe",
        "answer_body": "<p>It appears you just need to <code>join</code> the values:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['ytest'] = df['ytest'].str.join(', ')\n</code></pre>\n<p>If you want another delimiter between the individual values, replace <code>', '</code> with the value you need.</p>\n<p>If your rows contained string data, you could use</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['ytest']=df['ytest'].str.replace(&quot;[()]&quot;, &quot;&quot;, regex=True)\n</code></pre>\n<p>Or, if you do not want to use a regex:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['ytest']=df['ytest'].str.replace(&quot;(&quot;, &quot;&quot;, regex=False)\ndf['ytest']=df['ytest'].str.replace(&quot;)&quot;, &quot;&quot;, regex=False)\n</code></pre>\n<p>From the <a href=\"https://pandas.pydata.org/pandas-docs/version/1.1.5/user_guide/text.html\" rel=\"nofollow noreferrer\">Pandas 1.1.5 documentation</a>:</p>\n<blockquote>\n<p>If you do want literal replacement of a string (equivalent to <code>str.replace()</code>), you can set the optional <code>regex</code> parameter to <em>False</em>, rather than escaping each character. In this case both <code>pat</code> and <code>repl</code> must be strings</p>\n</blockquote>\n",
        "question_body": "<p>I'm trying to replace character &quot;(&quot; and &quot;)&quot; in my data frame with space, but got an error:</p>\n<pre><code>error: unbalanced parenthesis at position 1\n</code></pre>\n<p>Here is my code:</p>\n<pre><code>df['ytest']=df['ytest'].str.replace(&quot;(&quot;,&quot;&quot;)\ndf['ytest']=df['ytest'].str.replace(&quot;)&quot;,&quot;&quot;)\n</code></pre>\n<p>Here is the example of the dataframe:</p>\n<pre><code>| ytest                                                       |\n|=============================================================|\n|('pasal xx tahun 2002',)                                     |\n|('pasal 1 tahun 2009', 'pasal 2 2012', 'pasal 4', 'pasal 8') |\n|('pasal 1b',)                                                |\n|('pasal 16', 'pasal 9')                                      |\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "statistics"
        ],
        "owner": {
            "account_id": 4909810,
            "reputation": 360,
            "user_id": 12195048,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5377b2a65d3de430423119f509c7e974?s=128&d=identicon&r=PG&f=1",
            "display_name": "bhola prasad",
            "link": "https://stackoverflow.com/users/12195048/bhola-prasad"
        },
        "is_answered": true,
        "view_count": 18,
        "accepted_answer_id": 68167138,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1624900892,
        "creation_date": 1624898672,
        "last_edit_date": 1624900892,
        "question_id": 68167137,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68167137/how-to-calculate-weighted-mean-and-median-in-python",
        "title": "How to calculate weighted mean and median in python?",
        "answer_body": "<p>First, install the <a href=\"https://pypi.org/project/weightedstats/\" rel=\"nofollow noreferrer\">weightedstats</a> library in python.</p>\n<pre><code>pip install weightedstats\n</code></pre>\n<p>Then, do the following -</p>\n<p><strong>Weighted Mean</strong></p>\n<pre><code>ws.weighted_mean(state['Murder.Rate'], weights=state['Population'])\n4.445833981123394\n</code></pre>\n<p><strong>Weighted Median</strong></p>\n<pre><code>ws.weighted_median(state['Murder.Rate'], weights=state['Population'])\n4.4\n</code></pre>\n<p>It also has special weighted mean and median methods to use with numpy arrays. The above methods will work but in case if you need it.</p>\n<pre><code>my_data = [1, 2, 3, 4, 5]\nmy_weights = [10, 1, 1, 1, 9]\n\nws.numpy_weighted_mean(my_data, weights=my_weights)\nws.numpy_weighted_median(my_data, weights=my_weights)\n</code></pre>\n",
        "question_body": "<p>I have data in pandas DataFrame or NumPy array and want to calculate the weighted mean(average) or weighted median based on some weights in another column or array. I am looking for a simple solution rather than writing functions from scratch or copy-paste them everywhere I need them.</p>\n<p>The data looks like this -</p>\n<pre><code>state.head()\n    State    Population Murder.Rate Abbreviation\n0   Alabama     4779736     5.7     AL\n1   Alaska      710231      5.6     AK\n2   Arizona     6392017     4.7     AZ\n3   Arkansas    2915918     5.6     AR\n4   California  37253956    4.4     CA\n</code></pre>\n<p>And I want to calculate the <code>weighted mean or median</code> of <code>murder rate</code> which takes into account the different <code>populations</code> in the states.</p>\n<p>How can I do that?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 543843,
            "reputation": 11307,
            "user_id": 913098,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/e3e07b09128f889209b738276d42d7ef?s=128&d=identicon&r=PG",
            "display_name": "Gulzar",
            "link": "https://stackoverflow.com/users/913098/gulzar"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 68167031,
        "answer_count": 5,
        "score": 0,
        "last_activity_date": 1624900111,
        "creation_date": 1624896795,
        "last_edit_date": 1624897089,
        "question_id": 68166733,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68166733/how-to-sort-dataframe-by-some-columns-with-nans-as-if-they-were-one-column",
        "title": "How to sort dataframe by some columns with nans, as if they were one column?",
        "answer_body": "<p>So maybe try <code>sum</code> with <code>reindex</code></p>\n<pre><code>out = df.reindex(df[['a','b']].sum(1).sort_values().index).reset_index(drop=True)\nOut[70]: \n     a    b   c\n0  1.0  NaN  v5\n1  NaN  2.0  v2\n2  3.0  NaN  v1\n3  NaN  4.0  v6\n4  5.0  NaN  v3\n5  NaN  6.0  v4\n</code></pre>\n",
        "question_body": "<p>Please consider</p>\n<pre><code>import pandas as pd\nimport numpy as np\nnan = np.nan\n\ndf = pd.DataFrame({\n    'a': [3, nan, 5, nan, 1, nan],\n    'b': [nan, 2, nan, 6, nan, 4],\n    'c': ['v1', 'v2', 'v3', 'v4', 'v5', 'v6']\n})\n\nprint(df)\n</code></pre>\n<p>out:</p>\n<pre><code>     a    b   c\n0  3.0  NaN  v1\n1  NaN  2.0  v2\n2  5.0  NaN  v3\n3  NaN  6.0  v4\n4  1.0  NaN  v5\n5  NaN  4.0  v6\n</code></pre>\n<hr />\n<p>Guaranteed each row has a single non-nan value, I want to sort all rows by valid values on <code>['a', 'b']</code>.<br />\nRequired:</p>\n<pre><code>     a    b   c\n0  1.0  NaN   v5\n1  NaN  2.0   v2\n2  3.0  NaN   v1\n3  NaN  4.0   v6\n4  5.0  NaN   v3\n5  NaN  6.0   v4\n\n</code></pre>\n<hr />\n<p>Please notice I am looking for a solution extendible for more than just 2 columns, and that values in each columns don't have to be followed or preceded by <code>nan</code>s, meaning there may be consecutive <code>nan</code>s or <code>non-nan</code>s in any column, as long as each row has only a single <code>non-nan</code>.</p>\n",
        "input_data_frames": [
            "     a    b   c\n0  3.0  NaN  v1\n1  NaN  2.0  v2\n2  5.0  NaN  v3\n3  NaN  6.0  v4\n4  1.0  NaN  v5\n5  NaN  4.0  v6\n",
            "     a    b   c\n0  1.0  NaN   v5\n1  NaN  2.0   v2\n2  3.0  NaN   v1\n3  NaN  4.0   v6\n4  5.0  NaN   v3\n5  NaN  6.0   v4\n\n"
        ],
        "output_codes": [
            "out = df.reindex(df[['a','b']].sum(1).sort_values().index).reset_index(drop=True)\nOut[70]: \n     a    b   c\n0  1.0  NaN  v5\n1  NaN  2.0  v2\n2  3.0  NaN  v1\n3  NaN  4.0  v6\n4  5.0  NaN  v3\n5  NaN  6.0  v4\n"
        ],
        "ques_desc": "Please consider out: Guaranteed each row has a single non-nan value, I want to sort all rows by valid values on . Required: Please notice I am looking for a solution extendible for more than just 2 columns, and that values in each columns don't have to be followed or preceded by s, meaning there may be consecutive s or s in any column, as long as each row has only a single . ",
        "ans_desc": "So maybe try with ",
        "formatted_input": {
            "qid": 68166733,
            "link": "https://stackoverflow.com/questions/68166733/how-to-sort-dataframe-by-some-columns-with-nans-as-if-they-were-one-column",
            "question": {
                "title": "How to sort dataframe by some columns with nans, as if they were one column?",
                "ques_desc": "Please consider out: Guaranteed each row has a single non-nan value, I want to sort all rows by valid values on . Required: Please notice I am looking for a solution extendible for more than just 2 columns, and that values in each columns don't have to be followed or preceded by s, meaning there may be consecutive s or s in any column, as long as each row has only a single . "
            },
            "io": [
                "     a    b   c\n0  3.0  NaN  v1\n1  NaN  2.0  v2\n2  5.0  NaN  v3\n3  NaN  6.0  v4\n4  1.0  NaN  v5\n5  NaN  4.0  v6\n",
                "     a    b   c\n0  1.0  NaN   v5\n1  NaN  2.0   v2\n2  3.0  NaN   v1\n3  NaN  4.0   v6\n4  5.0  NaN   v3\n5  NaN  6.0   v4\n\n"
            ],
            "answer": {
                "ans_desc": "So maybe try with ",
                "code": [
                    "out = df.reindex(df[['a','b']].sum(1).sort_values().index).reset_index(drop=True)\nOut[70]: \n     a    b   c\n0  1.0  NaN  v5\n1  NaN  2.0  v2\n2  3.0  NaN  v1\n3  NaN  4.0  v6\n4  5.0  NaN  v3\n5  NaN  6.0  v4\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "calculated-columns"
        ],
        "owner": {
            "account_id": 6701770,
            "reputation": 365,
            "user_id": 5168014,
            "user_type": "registered",
            "accept_rate": 64,
            "profile_image": "https://www.gravatar.com/avatar/2b667ab89110268f86bf1fb8d6a047cf?s=128&d=identicon&r=PG&f=1",
            "display_name": "LinnK",
            "link": "https://stackoverflow.com/users/5168014/linnk"
        },
        "is_answered": true,
        "view_count": 40734,
        "accepted_answer_id": 31699045,
        "answer_count": 4,
        "score": 21,
        "last_activity_date": 1624897831,
        "creation_date": 1438168419,
        "last_edit_date": 1495200691,
        "question_id": 31698861,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/31698861/add-column-to-the-end-of-pandas-dataframe-containing-average-of-previous-data",
        "title": "Add column to the end of Pandas DataFrame containing average of previous data",
        "answer_body": "<p>You can take a copy of your df using <code>copy()</code> and then just call <code>mean</code> and pass params <code>axis=1</code> and <code>numeric_only=True</code> so that the mean is calculated row-wise and to ignore non-numeric columns, when you do the following the column is always added at the end:</p>\n\n<pre><code>In [68]:\n\nsummary_ave_data = df.copy()\nsummary_ave_data['average'] = summary_ave_data.mean(numeric_only=True, axis=1)\nsummary_ave_data\nOut[68]:\n                 Time         F7         F8         F9    average\n0 2015-07-29 00:00:00  43.005593 -56.509746  25.271271   3.922373\n1 2015-07-29 01:00:00  55.114918 -59.173852  31.849262   9.263443\n2 2015-07-29 02:00:00  63.990762 -64.699492  52.426017  17.239096\n</code></pre>\n",
        "question_body": "<p>I have a DataFrame <code>ave_data</code> that contains the following:</p>\n\n<pre><code>ave_data\n\nTime        F7           F8            F9  \n00:00:00    43.005593    -56.509746    25.271271  \n01:00:00    55.114918    -59.173852    31.849262  \n02:00:00    63.990762    -64.699492    52.426017\n</code></pre>\n\n<p>I want to add another column to this dataframe, containing the average of the values under column F7, F8 and F9 for each row.  </p>\n\n<p>The <code>ave_data</code> DataFrame might change size as my code reads from different Excel files later, so the method needs to be generic (i.e add the column containing the average always as the last column in the DataFrame, not in column number 4)</p>\n\n<pre><code>desired output\n\nTime        F7           F8            F9           Average\n00:00:00    43.005593    -56.509746    25.271271    4.25  \n01:00:00    55.114918    -59.173852    31.849262    9.26\n02:00:00    63.990762    -64.699492    52.426017    17.24\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "jupyter-notebook",
            "data-wrangling"
        ],
        "owner": {
            "account_id": 22071091,
            "reputation": 13,
            "user_id": 16332051,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2e5332dc728bf6c90ccf87e68a11975b?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dilan",
            "link": "https://stackoverflow.com/users/16332051/dilan"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 68166334,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624895152,
        "creation_date": 1624893878,
        "last_edit_date": 1624894195,
        "question_id": 68166039,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68166039/pandas-count-certain-value-in-a-column",
        "title": "Pandas Count certain Value in a column",
        "answer_body": "<p>If your column 'B' consists of lists then this should work:</p>\n<pre><code>import pandas as pd\n\ndf =pd.DataFrame({'A': [3,4,56,75,52],'B': [[0,0,0],[2,0,2],[1],[2,0],[0,0,0]]})\n\nlistWeSearch = [0,0,0]\ncounter = 0\n\ncounter = len([counter+1 for i in range(len(df)) if df.iloc[i]['B'] == listWeSearch])\n</code></pre>\n<p>Otherwise if the values of 'B' the other solutions proposed should work!</p>\n",
        "question_body": "<p>I have follwing dataframe:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>A</th>\n<th>B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>0,0,0</td>\n</tr>\n<tr>\n<td>4</td>\n<td>2,0,2</td>\n</tr>\n<tr>\n<td>56</td>\n<td>1</td>\n</tr>\n<tr>\n<td>75</td>\n<td>2,0</td>\n</tr>\n<tr>\n<td>52</td>\n<td>0,0,0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>How can I count the Total Number of <code>0,0,0</code> in column <strong>B</strong>? In my example above it would be: 2</p>\n<p>Thank you!!!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 210201,
            "reputation": 333,
            "user_id": 461682,
            "user_type": "registered",
            "accept_rate": 30,
            "profile_image": "https://i.stack.imgur.com/prQOR.jpg?s=128&g=1",
            "display_name": "sridharraman",
            "link": "https://stackoverflow.com/users/461682/sridharraman"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68165909,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624893794,
        "creation_date": 1624892248,
        "last_edit_date": 1624892381,
        "question_id": 68165624,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68165624/merging-of-pandas-dataframe-with-calculations",
        "title": "Merging of pandas dataframe with calculations",
        "answer_body": "<p>Another way:</p>\n<pre><code>out=df1.merge(df2,on=['id','date'],suffixes=('_1','_2'),how='left'))\n</code></pre>\n<p>Now do calculations:</p>\n<pre><code>out['avg']=out.eval(&quot;(avg_1*count_1+avg_2*count_2)/(count_1+count_2)&quot;)\nout['count']=out.eval(&quot;count_1+count_2&quot;)\nout=out.drop(out.filter(like='_').columns,1)\n</code></pre>\n<p>Finally:</p>\n<pre><code>df2.update(out)\n</code></pre>\n",
        "question_body": "<p>I have a couple of pandas dataframes.</p>\n<p>DF A:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>id</th>\n<th>date</th>\n<th>avg</th>\n<th>count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>27/06/2021</td>\n<td>10</td>\n<td>5</td>\n</tr>\n<tr>\n<td>1</td>\n<td>28/06/2021</td>\n<td>12</td>\n<td>4</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>DF B:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>id</th>\n<th>date</th>\n<th>avg</th>\n<th>count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>27/06/2021</td>\n<td>8</td>\n<td>5</td>\n</tr>\n<tr>\n<td>1</td>\n<td>28/06/2021</td>\n<td>6</td>\n<td>6</td>\n</tr>\n<tr>\n<td>1</td>\n<td>29/06/2021</td>\n<td>11</td>\n<td>10</td>\n</tr>\n<tr>\n<td>2</td>\n<td>27/06/2021</td>\n<td>3</td>\n<td>10</td>\n</tr>\n<tr>\n<td>2</td>\n<td>28/06/2021</td>\n<td>3</td>\n<td>10</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Basically, these are simplifications of intermediate tables aggregated from various Big Data sources. How can I merge these data frames so that the average for a id+date is correct (i.e. it is (avg1 * count1 + avg2 * count2)/(count1 + count2))</p>\n<p>The expected DF for the above two should be like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>id</th>\n<th>date</th>\n<th>avg</th>\n<th>count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>27/06/2021</td>\n<td>9</td>\n<td>10</td>\n</tr>\n<tr>\n<td>1</td>\n<td>28/06/2021</td>\n<td>8.4</td>\n<td>10</td>\n</tr>\n<tr>\n<td>1</td>\n<td>29/06/2021</td>\n<td>11</td>\n<td>10</td>\n</tr>\n<tr>\n<td>2</td>\n<td>27/06/2021</td>\n<td>3</td>\n<td>10</td>\n</tr>\n<tr>\n<td>2</td>\n<td>28/06/2021</td>\n<td>3</td>\n<td>10</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Thanks.</p>\n"
    },
    {
        "tags": [
            "python",
            "excel",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 18725227,
            "reputation": 3,
            "user_id": 13652094,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/fdacde423548d733f3961fb5d0e3593e?s=128&d=identicon&r=PG&f=1",
            "display_name": "starlord",
            "link": "https://stackoverflow.com/users/13652094/starlord"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 68162826,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624884160,
        "creation_date": 1624877771,
        "last_edit_date": 1624883793,
        "question_id": 68162110,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68162110/pandas-index-column",
        "title": "Pandas index column",
        "answer_body": "<p>It is Pandas' convention to show row index on the left.  You can't remove the index  (unless you set other kind of index, but still index there).  If you don't want to see the repeating indices (0 to 3000 repeating 3 times), you can reset the index to re-serialize the index numbers (0 to around 9002) by <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html\" rel=\"nofollow noreferrer\"><code>reset_index()</code></a>, as follows:</p>\n<p>Assuming the name of your dataframe after concat is <code>df</code></p>\n<pre><code>df = df.reset_index(drop=True)    \n</code></pre>\n<h2>Edit</h2>\n<p>If you just don't want to see the row index and is fine to set your column <code>Symbol</code> as the index, you can use:</p>\n<pre><code>df = df.set_index('Symbol')    \n</code></pre>\n<p>Then, you will no longer see the original default range index. Note that in this way, rows of the same <code>Symbol</code> will be placed together.  You have to consider whether this is fine for you.</p>\n",
        "question_body": "<p>I am trying to concat three files into one xlsx file. I'm able to concat but the index column is not getting removed. My Dataframe after using concat function is having index column of all three xlsx file from 0 to 3000 for three times. How should I approach to resolve the issue?</p>\n<p>Details -</p>\n<p>I have amex-listing.xlsx file with 3 worksheet.</p>\n<p>sheet1 = nyse</p>\n<p>sheet2 = amex</p>\n<p>sheet3 = nasdaq</p>\n<p><strong>Code</strong> -</p>\n<pre><code>xls = pd.ExcelFile('amex-listing.xlsx')\nexchanges = xls.sheet_names\npd.set_option(&quot;display.max_columns&quot;,100)\nfinal_file = 'listing.xlsx'\nlistings = []\nfor exchange in exchanges:\n    listing = pd.read_excel('amex-listing.xlsx',sheet_name=exchange, na_values='NaN', index_col=False)\n    listing['exchange'] = exchange\n    sup_df = pd.DataFrame()\n    sup_df = listing.append(sup_df)\n    listings.append(sup_df)\n\nlistings = pd.concat(listings)\n</code></pre>\n<p><strong>Output dataframe-</strong></p>\n<p><a href=\"https://i.stack.imgur.com/1GjYA.jpg\" rel=\"nofollow noreferrer\">index_issue</a></p>\n<p><a href=\"https://i.stack.imgur.com/7VG34.png\" rel=\"nofollow noreferrer\">dataframe_view_after_concat</a></p>\n<p>How can I remove/ignore/drop unnamed index column? Any solution possible?</p>\n<p><strong>Output xlxs file having unnamed index column -</strong></p>\n<p><a href=\"https://i.stack.imgur.com/tZAqw.png\" rel=\"nofollow noreferrer\">amex-index</a></p>\n<p><a href=\"https://i.stack.imgur.com/uaVkM.png\" rel=\"nofollow noreferrer\">amex-nasdaq_index</a></p>\n<p><a href=\"https://i.stack.imgur.com/07Vpl.png\" rel=\"nofollow noreferrer\">nasdaq-nyse_index</a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 11098043,
            "reputation": 352,
            "user_id": 8147508,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dbf0ab4fedaca38f5f4ae3c2344f9b95?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dev Ananth",
            "link": "https://stackoverflow.com/users/8147508/dev-ananth"
        },
        "is_answered": true,
        "view_count": 23,
        "accepted_answer_id": 68162029,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624877427,
        "creation_date": 1624877015,
        "question_id": 68161955,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68161955/pandas-get-column-values-of-last-n-records-as-a-list-from-another-dataframe-base",
        "title": "Pandas get column values of last N records as a list from another dataframe based on current row value",
        "answer_body": "<p>IIUC, you can try:</p>\n<pre><code>my_df.set_index(['Student', 'Subject']).join(master_df.groupby(['Student', 'Subject']).agg(lambda x: list(x)[-3:])).reset_index()\n</code></pre>\n<p>OUTPUT:</p>\n<pre><code>   Student Subject                                              Score\n0     1000     ENG          [0.35104403633346903, 0.1978421698809576]\n1     1001     ENG                               [0.4159411753678969]\n2     1000    MATH                                                NaN\n3     1002    MATH                                                NaN\n4     1001    MATH  [0.4051361256846634, 0.23072043308688617, 0.67...\n</code></pre>\n",
        "question_body": "<p>I have a master dataframe</p>\n<pre><code>master_df = pd.DataFrame({'Student' : [1000, 1001, 1000, 1001, 1001, 1001],\n                      'Subject':  ['ENG','MATH','ENG','MATH','MATH','ENG'],                                 \n                          'Score' : np.random.random(6)})\n</code></pre>\n<p>df looks like below:</p>\n<pre><code>   Student  Subject Score\n0   1000    ENG     0.913371\n1   1001    MATH    0.806932\n2   1000    ENG     0.911395\n3   1001    MATH    0.292194\n4   1001    MATH    0.796219\n5   1001    ENG     0.071908\n</code></pre>\n<p>I have another dataframe</p>\n<pre><code>my_df = pd.DataFrame({'Student' : [1000, 1001, 1000, 1002, 1001],\n                      'Subject':  ['ENG','ENG','MATH','MATH','MATH']})\n</code></pre>\n<p>df looks like below:</p>\n<pre><code>    Student Subject\n0   1000    ENG\n1   1001    ENG\n2   1000    MATH\n3   1002    MATH\n4   1001    MATH\n</code></pre>\n<p>Expected result:</p>\n<pre><code>       Student  Subject Last_3_Scr\n    0   1000    ENG     [0.911395, 0.913371]\n    1   1001    ENG     [0.071908]\n    2   1000    MATH    []\n    3   1002    MATH    []\n    4   1001    MATH    [0.796219,0.292194,0.806932]\n</code></pre>\n<p>I tried</p>\n<pre><code>my_df = my_df.sort_values('Student')\nmy_df['Last_3_Scr'] = [x.agg(list) for x in\n                  master_df.groupby(['Student','Subject'])['Score'].rolling(3)]\n</code></pre>\n<p>But not working as expected. Any help please?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "csv",
            "batch-processing"
        ],
        "owner": {
            "account_id": 18983637,
            "reputation": 1,
            "user_id": 13854508,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gi66eIMv4gGCctcMI5uc3GkQsdj93ssFzPvMSsH=k-s128",
            "display_name": "Madhuvanthi Venkatesh",
            "link": "https://stackoverflow.com/users/13854508/madhuvanthi-venkatesh"
        },
        "is_answered": true,
        "view_count": 50,
        "accepted_answer_id": 68104193,
        "answer_count": 1,
        "score": -3,
        "last_activity_date": 1624876965,
        "creation_date": 1623950168,
        "last_edit_date": 1624876965,
        "question_id": 68023854,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68023854/how-to-process-list-of-gcp-points-from-the-aerial-image-in-python",
        "title": "How to process list of gcp points from the aerial image in python?",
        "answer_body": "<p>Alright, I found the solution for this. I am posting it.</p>\n<pre><code># Read the coordinates in the CSV file\nf=pd.read_csv(&quot;/content/drive/MyDrive/Input/00390_00390_modified.tif.points&quot;)\nkeep_col = ['mapX','mapY','pixelX', 'pixelY', 'enable', 'dX','dY', 'residual']\nnew_f = f[keep_col]\ndf = new_f.drop(columns=['enable','dX', 'dY', 'residual'])\ncol=['mapX','mapY', 'pixelX','pixelY']\nmodified_df = df[col]\nmodified_df['pixelY'] = modified_df['pixelY'] *(-1) \n\n# Create an empty GCP list\ngcp_list=[]\n\n# GCP coordinates list  \nfor index, rows in modified_df.iterrows():\n   gcps = gdal.GCP(rows.mapX, rows.mapY, 1, rows.pixelX, rows.pixelY )\n   gcp_list.append(gcps)\n   print(gcps)\n</code></pre>\n",
        "question_body": "<p>I have gcp points that I am exporting from QGIS and I would like to georeference several maps using the gdal module in python by using these points.</p>\n<p>The following is the code to read the gcp points from the csv file and they will be used for transforming the data in the gdal function.</p>\n<pre><code>import pandas as pd\nf=pd.read_csv(&quot;/content/drive/MyDrive/00390_00390_modified.tif.points&quot;)\nkeep_col = ['mapX','mapY','pixelX', 'pixelY', 'enable', 'dX','dY', 'residual']\nnew_f = f[keep_col]\ndf = new_f.drop(columns=['enable','dX', 'dY', 'residual'])\n#df['gcp']='-gcp'\n#df['end']='\\ '\n#print(df)\ncol=['pixelY','pixelX','mapX','mapY']\nmodified_df = df[col]\nmodified_df['pixelY'] = modified_df['pixelY'] + 1200\nnew_df= modified_df.to_csv('/content/drive/MyDrive/modified.csv', index=False)\nprint(modified_df[col])\n</code></pre>\n<p>The output of this code is,</p>\n<pre><code>        pixelY       pixelX          mapX          mapY\n0    553.956835    19.352518  6.776477e+06  3.485354e+06\n1    879.496403   663.237410  7.781406e+06  4.026885e+06\n2   1053.776978   752.446043  7.925402e+06  4.306009e+06\n3    751.618705  1047.230216  8.385932e+06  3.806277e+06\n4    340.467626   674.208633  7.795048e+06  3.193823e+06\n</code></pre>\n<p>These will be the input ground control points to the !gdaltranslate function.</p>\n<p>Is there any method to insert the coordinates in between -gcp and frontslash?</p>\n<p>I tried a lot, but unfortunately, both reading data and processing them through pandas data frame and CSV aren't working.</p>\n<pre><code>!gdal_translate \\\n-gcp    553.9568345 19.35251799 6776477.033 3485353.556 \\\n-gcp    879.4964029 663.2374101 7781405.588 4026884.535 \\\n-gcp    1053.776978 752.4460432 7925401.709 4306008.844 \\\n-gcp    751.618705  1047.230216 8385931.623 3806277.157 \\\n-gcp    340.4676259 674.2086331 7795047.763 3193823.491 \\\n-of GTiff \\\n/content/drive/MyDrive/00390_00390.tif \\\n/content/drive/MyDrive/map-with-gcps.tif\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "missing-data",
            "fillna"
        ],
        "owner": {
            "account_id": 19736278,
            "reputation": 37,
            "user_id": 14450691,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d49f615cced63690c2c9af9ca7ec5131?s=128&d=identicon&r=PG&f=1",
            "display_name": "saahmed",
            "link": "https://stackoverflow.com/users/14450691/saahmed"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68161712,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1624876574,
        "creation_date": 1624875592,
        "question_id": 68161652,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68161652/how-to-use-each-vector-entry-to-fill-nans-of-a-separate-groups-in-a-dataframe",
        "title": "How to use each vector entry to fill NAN&#39;s of a separate groups in a dataframe",
        "answer_body": "<p>Create dictionary by <code>Patient</code> values with missing values, <code>map</code> to original column and replace missing values only:</p>\n<pre><code>print (df)\n   Age  Patient     HR\n0   21        1    NaN\n1   21        1    NaN\n2   21        1    NaN\n3   30        2  100.0 &lt;- value is not replaced\n4   30        2    NaN\n5   24        3    NaN\n6   24        3    NaN\n7   24        3    NaN\n\n\np = df.loc[df.HR.isna(), 'Patient'].unique()\nvalsHR = [78.8, 82.3, 91.0]\n\ndf['HR'] = df['HR'].fillna(df['Patient'].map(dict(zip(p, valsHR))))\nprint (df)\n   Age  Patient     HR\n0   21        1   78.8\n1   21        1   78.8\n2   21        1   78.8\n3   30        2  100.0\n4   30        2   82.3\n5   24        3   91.0\n6   24        3   91.0\n7   24        3   91.0\n</code></pre>\n<p>If some groups has no NaNs:</p>\n<pre><code>print (df)\n   Age  Patient     HR\n0   21        1    NaN\n1   21        1    NaN\n2   21        1    NaN\n3   30        2  100.0 &lt;- group 2 is not replaced\n4   30        2  100.0 &lt;- group 2 is not replaced\n5   24        3    NaN\n6   24        3    NaN\n7   24        3    NaN\n\n\np = df.loc[df.HR.isna(), 'Patient'].unique()\nvalsHR = [78.8, 82.3, 91.0]\n\ndf['HR'] = df['HR'].fillna(df['Patient'].map(dict(zip(p, valsHR))))\nprint (df)\n   Age  Patient     HR\n0   21        1   78.8\n1   21        1   78.8\n2   21        1   78.8\n3   30        2  100.0\n4   30        2  100.0\n5   24        3   82.3\n6   24        3   82.3\n7   24        3   82.3\n</code></pre>\n",
        "question_body": "<p>Say I have a vector <code>ValsHR</code> which looks like this:</p>\n<p><code>valsHR=[78.8, 82.3, 91.0]</code></p>\n<p>And I have a dataframe <code>MainData</code></p>\n<pre><code>Age  Patient  HR             \n21   1        NaN\n21   1        NaN\n21   1        NaN\n30   2        NaN\n30   2        NaN\n24   3        NaN\n24   3        NaN\n24   3        NaN \n</code></pre>\n<p>I want to fill the NaNs so that the first value in valsHR will only fill in the NaNs for patient 1, the second will fill the NaNs for patient 2 and the third will fill in for patient 3.</p>\n<p>So far I've tried using this:\n<code>mainData['HR'] = mainData['HR'].fillna(ValsHR)</code> but it fills all the NaNs with the first value in the vector.</p>\n<p>I've also tried to use this:\n<code>mainData['HR'] = mainData.groupby('Patient').fillna(ValsHR)</code> fills the NaNs with values that aren't in the <code>valsHR</code> vector at all.</p>\n<p>I was wondering if anyone knew a way to do this?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21767463,
            "reputation": 23,
            "user_id": 16071117,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxxAgOyf3v7o4webh-9YzL517WKZhHDLzn7CI-M=k-s128",
            "display_name": "SURAJ NARAYAN",
            "link": "https://stackoverflow.com/users/16071117/suraj-narayan"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 68161764,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624876114,
        "creation_date": 1624876003,
        "question_id": 68161746,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68161746/how-to-get-datetime",
        "title": "How to get datetime",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_timedelta.html\" rel=\"nofollow noreferrer\"><code>to_timedelta</code></a> to variable for not overwrite original column and add it to datetimes created by <a href=\"http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html\" rel=\"nofollow noreferrer\"><code>to_datetime</code></a>:</p>\n<pre><code>delay = pd.to_timedelta(df['Delay'], unit='D')\ndf['Output_date'] = pd.to_datetime(df['Due_date'], dayfirst=True).add(delay)\n</code></pre>\n",
        "question_body": "<p>Well Sorry about the title...\nActually I'm trying to do something like the below table.</p>\n<p><a href=\"https://i.stack.imgur.com/u4dxH.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/u4dxH.png\" alt=\"enter image description here\" /></a></p>\n<p>Delay is <strong>int64</strong> Dtype and Due_date is <strong>datetime64[ns]</strong> Dtype</p>\n<p>First I try to convert the delay column in datetime using this:</p>\n<pre><code>df['Delay'] = pd.to_datetime(df['Delay'], unit='D')\n</code></pre>\n<p>but it's returning default value i.e. 02-01-1970</p>\n<p>then I try to add directly using this:</p>\n<pre><code>df['Output_date'] = (df['Delay'].add(df['Due_date']))\n</code></pre>\n<p>but this one is also not working....\ncan anyone help me with this.</p>\n<p>I'm a student and new to this area.</p>\n<p>Thank you</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "account_id": 10134182,
            "reputation": 49,
            "user_id": 10215930,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4e77d460c13cb8292442c8f6a6d3c224?s=128&d=identicon&r=PG&f=1",
            "display_name": "Nitish",
            "link": "https://stackoverflow.com/users/10215930/nitish"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68161154,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624873811,
        "creation_date": 1624872431,
        "last_edit_date": 1624872821,
        "question_id": 68160832,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68160832/replace-empty-datetime-value-with-other-datetime-value-from-same-dataframe-based",
        "title": "Replace empty datetime value with other datetime value from same dataframe based on some condition",
        "answer_body": "<p>We can <code>fill</code> the <code>NaT</code> values in <code>start</code> after masking the values in <code>login</code> where the corresponding <code>Status</code> is not one of <code>Closed, Resolved</code></p>\n<pre><code>m = df['Status'].isin(['Resolved', 'Closed'])\ndf['start'] = df['start'].fillna(df['login'].mask(~m))\n</code></pre>\n<hr />\n<pre><code>  login                Status   start\n0 2021-05-28 09:29:35  Resolved 2021-05-28 09:29:35\n1 2021-05-28 11:46:11    Closed 2021-05-28 11:46:11\n2 2021-05-29 15:59:16       WIP                 NaT\n3 2021-05-30 10:43:57    Closed 2021-05-31 12:53:57\n4 2021-06-27 17:53:29  Resolved 2021-06-27 17:53:29\n</code></pre>\n",
        "question_body": "<p>I have a dataframe:</p>\n<pre><code>                  login   Status               start\n0   2021-05-28 09:29:35 Resolved                 NaT\n1   2021-05-28 11:46:11   Closed                 NaT\n2   2021-05-29 15:59:16      WIP                 NaT\n3   2021-05-30 10:43:57   Closed 2021-05-31 12:53:57\n4   2021-06-27 17:53:29 Resolved                 NaT\n</code></pre>\n<p>I want to fill <code>start</code> value with <code>login</code> value if start is <code>NULL</code> and Status is either Resolved or Closed.<br />\nExpected DataFrame:</p>\n<pre><code>                  login   Status               start\n0   2021-05-28 09:29:35 Resolved 2021-05-28 09:29:35\n1   2021-05-28 11:46:11   Closed 2021-05-28 11:46:11\n2   2021-05-29 15:59:16      WIP                 NaT\n3   2021-05-30 10:43:57   Closed 2021-05-31 12:53:57\n4   2021-06-27 17:53:29 Resolved 2021-06-27 17:53:29\n</code></pre>\n<p>iam unable to put condition for <code>Null</code> start values</p>\n<p>I tried to make a function :</p>\n<pre><code>def fun(row):       \n    if row.start.isna() and (row['Status'] == 'Resolved') or (row['Status' == 'Closed']):\n        return row['start']\n    else:\n        return row['login']\n</code></pre>\n<p>and then using <code>apply</code> to run the function:</p>\n<pre><code>df['start'] = df.apply(fun, axis=1)\n</code></pre>\n<p>But iam getting error :</p>\n<pre><code>AttributeError: 'Timestamp' object has no attribute 'isna'\n</code></pre>\n<p>How can we get the above result</p>\n<p>TIA</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20363411,
            "reputation": 15,
            "user_id": 14937262,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cca5877b7afc71360591b636179d69ba?s=128&d=identicon&r=PG&f=1",
            "display_name": "nb123",
            "link": "https://stackoverflow.com/users/14937262/nb123"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 68159801,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624873089,
        "creation_date": 1624867670,
        "question_id": 68159740,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68159740/how-to-populate-a-dataframe-column-based-on-a-lookup-of-other-columns",
        "title": "How to populate a dataframe column based on a lookup of other columns?",
        "answer_body": "<p>Here's one way:</p>\n<pre><code>df['Parent_age'] = df.Parent.map(dict(df[['Child' , 'Age']].values))\n\n# when Parent is not in Child column, then apply get_parent_age\ncond = df['Parent_age'].isnull()\ndf.loc[cond, 'Parent_age'] = df.loc[cond, 'Parent'].map(get_parent_age)\n\n</code></pre>\n",
        "question_body": "<p>I have created a sample dataframe using the following:</p>\n<pre><code>import pandas as pd\nfrom typing import Dict\n\ndata: Dict = {'Child': ['Alpha', 'Bravo', 'Charlie', 'Delta'],\n              'Age': [20, 30, 40, 50],\n              'Parent': ['Delta', 'Delta', 'Echo', 'Foxtrot']}\n\ndf = pd.DataFrame(data)\n\nprint(df)\n</code></pre>\n<p>The output of the above code snippet is as follows:</p>\n<pre><code>     Child  Age   Parent\n0    Alpha   20    Delta\n1    Bravo   30    Delta\n2  Charlie   40     Echo\n3    Delta   50  Foxtrot\n</code></pre>\n<p>I would like to add another column called 'Parent Age' that looks at the Child column to see if Parent is in it and if so, it fetches the corresponding Age from the Age column. If the Parent is not in the Child Column, the Parent Age column value should be populated by doing an extra function call to get_parent_age(name).</p>\n<p>Here is the sample output that I am looking for:</p>\n<pre><code>     Child  Age   Parent  Parent Age\n0    Alpha   20    Delta  50\n1    Bravo   30    Delta  50\n2  Charlie   40     Echo  get_parent_age(Echo)\n3    Delta   50  Foxtrot  get_parent_age(Foxtrot)\n</code></pre>\n<p>My real dataset has many more rows, but hopefully you get the gist. Thank you for your help!</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 19965788,
            "reputation": 143,
            "user_id": 14632409,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/6b59ada5cbd7c398f0f0dd398db38872?s=128&d=identicon&r=PG&f=1",
            "display_name": "Shivika Patel",
            "link": "https://stackoverflow.com/users/14632409/shivika-patel"
        },
        "is_answered": true,
        "view_count": 28,
        "accepted_answer_id": 68160559,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624872822,
        "creation_date": 1624871049,
        "last_edit_date": 1624872822,
        "question_id": 68160505,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68160505/count-the-number-of-files-with-identical-names-but-different-timestamps",
        "title": "Count the number of files with identical names but different timestamps?",
        "answer_body": "<p>IIUC, you can try:</p>\n<pre><code>df.filename = df.filename.str.replace('(\\d+_)', '', regex=True)\n# df.filename = df.filename.str.replace('(\\d{5}_)', '', regex=True)\ndf_final = df.groupby('filename', as_index=False).sum()\n</code></pre>\n<p>OUTPUT:</p>\n<pre><code>              filename  count\n0  phd_lab_sens.txt.gz    650\n</code></pre>\n",
        "question_body": "<p>I need to calculate the number of files that have the same filename but different timestamp in pandas dataframe.</p>\n<p>Example:</p>\n<pre><code>df:\n\nfilename                       count\nphd_20193_lab_sens.txt.gz       100\nphd_20194_lab_sens.txt.gz        50 \nphd_20198_lab_sens.txt.gz       300 \nphd_20199_lab_sens.txt.gz       200\n</code></pre>\n<p>Output:</p>\n<pre><code>df_final\n\nfilename                count\nphd_lab_sens.txt.gz     650\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "correlation",
            "analysis"
        ],
        "owner": {
            "account_id": 16196175,
            "reputation": 860,
            "user_id": 11693768,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/692ebec00b99a45dff3ea0d2542615fd?s=128&d=identicon&r=PG&f=1",
            "display_name": "anarchy",
            "link": "https://stackoverflow.com/users/11693768/anarchy"
        },
        "is_answered": true,
        "view_count": 21,
        "accepted_answer_id": 68160720,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624871973,
        "creation_date": 1624871744,
        "question_id": 68160668,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68160668/pandas-bin-data-based-on-a-column-then-find-correlations-for-each-dataframe",
        "title": "Pandas bin data based on a column then find correlations for each dataframe",
        "answer_body": "<p>you can create a dataframe dict like this:</p>\n<pre><code>df_dict = {f'df{index}': k for index, (_, k) in enumerate(\n    df.groupby(pd.cut(df.years_left, bins=range(0, 1000, 10))))}\n</code></pre>\n<p>And then you can access df's like:</p>\n<pre><code>df_dict['df1'] and so on ...\n</code></pre>\n<p><em>NOTE:</em> Instead of <code>dict comprehension</code> you can also use <code>list comprehension</code>.</p>\n",
        "question_body": "<p>I have the following dataframe,</p>\n<pre><code>id       sqft  years_left date         price\n0        1400  65         01-01-2021   xxx\n1        1200  49         01-01-1950   xxx\n.. \n950,000  1600  10         09-05-1990   xx\n\n\n</code></pre>\n<p>I want to run a correlation between years_left and price.</p>\n<p>But I want to bin the data by the year_left column into 100 bins, between 1 and 999 years, so like</p>\n<pre><code>df = df[df[years_left] &gt; 0 &amp;&amp; df[years_left] &lt; 10]\ndf2 = df[df[years_left] &gt; 11 &amp;&amp; df[years_left] &lt; 20]\n....\n</code></pre>\n<p>I have to manually create each dataframe. Or using a for loop to create the dataframe in a list.</p>\n<p>Is there a shortcut to do what I want? To bin the data into how ever many bins I want then find correlations within each bin?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "account_id": 3337078,
            "reputation": 157,
            "user_id": 9168000,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4f8b0ca12d7cf4951fffba8f055251cb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Manglu",
            "link": "https://stackoverflow.com/users/9168000/manglu"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 68155153,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624862721,
        "creation_date": 1624817816,
        "last_edit_date": 1624823349,
        "question_id": 68154081,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68154081/how-to-put-two-different-flags-based-on-two-thresholds-when-column-value-changes",
        "title": "How to put two different flags based on two thresholds when column value changes in a pandas Dataframe",
        "answer_body": "<h3>Let's approach by the following steps:</h3>\n<p><strong>1) Split the <code>work</code> id into 2 parts: <code>work_prefix</code> and <code>work_suffix</code>:</strong></p>\n<pre><code>df[['work_prefix', 'work_suffix']] = df['work'].str.split('_', expand=True)\n</code></pre>\n<p><strong>2) Then, define a set of boolean masks corresponding to the conditions. These boolean masks are set considering group boundary of same <code>order</code> using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>.groupby()</code></a>:</strong></p>\n<pre><code>threshold_sameprefix = -1.0       # given threshold value\nthreshold_diffprefix = 0.8        # given threshold value\n\nw_ne = df['work'] != df.groupby('order')['work'].shift()          # work id changed\nwp_eq = df['work_prefix'] == df.groupby('order')['work_prefix'].shift()   # same work prefix\nwp_ne = df['work_prefix'] != df.groupby('order')['work_prefix'].shift()   # different work prefix\n\nm1 = w_ne &amp; wp_eq &amp; (df['flag'] &gt; threshold_sameprefix)       # condition for 'F1'\nm2 = w_ne &amp; wp_ne &amp; (df['flag'] &gt; threshold_diffprefix)       # condition for 'F2'\n</code></pre>\n<p><strong>3) Finally, use <code>.loc</code> with the boolean masks to set up <code>flag1</code> with values <code>F1</code> and <code>F2</code>, as follows:</strong></p>\n<pre><code>df['flag1'] = ' '               # init flag1 to blank\ndf.loc[m1, 'flag1'] = 'F1'\ndf.loc[m2, 'flag1'] = 'F2'\n</code></pre>\n<p><strong>Input</strong></p>\n<pre><code>  vehicle order work      flag\n0     abc  abc1  1_1  0.000000\n1     abc  abc1  1_2 -1.166666\n2     abc  abc2  1_1  0.000000\n3     abc  abc2  1_2 -0.624999\n4     abc  abc3  1_1  0.000000\n5     abc  abc3  1_2 -2.020833\n6     abc  abc3  2_1  1.000000\n7     abc  abc3  2_2 -0.052082\n</code></pre>\n<p><strong>Output:</strong></p>\n<pre><code>  vehicle order work      flag work_prefix work_suffix flag1\n0     abc  abc1  1_1  0.000000           1           1      \n1     abc  abc1  1_2 -1.166666           1           2      \n2     abc  abc2  1_1  0.000000           1           1      \n3     abc  abc2  1_2 -0.624999           1           2    F1\n4     abc  abc3  1_1  0.000000           1           1      \n5     abc  abc3  1_2 -2.020833           1           2      \n6     abc  abc3  2_1  1.000000           2           1    F2\n7     abc  abc3  2_2 -0.052082           2           2    F1\n</code></pre>\n<p>Optionally, you can remove the 2 working columns <code>work_prefix</code> and <code>work_suffix</code> by:</p>\n<pre><code>df = df.drop(['work_prefix', 'work_suffix'], axis=1)\n</code></pre>\n<hr />\n<h3>Bonus Codes</h3>\n<p>To set up your first column <code>flag</code> more efficiently instead of using looping, you can use:</p>\n<pre><code>data = [['abc', 'abc1', '1_1',    '2021-06-01 06:00:00.035999', '2021-06-02 09:59:59.964000'],\n ['abc',  'abc1',  '1_2',  '2021-06-01 06:00:00.035999', '2021-06-02 09:59:59.964000'],\n ['abc',  'abc2',  '1_1',  '2021-06-01 06:00:00.035999', '2021-06-01 20:59:59.964001'],\n ['abc',  'abc2',  '1_2',  '2021-06-01 06:00:00.035999', '2021-06-01 20:59:59.964001'],\n ['abc',  'abc3',  '1_1',  '2021-06-01 06:00:00.035999', '2021-06-03 06:29:59.964000'],\n ['abc',  'abc3',  '1_2',  '2021-06-01 06:00:00.035999', '2021-06-03 06:29:59.964000'],\n ['abc',  'abc3',  '2_1',  '2021-06-04 06:30:00.000001', '2021-06-04 07:44:59.927999'],\n ['abc',  'abc3',  '2_2',  '2021-06-04 06:30:00.000001', '2021-06-04 07:44:59.927999']]\ndf = pd.DataFrame(data, columns = ['vehicle', 'order', 'work', 'Start', 'Finish'])\n\ndf['Start'] = pd.to_datetime(df['Start'])\ndf['Finish'] = pd.to_datetime(df['Finish'])\n</code></pre>\n<p><strong>Main codes to replace your codes with looping:</strong></p>\n<pre><code>df['flag'] = ((df['Start'] - df.groupby('order')['Finish'].shift()) / pd.Timedelta(days=1)).fillna(0)\n</code></pre>\n<p><strong>Result:</strong></p>\n<pre><code>print(df)\n\n  vehicle order work                      Start                     Finish      flag\n0     abc  abc1  1_1 2021-06-01 06:00:00.035999 2021-06-02 09:59:59.964000  0.000000\n1     abc  abc1  1_2 2021-06-01 06:00:00.035999 2021-06-02 09:59:59.964000 -1.166666\n2     abc  abc2  1_1 2021-06-01 06:00:00.035999 2021-06-01 20:59:59.964001  0.000000\n3     abc  abc2  1_2 2021-06-01 06:00:00.035999 2021-06-01 20:59:59.964001 -0.624999\n4     abc  abc3  1_1 2021-06-01 06:00:00.035999 2021-06-03 06:29:59.964000  0.000000\n5     abc  abc3  1_2 2021-06-01 06:00:00.035999 2021-06-03 06:29:59.964000 -2.020833\n6     abc  abc3  2_1 2021-06-04 06:30:00.000001 2021-06-04 07:44:59.927999  1.000000\n7     abc  abc3  2_2 2021-06-04 06:30:00.000001 2021-06-04 07:44:59.927999 -0.052082\n</code></pre>\n",
        "question_body": "<p>I have a data-frame, say <code>df</code> (last two columns I consider as <code>datetime64[ns]</code> not <code>str</code>),</p>\n<pre><code>data = [['abc', 'abc1', '1_1',    '2021-06-01 06:00:00.035999', '2021-06-02 09:59:59.964000'],\n ['abc',  'abc1',  '1_2',  '2021-06-01 06:00:00.035999', '2021-06-02 09:59:59.964000'],\n ['abc',  'abc2',  '1_1',  '2021-06-01 06:00:00.035999', '2021-06-01 20:59:59.964001'],\n ['abc',  'abc2',  '1_2',  '2021-06-01 06:00:00.035999', '2021-06-01 20:59:59.964001'],\n ['abc',  'abc3',  '1_1',  '2021-06-01 06:00:00.035999', '2021-06-03 06:29:59.964000'],\n ['abc',  'abc3',  '1_2',  '2021-06-01 06:00:00.035999', '2021-06-03 06:29:59.964000'],\n ['abc',  'abc3',  '2_1',  '2021-06-04 06:30:00.000001', '2021-06-04 07:44:59.927999'],\n ['abc',  'abc3',  '2_2',  '2021-06-04 06:30:00.000001', '2021-06-04 07:44:59.927999']]\n df = pd.DataFrame(data, columns = ['vehicle', 'order', 'work', 'Start', 'Finish'])\n</code></pre>\n<p>I want to find the time between two works. For example, I want to calculate the time between the finishing time of work <code>1_1 (vehicle: abc and order: abc1)</code> and starting time of work <code>1_2</code>. I am calculating it for each distinct <code>order</code>.</p>\n<pre><code>  vehicle  order    work             Start                           Finish\n0     abc  abc1     1_1        2021-06-01 06:00:00.035999     2021-06-02 09:59:59.964000\n1     abc  abc1     1_2        2021-06-01 06:00:00.035999     2021-06-02 09:59:59.964000\n2     abc  abc2     1_1        2021-06-01 06:00:00.035999     2021-06-01 20:59:59.964001\n3     abc  abc2     1_2        2021-06-01 06:00:00.035999     2021-06-01 20:59:59.964001\n4     abc  abc3     1_1        2021-06-01 06:00:00.035999     2021-06-03 06:29:59.964000\n5     abc  abc3     1_2        2021-06-01 06:00:00.035999     2021-06-03 06:29:59.964000\n6     abc  abc3     2_1        2021-06-04 06:30:00.000001     2021-06-04 07:44:59.927999\n7     abc  abc3     2_2        2021-06-04 06:30:00.000001     2021-06-04 07:44:59.927999\n</code></pre>\n<p>I have written one code for this and it is working.</p>\n<pre><code>po_unique = df['order'].unique()\nappended_data = []\nfor pos in po_unique:\n    x1 = df.copy()\n    x1 = x1.loc[x1['order'] == pos, :]\n    x1.reset_index(drop = True, inplace = True)\n    #print(x1)\n    aList = []\n    for i in range(len(x1) - 1):\n        t = (x1.Start[i + 1] - x1.Finish[i])/ dt.timedelta(hours=24)\n        aList.append(t)\n    aList.insert(0, 0)\n    x2 = x1.copy()\n    x2['flag'] = aList\n    appended_data.append(x2)\nappended_data = pd.concat(appended_data)\n\n</code></pre>\n<p>I would like to receive some views about the code. Is there any alternative way to do this?\nThe output for <code>appended_data[['order', 'work', 'flag']]</code> looks like</p>\n<pre><code>Out[112]: \n  order work      flag\n0  abc1  1_1     0.000000\n1  abc1  1_2     -1.166666\n0  abc2  1_1     0.000000\n1  abc2  1_2     -0.624999\n0  abc3  1_1      0.000000\n1  abc3  1_2     -2.020833\n2  abc3  2_1      1.000000\n3  abc3  2_2     -0.052082\n\n</code></pre>\n<p>Now I want to create another column <code>flag1</code> such that if value of the flag column is greater than some threshold value then it will put 'F' in this column. I can do this also by using <code>.apply()</code> function like</p>\n<pre><code>thresold = 0.9\nappended_data['flag1'] = appended_data.apply(lambda row: 'F' if row['flag'] &gt; thresold else ' ', axis = 1)\n</code></pre>\n<p>but if I want to put flag for two different thresholds, one is for &quot;inside&quot; like <code>1_1 to 1_2</code> and another one is for &quot;outside&quot; (when prefix changes) like <code>1_2 to 2_1</code>, then what to do. Say\n<code>threshold_sameprefix = -1.0</code>\n<code>threshold_diffprefix = 0.8</code></p>\n<p>Expected output</p>\n<pre><code>    vehicle order  work      flag     flag1\n     abc     abc1  1_1     0.000000      \n     abc     abc1  1_2     -1.166666      \n     abc     abc2  1_1     0.000000      \n     abc     abc2  1_2     -0.624999     F1 \n     abc     abc3  1_1     0.000000      \n     abc     abc3  1_2     -2.020833      \n     abc     abc3  2_1     1.000000      F2\n     abc     abc3  2_2     -0.052082     F1 \n\n</code></pre>\n<p>Please do not take minimum threshold and apply the logic what I did. I want to create a logic where I want to assign <strong>flag</strong> in an iterative way so that I can customize it.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "filtering",
            "dataframe"
        ],
        "owner": {
            "account_id": 2921032,
            "reputation": 615,
            "user_id": 2503283,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/49cd4b3be566093de663d2ba5f7de68d?s=128&d=identicon&r=PG",
            "display_name": "uchuujin",
            "link": "https://stackoverflow.com/users/2503283/uchuujin"
        },
        "is_answered": true,
        "view_count": 47654,
        "accepted_answer_id": 29836852,
        "answer_count": 4,
        "score": 57,
        "last_activity_date": 1624861750,
        "creation_date": 1429836511,
        "last_edit_date": 1495540477,
        "question_id": 29836836,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/29836836/how-do-i-filter-a-pandas-dataframe-based-on-value-counts",
        "title": "How do I filter a pandas DataFrame based on value counts?",
        "answer_body": "<p>Use <a href=\"http://pandas.pydata.org/pandas-docs/stable/groupby.html#filtration\" rel=\"noreferrer\">groupby filter</a>:</p>\n\n<pre><code>In [11]: df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])\n\nIn [12]: df\nOut[12]:\n   A  B\n0  1  2\n1  1  4\n2  5  6\n\nIn [13]: df.groupby(\"A\").filter(lambda x: len(x) &gt; 1)\nOut[13]:\n   A  B\n0  1  2\n1  1  4\n</code></pre>\n\n<p>I recommend reading the <a href=\"http://pandas.pydata.org/pandas-docs/stable/groupby.html#filtration\" rel=\"noreferrer\">split-combine-section of the docs</a>.</p>\n",
        "question_body": "<p>I'm working in Python with a pandas DataFrame of video games, each with a genre. I'm trying to remove any video game with a genre that appears less than some number of times in the DataFrame, but I have no clue how to go about this. I did find <a href=\"https://stackoverflow.com/questions/6796569/how-to-filter-a-dataframe-based-on-category-counts\">a StackOverflow question</a> that seems to be related, but I can't decipher the solution at all (possibly because I've never heard of R and my memory of functional programming is rusty at best).</p>\n\n<p>Help?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "postgresql",
            "dataframe",
            "psycopg2"
        ],
        "owner": {
            "account_id": 15001531,
            "reputation": 4181,
            "user_id": 10829044,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XkVFAldxdns/AAAAAAAAAAI/AAAAAAAAAAA/AKxrwcYw7mFQ9BqeKrYJYmarsmObRK9Lsw/mo/photo.jpg?sz=128",
            "display_name": "The Great",
            "link": "https://stackoverflow.com/users/10829044/the-great"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68158405,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624861049,
        "creation_date": 1624848075,
        "last_edit_date": 1624848471,
        "question_id": 68156958,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68156958/use-multiple-pandas-list-in-postgresql-query-to-filter-data",
        "title": "Use multiple pandas list in postgresql query to filter data",
        "answer_body": "<p><code>cursor.execute</code> expects it's second argument to be a sequence with one element per value placeholder.  To satisfy this requirement, make a new list by adding both lists together (or extending the first with the second).</p>\n<pre class=\"lang-py prettyprint-override\"><code>all_values = subj_list + class_list\nprint(cur.mogrify(sql, all_values))\n</code></pre>\n<p>gives</p>\n<pre class=\"lang-none prettyprint-override\"><code>b'select * from tablea where subject_id in (1, 2, 3, 4) and class_id in (4, 5, 6, 7, 8, 9, 10, 11, 12, 13)'\n</code></pre>\n",
        "question_body": "<p>I already referred this <a href=\"https://stackoverflow.com/questions/68066426/use-pandas-list-to-filter-data-using-postgresql-query\">post</a> and trying to extend it to work for multiple lists</p>\n<p>Basically, I would like to use multiple pandas list in a postgresql query (written in Jupyter notebook)</p>\n<pre><code>subj_list = [1,2,3,4]\nclass_list = [4,5,6,7,8,9,10,11,12,13]\n</code></pre>\n<p>I would like to both my lists in the below query. I tried the below</p>\n<pre><code>sql = &quot;select * from tablea where subject_id in ({}) and class_id in ({})&quot;\nsub_ids = ', '.join(['%s'] * len(subj_list))\nclass_ids = ', '.join(['%s'] * len(class_list))\nsql = sql.format(sub_ids ,class_ids )\ncur.execute(sql,subj_list,class_list)\n</code></pre>\n<p>I get an error as shown below</p>\n<blockquote>\n<p>TypeError                                 Traceback (most recent call\nlast)  in \n----&gt; 1 cur.execute(sql,subj_list,class_list)</p>\n<p>TypeError: function takes at most 2 arguments (3 given)</p>\n</blockquote>\n<p>I tried to extend <a href=\"https://stackoverflow.com/questions/68066426/use-pandas-list-to-filter-data-using-postgresql-query\">this post</a> for multiple lists but it doesn't seem to work.</p>\n<p>Might be am using it incorrectly. Can help me with this?</p>\n<p>Can experts here help me with the solution on how to use the multiple python list variables directly in the query?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "account_id": 20193145,
            "reputation": 45,
            "user_id": 14810812,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ff10b90db6929cd519d6db4926be26d5?s=128&d=identicon&r=PG&f=1",
            "display_name": "Christian Rueda",
            "link": "https://stackoverflow.com/users/14810812/christian-rueda"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 68148652,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624859439,
        "creation_date": 1624774861,
        "last_edit_date": 1624859439,
        "question_id": 68148541,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68148541/how-to-update-several-dataframes-values-with-another-dataframe-values",
        "title": "How to update several DataFrames values with another DataFrame values?",
        "answer_body": "<p>Create a <strong>mapping</strong> series by selecting the last row from <code>df3</code>, then <code>map</code> it on the column <code>name</code> and fill the nan values using the values from <code>p_mw</code> column</p>\n<pre><code>s = df3.iloc[-1]\n\ndf1['p_mw'] = df1['name'].map(s).fillna(df1['p_mw'])\ndf2['p_mw'] = df2['name'].map(s).fillna(df2['p_mw'])\n</code></pre>\n<p>If there are multiple dataframes that needed to be updated then we can use a for loop to avoid repetition of our code:</p>\n<pre><code>for df in (df1, df2):\n    df['p_mw'] = df['name'].map(s).fillna(df['p_mw'])\n</code></pre>\n<hr />\n<pre><code>&gt;&gt;&gt; df1\n\n   name  p_mw   type\n0  GH_1    60  Hidro\n1  GH_2    40  Hidro\n2  GH_3    90  Hidro\n\n&gt;&gt;&gt; df2\n   name  p_mw   type\n0  GT_1  20.0  Termo\n1  GT_2   0.0  Termo\n2  GF_1  10.0   Fict\n</code></pre>\n",
        "question_body": "<p>Please would you like to know how I can update two <code>DataFrames</code> <code>df1</code> y <code>df2</code> from another <code>DataFrame</code> <code>df3</code>. All this is done within a <code>for</code> loop that iterates over all the elements of the <code>DataFrame</code> <code>df3</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>for i in range(len(df3)):\n   df1.p_mw = ...\n   df2.p_mw = ...\n</code></pre>\n<p>The initial <code>DataFrames</code> <code>df1</code> and <code>df2</code> are as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df1 = pd.DataFrame([['GH_1', 10, 'Hidro'],\n                    ['GH_2', 20, 'Hidro'],\n                    ['GH_3', 30, 'Hidro']],\n                    columns= ['name','p_mw','type'])\n\ndf2 = pd.DataFrame([['GT_1', 40, 'Termo'],\n                    ['GT_2', 50, 'Termo'],\n                    ['GF_1', 10, 'Fict']],\n                    columns= ['name','p_mw','type'])\n</code></pre>\n<p>The <code>DataFrame</code> from which I want to update the data is:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df3 = pd.DataFrame([[150,57,110,20,10],\n                    [120,66,110,20,0],\n                    [90,40,105,20,0],\n                    [60,40,90,20,0]],\n                    columns= ['GH_1', 'GH_2', 'GH_3', 'GT_1', 'GT_2'])\n</code></pre>\n<p>As you can see the <code>DataFrame</code> <code>df3</code> contains data from the corresponding column <code>p_mw</code> for both<code> DataFrames</code> <code>df1</code> and <code>df2</code>. Furthermore, the DataFrame df2 has an element named GF_1 for which there is no update and should remain the same.</p>\n<p>After updating for the last iteration, the desired output is the following:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df1 = pd.DataFrame([['GH_1', 60, 'Hidro'],\n                    ['GH_2', 40, 'Hidro'],\n                    ['GH_3', 90, 'Hidro']],\n                    columns= ['name','p_mw','type'])\n\ndf2 = pd.DataFrame([['GT_1', 20, 'Termo'],\n                    ['GT_2', 0, 'Termo'],\n                    ['GF_1', 10, 'Fict']],\n                    columns= ['name','p_mw','type'])\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "integer"
        ],
        "owner": {
            "account_id": 19672854,
            "reputation": 29,
            "user_id": 14401723,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-mMaJZqFaWGg/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckHOl7aOPEE2PrNvK7sancXcf4Nyg/photo.jpg?sz=128",
            "display_name": "Sunny Reddy",
            "link": "https://stackoverflow.com/users/14401723/sunny-reddy"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 68148914,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624854492,
        "creation_date": 1624778489,
        "last_edit_date": 1624854492,
        "question_id": 68148898,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68148898/attributeerror-int-object-has-no-attribute-split-for-pandas",
        "title": "AttributeError: &#39;int&#39; object has no attribute &#39;split&#39; for pandas",
        "answer_body": "<p>You can try:</p>\n<p>via <code>str.split()</code> and <code>str.len()</code>:</p>\n<pre><code>df['doc_len']=df['Content'].str.split().str.len()\n</code></pre>\n<p><strong>OR</strong></p>\n<p>via <code>str.count()</code> and then add 1:</p>\n<pre><code>df['doc_len']=df['Content'].str.count(' ')+1\n</code></pre>\n",
        "question_body": "<p><strong>AttributeError: 'int' object has no attribute 'split'</strong></p>\n<p>Data is :</p>\n<pre><code>print(df)\n\n\n    Content               Page no\n0   My name is mark       3\n1   My name is jeff       3\n2   My name is bill       3\n</code></pre>\n<p>The code is :</p>\n<pre><code>df['doc_len'] = df['Content'].apply(lambda words: len(words.split()))\n</code></pre>\n<p>The error it's returning is :</p>\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-26-7d2f1de16b3d&gt; in &lt;module&gt;\n----&gt; 1 df['doc_len'] = df['Content'].apply(lambda words: len(words.split()))\n\n~\\t5\\lib\\site-packages\\pandas\\core\\series.py in apply(self, func, convert_dtype, args, **kwds)\n   4136             else:\n   4137                 values = self.astype(object)._values\n-&gt; 4138                 mapped = lib.map_infer(values, f, convert=convert_dtype)\n   4139 \n   4140         if len(mapped) and isinstance(mapped[0], Series):\n\npandas\\_libs\\lib.pyx in pandas._libs.lib.map_infer()\n\n&lt;ipython-input-26-7d2f1de16b3d&gt; in &lt;lambda&gt;(words)\n----&gt; 1 df['doc_len'] = df['Content'].apply(lambda words: len(words.split()))\n\nAttributeError: 'int' object has no attribute 'split'\n</code></pre>\n<p><strong>The Content column is object type and it's returning int object has no attribute split</strong></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22024896,
            "reputation": 27,
            "user_id": 16293054,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5a83f115085bff37e36dcc04d0a439fd?s=128&d=identicon&r=PG&f=1",
            "display_name": "littlebigship",
            "link": "https://stackoverflow.com/users/16293054/littlebigship"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68157063,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624853635,
        "creation_date": 1624845011,
        "question_id": 68156687,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68156687/how-to-strip-symbol-from-pandas-data-frame-column",
        "title": "How to strip symbol from pandas data frame column",
        "answer_body": "<p>Use str.extract or regex capture group to keep only the numbers -- and the decimal if there is one (otherwise you would have 45.00 become 4500).  If you have decimals, use astype(float), or if you have only integers, you can ignore the . and use astype(int).  Here are 2 ways to clean up your Price column, you only need one:</p>\n<pre><code>row1list = ['$500 -', 'www']\nrow2list = ['$4.00 -', 'xyz']\ndf = pd.DataFrame([row1list, row2list],\n                  columns=['Price', 'abc'])\n\n# option 1:  regex capture groups\ndf['Price'] = df['Price'].str.replace('([0-9]+)', r'\\1', regex=True).astype(float)\n\n# option 2:  extract\ndf['Price'] = df['Price'].str.extract('([0-9.]+)').astype(float)\n\n# print(df)\n#    Price  abc\n# 0  500.0  www\n# 1    4.0  xyz\n\n\n</code></pre>\n",
        "question_body": "<p>I'm trying to turn a column of text numbers into numeric values but there is a pesky &quot; -&quot; at the end of the values that I can't seem to get rid of.</p>\n<p>Here's my code</p>\n<pre><code>from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(driver.page_source)\nitems = soup.find(&quot;div&quot;, {&quot;class&quot;: &quot;items-grid-view&quot;})\n\nrows_processed=[]\nfor item in items.findAll(&quot;div&quot;, {&quot;class&quot;: &quot;item-cell&quot;}):\n    itemTitle = item.find(&quot;a&quot;, {&quot;class&quot;: &quot;item-title&quot;})\n    itemPromo = item.find(&quot;p&quot;, {&quot;class&quot;: &quot;item-promo&quot;})\n    itemPrice = item.find(&quot;li&quot;, {&quot;class&quot;: &quot;price-current&quot;})\n    row = []\n    \n    row.append(itemTitle.text)\n    row.append(itemPromo.text)\n    \n    offer_tag = itemPrice.find(&quot;a&quot;) \n    if offer_tag:\n        offer_tag.extract()\n        \n    row.append(itemPrice.text)\n\n        \n    rows_processed.append(row)\n\ndf = pd.DataFrame.from_records(rows_processed, columns=[&quot;Item Title &quot;, &quot;Status&quot;, &quot;Price&quot;])\ndf['Price'] = df['Price'].str.lstrip('$')\ndf['Price'] = df['Price'].str.rstrip(' -')\ndf = df.replace(',','', regex=True)\ndf.replace(to_replace =&quot;-&quot;,\n                 value =&quot;&quot;)\n\nisAvailable = &quot;Available&quot; in df[&quot;Status&quot;].values\nprint(isAvailable)\ndisplay(df)\n</code></pre>\n<p>With the commands I've used I've been able to strip a     &quot;C&quot;     &quot;$&quot;    &quot;,&quot;    and    &quot;(2 Offers)&quot; from the price value. But there's still a &quot; -&quot; after every number i.e.</p>\n<pre><code>500 -\n450 - \n600 - \n1200 -\netc\n</code></pre>\n<p>How do I strip the space and dash from every value in the data frame column?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by"
        ],
        "owner": {
            "account_id": 7984773,
            "reputation": 335,
            "user_id": 6024683,
            "user_type": "registered",
            "accept_rate": 0,
            "profile_image": "https://www.gravatar.com/avatar/a25b1d4334e8fb67c565efc9e4371f9a?s=128&d=identicon&r=PG&f=1",
            "display_name": "WILLIAM",
            "link": "https://stackoverflow.com/users/6024683/william"
        },
        "is_answered": true,
        "view_count": 44,
        "accepted_answer_id": 68157164,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624850622,
        "creation_date": 1624849745,
        "question_id": 68157109,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68157109/pandas-how-to-group-multiple-row-by-different-criteria",
        "title": "Pandas: How to group multiple row by different criteria",
        "answer_body": "<p>Create a dictionary that reverse map from <code>Team</code> to <code>Country</code> and then aggregate by <code>Country</code>:</p>\n<pre><code>df = pd.DataFrame(example)\n\ndf.Goals.groupby(\n  df.Team.map({v: k for k, lst in group_dict.items() for v in lst}).rename('Country')\n).sum().reset_index()\n\n#  Country  Goals\n#0   Italy     20\n#1      UK     39\n</code></pre>\n",
        "question_body": "<p>I am trying to group row by different condition and here is the example. Basically, what I want to try is group the <code>Team</code> name and put it into a new <code>Dataframe</code> with the <code>sum of goal</code>. I have try <code>groupby</code> but somehow cannot do what I want. How can I get the expected result? Thanks!</p>\n<pre><code>example = {'Team':['Arsenal', 'Manchester United', 'Arsenal',\n               'Arsenal', 'Chelsea', 'Manchester United',\n               'Manchester United', 'Chelsea', 'Chelsea', 'Chelsea',\n               'Juventus','Juventus'],\n                 \n       'Player':['Ozil', 'Pogba', 'Lucas', 'Aubameyang',\n                   'Hazard', 'Mata', 'Lukaku', 'Morata', \n                                     'Giroud', 'Kante',\n                'Ronaldo','Buffon'],\n                                       \n       'Goals':[5, 3, 6, 4, 9, 2, 0, 5, 2, 3, 20, 0] }\ngroup_dict = {'UK':['Arsenal', 'Manchester United', 'Chelsea'], 'Italy':['Juventus']}\n</code></pre>\n<p>Expected Result:</p>\n<pre><code>Country    Goals\nUK          39\nItaly       20\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 4651817,
            "reputation": 2791,
            "user_id": 3768495,
            "user_type": "registered",
            "accept_rate": 88,
            "profile_image": "https://www.gravatar.com/avatar/085b9585d2608c6356c01e6342ad43be?s=128&d=identicon&r=PG&f=1",
            "display_name": "user3768495",
            "link": "https://stackoverflow.com/users/3768495/user3768495"
        },
        "is_answered": true,
        "view_count": 242459,
        "accepted_answer_id": 40353394,
        "answer_count": 3,
        "score": 105,
        "last_activity_date": 1624839939,
        "creation_date": 1477965018,
        "question_id": 40353079,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/40353079/pandas-how-to-check-dtype-for-all-columns-in-a-dataframe",
        "title": "pandas how to check dtype for all columns in a dataframe?",
        "answer_body": "<p>The <strong>singular</strong> form <code>dtype</code> is used to check the data type for a single column. And the <strong><em>plural</em></strong> form <code>dtypes</code> is for data frame which returns data types for all columns. Essentially:</p>\n\n<p><strong><em>For a single column</em></strong>:</p>\n\n<pre><code>dataframe.column.dtype\n</code></pre>\n\n<p><strong><em>For all columns</em></strong>:</p>\n\n<pre><code>dataframe.dtypes\n</code></pre>\n\n<p><em>Example</em>:</p>\n\n<pre><code>import pandas as pd\ndf = pd.DataFrame({'A': [1,2,3], 'B': [True, False, False], 'C': ['a', 'b', 'c']})\n\ndf.A.dtype\n# dtype('int64')\ndf.B.dtype\n# dtype('bool')\ndf.C.dtype\n# dtype('O')\n\ndf.dtypes\n#A     int64\n#B      bool\n#C    object\n#dtype: object\n</code></pre>\n",
        "question_body": "<p>It seems that dtype only work for pandas.DataFrame.Series, right? Is there a function to display data types of all columns at once?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime",
            "time-series"
        ],
        "owner": {
            "account_id": 12416217,
            "reputation": 55,
            "user_id": 14251610,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2354393afcac16b162e09abfc21f4a11?s=128&d=identicon&r=PG&f=1",
            "display_name": "Peshal1067",
            "link": "https://stackoverflow.com/users/14251610/peshal1067"
        },
        "is_answered": true,
        "view_count": 90,
        "accepted_answer_id": 68136762,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1624833689,
        "creation_date": 1624623444,
        "last_edit_date": 1624685155,
        "question_id": 68130863,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68130863/comparing-two-timeseries-dataframes-based-on-some-conditions-in-pandas",
        "title": "comparing two timeseries dataframes based on some conditions in pandas",
        "answer_body": "<p>Input data:</p>\n<pre><code>&gt;&gt;&gt; df1\n                     value_1\ndate_1\n2017-10-11 00:00:00   5000.0\n2017-10-11 03:00:00   1500.0\n2017-10-11 06:00:00   1200.0\n2017-10-11 09:00:00      NaN\n\n&gt;&gt;&gt; df2\n                     value_2\ndate_2\n2017-10-11 00:00:00   1500.0\n2017-10-11 00:30:00   2050.0\n2017-10-11 00:50:00      NaN\n2017-10-11 01:20:00   2400.0\n2017-10-11 01:40:00   2500.0\n...\n2017-10-11 08:20:00   2400.0\n2017-10-11 08:50:00   2600.0\n2017-10-11 09:20:00      NaN\n2017-10-11 09:50:00   8000.0\n2017-10-11 10:20:00   9000.0\n</code></pre>\n<ol>\n<li>Fill <code>NaN</code> value from df2 by linear interpolation between <code>t-1</code> and <code>t+1</code>:</li>\n</ol>\n<pre><code>df2['value_2'] = df2['value_2'].interpolate()\n</code></pre>\n<ol start=\"2\">\n<li>Create an interval from df1 according to your rules:</li>\n</ol>\n<pre><code>ii = pd.IntervalIndex.from_tuples(\n         list(zip(df1.index - pd.DateOffset(hours=1, minutes=29),\n                  df1.index + pd.DateOffset(hours=1, minutes=30)))\n     )\n</code></pre>\n<ol start=\"3\">\n<li>Bin values into discrete intervals:</li>\n</ol>\n<pre><code>df1['interval'] = pd.cut(df1.index, bins=ii)\ndf2['interval'] = pd.cut(df2.index, bins=ii)\n</code></pre>\n<ol start=\"4\">\n<li>Merge the two dataframes on <code>interval</code>:</li>\n</ol>\n<pre><code>dfx = pd.merge(df2, df1, on='interval', how='left').set_index('interval')\ndfx = (dfx['value_2'].lt(2800) &amp; dfx['value_1'].gt(1600)) \\\n          .astype(int).to_frame('count').set_index(df2.index)\n</code></pre>\n<ol start=\"5\">\n<li>Append index of <code>df1</code> with as a freq of 90 minutes:</li>\n</ol>\n<pre><code>dti = df2.index.append(\n          pd.DatetimeIndex(df1.index.to_series().resample('90T').groups.keys())\n      ).sort_values().drop_duplicates()\ndfx = dfx.reindex(dti).ffill().astype(int)\n</code></pre>\n<ol start=\"6\">\n<li>Compute duration from <code>count</code> and reindex from <code>df2</code>:</li>\n</ol>\n<pre><code>dfx['duration'] = dfx.index.to_series().diff(-1).abs() \\\n                     .fillna(pd.Timedelta(0)).dt.components \\\n                     .apply(lambda x: f&quot;{x['hours']:02}:{x['minutes']:02}&quot;,\n                            axis='columns')\n\ndfx.loc[dfx['count'] == 0, 'duration'] = '00:00'\ndfx = dfx.reindex(df2.index)\n</code></pre>\n<p>Output result:</p>\n<pre><code>&gt;&gt;&gt; dfx\n                     count duration\ndate_2\n2017-10-11 00:00:00      1    00:30\n2017-10-11 00:30:00      1    00:20\n2017-10-11 00:50:00      1    00:30\n2017-10-11 01:20:00      1    00:10\n2017-10-11 01:40:00      0    00:00\n2017-10-11 02:20:00      0    00:00\n2017-10-11 02:50:00      0    00:00\n2017-10-11 03:00:00      0    00:00\n2017-10-11 03:20:00      0    00:00\n2017-10-11 03:50:00      0    00:00\n2017-10-11 04:20:00      0    00:00\n2017-10-11 04:50:00      0    00:00\n2017-10-11 05:20:00      0    00:00\n2017-10-11 05:50:00      0    00:00\n2017-10-11 06:00:00      0    00:00\n2017-10-11 06:20:00      0    00:00\n2017-10-11 06:50:00      0    00:00\n2017-10-11 07:20:00      0    00:00\n2017-10-11 07:50:00      1    00:30\n2017-10-11 08:20:00      1    00:30\n2017-10-11 08:50:00      1    00:10\n2017-10-11 09:20:00      0    00:00\n2017-10-11 09:50:00      0    00:00\n2017-10-11 10:20:00      0    00:00\n</code></pre>\n",
        "question_body": "<p>I have two timeseries dataframes <code>df1</code> and <code>df2</code>:</p>\n<pre><code>df1 = pd.DataFrame({'date_1':['10/11/2017 0:00','10/11/2017 03:00','10/11/2017 06:00','10/11/2017 09:00'],\n                  'value_1':[5000,1500,np.nan,2000]})\n\ndf1['date_1'] = pd.to_datetime(df1.date_1.astype(str), format='%m/%d/%Y %H:%M',errors ='coerce') \ndf1.index = pd.DatetimeIndex(df1.date_1)\ndf1.drop('date_1', axis = 1, inplace = True)\n</code></pre>\n<p>&amp;</p>\n<pre><code>df2 = pd.DataFrame({'date_2': ['2017-10-11 00:00:00', '2017-10-11 00:30:00','2017-10-11 00:50:00', '2017-10-11 01:20:00',\n                             '2017-10-11 01:40:00','2017-10-11 02:20:00','2017-10-11 02:50:00', '2017-10-11 03:00:00',\n                             '2017-10-11 03:20:00', '2017-10-11 03:50:00','2017-10-11 04:20:00', '2017-10-11 04:50:00',\n                             '2017-10-11 05:20:00', '2017-10-11 05:50:00','2017-10-11 06:00:00', '2017-10-11 06:20:00',\n                             '2017-10-11 06:50:00', '2017-10-11 07:20:00','2017-10-11 07:50:00', '2017-10-11 08:20:00',\n                             '2017-10-11 08:50:00', '2017-10-11 09:20:00','2017-10-11 09:50:00', '2017-10-11 10:20:00'],\n\n                  'value_2':[1500.0, 2050.0,  np.nan,  2400.0, \n                           2500.0,  2550.0,  2900.0,  np.nan,\n                           3200.0,  3500.0,  np.nan,  3600.0,\n                           2600.0,  2500.0,  2350.0,  2200.0,\n                           np.nan,  2100.0,  np.nan,  2400.0,\n                           2600.0,  np.nan,  8000.0,  9000.0]\n                    })\ndf2['date_2'] = pd.to_datetime(df2.date_2.astype(str), format='%Y-%m-%d %H:%M',errors ='coerce') \ndf2.index = pd.DatetimeIndex(df2.date_2)\ndf2.drop('date_2', axis = 1, inplace = True)\n</code></pre>\n<p>Both dataframes are observations on the same day but with different time resolution. <code>df1</code> has time resolution of <code>3 hours</code> whereas <code>df2</code> has time resolution of <code>30 minutes</code> or less.\nI am interested to create a new dataframe <code>dfx</code> by comparing above dataframes with certain conditions, and create two columns <code>count</code> and <code>duration</code> in <code>dfx</code>.</p>\n<ul>\n<li>firstly: look at <code>df_2['value_2']</code></li>\n<li>compare <code>df_2['value_2']</code> with <code>df_1['value_1']</code></li>\n<li>if <code>df_2['value_2']&lt;2800</code> for a timestamp &amp; <code>df_1['value_1'] &gt;1600</code> for a timestamp within nearest half of the resolution of <code>df1</code> i.e. <code>01:30</code> we count the <code>event</code> as <code>1</code> otherwise <code>0</code>.</li>\n<li>e.g. for a timestamps of <code>df2</code> <code>00:00:00 - 01:30:00 </code> compare <code>df_2['value_2']</code> values with<br />\n<code>df_1['value_1']</code> at <code>00:00:00</code></li>\n<li>for a timestamps of <code>df2</code> <code>01:31:00 - 03:00:00 </code> compare <code>df_2['value_2']</code> values with<br />\n<code>df_1['value_1']</code> at <code>03:00:00</code></li>\n<li>for a timestamps of <code>df2</code> <code>03:00:00 - 04:30:00 </code> compare <code>df_2['value_2']</code> values with<br />\n<code>df_1['value_1']</code> at <code>03:00:00</code></li>\n<li>for a timestamps of <code>df2</code> <code>04:31:00 - 06:00:00 </code> compare <code>df_2['value_2']</code> values with<br />\n<code>df_1['value_1']</code> at <code>06:00:00</code>\nand so on.\nwhere,</li>\n<li>if <code>df2['value_2] == np.nan</code> for a timestamp <code>t</code> replace the <code>nan</code> value with average of values at timestamps<code>t-1 &amp; t+1</code> and then make  the comparison.</li>\n<li>if <code>df1['value_1] == np.nan</code> for a timestamp <code>t</code> , give the corresponding <code>count</code> value 0.</li>\n</ul>\n<p>For the <code>duration</code> column in <code>dfx</code>:\n<code>dfx['duration] = df2.index[i+1] - df2.index[i]</code>\nfor <code>count</code> on marginal time stamps like <code>01:20:00</code>,\n<code>dfx['duration] = (df1.index[i] + 01:30) - df2.index[i]</code>\nwhere. <code>df1.index[i]</code> is the timestamp of <code>df1</code> with which comparison of df2 is made.</p>\n<p><strong>Desired output</strong></p>\n<pre><code>dfx = pd.DataFrame({'date_2': ['2017-10-11 00:00:00', '2017-10-11 00:30:00','2017-10-11 00:50:00', '2017-10-11 01:20:00',\n                             '2017-10-11 01:40:00','2017-10-11 02:20:00','2017-10-11 02:50:00', '2017-10-11 03:00:00',\n                             '2017-10-11 03:20:00', '2017-10-11 03:50:00','2017-10-11 04:20:00', '2017-10-11 04:50:00',\n                             '2017-10-11 05:20:00', '2017-10-11 05:50:00','2017-10-11 06:00:00', '2017-10-11 06:20:00',\n                             '2017-10-11 06:50:00', '2017-10-11 07:20:00','2017-10-11 07:50:00', '2017-10-11 08:20:00',\n                             '2017-10-11 08:50:00', '2017-10-11 09:20:00','2017-10-11 09:50:00', '2017-10-11 10:20:00'],\n\n                  'count':[1, 1,  1,  1, \n                           0,  0,  0, 0,\n                           0,  0,  0,  0,\n                           0,  0,  0,  0,\n                           0,  0,  1,  1,\n                           1,  0,  0,  0],\n                    \n                    'duration':['00:30','00:20','00:30','00:10',\n                                '00:00', '00:00', '00:00', '00:00',\n                                '00:00', '00:00', '00:00', '00:00',\n                                '00:00', '00:00', '00:00', '00:00',\n                                '00:00', '00:00', '00:30', '00:30',\n                                '00:10', '00:00', '00:00', '00:00']})\n                        \ndfx['date_2'] = pd.to_datetime(dfx.date_2.astype(str), format='%Y-%m-%d %H:%M',errors ='coerce') \ndfx.index = pd.DatetimeIndex(dfx.date_2)\ndfx.drop('date_2', axis = 1, inplace = True)\n</code></pre>\n<p>My question has become quite long in spite of my desire to shorten it. Please, bear with it.\nI would highly appreciate your kind help.</p>\n<p>Thanks!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "date"
        ],
        "owner": {
            "account_id": 21987595,
            "reputation": 51,
            "user_id": 16261585,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5f94e4c603f799cbd63a6c0dab575576?s=128&d=identicon&r=PG&f=1",
            "display_name": "gato",
            "link": "https://stackoverflow.com/users/16261585/gato"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 68155543,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624832964,
        "creation_date": 1624829146,
        "question_id": 68155390,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68155390/error-during-calculation-of-age-based-on-values-in-column-in-python-pandas",
        "title": "Error during calculation of age based on values in column in Python Pandas?",
        "answer_body": "<p>Next time provide such snipets in order to help you faster instead of recreating your data</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = pd.DataFrame(\n        columns=['col1'],\n        data = [['60100412345'],['70111243335']]\n     )\n</code></pre>\n<p>The following function is based on: <a href=\"https://github.com/arthurdejong/python-stdnum/blob/master/stdnum/pl/pesel.py\" rel=\"nofollow noreferrer\">https://github.com/arthurdejong/python-stdnum/blob/master/stdnum/pl/pesel.py</a></p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_birth_date(number):\n    year = int(number[0:2])\n    month = int(number[2:4])\n    day = int(number[4:6])\n    year += {\n        0: 1900,\n        1: 2000,\n        2: 2100,\n        3: 2200,\n        4: 1800,\n    }[month // 20]\n    month = month % 20\n    return pd.Timestamp(year, month, day)\n\n        \ntoday_date= pd.Timestamp(year=2021, month=6, day=30)\ndf[&quot;AGE&quot;] = (today_date - df.col1.apply(lambda x: get_birth_date(x))) / np.timedelta64(1, 'Y')\ndf[&quot;AGE&quot;] = df.AGE.astype(&quot;int&quot;)\ndf\ncol1    AGE\n0   60100412345 60\n1   70111243335 50\n</code></pre>\n",
        "question_body": "<p>I have Data Frame in Python Pandas like below:</p>\n<pre><code>col1\n-----------\n60100412345\n70111243335\n</code></pre>\n<p>And I try to create column &quot;age&quot; based on values in &quot;col1&quot; because:</p>\n<ul>\n<li>first two numbers is year</li>\n<li>next two values is month</li>\n<li>netxt two values is day</li>\n</ul>\n<p>So, 60100412345 is year = 1960, month = 10, day = 04</p>\n<p>And I use below code to calculate age:</p>\n<pre><code>today_date= pd.Timestamp(year=2021, month=6, day=30)\ndf[&quot;AGE&quot;] = (today_date - pd.to_datetime(df.col1.str[:6], format = '%y%m%d')) / np.timedelta64(1, 'Y')\ndf[&quot;AGE&quot;] = df.AGE.astype(&quot;int&quot;)\n</code></pre>\n<p>but I have an error like: ValueError: <code>unconverted data remains: 28</code></p>\n<p>How can I repare this error ? Or maybe do you have another idea how to calculate age based on values in col1 ?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 12892106,
            "reputation": 23,
            "user_id": 9323741,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-asM-JlZgzeQ/AAAAAAAAAAI/AAAAAAAAAPA/oI7gih05DEU/photo.jpg?sz=128",
            "display_name": "Fernando Fernandes",
            "link": "https://stackoverflow.com/users/9323741/fernando-fernandes"
        },
        "is_answered": true,
        "view_count": 53,
        "accepted_answer_id": 68155646,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624831567,
        "creation_date": 1624820973,
        "last_edit_date": 1624822042,
        "question_id": 68154495,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68154495/pandas-groupby-variable-time-intervals",
        "title": "Pandas groupby variable time intervals",
        "answer_body": "<p>Yes, this is possible, you'll just need to create a custom grouping function in order to handle the non-uniformity of your use case. In the solution below, I am first creating a new column <code>end_dt</code>, which we will later use as our innermost grouping index. In order to create this column, we are invoking a function <code>get_end_times()</code> using the <code>start_dt</code> column that will take each group (<code>machine</code>/<code>benchmark</code> combo) and call the <code>run_calc()</code> inner function. This function uses the first <code>start_dt</code> in the slice of the dataframe passed to the function to determine where to set the endpoint (1-hr later). It then checks which elements fall within that span and returns the set of <code>end_dt</code> that will be reassigned to the group that invoked the inner function. This iterates until all <code>start_dt</code> values have been assigned an <code>end_dt</code> value (checked via <code>(~f).all()</code>). See below for full implementation:</p>\n<pre><code>def run_calc(x):\n\n    i = (x - x.iloc[0]).dt.total_seconds()&gt;3600\n\n    x[~i] = x.iloc[0] + np.timedelta64(1, 'h')\n\n    return x, i\n\ndef get_end_times(group):\n\n    f = pd.Series([True]*len(group), index=group.index)\n\n    iterate = True\n\n    while iterate:\n        new, f = run_calc(group[f])\n        group[(~f).index] = new\n        if (~f).all(): iterate = False\n\n    return group\n\ndf['end_dt'] = df.groupby(['machine','benchmark'])['start_dt'].transform(get_end_times)\n\ndf.groupby(['machine','benchmark','end_dt']).agg({'start_dt': 'first', 'value1': 'sum', 'value2': 'sum', 'value3': 'sum'}) \\\n    .reset_index().set_index(['machine','benchmark','start_dt','end_dt'])\n</code></pre>\n<p>Yields:</p>\n<pre><code>                                                           value1  value2  \\\nmachine benchmark start_dt            end_dt                                \nA       bench1    2021-06-07 07:32:01 2021-06-07 08:32:01       0       0   \n                  2021-06-07 11:12:21 2021-06-07 12:12:21       1       0   \n        bench2    2021-06-17 12:28:57 2021-06-17 13:28:57       6       0   \n\n                                                           value3  \nmachine benchmark start_dt            end_dt                       \nA       bench1    2021-06-07 07:32:01 2021-06-07 08:32:01      10  \n                  2021-06-07 11:12:21 2021-06-07 12:12:21      16  \n        bench2    2021-06-17 12:28:57 2021-06-17 13:28:57       2  \n</code></pre>\n",
        "question_body": "<p>I am struggling to find the correct way to group a DataFrame using some constraints. I have the following Dataframe:</p>\n<pre><code>           start_dt  machine     benchmark      value1  value2  value3\n2021-06-07 07:32:01  A           bench1         0       0       0\n2021-06-07 07:32:37  A           bench1         0       0       0\n2021-06-07 07:33:13  A           bench1         0       0       0\n2021-06-07 07:33:49  A           bench1         0       0       0\n2021-06-07 07:34:26  A           bench1         0       0       0\n2021-06-07 08:30:26  A           bench1         0       0       10\n2021-06-07 11:12:21  A           bench1         0       0       6\n2021-06-07 12:05:21  A           bench1         1       0       10\n2021-06-17 12:28:57  A           bench2         0       0       0\n2021-06-17 12:29:29  A           bench2         0       0       0\n2021-06-17 12:33:09  A           bench2         3       0       1\n2021-06-17 12:33:48  A           bench2         3       0       1\n2021-06-17 12:35:17  A           bench2         0       0       0\n</code></pre>\n<p>I want to group base on machine, benchmark, and start_dt columns. However, it has some constraints on the start_dt column.\nstart_dt group criteria must be on 1h chunks. I have tried the following command:</p>\n<pre><code>df.groupby([&quot;machine&quot;, &quot;benchmark&quot;, pd.Grouper(key=&quot;start_dt&quot;, freq=&quot;1h&quot;, sort=True, origin=&quot;start&quot;)]).sum()\n</code></pre>\n<p>However, it will group the dataframe based on the first datetime on for all benchmaks, and I don't want this. What I would like is something like the following, where end_dt is start_dt + 1h.</p>\n<pre><code>machine benchmark          start_dt               end_dt   value1  value2  value3              \nA       bench1  2021-06-07 07:32:01  2021-06-07 08:32:01   0          0    10\n                2021-06-07 11:12:21  2021-06-07 12:12:21   1          0    16\n        bench2  2021-06-17 12:28:57  2021-06-17 13:28:57   6          0    2\n</code></pre>\n<p>For example for machine A and benchmark bench1 there are at least two time intervals</p>\n<p>2021-06-07 07:32:01  2021-06-07 08:32:01</p>\n<p>2021-06-07 11:12:21  2021-06-07 12:12:21</p>\n<p>but nothing in the middle, consequentially I would like to maintain the time intervals as they appear on the column instead of what pandas Grouper gives to me. Is it possible?</p>\n<p><strong>Edits:</strong></p>\n<ul>\n<li>The timestamps are unique</li>\n</ul>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 19765227,
            "reputation": 13,
            "user_id": 14473410,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a580f29a24e43d80767e7f486bf05ef8?s=128&d=identicon&r=PG&f=1",
            "display_name": "Nesha25",
            "link": "https://stackoverflow.com/users/14473410/nesha25"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 68137108,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1624826145,
        "creation_date": 1624651706,
        "last_edit_date": 1624826145,
        "question_id": 68136563,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68136563/create-dataframe-from-rows-under-a-row-with-a-certain-condition",
        "title": "Create dataframe from rows under a row with a certain condition",
        "answer_body": "<p>I'm sure I saw a similar case recently but couldn't find it.. There's probably a better solution, but here's one option that makes a new column that accumulates a counter you can use to split the data using <code>groupby</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport datetime\n\n# make a test df\ndata = [datetime.datetime.strptime('06/23/2021', &quot;%m/%d/%Y&quot;),\n'tony',\n'nikki',\n...]\n\ndf = pd.DataFrame({&quot;raw&quot;:data})\n\n# basically count +1 every time a datetime object is encountered\ndf[&quot;groups&quot;] = df.raw.apply(isinstance, args=(datetime.datetime,)).astype(int).cumsum()\n\n</code></pre>\n<p>Result:</p>\n<pre class=\"lang-py prettyprint-override\"><code>                    raw  groups\n0   2021-06-23 00:00:00       1\n1                  tony       1\n2                 nikki       1\n3                 james       1\n4   2021-06-24 00:00:00       2\n5                   amy       2\n6                  jose       2\n7   2021-06-25 00:00:00       3\n8                  tony       3\n9                  jose       3\n10                eddie       3\n11                 anna       3\n</code></pre>\n<p>Then you can get the individual groups via <code>df.groupby(&quot;groups&quot;)</code>.</p>\n",
        "question_body": "<p>I have an ugly data source that I am trying to clean up. I cannot make the source cleaner, I receive it like this. I am using Python and Pandas dataframes. The first column contains a cell with a datetime object followed by several cells with string, and then another datetime object, and then more strings. The number of strings is not known.</p>\n<p>I would like to make each group of data into its own dataframe, so that I may export each group neatly into separate sheets in a spreadsheet, which is required for this project.</p>\n<p>so, input dataframe has a first column something like this:</p>\n<pre><code>df[col1]\n-----\ndatetime.datetime.strptime('06/23/2021', &quot;%m/%d/%Y&quot;)\n'tony'\n'nikki'\n'james'\ndatetime.datetime.strptime('06/24/2021', &quot;%m/%d/%Y&quot;)\n'amy'\n'jose'\ndatetime.datetime.strptime('06/25/2021', &quot;%m/%d/%Y&quot;)\n'tony'\n'jose'\n'eddie'\n'anna'\n</code></pre>\n<p>And I would like to split it into several dataframes like this:</p>\n<pre><code>df1[col1]\n-----\n'tony'\n'nikki'\n'james'\n\ndf2[col1]\n-----\n'amy'\n'jose'\n\ndf3[col1]\n-----\n'tony'\n'jose'\n'eddie'\n'anna'\n</code></pre>\n<p>I am not sure how to go about it, without just iterating through the rows, which I understand to be a last resort approach while using pandas.</p>\n<p>iterating through the rows would be something like (this is pseudo code as this is part of my struggle)</p>\n<pre><code>strRows = []\ndfs = []\nfor index,row in df.iterrows():\n    \n    while row not contain datetime object:\n        # append row to list to add to new dataframe\n        strRows.append(row)\n    # create new dataFrame with saved rows\n    newDF = pd.DataFrame(strRows)\n    dfs.append(newDF)\n    \n\n    \n</code></pre>\n<p>Although ultimately I'd like to preserve the date information and name the new dataframe after that date, for now I am just concerned with extracting the rows.</p>\n<p>My question: Is <code>df.iterrows()</code> a good way to go about this, or is there a non-iterative approach that is preferential?</p>\n<p>I appreciate any advice. Thanks.</p>\n<h2>EDIT:</h2>\n<p>Based on advice here, and more general churning, I ended up creating a new column that contained the date itself, this way.</p>\n<pre><code>df[&quot;Day&quot;] = pd.to_datetime(df['Col1'], errors='coerce').fillna(method='ffill')\n</code></pre>\n<p>The <code>errors='coerce'</code> makes an error in parsing result in an ' NaT', and the <code>fillna(method='ffill')</code> makes the date fill in the whole column. This is important specifically because Col1 contains datetimes and strings, so when <code>to_datetime</code> encounters a string, I want it to give a <code>NaT</code>, so that the <code>fillna</code> can use the <code>ffill</code> to fill in the datetimes down the line.</p>\n<p>Then, since my ultimate goal was actually to create sheets in a spreadsheet, not separate dataframes, I used a dictionary created by a groupby to write to my xlsx directly.</p>\n<pre><code>df_days = {key: value for (key, value) in df.groupby('Day')}\n\nwith writer as writer:\n    for key in df_days:\n        df_days[key].to_excel(writer, key, index=False)\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "date",
            "datetime"
        ],
        "owner": {
            "account_id": 19766596,
            "reputation": 3,
            "user_id": 14474484,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/17e552694f9f3b01587bbf98f35bac6b?s=128&d=identicon&r=PG&f=1",
            "display_name": "m_so99",
            "link": "https://stackoverflow.com/users/14474484/m-so99"
        },
        "is_answered": true,
        "view_count": 23,
        "accepted_answer_id": 68154939,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624824929,
        "creation_date": 1624822182,
        "question_id": 68154629,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68154629/how-to-convert-results-from-sunset-and-sunrise-data-using-astral-package-into-a",
        "title": "How to convert results from sunset and sunrise data using astral package into a dataframe?",
        "answer_body": "<p>You can use list comprehension from <code>x</code>:</p>\n<pre><code>pd.DataFrame([sun(city.observer, date=z) for z in x])\n</code></pre>\n<p>Output:</p>\n<pre><code>                  dawn              sunrise                 noon  \\\n0 2020-12-09 15:16:... 2020-12-09 15:54:... 2020-12-09 20:03:...   \n1 2020-12-10 15:17:... 2020-12-10 15:55:... 2020-12-10 20:04:...   \n2 2020-12-11 15:18:... 2020-12-11 15:56:... 2020-12-11 20:04:...   \n\n                sunset                 dusk  \n0 2020-12-10 00:13:... 2020-12-10 00:51:...  \n1 2020-12-11 00:13:... 2020-12-11 00:51:...  \n2 2020-12-12 00:13:... 2020-12-12 00:51:...  \n</code></pre>\n",
        "question_body": "<p>I'm trying to find a way to convert my results I got from calculating the dawn and dusk times from a certain time interval using the <code>astral</code> package at a certain city into a dataframe. The problem is that once I got the results of the sunrise and sunset information, I have a hard time converting it into a dataframe for further analysis. My code is shown below right now</p>\n<pre><code>from astral import *\n\ndef get_astral_summary():\n  x=print((\n    f'Dawn:    {s[&quot;dawn&quot;]}\\n'\n    f'Sunrise: {s[&quot;sunrise&quot;]}\\n'\n    f'Noon:    {s[&quot;noon&quot;]}\\n'\n    f'Sunset:  {s[&quot;sunset&quot;]}\\n'\n    f'Dusk:    {s[&quot;dusk&quot;]}\\n'\n  ))\n  return x \n\ncity = LocationInfo(&quot;Surrey&quot;, &quot;Canada&quot;, 'US/Pacific',49.104599,-122.823509)\nx =pd.date_range(start=&quot;2020-12-09&quot;,end=&quot;2020-12-11&quot;)\n\nfor y in x:\n  s = sun(city.observer, date=y)\n  get_astral_summary()\n\n</code></pre>\n<p>Right now, the way I can see my results is making the function that prints out the results, the output is shown below:</p>\n<pre><code>Dawn:    2020-12-09 15:16:48.899052+00:00\nSunrise: 2020-12-09 15:54:44.342273+00:00\nNoon:    2020-12-09 20:03:37+00:00\nSunset:  2020-12-10 00:13:05.243283+00:00\nDusk:    2020-12-10 00:51:00.107776+00:00\n\nDawn:    2020-12-10 15:17:44.663065+00:00\nSunrise: 2020-12-10 15:55:43.641196+00:00\nNoon:    2020-12-10 20:04:04+00:00\nSunset:  2020-12-11 00:13:01.662368+00:00\nDusk:    2020-12-11 00:51:00.111312+00:00\n\nDawn:    2020-12-11 15:18:38.380512+00:00\nSunrise: 2020-12-11 15:56:40.604162+00:00\nNoon:    2020-12-11 20:04:32+00:00\nSunset:  2020-12-12 00:13:01.168222+00:00\nDusk:    2020-12-12 00:51:02.912291+00:00\n\n</code></pre>\n<p>The question arises: How do I put it into a dataframe where the time is formatted correctly? I noticed when I just called the variable <code>s</code>, the data type sort of looks like a dictionary like this shown below:</p>\n<pre><code>{'dawn': datetime.datetime(2020, 12, 11, 15, 18, 38, 380512, tzinfo=&lt;UTC&gt;),\n 'dusk': datetime.datetime(2020, 12, 12, 0, 51, 2, 912291, tzinfo=&lt;UTC&gt;),\n 'noon': datetime.datetime(2020, 12, 11, 20, 4, 32, tzinfo=&lt;UTC&gt;),\n 'sunrise': datetime.datetime(2020, 12, 11, 15, 56, 40, 604162, tzinfo=&lt;UTC&gt;),\n 'sunset': datetime.datetime(2020, 12, 12, 0, 13, 1, 168222, tzinfo=&lt;UTC&gt;)}\n\n</code></pre>\n<p>I noticed when I do</p>\n<pre><code>df = pd.DataFrame([s], columns=s.keys())\n</code></pre>\n<p>the dataframe doesnt give me the whole time interval, it only gives me this shown below</p>\n<pre><code>                              dawn  ...                             dusk\n0 2020-12-15 15:21:51.681536+00:00  ... 2020-12-16 00:51:41.956146+00:00\n\n[1 rows x 5 columns]\n</code></pre>\n<p>Can anyone provide me some suggestions to tackle this problem? Thanks!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 13471817,
            "reputation": 33,
            "user_id": 9720067,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1768258849904713/picture?type=large",
            "display_name": "aTc Creator",
            "link": "https://stackoverflow.com/users/9720067/atc-creator"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68154760,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624823340,
        "creation_date": 1624822406,
        "last_edit_date": 1624822731,
        "question_id": 68154651,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68154651/driving-column-from-another-column-using-python-pandas",
        "title": "Driving column From another Column using Python pandas",
        "answer_body": "<p>You can <code>extract</code> the values, then <code>ffill</code> and put empty strings on heading rows using <code>np.where</code>:</p>\n<pre><code>z = df['Value date'].str.extract('Deposits (.*)')\ndf['Currency'] = np.where(z.isna(), z.ffill(), '')\ndf\n</code></pre>\n<p>Output:</p>\n<pre><code>      Value date Currency\n0   Deposits HKD         \n1     01 Mar 201      HKD\n2    04 Mar 2019      HKD\n3    04 Mar 2019      HKD\n4    05 Mar 2019      HKD\n5    05 Mar 2019      HKD\n6   Deposits SGD         \n7    02 Mar 2019      SGD\n8   Deposits USD         \n9    01 Mar 2019      USD\n10   12 Mar 2019      USD\n11   29 Mar 2019      USD\n</code></pre>\n",
        "question_body": "<p>Can anyone please help me on this how I can drive a currency column from value date section header which is shown below using pandas? I try but fail to get it.</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Value date</th>\n<th>Currency(Driven form Value date)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Deposits HKD</td>\n<td></td>\n</tr>\n<tr>\n<td>01 Mar 201</td>\n<td>HKD</td>\n</tr>\n<tr>\n<td>04 Mar 2019</td>\n<td>HKD</td>\n</tr>\n<tr>\n<td>04 Mar 2019</td>\n<td>HKD</td>\n</tr>\n<tr>\n<td>05 Mar 2019</td>\n<td>HKD</td>\n</tr>\n<tr>\n<td>05 Mar 2019</td>\n<td>HKD</td>\n</tr>\n<tr>\n<td>Deposits SGD</td>\n<td></td>\n</tr>\n<tr>\n<td>02 Mar 2019</td>\n<td>SGD</td>\n</tr>\n<tr>\n<td>Deposits USD</td>\n<td></td>\n</tr>\n<tr>\n<td>01 Mar 2019</td>\n<td>USD</td>\n</tr>\n<tr>\n<td>12 Mar 2019</td>\n<td>USD</td>\n</tr>\n<tr>\n<td>29 Mar 2019</td>\n<td>USD</td>\n</tr>\n</tbody>\n</table>\n</div>"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 7613018,
            "reputation": 2067,
            "user_id": 5773335,
            "user_type": "registered",
            "accept_rate": 55,
            "profile_image": "https://www.gravatar.com/avatar/d441b917fd42fe7bac494f9547103bcd?s=128&d=identicon&r=PG&f=1",
            "display_name": "milan",
            "link": "https://stackoverflow.com/users/5773335/milan"
        },
        "is_answered": true,
        "view_count": 66,
        "accepted_answer_id": 68154298,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624819421,
        "creation_date": 1624614639,
        "last_edit_date": 1624668912,
        "question_id": 68129036,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68129036/show-commit-count-without-any-duplication-in-pandas",
        "title": "show commit count without any duplication in pandas",
        "answer_body": "<p>I can't replicate how you've made that pandas Dataframe from the code you've provided. But say I have a dummy dataframe:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'SHA':['1', '1', '1', '2', '2','3','4','4'],\n                    'Author':['Bob', 'Bob', 'Bob', 'Harry', 'Harry','Mary','Bob','Bob']})\n\n   SHA  Author\n0   1   Bob\n1   1   Bob\n2   1   Bob\n3   2   Harry\n4   2   Harry\n5   3   Mary\n6   4   Bob\n7   4   Bob\n</code></pre>\n<p>I can get the number of commits made per author by doing:</p>\n<pre><code>df.groupby(['Author'])['SHA'].nunique()\n</code></pre>\n<p>This creates a <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\">DataFrameGroupby</a> object that we use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nunique.html\" rel=\"nofollow noreferrer\">nunique</a> on the <code>SHA</code> column to get the number of counts of unique commits.</p>\n",
        "question_body": "<p>I fetched logs from git using <code>git log --all --numstat --pretty=format:'--%h--%ad--%aN' --no-merges &gt; ../git.log</code> and saved to a git.log file. The purpose of this to read the logs and find out stuffs like commit count of each author, total lines of code written by each other, total lines added, delete, contributions by year, month, day and many more.</p>\n<p>For now, I could read the data and formatted it in a csv. However, the problem is the duplication of commit hash(sha) but its equally important as well. In the file format you see</p>\n<pre><code>--568dc3532--Thu Jun 17 14:29:30 2021 -0400--Sebastian Markb\u00e5ge\n1   1   fixtures/blocks/src/index.js\n2   2   fixtures/concurrent/time-slicing/src/index.js\n1   1   fixtures/devtools/scheduling-profiler/app.js\n1   1   fixtures/dom/src/__tests__/wrong-act-test.js\n0   1   packages/react-dom/index.classic.fb.js\n0   1   packages/react-dom/index.js\n0   1   packages/react-dom/index.modern.fb.js\n\n--43f4cc160--Thu Jun 17 13:56:18 2021 +0100--Dan Abramov\n10  19  packages/react-reconciler/src/__tests__/ReactSuspense-test.internal.js\n\n--d0f348dc1--Wed Jun 16 19:44:44 2021 -0400--Brian Vaughn\n27  28  packages/react-reconciler/src/ReactFiberCommitWork.new.js\n27  28  packages/react-reconciler/src/ReactFiberCommitWork.old.js\n58  0   packages/react-reconciler/src/__tests__/ReactSuspense-test.internal.js\n</code></pre>\n<p>in the commit hash 568dc3532, there are changes in 7 different files and the insertion and deletion is also depicted on each file which is supremely important for my analysis. But, when I calculate commit count for all author with this statement</p>\n<pre><code>commit_data.Author.value_counts().head(10)\n</code></pre>\n<p>This is the output of it</p>\n<pre><code>Dan Abramov           14294\nPaul O\u2019Shannessy      12090\nBrian Vaughn          11250\nTravis CI              9423\nSebastian Markb\u00e5ge     6711\nAndrew Clark           6028\nSophie Alpert          4849\nDominic Gannaway       3101\nVjeux                  2606\nCircle CI              1293\nName: Author, dtype: int64\n</code></pre>\n<p>I get the count of duplicated commit as well. This is how my csv looks like</p>\n<p><a href=\"https://i.stack.imgur.com/i9kEW.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/i9kEW.png\" alt=\"enter image description here\" /></a></p>\n<p>So, my question is how can I show the count of commits by each author without any duplication.</p>\n<p>Here is the csv</p>\n<p><a href=\"https://gist.github.com/SanskarSans/b7a756f1fe8ee71619950f633c87cd41\" rel=\"nofollow noreferrer\">https://gist.github.com/SanskarSans/b7a756f1fe8ee71619950f633c87cd41</a></p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 473227,
            "reputation": 123,
            "user_id": 882206,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5256ffe0e38f1e15598de7b5502b542e?s=128&d=identicon&r=PG&f=1",
            "display_name": "PythonLearner",
            "link": "https://stackoverflow.com/users/882206/pythonlearner"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 68152961,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624818189,
        "creation_date": 1624808842,
        "question_id": 68152834,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68152834/pandas-dataframe-add-new-column-with-calculated-values-based-on-previous-row",
        "title": "Pandas DataFrame: Add new column with calculated values based on previous row",
        "answer_body": "<p>Your formula is the same as accumulation of the value of <code>val3</code> everyday (probably thinking in Excel style of formula).  As such, you can try using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cumsum.html\" rel=\"nofollow noreferrer\"><code>cumsum()</code></a>, as follows:</p>\n<pre><code>df['calculated'] = df['val3'].cumsum()\n</code></pre>\n<p>You got the error because when you are going to define the column <code>calculated</code>, it is derived also from column values of <code>calculated</code> which is yet to be defined. Hence, the error.</p>\n<p>You can use the code above to get the same result directly without relying on a column not yet defined.</p>\n<p><strong>Result:</strong></p>\n<pre><code>print(df)\n\n                 name  rating      val1      val2      val3  calculated\n2020-10-16  cool_name    23.0  1.700079  1.515385  0.184694    0.184694\n2020-10-19  cool_name    -3.0  1.230071  1.289615 -0.059545    0.125149\n2020-10-20  cool_name   -11.0  0.007064  0.675135 -0.668071   -0.542922\n2020-10-21  cool_name   -21.0 -2.093643 -0.408622 -1.685021   -2.227943\n2020-10-22  cool_name    -5.0 -2.384278 -0.638191 -1.746087   -3.974030\n</code></pre>\n",
        "question_body": "<p>I have the following Pandas DataFrame df:</p>\n<pre><code>            name          rating    val1     val2        val3\n&lt;DATE&gt;                                                 \n2020-10-16  cool_name      23.0  1.700079  1.515385  0.184694\n2020-10-19  cool_name      -3.0  1.230071  1.289615 -0.059545\n2020-10-20  cool_name     -11.0  0.007064  0.675135 -0.668071\n2020-10-21  cool_name     -21.0 -2.093643 -0.408622 -1.685021\n2020-10-22  cool_name      -5.0 -2.384278 -0.638191 -1.746087\n\n\n       \n</code></pre>\n<p>How can I add a new column called &quot;calculated&quot; which is calculated this way:</p>\n<pre><code>df['calculated'] = df['calculated'(previous day)] + df['val3'(current day)]\n</code></pre>\n<p>If I try to do it this way,  I'll receive a key error (I am not even sure if it is shift(1) or shift(-1)):</p>\n<pre><code>df['calculated'] = df['calculated'].shift() + df['val3']\n</code></pre>\n<p>I think this is due to the fact that the first row doesn't have a previous row with &quot;calculated&quot; . Howerver, I don't know how to solve this problems.</p>\n<p>I tried various solutions and searched for answers, but unfortunately I'm stuck. Any help would be highly appreciated.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22061685,
            "reputation": 41,
            "user_id": 16324245,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3a04d2d2ea5d5402844e970fb4426307?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mark J.",
            "link": "https://stackoverflow.com/users/16324245/mark-j"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 68149099,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1624815516,
        "creation_date": 1624772756,
        "last_edit_date": 1624815516,
        "question_id": 68148356,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68148356/how-to-form-sentences-from-single-words-in-a-dataframe",
        "title": "How to form sentences from single words in a dataframe?",
        "answer_body": "<p>You could try this:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Initialize variables\nnew_data = {&quot;start_time&quot;: [], &quot;end_time&quot;: [], &quot;sentence&quot;: []}\nsentence = []\nstart_time = None\n\n# Iterate on the dataframe\nfor i, row in df.iterrows():\n    # Initialize start_time\n    if not start_time:\n        start_time = row[&quot;start_time&quot;]\n    if (\n        not row[&quot;word&quot;].endswith(&quot;?&quot;)\n        and not row[&quot;word&quot;].endswith(&quot;!&quot;)\n        and not row[&quot;word&quot;].endswith(&quot;S.&quot;)\n    ):\n        # If word is not ending a phrase, get it\n        sentence.append(row[&quot;word&quot;])\n    else:\n        # Pause iteration and update new_data with start_time, end_time\n        # and completed sentence\n        new_data[&quot;start_time&quot;].append(start_time)\n        new_data[&quot;end_time&quot;].append(row[&quot;end_time&quot;])\n        sentence.append(row[&quot;word&quot;])\n        new_data[&quot;sentence&quot;].append(&quot; &quot;.join(sentence))\n        # Reset variables\n        start_time = None\n        sentence = []\n\nnew_df = pd.DataFrame(new_data, columns=[&quot;start_time&quot;, &quot;end_time&quot;, &quot;sentence&quot;])\n\nprint(new_df)\n# Outputs\n   start_time  end_time        sentence\n0         0.1       0.6  WHERE ARE YOU?\n1         0.7       1.4      I AM U. S.\n2         1.5       1.8       OK, COOL!\n3         2.1       2.4      YES IT IS.\n</code></pre>\n",
        "question_body": "<p>I'm trying to form sentences from single words in a dataframe (sometimes ending with .?!), and recognize that U. or S. is not the end of the sentence.</p>\n<pre class=\"lang-py prettyprint-override\"><code>data = {\n    &quot;start_time&quot;: [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.9, 2.1, 2.3, 2.5],\n    &quot;end_time&quot;: [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4],\n    &quot;word&quot;: [\n        &quot;WHERE&quot;,\n        &quot;ARE&quot;,\n        &quot;YOU?&quot;,\n        &quot;I&quot;,\n        &quot;AM&quot;,\n        &quot;U.&quot;,\n        &quot;S.&quot;,\n        &quot;OK,&quot;,\n        &quot;COOL!&quot;,\n        &quot;YES&quot;,\n        &quot;IT&quot;,\n        &quot;IS.&quot;,\n    ],\n}\ndf = pd.DataFrame(data, columns=[&quot;start_time&quot;, &quot;end_time&quot;, &quot;word&quot;])\n</code></pre>\n<p>The dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>s_time e_time word\n\n0.1 0.2 WHERE\n\n0.3 0.4 ARE\n\n0.5 0.6 YOU?\n\n0.7 0.8 I\n\n0.9 1.0 AM\n\n1.1 1.2 U.\n\n1.3 1.4 S.\n\n1.5 1.6 OK,\n\n1.7 1.8 COOL!\n\n1.9 2.0 YES\n\n2.1 2.2 IT\n\n2.3 2.4 IS.\n</code></pre>\n<p>The result I want to get looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>s_time e_time sentence\n\n0.1 0.6 WHERE ARE YOU?\n\n0.7 1.4 I AM U. S.\n\n1.5 1.8 OK, COOL!\n\n1.9 2.4 YES IT IS.\n</code></pre>\n<p>I am stuck with how to get U. S. in one sentence.</p>\n<p>Any suggestion would be much appreciated and really thanks for anyone help!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "function",
            "user-defined-functions"
        ],
        "owner": {
            "account_id": 22043055,
            "reputation": 53,
            "user_id": 16308567,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GiFR9NhVy6x45kWD5V3tbOXBSsw7VOBsa5QB8oAPw=k-s128",
            "display_name": "vish vas chauhan Diary",
            "link": "https://stackoverflow.com/users/16308567/vish-vas-chauhan-diary"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 68153365,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624813373,
        "creation_date": 1624811755,
        "question_id": 68153268,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68153268/how-to-apply-def-function-in-full-dataframe",
        "title": "How to apply def function in full dataframe?",
        "answer_body": "<p>The functions do different things.</p>\n<p>The first option works because you're iterating over each column and applying np.where to each column once.</p>\n<pre><code>for c in df.columns:\n    df[c] = np.where(df[c] &gt; 45, df[c] + 100, df[c])\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>   age1  age2\n0    23    10\n1    45    20\n2    21   150\n</code></pre>\n<hr />\n<p>In this case:</p>\n<pre><code>def fun(x):\n  l = list(df.columns)\n  for c in l:\n    df[c]=np.where(df[c]&gt;45,df[c]+100,df[c])\n  return x\ndf.apply(fun)\n</code></pre>\n<p>The function <code>fun</code> is called for every column (via <code>apply</code>), but you're doing the complete operation each time.</p>\n<p>This is roughly equivalent to:</p>\n<pre><code>for _ in df.columns:\n    for c in df.columns:\n        df[c] = np.where(df[c] &gt; 45, df[c] + 100, df[c])\n</code></pre>\n<p>Notice the nested looping.</p>\n<p>Hence why it produces <code>df</code>:</p>\n<pre><code>   age1  age2\n0    23    10\n1    45    20\n2    21   250\n</code></pre>\n<hr />\n<p>The last option is close:</p>\n<pre><code>def f(x):\n  val=[]\n  if x&gt;=40:\n      val = x+100\n  else:\n      val = x\n  return val\n\ndf.apply(f,axis=1)\n</code></pre>\n<p>However x is a Series of values (DataFrame column) which means that <code>x &gt;= 40</code> does not work leading to an Error:</p>\n<pre class=\"lang-none prettyprint-override\"><code>ValueError: The truth value of a Series is ambiguous. \nUse a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n<p>And can be modified just slightly to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.applymap.html\" rel=\"nofollow noreferrer\"><code>applymap</code></a> which applies the function to every cell in the DataFrame:</p>\n<pre><code>def f(x):\n    if x &gt; 45:  # Changed the bound to match the np.where condition\n        val = x + 100\n    else:\n        val = x\n    return val\n\ndf = df.applymap(f)\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>   age1  age2\n0    23    10\n1    45    20\n2    21   150\n</code></pre>\n<hr />\n<p>However, the more pandas approach here would be to use something like <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mask.html\" rel=\"nofollow noreferrer\"><code>DataFrame.mask</code></a>:</p>\n<pre><code>df = df.mask(df &gt; 45, df + 100)\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>   age1  age2\n0    23    10\n1    45    20\n2    21   150\n</code></pre>\n",
        "question_body": "<p>I need help to correct the function. I am confused for two things.</p>\n<ol>\n<li>how to put for loop into def function.</li>\n<li>Please, Correct my other function. It works only on single column</li>\n</ol>\n<pre><code>raw_data = {'age1': [23,45,21],'age2': [10,20,50]}\ndf = pd.DataFrame(raw_data, columns = ['age1','age2'])\ndf\n</code></pre>\n<p>It works well.</p>\n<pre><code>l = list(df.columns)\nfor c in l:\n  df[c]=np.where(df[c]&gt;45,df[c]+100,df[c])\n</code></pre>\n<ol>\n<li>It does not work properly and add more value than 100. What is wrong here.</li>\n</ol>\n<pre><code>def fun(x):\n  l = list(df.columns)\n  for c in l:\n    df[c]=np.where(df[c]&gt;45,df[c]+100,df[c])\n  return x\ndf.apply(fun)\n</code></pre>\n<ol start=\"2\">\n<li>Why i can't apply this function on full dataframe. Please correct...</li>\n</ol>\n<pre><code>def f(x):\n  val=[]\n  if x&gt;=40:\n      val = x+100\n  else:\n      val = x\n  return val\ndf.apply(f,axis=1)\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 1110538,
            "reputation": 12563,
            "user_id": 1101083,
            "user_type": "registered",
            "accept_rate": 70,
            "profile_image": "https://i.stack.imgur.com/ycwVS.png?s=128&g=1",
            "display_name": "Himanshu Yadav",
            "link": "https://stackoverflow.com/users/1101083/himanshu-yadav"
        },
        "is_answered": true,
        "view_count": 45,
        "accepted_answer_id": 68153142,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624811181,
        "creation_date": 1624809271,
        "question_id": 68152914,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68152914/pandas-apply-agg-on-two-columns-at-a-time",
        "title": "Pandas: Apply agg on two columns at a time",
        "answer_body": "<p>Depending on what you're looking for, as <a href=\"https://stackoverflow.com/questions/68152914/pandas-apply-agg-on-two-columns-at-a-time#comment120453364_68152914\">@anky</a> suggests, a standard <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html#pandas-core-groupby-dataframegroupby-aggregate\" rel=\"nofollow noreferrer\"><code>groupby agg</code></a> with selecting the desired columns may work:</p>\n<pre><code>df_collect = df.groupby('col1', as_index=False)[['col2', 'col3']].agg(set)\n</code></pre>\n<p><code>df_collect</code>:</p>\n<pre><code>   col1                         col2                         col3\n0     1           {1, 2, 5, 6, 7, 9}        {1, 2, 3, 4, 5, 6, 9}\n1     2  {1, 2, 3, 4, 5, 6, 7, 8, 9}     {1, 3, 4, 5, 6, 7, 8, 9}\n2     3              {2, 3, 6, 7, 8}        {2, 4, 5, 6, 7, 8, 9}\n3     4     {1, 2, 3, 4, 6, 7, 8, 9}        {1, 4, 5, 6, 7, 8, 9}\n4     5           {1, 3, 4, 5, 6, 7}           {1, 2, 3, 4, 8, 9}\n5     6              {2, 3, 5, 6, 7}           {1, 3, 4, 6, 7, 9}\n6     7        {1, 2, 4, 5, 7, 8, 9}           {1, 3, 4, 5, 6, 8}\n7     8        {1, 2, 4, 5, 6, 7, 8}  {1, 2, 3, 4, 5, 6, 7, 8, 9}\n8     9           {1, 3, 4, 7, 8, 9}              {2, 4, 5, 6, 9}\n</code></pre>\n<p>Or, for something more similar to the way <code>PySpark</code> looks, use <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#named-aggregation\" rel=\"nofollow noreferrer\">Named Aggregation</a> to incorporate aliasing, column selection, and separate aggregation options:</p>\n<pre><code>df_collect = (\n    df.groupby('col1', as_index=False)\n        .agg(Col2Arr=('col2', set), Col3Arr=('col3', set))\n)\n</code></pre>\n<p><code>df_collect</code>:</p>\n<pre><code>   col1                      Col2Arr                      Col3Arr\n0     1           {1, 2, 5, 6, 7, 9}        {1, 2, 3, 4, 5, 6, 9}\n1     2  {1, 2, 3, 4, 5, 6, 7, 8, 9}     {1, 3, 4, 5, 6, 7, 8, 9}\n2     3              {2, 3, 6, 7, 8}        {2, 4, 5, 6, 7, 8, 9}\n3     4     {1, 2, 3, 4, 6, 7, 8, 9}        {1, 4, 5, 6, 7, 8, 9}\n4     5           {1, 3, 4, 5, 6, 7}           {1, 2, 3, 4, 8, 9}\n5     6              {2, 3, 5, 6, 7}           {1, 3, 4, 6, 7, 9}\n6     7        {1, 2, 4, 5, 7, 8, 9}           {1, 3, 4, 5, 6, 8}\n7     8        {1, 2, 4, 5, 6, 7, 8}  {1, 2, 3, 4, 5, 6, 7, 8, 9}\n8     9           {1, 3, 4, 7, 8, 9}              {2, 4, 5, 6, 9}\n</code></pre>\n<hr />\n<p>Sample data used:</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\nnp.random.seed(5)\ndf = pd.DataFrame(np.random.randint(1, 10, (100, 3)),\n                  columns=[1, 2, 3]).add_prefix('col')\n</code></pre>\n<p><code>df.head(10)</code>:</p>\n<pre><code>   col1  col2  col3\n0     4     7     7\n1     1     9     5\n2     8     1     1\n3     8     2     6\n4     8     1     2\n5     5     7     3\n6     2     3     8\n7     1     6     1\n8     1     5     5\n9     4     3     5\n</code></pre>\n",
        "question_body": "<p>I am migrating some of pySpark code into Pandas and stuck with implementing <code>collect_set</code> on two columns. <br/>\npySpark code looks like this:</p>\n<p><code>df_collect = df.groupBy('col1').agg(collect_set('col2').alias('Col2Arr'), collect_set('col3').alias('Col3Arr'))</code></p>\n<p>I can easily implement for one of the columns by calling <code>lambda</code> function on <code>agg</code> but can't do it on two columns at the same time:</p>\n<p><code>df_collect = df.groupby('col1')['col2'].agg({'Col2Arr': lambda x: set(x)})</code></p>\n<p>I tried:</p>\n<p><code>df.groupby('col1').agg(Col2Arr = lambda x: set(x['col2']), Col3Arr = lambda x: set(x['col3']))</code></p>\n<p>and</p>\n<pre><code>def count_set(x):\n    d = {}\n    d['Col2Arr'] = lambda a: set(a['col2'])\n    d['Col3Arr'] = lambda a: set(a['col3'])\n    return pd.Series(d, index=['Col2Arr', 'Col3Arr'])\n\ndf.groupby('col1').apply(count_set)\n\n</code></pre>\n<p>Nothing seem to work. Not sure what I am missing here.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 19084350,
            "reputation": 11,
            "user_id": 13935932,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-gUkNxSd7h9o/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckg5So1rhjEPuBPNBTPCyqpFW0c7g/photo.jpg?sz=128",
            "display_name": "Jforpython",
            "link": "https://stackoverflow.com/users/13935932/jforpython"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68152939,
        "answer_count": 3,
        "score": -1,
        "last_activity_date": 1624809425,
        "creation_date": 1624808748,
        "question_id": 68152816,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68152816/how-to-sort-a-dataframe-according-to-another-dataframe-column-values",
        "title": "how to sort a dataframe according to another dataframe column values?",
        "answer_body": "<p>Here\u2019s one way:</p>\n<pre><code>df2.TestCaseName = pd.Categorical(df2.TestCaseName,\n                                  categories=df1.TestCaseName.values,\n                                  ordered=True)\n\ndf2 = df2.sort_values('TestCaseName')\n</code></pre>\n",
        "question_body": "<p>I have two data frames df1 and df2</p>\n<pre><code>df1 = pd.dataframe(&quot;TestCaseName&quot; : ['B', 'D', 'A', 'E', 'C'])\n\n  TestCaseName\n0     B\n1     D\n2     A\n3     E\n4     C\n</code></pre>\n<p>and another data frame</p>\n<pre><code>df2 = pd.dataframe({&quot;TestCaseName&quot; : ['A', 'B', 'C', 'D', 'E'], &quot;NameSpace&quot; : ['T2'. 'T3', 'T6', 'T1', 'T8'])\n\n   TestCaseName  NameSpace    \n\n 0      A           T2\n 1      B           T3\n 2      C           T6\n 3      D           T1\n 4      E           T8\n</code></pre>\n<p>What i want is sort the test case name of df2 according to df1.</p>\n<p>Here is what i have tried;</p>\n<pre><code>df2 = df2.set_index('TestCaseName')\ndf2 = df2.reindex(index=df1['TestCaseName'])\ndf2 = df2.reset_index()\n</code></pre>\n<p>Which is giving me error ValueError: cannot reindex from a duplicate axis</p>\n<p>Desired Output:</p>\n<pre><code>  TestCaseName NameSpace\n0     B           T3\n1     D           T1 \n2     A           T2\n3     E           T8\n4     C           T6\n</code></pre>\n<p>Can someone tell me what am i doing wrong or suggest any better idea?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "split"
        ],
        "owner": {
            "account_id": 18064507,
            "reputation": 191,
            "user_id": 13130804,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/0833a5cc2363e5ba081789439efe8bc6?s=128&d=identicon&r=PG&f=1",
            "display_name": "Miguel Gonzalez",
            "link": "https://stackoverflow.com/users/13130804/miguel-gonzalez"
        },
        "is_answered": true,
        "view_count": 280,
        "accepted_answer_id": 63892161,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624809362,
        "creation_date": 1600118551,
        "question_id": 63891996,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/63891996/remove-words-starting-with-in-a-column-from-a-dataframe",
        "title": "remove words starting with &quot;@&quot; in a column from a dataframe",
        "answer_body": "<pre><code>Please `str.replace` string starting with `@`\n</code></pre>\n<p><strong>Sample Data</strong></p>\n<pre><code>                                       text\n0  News via @livemint: @RBI bars banks from links\n1      Newsfeed from @oayments_source: How Africa\n2                   is that bitcoin? not my thing\n\n\n tweetscrypto['clean_text']=tweetscrypto['text'].str.replace('(\\@\\w+.*?)',&quot;&quot;)\n</code></pre>\n<p>Still, can capture <code>@</code> without escaping as noted by <code>@baxx</code></p>\n<pre><code>tweetscrypto['clean_text']=tweetscrypto['text'].str.replace('(@\\w+.*?)',&quot;&quot;)\n\n                    clean_text\n0  News via :  bars banks from links\n1         Newsfeed from : How Africa\n2      is that bitcoin? not my thing\n</code></pre>\n",
        "question_body": "<p>I have a dataframe called tweetscrypto and I am trying to remove all the words from the column &quot;text&quot; starting with the character &quot;@&quot; and gather the result in a new column &quot;clean_text&quot;. The rest of the words should stay exactly the same:</p>\n<p><a href=\"https://i.stack.imgur.com/gMQ69.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/gMQ69.png\" alt=\"enter image description here\" /></a></p>\n<pre><code>tweetscrypto['clean_text'] = tweetscrypto['text'].apply(filter(lambda x:x[0]!='@', x.split()))\n</code></pre>\n<p>it does not seem to work. Can somebody help?</p>\n<p>Thanks in advance</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 3124884,
            "reputation": 121,
            "user_id": 2643948,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/3423e43b576ab333117f731daef5aad9?s=128&d=identicon&r=PG&f=1",
            "display_name": "RebeccaKennedy",
            "link": "https://stackoverflow.com/users/2643948/rebeccakennedy"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 68150849,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624808663,
        "creation_date": 1624788441,
        "question_id": 68150020,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
        "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
        "answer_body": "<h3>Numpy approach</h3>\n<p>We can define a function <code>first_value</code> which takes a <code>2D</code> array and <code>offset</code> (n) as input arguments and returns <code>1D</code> array. Basically, for each row it returns the <code>nth</code> value after the first <code>non-nan</code> value</p>\n<pre><code>def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i &gt;= arr.shape[1])] = np.nan\n    return vals\n</code></pre>\n<h3>Pandas approach</h3>\n<p>We can <code>stack</code> the dataframe to reshape then group the dataframe on <code>level=0</code> and aggregate using <code>nth</code>, then <code>reindex</code> to conform the index of aggregated frame according to original frame</p>\n<pre><code>def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n</code></pre>\n<h3>Sample run</h3>\n<pre><code>&gt;&gt;&gt; first_valid(df, 0)\nDate\n1     25.0\n2     29.0\n3     33.0\n4     31.0\n5     30.0\n6     35.0\n7     31.0\n8     33.0\n9     26.0\n10    27.0\n11    35.0\n12    33.0\n13    28.0\n14    25.0\n15    25.0\n16    26.0\n17    34.0\n18    28.0\n19    34.0\n20    28.0\ndtype: float64\n\n\n&gt;&gt;&gt; first_valid(df, 1)\nDate\n1      NaN\n2      NaN\n3      NaN\n4     35.0\n5     34.0\n6     34.0\n7     26.0\n8     25.0\n9     31.0\n10    26.0\n11    25.0\n12    35.0\n13    25.0\n14    25.0\n15    26.0\n16    31.0\n17    29.0\n18    29.0\n19    26.0\n20    30.0\ndtype: float64\n\n&gt;&gt;&gt; first_valid(df, 2)\nDate\n1      NaN\n2      NaN\n3      NaN\n4      NaN\n5      NaN\n6      NaN\n7      NaN\n8     31.0\n9      NaN\n10    28.0\n11    29.0\n12    28.0\n13    35.0\n14    28.0\n15    31.0\n16    27.0\n17    25.0\n18    31.0\n19     NaN\n20     NaN\ndtype: float64\n</code></pre>\n<h3>Performance</h3>\n<pre><code># Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p><strong>Numpy based approach is approximately <code>300x</code> faster than the <code>OP's</code> given approach</strong> while pandas based approach is approximately <code>22x</code> faster</p>\n",
        "question_body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "input_data_frames": [
            "f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n",
            "f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n"
        ],
        "output_codes": [
            "def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan\n    return vals\n",
            "def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n",
            ">>> first_valid(df, 0)\nDate\n1     25.0\n2     29.0\n3     33.0\n4     31.0\n5     30.0\n6     35.0\n7     31.0\n8     33.0\n9     26.0\n10    27.0\n11    35.0\n12    33.0\n13    28.0\n14    25.0\n15    25.0\n16    26.0\n17    34.0\n18    28.0\n19    34.0\n20    28.0\ndtype: float64\n\n\n>>> first_valid(df, 1)\nDate\n1      NaN\n2      NaN\n3      NaN\n4     35.0\n5     34.0\n6     34.0\n7     26.0\n8     25.0\n9     31.0\n10    26.0\n11    25.0\n12    35.0\n13    25.0\n14    25.0\n15    26.0\n16    31.0\n17    29.0\n18    29.0\n19    26.0\n20    30.0\ndtype: float64\n\n>>> first_valid(df, 2)\nDate\n1      NaN\n2      NaN\n3      NaN\n4      NaN\n5      NaN\n6      NaN\n7      NaN\n8     31.0\n9      NaN\n10    28.0\n11    29.0\n12    28.0\n13    35.0\n14    28.0\n15    31.0\n16    27.0\n17    25.0\n18    31.0\n19     NaN\n20     NaN\ndtype: float64\n",
            "# Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"
        ],
        "ques_desc": "I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. ",
        "ans_desc": "Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster ",
        "formatted_input": {
            "qid": 68150020,
            "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
            "question": {
                "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
                "ques_desc": "I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. "
            },
            "io": [
                "f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n",
                "f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n"
            ],
            "answer": {
                "ans_desc": "Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster ",
                "code": [
                    "def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan\n    return vals\n",
                    "def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n",
                    "# Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "emoji"
        ],
        "owner": {
            "account_id": 19144771,
            "reputation": 31,
            "user_id": 14946125,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/1he8s.jpg?s=128&g=1",
            "display_name": "AnandP2812",
            "link": "https://stackoverflow.com/users/14946125/anandp2812"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68151381,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624802922,
        "creation_date": 1624796864,
        "question_id": 68151158,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68151158/pandas-not-listing-every-single-unique-value-in-a-column",
        "title": "Pandas not listing every single unique value in a column",
        "answer_body": "<p>You can create a second data frame where you have removed all duplicate function.\nFor example:</p>\n<pre><code>df_unique = df.drop_duplicates()\n\nfor i in df_unique.index:\n   print(df_unique['Emojis'][i])\n</code></pre>\n<p>Here is the documentation:\n<a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html\" rel=\"nofollow noreferrer\">Pandas remove Duplicates</a></p>\n",
        "question_body": "<p>I am trying to list every single unique value in a single column, so I can copy and paste them. But, when I do it, it only seems to list the first 1000 unique values in my column.</p>\n<p>When I count the number of unique values in my column, I get 2038:</p>\n<pre><code>df['Emojis'].nunique()\n\n2038\n</code></pre>\n<p>Then, when I use this code to list all the unique values in my column, it only seems to list the first <strong>1000</strong> unique values, in which it is then followed by a <code>...</code></p>\n<pre><code>df['Emojis'].tolist()\n</code></pre>\n<pre><code>['\ud83d\ude01',\n '\ud83d\ude0c',\n '\ud83d\udc94',\n '\ud83d\udca4',\n '\ud83d\udcaf',\n '\ud83d\ude02',\n '\u263a',\n '\ud83d\ude01',\n '\ud83d\ude29',\n '\ud83d\ude4a',\n '\ud83d\ude29',\n '\ud83d\udc4d',\n '\ud83d\ude02',\n '\ud83d\ude47',\n '\ud83d\ude18',\n '\ud83c\udf3b',\n '\ud83d\ude2d',\n '\ud83d\ude02',\n '\ud83d\ude0f',\n '\ud83d\udc93',\n '\ud83d\udc15',\n '\ud83d\udc4d',\n '\ud83d\udd25',\n '\ud83c\udf82',\n '\ud83d\ude12',\n '\ud83d\ude05',\n '\ud83d\udc9e',\n '\ud83d\ude1e',\n '\ud83c\udf1f',\n '\ud83d\ude1e',\n '\ud83d\ude29',\n '\ud83d\udc80',\n '\ud83d\ude12',\n '\ud83d\ude2d',\n '\ud83d\ude22',\n '\ud83d\ude02',\n '\ud83d\ude0a',\n '\ud83d\ude09',\n '\ud83d\ude2e',\n '\ud83d\ude0b',\n '\ud83d\ude33',\n '\ud83d\ude31',\n '\ud83d\ude02',\n '\ud83d\ude1c',\n '\ud83d\ude00',\n '\ud83d\ude2a',\n '\ud83d\udc4f',\n '\ud83d\udc99',\n '\ud83d\ude0e',\n '\ud83d\udc3e',\n '\ud83d\ude02',\n '\ud83d\udc40',\n '\ud83d\ude1e',\n '\ud83d\ude03',\n '\u2764',\n '\ud83d\ude0c',\n '\u2764',\n '\ud83c\udf39',\n '\ud83c\udfc3',\n '\ud83d\ude0a',\n '\ud83d\udc99',\n '\ud83d\ude32',\n '\ud83d\ude0a',\n '\ud83d\ude2b',\n '\ud83d\udc94',\n '\ud83d\ude4f',\n '\ud83d\ude0d',\n '\u2764',\n '\ud83d\ude1e',\n '\ud83d\ude4f',\n '\ud83c\udf55',\n '\ud83d\ude0f',\n '\ud83d\udc40',\n '\ud83d\ude02',\n '\ud83d\ude1a',\n '\ud83d\ude09',\n '\u2728',\n '\ud83d\ude2d',\n '\ud83d\udc9c',\n '\ud83c\udfb6',\n '\ud83d\ude00',\n '\ud83d\ude07',\n '\ud83d\udc9d',\n '\ud83d\ude2d',\n '\ud83d\ude0e',\n '\ud83d\udc4c',\n '\ud83d\udc9b',\n '\ud83d\ude02',\n '\ud83d\ude4c',\n '\ud83d\udc96',\n '\ud83d\ude0c',\n '\u2764',\n '\ud83d\ude0d',\n '\ud83d\udc40',\n '\ud83d\udc9a',\n '\ud83d\ude02',\n '\u2764',\n '\ud83d\ude31',\n '\ud83d\ude31',\n '\ud83d\ude22',\n '\u2728',\n '\ud83d\ude1f',\n '\ud83d\udc9c',\n '\ud83d\ude0d',\n '\u2665',\n '\ud83d\ude31',\n '\ud83d\ude0f',\n '\ud83d\udc95',\n '\ud83d\udc4a',\n '\ud83d\ude08',\n '\ud83d\ude12',\n '\ud83d\udc4d',\n '\ud83d\ude0b',\n '\ud83d\ude0a',\n '\ud83d\ude0a',\n '\ud83d\ude0c',\n '\ud83d\ude05',\n '\ud83d\ude0d',\n '\ud83d\ude14',\n '\ud83d\ude0d',\n '\u2665',\n '\ud83d\ude18',\n '\u2764',\n '\ud83d\ude22',\n '\ud83d\ude02',\n '\ud83c\udf0d',\n '\ud83d\ude0e',\n '\ud83d\udc40',\n '\ud83d\ude0a',\n '\ud83d\ude02',\n '\ud83d\ude0a',\n '\ud83d\ude0d',\n '\ud83d\ude2d',\n '\u2764',\n '\u2764',\n '\ud83d\ude22',\n '\ud83c\udf42',\n '\ud83d\ude05',\n '\ud83d\ude05',\n '\ud83d\udc8b',\n '\ud83d\ude05',\n '\ud83d\ude29',\n '\ud83d\udcf7',\n '\u263a',\n '\ud83d\udc97',\n '\ud83d\ude01',\n '\ud83d\ude0a',\n '\ud83d\ude48',\n '\ud83d\udc40',\n '\ud83d\ude1c',\n '\ud83d\udc99',\n '\ud83d\ude02',\n '\ud83d\ude39',\n '\ud83d\udc98',\n '\ud83d\ude02',\n '\ud83d\ude0d',\n '\ud83d\udc4d',\n '\ud83d\ude09',\n '\ud83d\ude01',\n '\ud83d\ude2d',\n '\ud83d\ude02',\n '\ud83d\udc9c',\n '\u2764',\n '\ud83d\udc40',\n '\ud83d\ude1f',\n '\ud83d\ude04',\n '\ud83d\udc4d',\n '\ud83d\ude48',\n '\ud83d\ude05',\n '\ud83d\ude2d',\n '\ud83d\ude4f',\n '\ud83d\ude11',\n '\ud83d\udcaf',\n '\ud83d\ude2d',\n '\ud83d\udc83',\n '\ud83d\udc9c',\n '\ud83d\ude06',\n '\ud83d\ude2d',\n '\u2764',\n '\ud83d\ude02',\n '\ud83d\ude4f',\n '\ud83d\ude06',\n '\ud83d\udd25',\n '\ud83d\ude02',\n '\ud83d\ude18',\n '\ud83c\udfb6',\n '\ud83c\udfa5',\n '\ud83d\udc96',\n '\ud83d\ude02',\n '\ud83d\udc99',\n '\u2665',\n '\ud83d\ude0a',\n '\ud83d\ude02',\n '\ud83d\udd25',\n '\ud83d\ude0d',\n '\ud83d\ude0b',\n '\ud83d\udc99',\n '\ud83d\ude2d',\n '\u2728',\n '\ud83d\ude0a',\n '\ud83d\udc9e',\n '\ud83c\udf7b',\n '\ud83d\ude14',\n '\ud83d\ude04',\n '\ud83d\udc95',\n '\ud83d\ude0b',\n '\ud83d\ude0e',\n '\ud83d\ude02',\n '\ud83d\ude18',\n '\ud83d\ude01',\n '\ud83d\ude06',\n '\ud83d\ude08',\n '\ud83e\udd90',\n '\ud83d\ude0a',\n '\ud83d\ude02',\n '\u2764',\n '\ud83d\ude02',\n '\ud83d\ude0a',\n '\ud83d\udc9a',\n '\ud83d\ude04',\n '\ud83d\udc80',\n '\ud83d\udc9c',\n '\ud83d\ude1e',\n '\ud83d\ude2e',\n '\ud83d\ude04',\n '\u2600',\n '\ud83c\udf82',\n '\ud83d\ude02',\n '\ud83d\ude1d',\n '\ud83d\udcf7',\n '\u2764',\n '\ud83d\ude36',\n '\ud83d\udcf7',\n '\ud83d\ude23',\n '\ud83d\ude22',\n '\ud83d\ude02',\n '\ud83d\ude0f',\n '\ud83d\ude0d',\n '\u2764',\n '\ud83d\ude02',\n '\ud83d\ude2d',\n '\ud83d\ude1b',\n '\u2665',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\ude0d',\n '\ud83c\udfb6',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\ude04',\n '\ud83d\ude06',\n '\ud83d\ude0d',\n '\ud83d\udcaf',\n '\ud83d\ude02',\n '\u2764',\n '\ud83d\ude02',\n '\ud83d\udc15',\n '\ud83d\ude02',\n '\ud83d\udd90',\n '\ud83d\udc93',\n '\u2764',\n '\ud83d\udc6d',\n '\ud83d\ude10',\n '\ud83d\udca5',\n '\ud83d\ude0e',\n '\ud83d\ude0d',\n '\ud83d\ude02',\n '\ud83d\ude0b',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\ude06',\n '\ud83d\udcaa',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\ude4f',\n '\ud83d\udc97',\n '\u2764',\n '\ud83d\ude48',\n '\ud83d\ude0d',\n '\ud83d\udc4e',\n '\ud83d\ude18',\n '\u2615',\n '\ud83d\ude0d',\n '\ud83c\udf39',\n '\ud83d\ude2d',\n '\ud83d\ude02',\n '\ud83d\ude11',\n '\u2764',\n '\ud83d\udc51',\n '\ud83d\ude37',\n '\ud83d\ude01',\n '\ud83d\ude02',\n '\ud83d\ude22',\n '\ud83d\udc9c',\n '\u26a1',\n '\ud83d\udc9c',\n '\ud83d\ude0d',\n '\ud83d\udc4d',\n '\u263a',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\ude1b',\n '\ud83d\ude0a',\n '\u2728',\n '\ud83d\ude18',\n '\ud83c\udf3a',\n '\ud83d\udd25',\n '\ud83d\ude2e',\n '\ud83d\ude18',\n '\ud83d\ude02',\n '\ud83d\ude0a',\n '\ud83d\udc95',\n '\ud83d\ude2d',\n '\ud83d\ude09',\n '\ud83d\ude2d',\n '\ud83d\udc80',\n '\ud83d\ude1e',\n '\ud83d\ude18',\n '\ud83d\ude0d',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\udc4c',\n '\ud83d\udc4d',\n '\ud83d\ude29',\n '\ud83d\ude0e',\n '\ud83d\ude02',\n '\ud83e\udd70\u2764',\n '\ud83d\ude1d',\n '\ud83d\udcaf',\n '\ud83d\ude0a',\n '\ud83d\udc7d',\n '\ud83c\udf7b',\n '\ud83d\ude18',\n '\ud83d\ude0a',\n '\ud83d\ude2a',\n '\ud83d\ude02',\n '\ud83d\udc99',\n '\ud83d\ude29',\n '\ud83d\udcaf',\n '\ud83d\ude09',\n '\ud83d\ude02',\n '\ud83d\ude06',\n '\ud83d\ude0a',\n '\ud83d\udc45',\n '\ud83d\ude0a',\n '\ud83d\udc7b',\n '\ud83d\ude0d',\n '\ud83d\ude2d',\n '\ud83d\ude0e',\n '\ud83d\udc95',\n '\ud83d\ude48',\n '\ud83d\ude02',\n '\ud83d\udc4d',\n '\ud83d\udc9a',\n '\ud83d\ude0c',\n '\ud83d\udc4d',\n '\ud83d\ude21',\n '\ud83d\ude0a',\n '\ud83d\ude0a',\n '\ud83d\ude2d',\n '\ud83d\ude0a',\n '\ud83d\ude18',\n '\ud83d\ude22',\n '\ud83d\ude29',\n '\ud83d\udc95',\n '\ud83c\udf3b',\n '\ud83c\udfb6',\n '\ud83c\udfb6',\n '\ud83d\ude0a',\n '\ud83d\ude2d',\n '\ud83d\ude02',\n '\ud83d\ude09',\n '\ud83d\ude14',\n '\ud83d\ude02',\n '\u263a',\n '\ud83d\udcaf',\n '\u2665',\n '\ud83d\ude01',\n '\u2764',\n '\ud83d\udc4d',\n '\ud83d\ude23',\n '\ud83d\udc94',\n '\ud83d\udc95',\n '\u00ae',\n '\ud83d\ude2d',\n '\ud83d\udc40',\n '\ud83d\udc96',\n '\ud83d\udc96',\n '\ud83d\udc4d',\n '\ud83d\ude02',\n '\ud83d\ude09',\n '\ud83d\ude0a',\n '\u2600',\n '\u2764',\n '\ud83d\ude12',\n '\ud83d\ude18',\n '\ud83d\ude05',\n '\ud83d\udcf7',\n '\ud83d\ude35',\n '\u2728',\n '\ud83d\ude4f',\n '\ud83d\udc4d',\n '\ud83d\ude2d',\n '\ud83d\ude2d',\n '\ud83d\ude2d',\n '\ud83d\ude0d',\n '\u263a',\n '\ud83d\udcab',\n '\ud83d\ude0d',\n '\ud83d\udca9',\n '\ud83d\ude02',\n '\ud83d\udc97',\n '\u2764',\n '\ud83d\ude0d',\n '\ud83d\ude2c',\n '\ud83d\ude02',\n '\ud83d\ude2d',\n '\ud83d\ude46',\n '\ud83d\udcaf',\n '\ud83d\ude35',\n '\ud83d\ude2a',\n '\ud83d\ude2c',\n '\ud83d\ude0a',\n '\ud83d\ude24',\n '\ud83d\udc96',\n '\ud83d\udcaa',\n '\ud83d\ude4f',\n '\ud83c\udf0d',\n '\ud83d\ude06',\n '\ud83d\ude33',\n '\ud83d\udc80',\n '\ud83c\udf89',\n '\ud83d\udc9c',\n '\ud83d\ude18',\n '\ud83d\ude4f',\n '\ud83d\ude08',\n '\ud83c\udf89',\n '\ud83d\udc95',\n '\ud83d\ude32',\n '\ud83d\ude29',\n '\ud83d\ude02',\n '\ud83d\ude09',\n '\ud83c\udfb5',\n '\ud83d\udc83',\n '\ud83d\ude18',\n '\ud83d\ude18',\n '\ud83d\ude2c',\n '\ud83d\ude02',\n '\ud83d\ude01',\n '\ud83d\ude02',\n '\ud83d\udcaf',\n '\ud83d\udc4d',\n '\ud83d\ude05',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\ude18',\n '\ud83d\udc9b',\n '\ud83d\udc9a',\n '\ud83d\ude2d',\n '\u2728',\n '\ud83d\ude06',\n '\ud83d\ude0a',\n '\ud83d\ude12',\n '\ud83d\ude0c',\n '\ud83d\ude12',\n '\ud83d\ude48',\n '\ud83d\udc95',\n '\ud83d\ude0d',\n '\ud83d\ude1e',\n '\u2764',\n '\ud83d\ude34',\n '\ud83d\ude2d',\n '\ud83d\ude2d',\n '\ud83d\ude33',\n '\ud83d\ude14',\n '\ud83d\ude24',\n '\ud83d\ude2d',\n '\ud83d\ude02',\n '\ud83d\ude2d',\n '\ud83d\ude34',\n '\ud83d\ude02',\n '\u2764',\n '\ud83d\udca5',\n '\u2665',\n '\ud83d\udd25',\n '\ud83d\ude12',\n '\ud83d\ude0e',\n '\ud83d\ude32',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\udc79',\n '\ud83d\ude0e',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\udc4c',\n '\ud83d\ude24',\n '\ud83d\ude0a',\n '\ud83d\ude02',\n '\ud83d\ude0d',\n '\ud83d\ude02',\n '\ud83d\ude2d',\n '\ud83d\ude02',\n '\ud83d\ude4f',\n '\ud83d\ude0d',\n '\ud83d\ude2d',\n '\ud83d\ude2d',\n '\ud83d\udc99',\n '\ud83d\udc9e',\n '\ud83d\ude0c',\n '\ud83d\ude14',\n '\ud83d\ude1b',\n '\ud83d\ude0a',\n '\ud83d\ude02',\n '\ud83d\udc96',\n '\ud83d\ude02',\n '\ud83d\udc4d',\n '\ud83d\udc85',\n '\ud83d\udc96',\n '\ud83d\udc9e',\n '\ud83d\udc95',\n '\ud83d\udc4b',\n '\ud83d\udc4d',\n '\ud83d\ude02',\n '\ud83d\udc9c',\n '\ud83c\udfa7',\n '\ud83d\ude11',\n '\ud83d\ude18',\n '\ud83d\ude21',\n '\ud83d\ude26',\n '\ud83d\udd25',\n '\ud83d\ude21',\n '\ud83d\ude3f',\n '\ud83d\udcf7',\n '\u26a1',\n '\ud83d\udc90',\n '\ud83d\ude0d',\n '\ud83d\ude29',\n '\ud83d\ude1e',\n '\ud83d\ude02',\n '\ud83d\ude2d',\n '\ud83d\ude0a',\n '\ud83d\ude0d',\n '\ud83d\ude02',\n '\ud83c\udf3c',\n '\ud83d\udc95',\n '\ud83d\udc9b',\n '\ud83d\ude11',\n '\ud83d\ude15',\n '\u2764',\n '\ud83d\ude00',\n '\ud83d\ude0e',\n '\ud83d\udc96',\n '\ud83d\ude1e',\n '\ud83d\ude02',\n '\u2728',\n '\ud83d\udd25',\n '\ud83d\udc93',\n '\ud83c\udf7b',\n '\ud83d\udca5',\n '\u2764',\n '\ud83d\ude20',\n '\ud83c\udfb5',\n '\ud83d\ude0a',\n '\ud83c\udf89',\n '\ud83d\ude0c',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\ude29',\n '\u2728',\n '\ud83d\ude2d',\n '\u2764',\n '\ud83d\ude14',\n '\ud83d\ude03',\n '\ud83d\ude02',\n '\ud83d\ude1d',\n '\ud83d\ude29',\n '\ud83d\udc95',\n '\ud83d\ude29',\n '\ud83d\udc51',\n '\ud83d\udc40',\n '\ud83d\ude02',\n '\ud83d\ude3f',\n '\ud83d\ude05',\n '\ud83d\ude4f',\n '\ud83d\ude07',\n '\ud83d\ude06',\n '\ud83d\ude02',\n '\ud83c\udf81',\n '\ud83d\ude31',\n '\ud83d\ude0a',\n '\ud83d\ude0a',\n '\u2728',\n '\ud83d\udc98',\n '\ud83d\ude0f',\n '\ud83d\udcaf',\n '\ud83d\ude25',\n '\ud83d\ude09',\n '\ud83d\ude1e',\n '\ud83d\ude2d',\n '\ud83d\ude05',\n '\ud83c\udfb6',\n '\ud83d\ude0a',\n '\ud83d\ude0d',\n '\u2764',\n '\ud83d\udc40',\n '\ud83d\ude14',\n '\ud83d\ude02',\n '\ud83c\udfb6',\n '\ud83d\udc45',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83c\udf1a',\n '\ud83d\udcf9',\n '\ud83d\udc4d',\n '\ud83d\ude2e',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\ude2d',\n '\ud83d\ude18',\n '\ud83d\ude2d',\n '\ud83d\ude0c',\n '\u2764',\n '\ud83d\ude01',\n '\ud83d\ude0d',\n '\ud83d\udcf7',\n '\ud83d\ude1d',\n '\ud83d\ude2d',\n '\u263a',\n '\ud83d\udc40',\n '\ud83d\ude0e',\n '\ud83d\ude05',\n '\ud83d\ude0e',\n '\ud83d\udcaf',\n '\u2728',\n '\ud83d\ude0a',\n '\ud83d\udc9c',\n '\ud83d\udc95',\n '\ud83d\ude1c',\n '\ud83d\ude02',\n '\ud83d\ude14',\n '\ud83d\ude02',\n '\ud83d\udc4d',\n '\u2615',\n '\ud83d\udca5',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\ude0f',\n '\ud83d\ude29',\n '\ud83d\udc4d',\n '\ud83d\ude18',\n '\ud83d\ude1c',\n '\ud83d\udc95',\n '\ud83d\ude0d',\n '\ud83d\ude33',\n '\u2764',\n '\ud83d\udc9e',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\udc9c',\n '\ud83d\udcaf',\n '\ud83d\ude34',\n '\ud83d\ude4f',\n '\u2764',\n '\ud83d\udc40',\n '\ud83d\udc93',\n '\ud83d\udc15',\n '\ud83d\ude0d',\n '\ud83d\ude02',\n '\ud83d\udc80',\n '\ud83d\ude09',\n '\ud83d\ude18',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\u2764',\n '\ud83d\ude2d',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\u2615',\n '\ud83d\ude02',\n '\ud83d\ude2a',\n '\ud83d\ude0d',\n '\ud83d\ude06',\n '\ud83d\ude15',\n '\u2764',\n '\ud83d\ude1b',\n '\ud83d\ude02',\n '\ud83d\ude1c',\n '\ud83d\ude22',\n '\ud83d\udc79',\n '\ud83d\ude80',\n '\ud83d\ude0d',\n '\ud83d\ude14',\n '\ud83d\ude18',\n '\ud83d\ude02',\n '\ud83d\ude2d',\n '\ud83d\ude11',\n '\u2764',\n '\ud83d\ude45',\n '\ud83d\udcaa',\n '\ud83d\ude2d',\n '\ud83d\ude02',\n '\ud83d\ude0d',\n '\ud83d\ude12',\n '\ud83d\ude30',\n '\ud83d\ude0d',\n '\ud83d\ude02',\n '\ud83d\ude0d',\n '\ud83d\ude39',\n '\ud83d\udc97',\n '\ud83d\ude29',\n '\ud83d\ude02',\n '\ud83d\ude0d',\n '\ud83d\ude03',\n '\ud83d\ude02',\n '\ud83d\ude2d',\n '\ud83d\ude31',\n '\ud83d\ude14',\n '\ud83d\ude24',\n '\ud83d\ude1c',\n '\ud83d\ude0d',\n '\ud83d\ude2d',\n '\ud83d\ude1b',\n '\ud83d\ude18',\n '\ud83d\udc9c',\n '\ud83d\udc94',\n '\ud83d\udc9c',\n '\ud83d\ude02',\n '\ud83d\ude11',\n '\ud83d\udc93',\n '\ud83d\ude02',\n '\ud83d\udc9c',\n '\ud83d\ude0e',\n '\ud83d\ude01',\n '\ud83d\udcaf',\n '\ud83d\ude33',\n '\ud83d\ude11',\n '\u2614',\n '\ud83d\ude24',\n '\ud83d\udcb8',\n '\ud83c\udfa7',\n '\ud83d\ude02',\n '\ud83d\ude29',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\u2764',\n '\ud83d\ude02',\n '\ud83d\ude08',\n '\ud83d\udc85',\n '\ud83d\ude09',\n '\ud83d\udc97',\n '\ud83d\ude01',\n '\ud83d\ude02',\n '\ud83d\ude0d',\n '\ud83d\udc93',\n '\ud83d\ude12',\n '\ud83d\udc99',\n '\ud83d\ude0a',\n '\ud83d\ude0d',\n '\ud83d\ude4f',\n '\ud83d\ude02',\n '\ud83d\udcaf',\n '\ud83d\udcf7',\n '\ud83d\udcf7',\n '\ud83d\ude02',\n '\ud83d\ude0a',\n '\ud83d\ude14',\n '\ud83d\udc40',\n '\ud83d\ude1e',\n '\u2764',\n '\ud83d\udc4d',\n '\ud83d\udc9d',\n '\ud83d\ude01',\n '\ud83d\ude07',\n '\ud83d\ude37',\n '\ud83d\ude21',\n '\ud83d\udc9a',\n '\ud83d\udc9b',\n '\ud83d\ude0e',\n '\ud83d\ude0e',\n '\ud83d\ude02',\n '\ud83d\ude03',\n '\ud83d\udc4d',\n '\ud83d\ude01',\n '\ud83d\udc99',\n '\ud83d\ude09',\n '\ud83d\udd25',\n '\ud83d\ude02',\n '\ud83d\ude33',\n '\ud83d\ude02',\n '\u2764',\n '\ud83d\ude02',\n '\ud83d\ude2d',\n '\ud83d\ude29',\n '\ud83d\udc99',\n '\ud83d\udc9b',\n '\ud83d\ude0a',\n '\ud83d\ude4f',\n '\ud83d\ude14',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83c\udf37',\n '\ud83d\udc9b',\n '\ud83d\ude18',\n '\ud83d\ude1c',\n '\u2764',\n '\ud83c\udf4c',\n '\ud83d\udcab',\n '\ud83d\ude18',\n '\ud83d\ude48',\n '\u2764',\n '\ud83d\udc97',\n '\ud83d\ude0d',\n '\ud83d\udc4f',\n '\u2764',\n '\ud83d\ude36',\n '\ud83c\udf1f',\n '\ud83d\udc44',\n '\ud83d\udd25',\n '\ud83d\udc96',\n '\ud83d\udc9b',\n '\ud83c\udf89',\n '\ud83c\udfa5',\n '\ud83d\udc83',\n '\ud83d\udc94',\n '\u263a',\n '\ud83c\udf0e\ud83c\udde7\ud83c\uddef\ud83c\uddf1\ud83c\uddfa\ud83c\udde6\ud83c\uddec',\n '\ud83d\ude0d',\n '\ud83c\udf81',\n '\ud83d\ude0d',\n '\ud83d\ude80',\n '\ud83d\ude2d',\n '\ud83d\ude31',\n '\ud83d\udcaf',\n '\ud83d\ude02',\n '\ud83d\ude01',\n '\u2600',\n '\ud83d\udc4f',\n '\ud83d\ude01',\n '\ud83d\udc4d',\n '\ud83d\ude35',\n '\ud83c\udf89',\n '\ud83d\ude12',\n '\u2764',\n '\ud83d\udc96',\n '\ud83d\ude2d',\n '\ud83d\ude2d',\n '\ud83d\udcaf',\n '\ud83d\ude2c',\n '\ud83d\ude0d',\n '\ud83d\udcab',\n '\ud83d\ude02',\n '\ud83d\ude22',\n '\ud83d\ude2d',\n '\u2764',\n '\ud83d\udc9c',\n '\u2614',\n '\ud83c\udf89',\n '\ud83d\ude0a',\n '\ud83d\ude1b',\n '\ud83d\ude02',\n '\ud83d\ude02',\n '\ud83d\udc3b',\n '\ud83d\ude30',\n '\ud83e\udd14',\n '\ud83d\ude0a',\n '\ud83d\ude02',\n '\ud83d\udc4a',\n '\ud83d\udd25',\n '\ud83d\ude0f',\n '\ud83c\udf51',\n '\ud83c\uddf3\ud83c\uddf1\ud83c\udf77',\n '\ud83d\udcab',\n '\ud83d\ude02',\n '\ud83d\ude0d',\n '\ud83d\ude18',\n '\ud83c\udf88',\n '\ud83d\ude0a',\n '\ud83d\udc99',\n '\ud83d\udcaf',\n '\ud83d\udc95',\n '\ud83d\ude02',\n '\ud83d\udc9c',\n '\ud83d\ude0c',\n '\ud83d\ude02',\n '\ud83c\udf39',\n '\ud83d\ude0a',\n '\ud83d\ude35',\n '\ud83d\ude0c',\n '\ud83d\udc97',\n '\ud83d\udc40',\n '\ud83d\udc4d',\n '\ud83d\ude2d',\n '\ud83d\ude29',\n '\ud83d\ude1e',\n '\ud83d\udc40',\n '\ud83d\udcb0',\n '\u2764',\n '\ud83c\udf88',\n '\ud83d\udc96',\n '\ud83d\ude02',\n '\ud83c\udfb6',\n '\ud83d\udc37',\n '\ud83d\ude18',\n '\ud83d\ude2e',\n '\ud83d\ude99',\n '\ud83c\udfa4',\n '\ud83d\ude2b',\n '\ud83d\ude11',\n '\ud83d\ude0f',\n '\u2764',\n '\ud83d\ude18',\n '\ud83d\ude09',\n '\ud83d\udc9b',\n '\ud83c\uddf5\ud83c\uddf9\u2764',\n '\ud83d\udc94',\n '\ud83d\udc96',\n '\ud83c\udf7b',\n '\ud83d\ude22',\n '\ud83d\udd17',\n '\ud83d\udc95',\n '\ud83d\ude02',\n '\ud83d\ude4f',\n '\ud83d\udc9e',\n '\ud83d\udc9c',\n '\ud83d\ude2d',\n '\ud83d\udc4d',\n '\ud83d\ude14',\n '\ud83d\ude21',\n '\ud83c\udf19',\n '\ud83d\udc40',\n '\ud83d\udc8c',\n '\ud83d\udc96',\n '\ud83d\ude24',\n '\ud83d\ude13',\n '\ud83d\udc80',\n '\ud83d\ude34',\n '\u2764',\n '\ud83d\ude26',\n '\ud83d\ude22',\n '\ud83d\ude2a',\n '\ud83d\ude02',\n '\ud83d\ude1c',\n '\ud83d\ude0d',\n '\ud83d\ude0d',\n '\ud83d\ude2d',\n '\ud83d\ude29',\n '\ud83d\udc4c',\n '\ud83d\udc94',\n '\u2764',\n '\ud83d\udc9a',\n '\ud83d\ude0a',\n '\ud83d\udcde',\n '\ud83d\udc9b',\n '\u2764',\n '\ud83d\ude0c',\n '\ud83d\udc40',\n '\ud83d\ude10',\n '\ud83d\udd25',\n '\ud83d\udc9a',\n '\ud83d\ude02',\n '\ud83d\udc97',\n '\ud83d\udc80',\n '\ud83d\udca6',\n '\ud83d\ude1d',\n '\ud83d\udc95',\n '\ud83d\ude29',\n '\ud83d\ude1f',\n '\ud83c\udf47',\n '\ud83d\udc96',\n '\ud83d\ude00',\n '\ud83d\ude01',\n '\ud83d\ude07',\n '\ud83d\udc95',\n '\ud83d\ude2a',\n '\u270a',\n '\ud83d\udcad',\n '\ud83d\ude02',\n '\ud83d\ude0a',\n '\ud83d\udc4a',\n '\ud83d\ude01',\n '\ud83d\ude0d',\n '\ud83d\ude05',\n '\ud83d\udcde',\n '\ud83c\udf89',\n '\ud83d\ude09',\n '\ud83d\ude04',\n '\ud83d\udc9c',\n '\ud83d\ude33',\n '\ud83d\ude00',\n '\ud83d\ude0c',\n '\ud83d\ude02',\n '\ud83c\udfa7',\n '\ud83d\udd25',\n '\ud83d\ude02',\n '\ud83d\ude03',\n '\ud83d\ude0f',\n '\ud83d\ude02',\n '\u270c',\n '\ud83d\ude0f',\n ...]\n\n</code></pre>\n<p>I am unsure why it does not list all 2038 unique values, which I need.</p>\n<p>And...apologies for showing the full output, but I feel it is necessary to show the output for this issues I am having.</p>\n<p>Therefore, is there a way to list all 2038 unique values in my column, or extract them all?</p>\n<p>Thanks.</p>\n"
    },
    {
        "tags": [
            "python",
            "excel",
            "pandas",
            "dataframe",
            "xlsx"
        ],
        "owner": {
            "account_id": 3763598,
            "reputation": 3897,
            "user_id": 3127764,
            "user_type": "registered",
            "accept_rate": 64,
            "profile_image": "https://i.stack.imgur.com/htQno.png?s=128&g=1",
            "display_name": "HaPsantran",
            "link": "https://stackoverflow.com/users/3127764/hapsantran"
        },
        "is_answered": true,
        "view_count": 445437,
        "accepted_answer_id": 26521726,
        "answer_count": 11,
        "score": 258,
        "last_activity_date": 1624789971,
        "creation_date": 1414038105,
        "last_edit_date": 1612535857,
        "question_id": 26521266,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/26521266/using-pandas-to-pd-read-excel-for-multiple-worksheets-of-the-same-workbook",
        "title": "Using Pandas to pd.read_excel() for multiple worksheets of the same workbook",
        "answer_body": "<p>Try <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#excelfile-class\" rel=\"noreferrer\"><code>pd.ExcelFile</code></a>:</p>\n\n<pre><code>xls = pd.ExcelFile('path_to_file.xls')\ndf1 = pd.read_excel(xls, 'Sheet1')\ndf2 = pd.read_excel(xls, 'Sheet2')\n</code></pre>\n\n<p>As noted by @HaPsantran, the entire Excel file is read in during the <code>ExcelFile()</code> call (there doesn't appear to be a way around this). This merely saves you from having to read the same file in each time you want to access a new sheet.</p>\n\n<p>Note that the <code>sheet_name</code> argument to <code>pd.read_excel()</code> can be the name of the sheet (as above), an integer specifying the sheet number (eg 0, 1, etc), a list of sheet names or indices, or <code>None</code>. If a list is provided, it returns a dictionary where the keys are the sheet names/indices and the values are the data frames. The default is to simply return the first sheet (ie, <code>sheet_name=0</code>).</p>\n\n<p>If <code>None</code> is specified, <strong>all</strong> sheets are returned, as a <code>{sheet_name:dataframe}</code> dictionary.</p>\n",
        "question_body": "<p>I have a large spreadsheet file (.xlsx) that I'm processing using python pandas. It happens that I need data from two tabs in that large file. One of the tabs has a ton of data and the other is just a few square cells.</p>\n\n<p>When I use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.excel.read_excel.html\">pd.read_excel()</a> on <em>any</em> worksheet, it looks to me like the whole file is loaded (not just the worksheet I'm interested in). So when I use the method twice (once for each sheet), I effectively have to suffer the whole workbook being read in twice (even though we're only using the specified sheet).</p>\n\n<p>Am I using it wrong or is it just limited in this way?</p>\n\n<p>Thank you!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "date",
            "datetime"
        ],
        "owner": {
            "account_id": 17990565,
            "reputation": 125,
            "user_id": 13074756,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/e4ad85d3c9a842e44dedb4b90814129c?s=128&d=identicon&r=PG&f=1",
            "display_name": "user13074756",
            "link": "https://stackoverflow.com/users/13074756/user13074756"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 68149983,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624788750,
        "creation_date": 1624786928,
        "last_edit_date": 1624787505,
        "question_id": 68149831,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68149831/find-if-date-is-weekend-or-weekday-in-pandas-dataframe",
        "title": "Find if Date is weekend or weekday in Pandas DataFrame",
        "answer_body": "<p>You can first form a <code>dates</code> series from your <code>Year</code>, <code>Month</code> and <code>Day</code> columns:</p>\n<pre><code>dates = pd.to_datetime({&quot;year&quot;: df.Year, &quot;month&quot;: df.Month, &quot;day&quot;: df.Day})\n</code></pre>\n<p>then you can check the <code>dayofweek</code> (note the <code>dt</code> accessor before it):</p>\n<pre><code>df[&quot;Day of Week&quot;] = dates.dt.dayofweek\n</code></pre>\n<p>and form the <code>is_weekend</code> column with your logic:</p>\n<pre><code>df[&quot;Is Weekend&quot;] = dates.dt.dayofweek &gt; 4\n</code></pre>\n<p>where we don't need <code>apply</code> anymore since we have a unified <code>dates</code> series and it supports this kind of all-at-once comparison.</p>\n",
        "question_body": "<p>I have a Dataframe, df, with 3 columns: Year, Month and Date. I want to use this information to determine if this date falls on a weekend or weekday and which day of the week it is.</p>\n<p>I am trying to use the below code to determine if its weekend or weekday but it is not working</p>\n<pre><code>from datetime import datetime\nfrom datetime import date\n\ndef is_weekend(d = datetime.today()):\n  return d.weekday() &gt; 4\n</code></pre>\n<pre><code>df['weekend'] = df.apply(lambda x: is_weekend(date(x['Year'], x['Month'], x['Day'])), axis=1)\n</code></pre>\n<p>I am getting this error:</p>\n<pre><code>TypeError: cannot convert the series to &lt;class 'int'&gt;\n</code></pre>\n<p>Apart from this, how do I find out if this date falls on which day of the week.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "mean",
            "standard-deviation"
        ],
        "owner": {
            "account_id": 16634018,
            "reputation": 297,
            "user_id": 12021183,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/35b529ce45febdad8479b04886ad7259?s=128&d=identicon&r=PG&f=1",
            "display_name": "raven",
            "link": "https://stackoverflow.com/users/12021183/raven"
        },
        "is_answered": true,
        "view_count": 58,
        "accepted_answer_id": 68148938,
        "answer_count": 2,
        "score": 4,
        "last_activity_date": 1624779819,
        "creation_date": 1624774001,
        "question_id": 68148468,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68148468/retrieving-the-average-of-averages-in-python-dataframe",
        "title": "Retrieving the average of averages in Python DataFrame",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def fn(x):\n    _100_means = [x.sample(10).mean() for i in range(100)]\n    return {\n        &quot;mean_of_100_means&quot;: np.mean(_100_means),\n        &quot;total_sd&quot;: np.std(_100_means),\n    }\n\n\nprint(df.groupby(&quot;year&quot;)[&quot;count&quot;].apply(fn).unstack().reset_index())\n</code></pre>\n<p>EDIT: Changed the computation of means.</p>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>    year  mean_of_100_means   total_sd\n0   1983             48.986   8.330787\n1   1984             48.479  10.384896\n2   1985             48.957   7.854900\n3   1986             50.821  10.303847\n4   1987             50.198   9.835832\n5   1988             47.497   8.678749\n6   1989             46.763   9.197387\n7   1990             49.696   8.837589\n8   1991             46.979   8.141969\n9   1992             48.555   8.603597\n10  1993             50.220   8.263946\n11  1994             48.735   9.954741\n12  1995             49.759   8.532844\n13  1996             49.832   8.998654\n14  1997             50.306   9.038316\n15  1998             49.513   9.024341\n16  1999             50.532   9.883166\n17  2000             49.195   9.177008\n18  2001             50.731   8.309244\n19  2002             48.792   9.680028\n20  2003             50.251   9.384759\n21  2004             50.522   9.269677\n22  2005             48.090   8.964458\n23  2006             49.529   8.250701\n24  2007             47.192   8.682196\n25  2008             50.124   9.337356\n26  2009             47.988   8.053438\n</code></pre>\n<hr />\n<p>The dataframe was created:</p>\n<pre class=\"lang-py prettyprint-override\"><code>data = []\nfor y in range(1983, 2010):\n    for i in np.random.randint(0, 100, size=1000):\n        data.append({&quot;year&quot;: y, &quot;count&quot;: i})\ndf = pd.DataFrame(data)\n</code></pre>\n",
        "question_body": "<p>I have a mass <code>pandas</code> DataFrame <code>df</code>:</p>\n<pre><code>year          count\n1983          5\n1983          4\n1983          7\n...\n2009          8\n2009          11\n2009          30\n</code></pre>\n<p>and I aim to sample 10 data points per <code>year</code> 100 times and get the mean and standard deviation of <code>count</code> per year. The signs of the <code>count</code> values are determined randomly.</p>\n<hr />\n<p>I want to randomly sample 10 data per <code>year</code>, which can be done by:</p>\n<pre><code>new_df = pd.DataFrame(columns=['year', 'count'])\nref = df.year.unique()\n\nfor i in range(len(ref)):\n  appended_df = df[df['year'] == ref[i]].sample(n=10)\n  new_df = pd.concat([new_df,appended_df])\n</code></pre>\n<p>Then, I assign a sign to <code>count</code> randomly (so that by random chance the <code>count</code> could be positive or negative) and rename it to <code>value</code>, which can be done by:</p>\n<pre><code>vlist = []\n\nfor i in range(len(new_df)):\n  if randint(0,1) == 0:\n    vlist.append(new_df.count.iloc[i])\n  else:\n    vlist.append(new_df.count.iloc[i] * -1)\n\nnew_data['value'] = vlist\n</code></pre>\n<p>Getting a mean and standard deviation per each <code>year</code> is quite simple:</p>\n<pre><code>xdf = new_data.groupby(&quot;year&quot;).agg([np.mean, np.std]).reset_index()\n</code></pre>\n<p>But I can't seem to find an optimal way to try this sampling 100 times per <code>year</code>, store the mean values, and get the mean and standard deviation of those 100 means per year. I could think of using <code>for</code> loop, but it would take too much of a runtime.</p>\n<p>Essentially, the output should be in the form of the following (the <code>value</code>s are arbitrary here):</p>\n<pre><code>year      mean_of_100_means  total_sd\n1983      4.22               0.43\n1984      -6.39              1.25\n1985      2.01               0.04\n...\n2007      11.92              3.38\n2008      -5.27              1.67\n2009      1.85               0.99\n</code></pre>\n<p>Any insights would be appreciated.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "select"
        ],
        "owner": {
            "account_id": 529574,
            "reputation": 18721,
            "user_id": 1610626,
            "user_type": "registered",
            "accept_rate": 70,
            "profile_image": "https://www.gravatar.com/avatar/d4738de350bcaf1777b12e8b77f7d018?s=128&d=identicon&r=PG",
            "display_name": "user1234440",
            "link": "https://stackoverflow.com/users/1610626/user1234440"
        },
        "is_answered": true,
        "view_count": 2904834,
        "protected_date": 1514966488,
        "accepted_answer_id": 11287278,
        "answer_count": 21,
        "score": 1393,
        "last_activity_date": 1624779543,
        "creation_date": 1341176596,
        "last_edit_date": 1612796046,
        "question_id": 11285613,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/11285613/selecting-multiple-columns-in-a-pandas-dataframe",
        "title": "Selecting multiple columns in a Pandas dataframe",
        "answer_body": "<p>The column names (which are strings) cannot be sliced in the manner you tried.</p>\n<p>Here you have a couple of options. If you know from context which variables you want to slice out, you can just return a view of only those columns by passing a list into the <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#basics\" rel=\"noreferrer\"><code>__getitem__</code> syntax</a> (the []'s).</p>\n<pre><code>df1 = df[['a', 'b']]\n</code></pre>\n<p>Alternatively, if it matters to index them numerically and not by their name (say your code should automatically do this without knowing the names of the first two columns) then you can do this instead:</p>\n<pre><code>df1 = df.iloc[:, 0:2] # Remember that Python does not slice inclusive of the ending index.\n</code></pre>\n<p>Additionally, you should familiarize yourself with the idea of a view into a Pandas object vs. a copy of that object. The first of the above methods will return a new copy in memory of the desired sub-object (the desired slices).</p>\n<p>Sometimes, however, there are indexing conventions in Pandas that don't do this and instead give you a new variable that just refers to the same chunk of memory as the sub-object or slice in the original object. This will happen with the second way of indexing, so you can modify it with the <code>.copy()</code> method to get a regular copy. When this happens, changing what you think is the sliced object can sometimes alter the original object. Always good to be on the look out for this.</p>\n<pre><code>df1 = df.iloc[0, 0:2].copy() # To avoid the case where changing df1 also changes df\n</code></pre>\n<p>To use <code>iloc</code>, you need to know the column positions (or indices). As the column positions may change, instead of hard-coding indices, you can use <code>iloc</code> along with <code>get_loc</code> function of <code>columns</code> method of dataframe object to obtain column indices.</p>\n<pre><code>{df.columns.get_loc(c): c for idx, c in enumerate(df.columns)}\n</code></pre>\n<p>Now you can use this dictionary to access columns through names and using <code>iloc</code>.</p>\n",
        "question_body": "<p>I have data in different columns, but I don't know how to extract it to save it in another variable.</p>\n<pre><code>index  a   b   c\n1      2   3   4\n2      3   4   5\n</code></pre>\n<p>How do I select <code>'a'</code>, <code>'b'</code> and save it in to df1?</p>\n<p>I tried</p>\n<pre><code>df1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n</code></pre>\n<p>None seem to work.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22061685,
            "reputation": 41,
            "user_id": 16324245,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3a04d2d2ea5d5402844e970fb4426307?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mark J.",
            "link": "https://stackoverflow.com/users/16324245/mark-j"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68146760,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624779427,
        "creation_date": 1624746788,
        "last_edit_date": 1624779427,
        "question_id": 68146715,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68146715/how-to-compose-each-word-in-the-dataframe-into-a-sentence-and-generate-the-next",
        "title": "How to compose each word in the dataframe into a sentence, and generate the next sentence after the period or question mark?",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import re\n\npattern = re.compile(r&quot;\\.|\\!|\\?$&quot;)\n\ndf_out = df.groupby(\n    df.word.apply(lambda x: bool(pattern.search(x))).shift().fillna(0).cumsum()\n).agg({&quot;start_time&quot;: &quot;first&quot;, &quot;end_time&quot;: &quot;last&quot;, &quot;word&quot;: &quot; &quot;.join})\nprint(df_out)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>      start_time  end_time            word\nword                                      \n0            0.1       0.6        I AM OK.\n1            0.7       1.2  HOW ABOUT YOU?\n2            1.3       1.4             OK!\n</code></pre>\n",
        "question_body": "<p>How to compose each word in the dataframe into a sentence, and generate the next sentence after the period or a question mark?</p>\n<p>the original dataframe looks like this:</p>\n<pre><code>start_time  end_time words\n\n0.1           0.2     I\n\n0.3           0.4     AM\n\n0.5           0.6     GOOD.\n\n0.7           0.8     HOW\n\n0.9           1.0     ABOUT\n\n1.1           1.2     YOU?\n\n1.3           1.4      OK!\n</code></pre>\n<p>the result I want to get looks like this:</p>\n<pre><code>start_time  end_time   words\n\n0.1          0.6     I AM GOOD.\n\n0.7          1.2     HOW ABOUT YOU?\n\n1.3          1.4     OK!\n</code></pre>\n<p>This is my dataframe:</p>\n<pre><code>    data = {'start_time': [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3],\n       'end_time': [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4],\n       'word':['I','AM','OK.','HOW', 'ABOUT', 'YOU?','OK!']}\ndf = pd.DataFrame(data, columns = ['start_time', 'end_time','word'])\n</code></pre>\n<p>Is there any suggested algorithm to help this problem, thank you very much!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 2716826,
            "reputation": 2307,
            "user_id": 2344011,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b13fbc3920412c1e5f8954f4f31faae0?s=128&d=identicon&r=PG",
            "display_name": "Gejun",
            "link": "https://stackoverflow.com/users/2344011/gejun"
        },
        "is_answered": true,
        "view_count": 254045,
        "accepted_answer_id": 36685531,
        "answer_count": 6,
        "score": 153,
        "last_activity_date": 1624776458,
        "creation_date": 1460941972,
        "last_edit_date": 1527574200,
        "question_id": 36684013,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe",
        "title": "extract column value based on another column pandas dataframe",
        "answer_body": "<p>You could use <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html\" rel=\"noreferrer\"><code>loc</code></a> to get series which satisfying your condition and then <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html\" rel=\"noreferrer\"><code>iloc</code></a> to get first element:</p>\n\n<pre><code>In [2]: df\nOut[2]:\n    A  B\n0  p1  1\n1  p1  2\n2  p3  3\n3  p2  4\n\nIn [3]: df.loc[df['B'] == 3, 'A']\nOut[3]:\n2    p3\nName: A, dtype: object\n\nIn [4]: df.loc[df['B'] == 3, 'A'].iloc[0]\nOut[4]: 'p3'\n</code></pre>\n",
        "question_body": "<p>I am kind of getting stuck on extracting value of one variable conditioning on another variable. For example, the following dataframe:</p>\n\n<pre><code>A  B\np1 1\np1 2\np3 3\np2 4\n</code></pre>\n\n<p>How can I get the value of <code>A</code> when <code>B=3</code>? Every time when I extracted the value of <code>A</code>, I got an object, not a string. </p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "loops"
        ],
        "owner": {
            "account_id": 19326816,
            "reputation": 43,
            "user_id": 14129286,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/46b590f7c23d90222ba8aaaa9a55005d?s=128&d=identicon&r=PG&f=1",
            "display_name": "siiddd",
            "link": "https://stackoverflow.com/users/14129286/siiddd"
        },
        "is_answered": true,
        "view_count": 62,
        "closed_date": 1624080570,
        "accepted_answer_id": 68043889,
        "answer_count": 5,
        "score": 2,
        "last_activity_date": 1624770230,
        "creation_date": 1624078200,
        "question_id": 68043719,
        "link": "https://stackoverflow.com/questions/68043719/run-a-loop-to-generate-variable-names-in-python",
        "closed_reason": "Duplicate",
        "title": "Run a loop to generate variable names in Python",
        "answer_body": "<p>Use the inbuilt <code>glob</code> package</p>\n<pre><code>from glob import glob\n\nfullpath = f'C:\\Users\\siddhn\\Desktop\\phone[1-6].csv'\ndfs = [pd.read_csv(file) for file in glob(fullpath)]\n\nprint(dfs[0])\n</code></pre>\n",
        "question_body": "<p><strong>I'm trying to import datasets which have the following filenames (phone1, phone2, etc)</strong></p>\n<pre><code>df1 = pd.read_csv(r'C:\\Users\\...\\phone1.csv')\ndf2 = pd.read_csv(r'C:\\Users\\...\\phone2.csv')\ndf3 = pd.read_csv(r'C:\\Users\\...\\phone3.csv')\ndf4 = pd.read_csv(r'C:\\Users\\...\\phone4.csv')\ndf5 = pd.read_csv(r'C:\\Users\\...\\phone5.csv')\ndf6 = pd.read_csv(r'C:\\Users\\...\\phone6.csv')\n</code></pre>\n<p><strong>I tried the following code</strong></p>\n<pre><code>for i in range(1, 7):\n    'df'+i = pd.read_csv(r'C:\\Users\\siddhn\\Desktop\\phone'+str(i)+'.csv', engine = 'python')\n</code></pre>\n<p>But I get an error saying that <strong>cannot assign to operator</strong></p>\n<p><strong>How to import the datasets using a loop.?</strong></p>\n"
    },
    {
        "tags": [
            "python",
            "string",
            "pandas",
            "dataframe",
            "correlation"
        ],
        "owner": {
            "user_type": "does_not_exist",
            "display_name": "user10784511"
        },
        "is_answered": true,
        "view_count": 2022,
        "accepted_answer_id": 55397423,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624770069,
        "creation_date": 1553766967,
        "last_edit_date": 1553774693,
        "question_id": 55394673,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/55394673/how-to-find-the-correlation-between-two-strings-in-pandas",
        "title": "How to find the correlation between two strings in pandas",
        "answer_body": "<p>There are a few steps here, the first thing you need to do is extract some sort of vector for each word.</p>\n\n<p>A good way is using gensim word2vec (you need to download the files from <a href=\"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\" rel=\"nofollow noreferrer\">here</a>):</p>\n\n<pre><code>from gensim.models import KeyedVectors\n\nmodel = KeyedVectors.load_word2vec_format('data/GoogleGoogleNews-vectors-negative300.bin', binary=True)\n</code></pre>\n\n<p>after getting the pretrained vectors you need to extract the vector for each word:</p>\n\n<pre><code>vector = model['plant']\n</code></pre>\n\n<p>or in the pandas column example:</p>\n\n<pre><code>df['Vectors'] = df['Keyword'].apply(lambda x: model[x])\n</code></pre>\n\n<p>Once this is done you can calculate the distance between two vectors using a number of methodologies, eg euclidean distance:</p>\n\n<pre><code>from sklearn.metrics.pairwise import euclidean_distances\ndistances = euclidean_distances(list(df['Vectors']))\n</code></pre>\n\n<p>distances will be a matrix, with 0 on the diagonal and the distance of all words from each other. The closer a distance is to 0, the more similar the words are.</p>\n\n<p>You can use different models and different distance metrics, but you can use this as a starting point.</p>\n",
        "question_body": "<p>I have df of string values </p>\n\n<pre><code>   Keyword\n    plant\n    cell\n    cat\n    Pandas\n</code></pre>\n\n<p>And I want to find the relationship or correlation between these two string values.</p>\n\n<p>I have used pandas <code>corr = df1.corrwith(df2,axis=0)</code>.\nBut this is useful for to find the correlation between the numerical values but I want to see whether the two strings are related by finding the correlation distance. How can I do that?</p>\n"
    },
    {
        "tags": [
            "python",
            "for-loop",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 5401746,
            "reputation": 3385,
            "user_id": 4301459,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://i.stack.imgur.com/85S0R.png?s=128&g=1",
            "display_name": "Blue Moon",
            "link": "https://stackoverflow.com/users/4301459/blue-moon"
        },
        "is_answered": true,
        "view_count": 293724,
        "accepted_answer_id": 31675177,
        "answer_count": 5,
        "score": 80,
        "last_activity_date": 1624768265,
        "creation_date": 1438081774,
        "last_edit_date": 1438082468,
        "question_id": 31674557,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/31674557/how-to-append-rows-in-a-pandas-dataframe-in-a-for-loop",
        "title": "How to append rows in a pandas dataframe in a for loop?",
        "answer_body": "<p>Suppose your data looks like this:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\nnp.random.seed(2015)\ndf = pd.DataFrame([])\nfor i in range(5):\n    data = dict(zip(np.random.choice(10, replace=False, size=5),\n                    np.random.randint(10, size=5)))\n    data = pd.DataFrame(data.items())\n    data = data.transpose()\n    data.columns = data.iloc[0]\n    data = data.drop(data.index[[0]])\n    df = df.append(data)\nprint('{}\\n'.format(df))\n# 0   0   1   2   3   4   5   6   7   8   9\n# 1   6 NaN NaN   8   5 NaN NaN   7   0 NaN\n# 1 NaN   9   6 NaN   2 NaN   1 NaN NaN   2\n# 1 NaN   2   2   1   2 NaN   1 NaN NaN NaN\n# 1   6 NaN   6 NaN   4   4   0 NaN NaN NaN\n# 1 NaN   9 NaN   9 NaN   7   1   9 NaN NaN\n</code></pre>\n<p>Then it could be replaced with</p>\n<pre><code>np.random.seed(2015)\ndata = []\nfor i in range(5):\n    data.append(dict(zip(np.random.choice(10, replace=False, size=5),\n                         np.random.randint(10, size=5))))\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre>\n<p>In other words, do not form a new DataFrame for each row. Instead, collect all the data in a list of dicts, and then call <code>df = pd.DataFrame(data)</code> once at the end, outside the loop.</p>\n<p>Each call to <code>df.append</code> requires allocating space for a new DataFrame with one extra row, copying all the data from the original DataFrame into the new DataFrame, and then copying data into the new row. All that allocation and copying makes calling <code>df.append</code> in a loop very inefficient. The time cost of copying <a href=\"http://stackoverflow.com/a/36489724/190597\">grows quadratically</a> with the number of rows. Not only is the call-DataFrame-once code easier to write, its performance will be much better -- the time cost of copying grows linearly with the number of rows.</p>\n",
        "question_body": "<p>I have the following for loop:</p>\n\n<pre><code>for i in links:\n     data = urllib2.urlopen(str(i)).read()\n     data = json.loads(data)\n     data = pd.DataFrame(data.items())\n     data = data.transpose()\n     data.columns = data.iloc[0]\n     data = data.drop(data.index[[0]])\n</code></pre>\n\n<p>Each dataframe so created has most columns in common with the others but not all of them. Moreover, they all have just one row. What I need to to is to add to the dataframe all the distinct columns and each row from each dataframe produced by the for loop</p>\n\n<p>I tried pandas concatenate or similar but nothing seemed to work. Any idea? Thanks.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "duplicates"
        ],
        "owner": {
            "account_id": 2323269,
            "reputation": 4611,
            "user_id": 2038360,
            "user_type": "registered",
            "accept_rate": 66,
            "profile_image": "https://i.stack.imgur.com/4Xgmo.png?s=128&g=1",
            "display_name": "gabboshow",
            "link": "https://stackoverflow.com/users/2038360/gabboshow"
        },
        "is_answered": true,
        "view_count": 21275,
        "accepted_answer_id": 47181183,
        "answer_count": 2,
        "score": 12,
        "last_activity_date": 1624756670,
        "creation_date": 1510147697,
        "last_edit_date": 1516033172,
        "question_id": 47180983,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/47180983/find-duplicate-rows-in-a-pandas-dataframe",
        "title": "find duplicate rows in a pandas dataframe",
        "answer_body": "<p>Use <code>groupby</code>, create a new column of indexes, and then call <code>duplicated</code>:</p>\n\n<pre><code>df['index_original'] = df.groupby(['col1', 'col2']).col1.transform('idxmin')    \ndf[df.duplicated(subset=['col1','col2'], keep='first')]\n\n   col1  col2  index_original\n2     1     2               0\n4     1     2               0\n</code></pre>\n\n<hr>\n\n<p><strong>Details</strong></p>\n\n<p>I <code>groupby</code> first two columns and then call <code>transform</code> + <code>idxmin</code> to get the first index of each group.</p>\n\n<pre><code>df.groupby(['col1', 'col2']).col1.transform('idxmin') \n\n0    0\n1    1\n2    0\n3    3\n4    0\nName: col1, dtype: int64\n</code></pre>\n\n<p><code>duplicated</code> gives me a boolean mask of values I want to keep:</p>\n\n<pre><code>df.duplicated(subset=['col1','col2'], keep='first')\n\n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n</code></pre>\n\n<p>The rest is just <em>boolean indexing</em>.</p>\n",
        "question_body": "<p>I am trying to find duplicates rows in a pandas dataframe.</p>\n\n<pre><code>df=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\n\ndf\nOut[15]: \n   col1  col2\n0     1     2\n1     3     4\n2     1     2\n3     1     4\n4     1     2\n\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\nduplicate = df.loc[duplicate_bool == True]\n\nduplicate\nOut[16]: \n   col1  col2\n2     1     2\n4     1     2\n</code></pre>\n\n<p>Is there a way to add a column referring to the index of the first duplicate (the one kept)</p>\n\n<pre><code>duplicate\nOut[16]: \n   col1  col2  index_original\n2     1     2               0\n4     1     2               0\n</code></pre>\n\n<p>Note: df could be very very big in my case....</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "matplotlib"
        ],
        "owner": {
            "account_id": 21213172,
            "reputation": 19,
            "user_id": 15601343,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/971b26dade4e1c1ecf4a437f09be107f?s=128&d=identicon&r=PG&f=1",
            "display_name": "iXXIX",
            "link": "https://stackoverflow.com/users/15601343/ixxix"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68147083,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1624753995,
        "creation_date": 1624747436,
        "last_edit_date": 1624747819,
        "question_id": 68146776,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68146776/plot-groupby-percentage-dataframe",
        "title": "Plot groupby percentage dataframe",
        "answer_body": "<p>IIUC let's try with some sample data:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(5)\nn = 100\ndf = pd.DataFrame({'user': np.random.choice(list(&quot;ABCD&quot;), size=n),\n                   'answer': np.random.choice([1, 0, -1], size=n)})\n</code></pre>\n<p><code>df.head()</code>:</p>\n<pre><code>  user  answer\n0    D       1\n1    C       0\n2    D      -1\n3    B       1\n4    C       1\n</code></pre>\n<hr />\n<h3>Option 1</h3>\n<p>Filter out the <code>-1</code> values and use named aggregation to get the &quot;good answers&quot; and &quot;total answers&quot;:</p>\n<pre><code>plot_df = df[df['answer'].ne(-1)].groupby('user').aggregate(\n    good_answer=('answer', 'sum'),\n    total_answer=('answer', 'size')\n)\n</code></pre>\n<p><code>plot_df</code>:</p>\n<pre><code>      good_answer  total_answer\nuser                           \nA               9            15\nB              11            20\nC              15            19\nD               7            14\n</code></pre>\n<p>Use division and multiplication to get percentage:</p>\n<pre><code>plot_df['pct'] = (plot_df['good_answer'] / plot_df['total_answer'] * 100)\n</code></pre>\n<p><code>plot_df</code>:</p>\n<pre><code>      good_answer  total_answer        pct\nuser                                      \nA               9            15  60.000000\nB              11            20  55.000000\nC              15            19  78.947368\nD               7            14  50.000000\n</code></pre>\n<p>Then this can be plotted with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html#pandas-dataframe-plot\" rel=\"nofollow noreferrer\"><code>DataFrame.plot</code></a>:</p>\n<pre><code>ax = plot_df.plot(\n    y='pct', kind='bar', rot=0,\n    title='Percentage of Good Answers',\n    ylim=[0, 100],\n    label='Percent Good'\n)\n\n# Add Labels on Top of Bars\nfor container in ax.containers:\n    ax.bar_label(container, fmt='%.2f%%')\n\nplt.show()\n</code></pre>\n<hr />\n<h3>Option 2</h3>\n<p>If <em>just</em> the percentage is needed <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.mean.html#pandas-core-groupby-groupby-mean\" rel=\"nofollow noreferrer\"><code>groupby mean</code></a> can be used to get to the resulting plot directly after filtering out the <code>-1</code>s:</p>\n<pre><code>plot_df = df[df['answer'].ne(-1)].groupby('user')['answer'].mean().mul(100)\n\nax = plot_df.plot(\n    kind='bar', rot=0,\n    title='Percentage of Good Answers',\n    ylim=[0, 100],\n    label='Percent Good'\n)\n# Add Labels on Top of Bars\nfor container in ax.containers:\n    ax.bar_label(container, fmt='%.2f%%')\n\nplt.show()\n</code></pre>\n<p><code>plot_df</code>:</p>\n<pre><code>         answer\nuser           \nA     60.000000\nB     55.000000\nC     78.947368\nD     50.000000\n</code></pre>\n<hr />\n<p>Both options Produce:</p>\n<p><a href=\"https://i.stack.imgur.com/ElebT.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ElebT.png\" alt=\"plot\" /></a></p>\n<hr />\n<p>All Together:</p>\n<pre><code>import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(5)\nn = 100\ndf = pd.DataFrame({'user': np.random.choice(list(&quot;ABCD&quot;), size=n),\n                   'answer': np.random.choice([1, 0, -1], size=n)})\n\nplot_df = df[df['answer'].ne(-1)].groupby('user').aggregate(\n    good_answer=('answer', 'sum'),\n    total_answer=('answer', 'size')\n)\nplot_df['pct'] = (plot_df['good_answer'] / plot_df['total_answer'] * 100)\n\nax = plot_df.plot(\n    y='pct', kind='bar', rot=0,\n    title='Percentage of Good Answers',\n    ylim=[0, 100],\n    label='Percent Good'\n)\n\n# Add Labels on Top of Bars\nfor container in ax.containers:\n    ax.bar_label(container, fmt='%.2f%%')\n\nplt.show()\n</code></pre>\n",
        "question_body": "<p>I didn't  find a complete answer to what i want to do:</p>\n<p>I have a dataframe. I want to group by user and their answers to a survey, sum all of their good answers/total of their answers, display it in % and plot the result.</p>\n<p>I have an answer column which contains : 1,0 or -1. I want to filter it in order to exclude -1.</p>\n<p>Here is what i did so far :</p>\n<pre><code>df_sample.groupby('user').filter(lambda x : x['answer'].mean() &gt;-1)\n\n</code></pre>\n<p>or :</p>\n<pre><code>a = df_sample.loc[df_sample['answer']!=-1,['user','answer']]\nb = a.groupby(['user','answer']).agg({'answer' : 'sum'})\n</code></pre>\n<p>See it's uncomplete. Thank you for any suggestion that you may have.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 15701822,
            "reputation": 139,
            "user_id": 11329736,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/70297905da5778f5945e8dfe6b8abaf4?s=128&d=identicon&r=PG&f=1",
            "display_name": "justinian482",
            "link": "https://stackoverflow.com/users/11329736/justinian482"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68146504,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1624745522,
        "creation_date": 1624744233,
        "question_id": 68146479,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68146479/add-data-from-one-column-to-another-column-on-every-other-row",
        "title": "Add data from one column to another column on every other row",
        "answer_body": "<p>I think you're looking for <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.stack.html#pandas-dataframe-stack\" rel=\"nofollow noreferrer\"><code>DataFrame.stack()</code></a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df2[&quot;column&quot;] = df1.stack().reset_index(drop=True)\nprint(df2)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>                  column\n0  ABL1_sgABL1_130854834\n1   CTTAGGCTATAATCACAATG\n2  ABL1_sgABL1_130862824\n3   GGTTCATCATCATTCAACGG\n4  ABL1_sgABL1_130872883\n5   TCAGTGATGATATAGAACGG\n6  ABL1_sgABL1_130884018\n7   TTGCTCCCTCGAAAAGAGCG\n</code></pre>\n",
        "question_body": "<p>I have two data frames:</p>\n<pre><code>import pandas as pd\nimport numpy as np\nsgRNA = pd.Series([&quot;ABL1_sgABL1_130854834&quot;,&quot;ABL1_sgABL1_130862824&quot;,&quot;ABL1_sgABL1_130872883&quot;,&quot;ABL1_sgABL1_130884018&quot;])\nsequence = pd.Series([&quot;CTTAGGCTATAATCACAATG&quot;,&quot;GGTTCATCATCATTCAACGG&quot;,&quot;TCAGTGATGATATAGAACGG&quot;,&quot;TTGCTCCCTCGAAAAGAGCG&quot;])\ndf1=pd.DataFrame(sgRNA,columns=[&quot;sgRNA&quot;])\ndf1[&quot;sequence&quot;]=sequence\n\ndf2=pd.DataFrame(columns=[&quot;column&quot;],\n                    index=np.arange(len(df1) * 2))\n</code></pre>\n<p>I want to add values from both columns from df1 to df2 every other row, like this:</p>\n<pre><code>ABL1_sgABL1_130854834\nCTTAGGCTATAATCACAATG\nABL1_sgABL1_130862824\nGGTTCATCATCATTCAACGG\nABL1_sgABL1_130872883\nTCAGTGATGATATAGAACGG\nABL1_sgABL1_130884018\nTTGCTCCCTCGAAAAGAGCG\n</code></pre>\n<p>To do this for <code>df1[&quot;sgRNA&quot;]</code> I used this code:</p>\n<pre><code>df2.iloc[0::2, :]=df1[&quot;sgRNA&quot;]\n</code></pre>\n<p>But I get this error:\n<code>ValueError: could not broadcast input array from shape (4,) into shape (4,1)</code>.\nWhat am I doing wrong?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20784477,
            "reputation": 18,
            "user_id": 15264040,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-7W9G7MbqfSk/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucm4ksvD502HMauzBjhEmwdLdboJ8w/s96-c/photo.jpg?sz=128",
            "display_name": "Ashleigh Wang",
            "link": "https://stackoverflow.com/users/15264040/ashleigh-wang"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 68145929,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624738412,
        "creation_date": 1624716425,
        "last_edit_date": 1624717042,
        "question_id": 68143099,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68143099/read-a-txt-into-a-dataframe-with-only-field-locations-act-as-delimiter",
        "title": "Read a txt into a DataFrame with only field locations act as delimiter",
        "answer_body": "<p>Use <code>read_fwf</code> to read your file and <code>colspecs</code> parameters like this:</p>\n<pre><code>positions = [1, 11, 71, 73, 74, 82, 86, 87, 91, 93, 97, 98, 99,\n             100, 150, 160, 162, 212, 242, 257, 327, 328, 339, 348]\nstart = [0] + positions[1:]\nstop = positions[1:] + [-1]\n\ndf = pd.read_fwf('file.txt', colspecs=list(zip(start, stop)), header=None)\n</code></pre>\n<pre><code>&gt;&gt;&gt; df.T  # transpose for a better display\n                 0\n0      DIDIDI12354\n1        John, Doe\n2               @@\n3                4\n4         7385y437\n5             85yH\n6                U\n7             IBBI\n8               BJ\n9             IBIB\n10               U\n11               B\n12               K\n13  @@SUBUKBCSHB77\n14      NUIKBHUS12\n15              34\n16             NaN\n17          Unit 7\n18              45\n19    NON'EXIST RD\n20               A\n21             NaN\n22             NaN\n23             NaN\n</code></pre>\n",
        "question_body": "<p>I am trying to read a txt file into a DataFrame using <code>pd.read_csv()</code>.</p>\n<p>However I soon realized that I cannot use a simple delimiter. The txt file is formatted as shown below, each of the fields could contain a comma, space, or other characters that usually act as a delimiter.</p>\n<p>The only way to separate the fields seems to be using the position provided in the picture.</p>\n<p>I wonder if there's a way that I can perform the task?</p>\n<p>Thanks!</p>\n<p><a href=\"https://i.stack.imgur.com/bbelT.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/bbelT.png\" alt=\"enter image description here\" /></a></p>\n<p>Edit:\n<strong>An example line would be like</strong>:</p>\n<pre><code>DIDIDI12354John, Doe                                                   @@47385y43785yHUIBBIBJIBIBUBK@@SUBUKBCSHB77                                    NUIKBHUS1234                                                  Unit 7                        45             NON'EXIST RD                                                          A                             \n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21757516,
            "reputation": 99,
            "user_id": 16062635,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ead3dfd6f9198dfc2734ca718f3e6883?s=128&d=identicon&r=PG&f=1",
            "display_name": "Harry Jones",
            "link": "https://stackoverflow.com/users/16062635/harry-jones"
        },
        "is_answered": true,
        "view_count": 81,
        "accepted_answer_id": 68145531,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1624734595,
        "creation_date": 1624678783,
        "question_id": 68139013,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68139013/combine-multiple-columns-of-text-of-multiple-rows-in-pandas",
        "title": "Combine multiple columns of text of multiple rows in pandas",
        "answer_body": "<p>Use <code>groupby</code> with <code>sort=False</code> and <code>Series.unique</code>. After that join back and use list comprehension to construct string out of dict and assign to <code>all</code> column</p>\n<pre><code>l1 = (df[['product','plan']].join(df.groupby(['product', 'plan'], sort=False)\n                                    .transform(lambda x: '-'.join(x.unique()))).to_dict('records'))\ndf['all'] = [';'.join(k+':'+v for k, v in x.items()) for x in l1]\n\nOut[528]:\n     product    brand continent plan                                                       all\n0     laptop       lg      n_am   P1    product:laptop;plan:P1;brand:lg-samsung;continent:n_am\n1     laptop  samsung      n_am   P1    product:laptop;plan:P1;brand:lg-samsung;continent:n_am\n2     laptop    apple        eu   P3           product:laptop;plan:P3;brand:apple;continent:eu\n3         tv       lg        eu   P3                  product:tv;plan:P3;brand:lg;continent:eu\n4         tv  samsung        eu   P2  product:tv;plan:P2;brand:samsung-apple;continent:eu-n_am\n5         tv    apple      n_am   P2  product:tv;plan:P2;brand:samsung-apple;continent:eu-n_am\n6         tv  samsung      n_am   P1           product:tv;plan:P1;brand:samsung;continent:n_am\n7  cellphone       lg        eu   P3           product:cellphone;plan:P3;brand:lg;continent:eu\n8  cellphone    apple      n_am   P2      product:cellphone;plan:P2;brand:apple;continent:n_am\n9  cellphone    apple        eu   P1        product:cellphone;plan:P1;brand:apple;continent:eu\n</code></pre>\n",
        "question_body": "<blockquote>\n<p>I want to combine multiple columns of text from multiple row groups. I\nknow that the join joins into a single cell but it doesn't read by\nproduct. I want a single cell to have the information for that product\nin different brands and continents by the plan.\nFor example: Within the laptop product what are the brands and\ncontinents in plan 1, in plan 2 , 3 and so on?</p>\n</blockquote>\n<pre><code>df['all'] = df[['product', 'brand', ...]].agg('-'.join, axis=1) \n</code></pre>\n<blockquote>\n<p>Here the dataframe:</p>\n</blockquote>\n<pre><code>product     brand     continent    plan\nlaptop         lg       n_am         P1\nlaptop    samsung       n_am         P1\nlaptop      apple         eu         P3\ntv             lg         eu         P3\ntv        samsung         eu         P2\ntv          apple       n_am         P2\ntv        samsung       n_am         P1\ncellphone      lg         eu         P3\ncellphone   apple       n_am         P2\ncellphone   apple         eu         P1\n</code></pre>\n<blockquote>\n<p>Expected dataframe:</p>\n</blockquote>\n<pre><code>product     brand     continent    plan   all\nlaptop         lg       n_am         P1   product: laptop; plan: P1; brand: lg-samsung; continent: n_am\nlaptop    samsung       n_am         P1   product: laptop; plan: P1; brand: lg-samsung; continent: n_am\nlaptop      apple         eu         P3   product: laptop; plan: P3; brand: apple; continent: eu\ntv             lg         eu         P3   product: tv;   plan: P3; brand: lg; continent: eu\ntv        samsung         eu         P2   product: tv;   plan: P2; brand: samsung-apple; continent: n_am-eu\ntv          apple       n_am         P2   product: tv;   plan: P2; brand: samsung-apple; continent: n_am-eu\ntv        samsung       n_am         P1   product: tv;   plan: P1; brand: samsung; continent: n_am\ncellphone      lg         eu         P3   product: cellphone; plan: P3; brand: lg; continent: eu\ncellphone   apple       n_am         P2   product: cellphone; plan: P2; brand: apple; continent: n_am\ncellphone   apple         eu         P1   product: cellphone; plan: P1; brand: apple; continent: eu\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 451722,
            "reputation": 13084,
            "user_id": 847773,
            "user_type": "registered",
            "accept_rate": 69,
            "profile_image": "https://i.stack.imgur.com/IraOK.jpg?s=128&g=1",
            "display_name": "Borut Flis",
            "link": "https://stackoverflow.com/users/847773/borut-flis"
        },
        "is_answered": true,
        "view_count": 83,
        "accepted_answer_id": 68140886,
        "answer_count": 4,
        "score": 5,
        "last_activity_date": 1624729261,
        "creation_date": 1624697054,
        "last_edit_date": 1624697215,
        "question_id": 68140665,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68140665/drop-all-rows-that-have-all-na-values-after-last-row-that-is-not-na",
        "title": "Drop all rows that have all NA values after last row that is not NA",
        "answer_body": "<p>You can get the index of the last row that have at least one value not NaN and just slice the dataset until that point:</p>\n<pre><code>df=df.replace('None', np.nan)\nids = df[df.notnull().any(axis=1)].index\nlast_id = ids[-1]\n\nres = df.loc[:last_id, :]\n\nprint(res)\n</code></pre>\n<p>Output:</p>\n<pre><code>     0   1   2   3        4\n0  2.0 NaN NaN NaN  21041.0\n1  1.0 NaN NaN NaN   3003.0\n2  2.0 NaN NaN NaN   1210.0\n3  NaN NaN NaN NaN      NaN\n4  2.0 NaN NaN NaN      NaN\n</code></pre>\n",
        "question_body": "<pre><code>          0     1     2     3        4  \n0        2.0  None  None  None  21041.0  \n1        1.0  None  None  None   3003.0  \n2        2.0  None  None  None   1210.0  \n3        NaN  None  None  None      NaN  \n4        2    None  None  None      NaN \n5        NaN  None  None  None      NaN\n6        NaN  None  None  None      NaN  \n</code></pre>\n<p>So I would drop 5 and 6 but keep 3, even though all values are NaN.</p>\n<p>I know of:</p>\n<pre><code>df.dropna(axis = 0, how = 'all', inplace = True)\n</code></pre>\n<p>This would delete 3 as well. I guess I need to combine with some other operation.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "csv",
            "duplicates"
        ],
        "owner": {
            "account_id": 21764439,
            "reputation": 25,
            "user_id": 16068550,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d609a30b0ff52393d273cc9242a091ad?s=128&d=identicon&r=PG&f=1",
            "display_name": "buckenup",
            "link": "https://stackoverflow.com/users/16068550/buckenup"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 68141674,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624727210,
        "creation_date": 1624703976,
        "last_edit_date": 1624727210,
        "question_id": 68141498,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68141498/pandas-dropping-duplicates-doesnt-drop-last-duplicate",
        "title": "Pandas dropping duplicates doesn&#39;t drop last duplicate",
        "answer_body": "<p>Because of <code>mode='a'</code> you can't remove previous duplicates after several execution of your function. Here is a code for your expected behaviour:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom datetime import datetime\n\n\ndef date_to_csv(): \n     df = pd.read_csv('test.csv') \n     df = df.append({'Date': str(datetime.now().date()), 'Price': randint(1, 100)}, ignore_index=True) \n     df.to_csv('test.csv', index=False) \n</code></pre>\n",
        "question_body": "<p>Setting keep=False should remove all duplicates but if I run my function is still returns a duplicate of the previous row</p>\n<pre><code>def date_to_csv():\n   import pandas as pd\n   from random import randint\n   df = pd.read_csv(&quot;test.csv&quot;)\n   df = df.append({'Date': datetime.date.today(), 'Price': randint(1,100)}, ignore_index=True)\n   result_df = df.drop_duplicates(keep=False)\n   result_df.to_csv('test.csv', mode='a', index=False, header=None)\n</code></pre>\n<p>If my csv file is empty with only the column headers 'Date' and 'Price' and I run my function 3 times it returns this in csv:</p>\n<pre><code>Date,Price\n2021-06-26,74\n2021-06-26,74\n2021-06-26,51\n2021-06-26,51\n2021-06-26,13\n</code></pre>\n<p>When I expect it to return something like this:</p>\n<pre><code>Date,Price\n2021-06-26,74\n2021-06-26,51\n2021-06-26,13\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "list",
            "pandas",
            "dataframe",
            "text"
        ],
        "owner": {
            "account_id": 6132206,
            "reputation": 1563,
            "user_id": 4782060,
            "user_type": "registered",
            "accept_rate": 54,
            "profile_image": "https://www.gravatar.com/avatar/3f8db094db7536fc8f5826de08973831?s=128&d=identicon&r=PG&f=1",
            "display_name": "jartymcfly",
            "link": "https://stackoverflow.com/users/4782060/jartymcfly"
        },
        "is_answered": true,
        "view_count": 12244,
        "accepted_answer_id": 50904575,
        "answer_count": 4,
        "score": 4,
        "last_activity_date": 1624719755,
        "creation_date": 1529306573,
        "question_id": 50904444,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/50904444/how-can-i-convert-a-pandas-dataframe-from-a-raw-text-in-python",
        "title": "How can I convert a pandas dataframe from a raw text in Python?",
        "answer_body": "<p>code:</p>\n\n<pre><code>df = [\n    'Timestamp;T;Pressure [bar];Input line pressure [bar];Speed [rpm];Angular Position [degree];Wheel speed [rpm];Wheel angular position [degree];',\n    ';1;5,281;5,303;219,727;10,283;216,363;45;',\n    ';1;5,273;5,277;219,727;11,602;216,363;45;',\n    ';1;5,288;5,293;205,078;12,832;216,363;45;',\n    ';1;5,316;5,297;219,727;14,15;216,363;45;',\n    ';1;5,314;5,307;219,727;15,469;216,363;45;',\n    ';1;5,288;5,3;219,727;16,787;216,363;45;',\n    ';1;5,318000000000001;5,31;219,727;18,105;216,363;45;',\n    ';1;5,304;5,3;219,727;19,424;216,388;56,25;',\n    ';1;5,291;5,29;219,947;20,742;216,388;56,25;',\n    ';1;5,316;5,297;219,507;22,061;216,388;56,25;']\n\nmat = [n.split(';') for n in df]\nprint(mat)\nnewdf1 = pd.DataFrame(mat)\nnewdf1.columns = newdf1.iloc[0]\nnewdf1 = newdf1.reindex(newdf1.index.drop(0))\n# newdf2 = pd.DataFrame.from_dict(df)\nprint(newdf1)\n</code></pre>\n\n<p>output:</p>\n\n<pre><code>0  Timestamp  T     Pressure [bar] Input line pressure [bar] Speed [rpm]  \\\n1             1              5,281                     5,303     219,727   \n2             1              5,273                     5,277     219,727   \n3             1              5,288                     5,293     205,078   \n4             1              5,316                     5,297     219,727   \n5             1              5,314                     5,307     219,727   \n6             1              5,288                       5,3     219,727   \n7             1  5,318000000000001                      5,31     219,727   \n8             1              5,304                       5,3     219,727   \n9             1              5,291                      5,29     219,947   \n10            1              5,316                     5,297     219,507   \n\n0  Angular Position [degree] Wheel speed [rpm]  \\\n1                     10,283           216,363   \n2                     11,602           216,363   \n3                     12,832           216,363   \n4                      14,15           216,363   \n5                     15,469           216,363   \n6                     16,787           216,363   \n7                     18,105           216,363   \n8                     19,424           216,388   \n9                     20,742           216,388   \n10                    22,061           216,388   \n\n0  Wheel angular position [degree]    \n1                               45    \n2                               45    \n3                               45    \n4                               45    \n5                               45    \n6                               45    \n7                               45    \n8                            56,25    \n9                            56,25    \n10                           56,25 \n</code></pre>\n",
        "question_body": "<p>I have a text file containing data like this, formatted in a list, where the first element is a string containing the column names sepparated by ';', and the next elements are the value rows:</p>\n\n<pre><code>['Timestamp;T;Pressure [bar];Input line pressure [bar];Speed [rpm];Angular Position [degree];Wheel speed [rpm];Wheel angular position [degree];',\n';1;5,281;5,303;219,727;10,283;216,363;45;',\n';1;5,273;5,277;219,727;11,602;216,363;45;',\n';1;5,288;5,293;205,078;12,832;216,363;45;',\n';1;5,316;5,297;219,727;14,15;216,363;45;',\n';1;5,314;5,307;219,727;15,469;216,363;45;',\n';1;5,288;5,3;219,727;16,787;216,363;45;',\n';1;5,318000000000001;5,31;219,727;18,105;216,363;45;',\n';1;5,304;5,3;219,727;19,424;216,388;56,25;',\n';1;5,291;5,29;219,947;20,742;216,388;56,25;',\n';1;5,316;5,297;219,507;22,061;216,388;56,25;']\n</code></pre>\n\n<p>How can I convert this list of text into a pandas dataframe?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21987595,
            "reputation": 51,
            "user_id": 16261585,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5f94e4c603f799cbd63a6c0dab575576?s=128&d=identicon&r=PG&f=1",
            "display_name": "gato",
            "link": "https://stackoverflow.com/users/16261585/gato"
        },
        "is_answered": true,
        "view_count": 37,
        "closed_date": 1624709196,
        "accepted_answer_id": 68142231,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624716284,
        "creation_date": 1624708980,
        "question_id": 68142161,
        "link": "https://stackoverflow.com/questions/68142161/how-to-create-new-column-based-on-first-column-taking-into-account-size-of-lette",
        "closed_reason": "Duplicate",
        "title": "How to create new column based on first column taking into account size of letter and list in Python Pandas?",
        "answer_body": "<p>You can try via <code>str.title()</code>,<code>str.contains()</code> and <code>astype()</code> method:</p>\n<pre><code>df['col2']=df['col1'].str.title().str.contains('|'.join(bad_list)).astype(int)\n</code></pre>\n<p>output of <code>df</code>:</p>\n<pre><code>    col1            col2\n0   John Simon prd  0\n1   agc Ann White   0\n2   BeN and Ann     1  \n</code></pre>\n<p><strong>Step by step breakdown of code:</strong></p>\n<p>Since your list i.e bad_list contains word in format(Ist word is capital and rest all small) so we convert whole Series('col1') like that by using <code>Series.str.title()</code> so now the Series('col1') looks like:</p>\n<pre><code>0    John Simon Prd\n1     Agc Ann White\n2       Ben And Ann\nName: col1, dtype: object\n</code></pre>\n<p>Then we use <code>str.contains()</code> that gives us a boolean series after checking if any of the element inside bad_list is present in the row of Series('col1'):</p>\n<pre><code>0    False\n1    False\n2     True\nName: col1, dtype: bool\n</code></pre>\n<p><strong>Note:</strong></p>\n<p>here the code inside <code>contains()</code> method:</p>\n<pre><code>'|'.join(bad_list)\n#giving you a string(output of above code):\n'Ben|Wayne'\n</code></pre>\n<p>Finally we are typecasting boolean Series to int via <code>astype()</code> method:</p>\n<pre><code>0    0\n1    0\n2    1\nName: col1, dtype: int32\n</code></pre>\n<p><strong>OR</strong></p>\n<p>another way is to use <code>IGNORECASE</code> flag from <code>re</code> module as suggested by @seanbean in comments:</p>\n<pre><code>from re import IGNORECASE\n\ndf['col2']=df['col1'].str.contains('|'.join(bad_list), flags=IGNORECASE).astype(int)\n</code></pre>\n",
        "question_body": "<p>I have DataFrame in Python Pandas like below:</p>\n<pre><code>col1\n--------\nJohn Simon prd\nagc Ann White\nBeN and Ann\n\nbad_list = [&quot;Ben&quot;, &quot;Wayne&quot;]\n</code></pre>\n<p>And I need to ake something like: create new column &quot;col2&quot; and if value in &quot;col1&quot; has value from bad_list give &quot;1&quot; in &quot;col2&quot; for this row and 0 if not.</p>\n<p><em><strong>Be aware thatn size of letter in bad_list and &quot;col1&quot; should be ignored, for example in &quot;col1&quot; is value &quot;BeN&quot; and on bad_list is &quot;Ben&quot; so it should also have value &quot;1&quot; in &quot;col2&quot;</strong></em></p>\n<p>So as a result based on Data Frame and conditions above I need as a result like below:</p>\n<pre><code>col1            | col2\n----------------|------\nJohn Simon prd  |0\narc Ann White   |0\nBeN and Ann     |1\n</code></pre>\n<p>last row in &quot;col2&quot; has value &quot;1&quot; because &quot;Ben&quot; is on bad_list nevermind that in &quot;col1&quot; is written as BeN.\nHow can I do that in Python Pandas?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20282623,
            "reputation": 47,
            "user_id": 14875983,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/db10dbee6bf2c5915d68177ca551926b?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mohamed",
            "link": "https://stackoverflow.com/users/14875983/mohamed"
        },
        "is_answered": true,
        "view_count": 63,
        "accepted_answer_id": 67451411,
        "answer_count": 5,
        "score": 0,
        "last_activity_date": 1624700188,
        "creation_date": 1620501583,
        "last_edit_date": 1620507082,
        "question_id": 67451313,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67451313/how-to-change-the-boolean-operators-t-f-to-zeros-and-ones-without-any-changing",
        "title": "How to change the Boolean operators (t/f) to zeros and ones without any changing in the missing values (NaN)?",
        "answer_body": "<p>Why don't you use the <code>boolean</code> dtype with its three states (True, False, NA) and take advantage of boolean masks?</p>\n<pre><code>b_cols = [col for col in df if set([&quot;f&quot;, &quot;t&quot;]).intersection(df[col].unique())]\nd_cols = df.select_dtypes(&quot;object&quot;).columns.difference(b_cols).tolist()\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>df[b_cols] = df[b_cols].replace({&quot;f&quot;: False, &quot;t&quot;: True}).astype(&quot;boolean&quot;)\nprint(df)\n\n     X1     X2     X3 X4\n0  3030  False   True  a\n1  3456   True    NaN  b\n2  3567  False  False  a\n3  4568  False   True  b\n\n\ndf = pd.get_dummies(df, columns=d_cols)\nprint(df)\n\n     X1     X2     X3  X4_a  X4_b\n0  3030  False   True     1     0\n1  3456   True    NaN     0     1\n2  3567  False  False     1     0\n3  4568  False   True     0     1\n</code></pre>\n",
        "question_body": "<p>I need to convert all columns in a dataframe (3700 rows x 22 columns) which including Boolean operators: true and false (t and f) to Zeros and ones with ignoring the missing values. Please note that I read the data from csv file.</p>\n<p>For Example:</p>\n<pre class=\"lang-none prettyprint-override\"><code>df = \n\n   X1       X2       X3    X4\n\n 3030       f        t     a\n\n 3456       t       NaN    b\n\n 3567       f        f     a\n\n 4568       f        t     b\n</code></pre>\n<p>After running this code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>feat_cols = []\n\nfor col in df.columns:\n\n    feat_cols.append (col)\n\nprint (feat_cols) \n\nfor feat in feat_cols:\n\n    if ((df[feat]=='f')).any():\n       df[feat] = \n       np.where(df[feat]==&quot;f&quot;,&quot;False&quot;,df[feat])\n    if ((df[feat]=='t')).any():\n        df[feat] = \n        np.where(df[feat]==&quot;t&quot;,&quot;True&quot;,df[feat])\n\nfor feat in df.columns:\n\n    if feat != 'True' and feat!= 'False': \n\n        df_Dummies = pd.get_dummies(df,dummy_na=True)\n    \n</code></pre>\n<p>I got this dataframe:</p>\n<pre class=\"lang-none prettyprint-override\"><code>df = \n\n   X1       X2_False   X2_True      X3_False  X3_True   X3_NaN    X4_a   X4_b    \n \n 3030          1         0            0          1        0        1      0 \n \n 3456          0         1            0          0        1        0      1\n\n 3567          1         0            1          0        0        1      0\n\n 4568          1         0            0          1        0        0      1\n</code></pre>\n<p>and this is the output which I am looking for:</p>\n<pre class=\"lang-none prettyprint-override\"><code>df = \n\n   X1       X2          X3      X4_a    X4_b\n\n 3030       False      True      1       0 \n\n 3456       Ture       NAN       0       1\n\n 3567       False      True      1       0\n\n 4568       False      True      0       1\n</code></pre>\n<p>I need your help in what should I change in the code above to get the dataframe above.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 16450471,
            "reputation": 15,
            "user_id": 11884017,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/0886955864e9fd5cb11ac34a00dce38c?s=128&d=identicon&r=PG&f=1",
            "display_name": "poukpouk",
            "link": "https://stackoverflow.com/users/11884017/poukpouk"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 68140832,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624698581,
        "creation_date": 1624696728,
        "last_edit_date": 1624697315,
        "question_id": 68140630,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68140630/why-the-dataframe-is-altered-using-isin",
        "title": "Why the dataframe is altered using isin?",
        "answer_body": "<p>This is something that Pandas does with large or small numbers. It uses scientific notation. You could fix it by simply round your numbers. You could even add a number between the parentheses of the <code>round</code> method to round to a specific number of decimals.</p>\n<pre><code>n_decimals = 0\ndf = df.round(n_decimals)  # or df = df.round() since zero is default\n</code></pre>\n<p>You could also change the Pandas configuration. Change the number of decimals yourself that Pandas should show.</p>\n<pre><code>pd.set_option('display.float_format', lambda x: '%.5f' % x)\n</code></pre>\n<p>You could also just convert the float numbers to integers when you don't care about decimals.</p>\n<pre><code>for column in [c for c in df.columns if df.startswith('2')]:\n    df.loc[:, column] = df.loc[:, column].apply(int)\n</code></pre>\n<p>More about this <a href=\"https://re-thought.com/how-to-suppress-scientific-notation-in-pandas/\" rel=\"nofollow noreferrer\">Pandas notation</a>.</p>\n",
        "question_body": "<p>I don't understand what is happening. It was working fine, and suddently, it's not.\nI got a dataframe looking like this:</p>\n<p><a href=\"https://i.stack.imgur.com/SZuWZ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/SZuWZ.png\" alt=\"enter image description here\" /></a></p>\n<p>And when I try to filter the indicators, the data is altered to things looking like this:</p>\n<p><a href=\"https://i.stack.imgur.com/DHI33.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/DHI33.png\" alt=\"enter image description here\" /></a></p>\n<p>This is the code I use to filter the indicators and I expect to keep the same data</p>\n<pre><code>dfCountry = data.copy()\ngoodIndicators = ['GDP per capita (current US$)', 'Internet users (per 100 people)', 'other indicators']\ndfCountry = dfCountry[dfCountry[&quot;Indicator Name&quot;].isin(goodIndicators)]\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "emoji"
        ],
        "owner": {
            "account_id": 19144771,
            "reputation": 31,
            "user_id": 14946125,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/1he8s.jpg?s=128&g=1",
            "display_name": "AnandP2812",
            "link": "https://stackoverflow.com/users/14946125/anandp2812"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68138058,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624698552,
        "creation_date": 1624663509,
        "question_id": 68137947,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68137947/getting-a-blank-column-when-trying-to-extract-emojis-from-text",
        "title": "Getting a blank column when trying to extract emojis from text",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import re\nimport emoji\n\npattern = re.compile(r&quot;|&quot;.join(map(re.escape, emoji.UNICODE_EMOJI[&quot;en&quot;])))\n\ndf[&quot;Emojis&quot;] = df[&quot;Sentence&quot;].apply(lambda x: &quot;&quot;.join(pattern.findall(x)))\ndf[&quot;Sentence&quot;] = df[&quot;Sentence&quot;].apply(lambda x: pattern.sub(&quot;&quot;, x))\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>               Sentence  Emojis\n0        You look  good      \ud83d\ude0c\n1              Love you      \u2764\ufe0f\n2   I am so happy today      \ud83e\udd29\n</code></pre>\n",
        "question_body": "<p>I am trying to extract emojis from a sentence and add them into a new column, but when I do it, the new column just contains nothing, in which the emojis are still in the sentence.</p>\n<p>For reference, my dataset looks like this - but contains over 70,000 sentences similar to this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Sentence</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>You look \ud83d\ude0c good</td>\n</tr>\n<tr>\n<td>Love you \u2764\ufe0f</td>\n</tr>\n<tr>\n<td>I am so happy today \ud83e\udd29</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>So far, I have tried <a href=\"https://stackoverflow.com/questions/63762570/extract-emoji-from-series-of-text\">this</a> method:</p>\n<pre><code>import pandas as pd \nimport emoji\n\ndf['emojis'] = df['Sentence'].apply(lambda row: ''.join(c for c in row if c in emoji.UNICODE_EMOJI))\ndf\n</code></pre>\n<p>And <a href=\"https://stackoverflow.com/questions/63762570/extract-emoji-from-series-of-text\">this</a> method:</p>\n<pre><code>def extract_emojis(text):\n    return ''.join(c for c in text if c in emoji.UNICODE_EMOJI)\n\ndf['emojis'] = df['Sentence'].apply(extract_emojis)\ndf\n</code></pre>\n<p>However, when I try them, my final output seems to be this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Sentence</th>\n<th>Emojis</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>You look \ud83d\ude0c good</td>\n<td></td>\n</tr>\n<tr>\n<td>Love you \u2764\ufe0f</td>\n<td></td>\n</tr>\n<tr>\n<td>I am so happy today \ud83e\udd29</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Hence, I want my output to look like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Sentence</th>\n<th>Emojis</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>You look good</td>\n<td>\ud83d\ude0c</td>\n</tr>\n<tr>\n<td>Love you</td>\n<td>\u2764\ufe0f</td>\n</tr>\n<tr>\n<td>I am so happy today</td>\n<td>\ud83e\udd29</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>As well  as that, I have also tried <a href=\"https://stackoverflow.com/questions/67912641/how-to-extract-emojis-from-text-and-then-add-them-to-a-new-column\">this</a> method, which is exactly what I want to do:</p>\n<pre><code>import pandas as pd\nimport emoji as emj\n\ndef extract_emoji(df):\n    df[&quot;emoji&quot;] = &quot;&quot;\n    for index, row in df.iterrows():\n        for emoji in EMOJIS:\n            if emoji in row[&quot;Sentence&quot;]:\n                row[&quot;Sentence&quot;] = row[&quot;Sentence&quot;].replace(emoji, &quot;&quot;)\n                row[&quot;emoji&quot;] += emoji\n\nextract_emoji(df)\nprint(df.to_string())\n</code></pre>\n<p>Though, with the method above, the code does not seem to fully execute, and I think it cannot handle so many rows in the dataset; hence, I have over 70,000 sentences, which need the emojis extracting.</p>\n<p>As you can see, I am nearly there, but not fully.</p>\n<p>These three methods have not fully worked for me, and I require some additional help.</p>\n<p>In summary, I just want to extract the emojis from each sentence and add them into a new column - if this is possible.</p>\n<p>Thank you very much.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "html-parsing"
        ],
        "owner": {
            "account_id": 8710393,
            "reputation": 288,
            "user_id": 6516735,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/87d0c0185695948dc5631da81a776df4?s=128&d=identicon&r=PG",
            "display_name": "schaefferda",
            "link": "https://stackoverflow.com/users/6516735/schaefferda"
        },
        "is_answered": true,
        "view_count": 21189,
        "accepted_answer_id": 38486953,
        "answer_count": 2,
        "score": 15,
        "last_activity_date": 1624698130,
        "creation_date": 1469033731,
        "question_id": 38486477,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/38486477/get-html-table-into-pandas-dataframe-not-list-of-dataframe-objects",
        "title": "Get HTML table into pandas Dataframe, not list of dataframe objects",
        "answer_body": "<p>From <a href=\"http://pandas.pydata.org/pandas-docs/version/0.17.1/io.html#io-read-html\" rel=\"noreferrer\">http://pandas.pydata.org/pandas-docs/version/0.17.1/io.html#io-read-html</a>,  \"read_html returns a list of DataFrame objects, even if there is only a single table contained in the HTML content\".</p>\n\n<p>So <code>df = df[0].dropna(axis=0, thresh=4)</code> should do what you want.</p>\n",
        "question_body": "<p>I apologize if this question has been answered elsewhere but I have been unsuccessful in finding a satisfactory answer here or elsewhere.</p>\n\n<p>I am somewhat new to python and pandas and having some difficulty getting HTML data into a pandas dataframe. In the pandas documentation it says .read_html() returns a list of dataframe objects, so when I try to do some data manipulation to get rid of the some samples I get an error.</p>\n\n<p>Here is my code to read the HTML:</p>\n\n<pre><code>df = pd.read_html('http://espn.go.com/nhl/statistics/player/_/stat/points/sort/points/year/2015/seasontype/2', header = 1)\n</code></pre>\n\n<p>Then I try to clean it up:</p>\n\n<pre><code>df = df.dropna(axis=0, thresh=4)\n</code></pre>\n\n<p>And I received the following error:</p>\n\n<pre><code>Traceback (most recent call last): File \"module4.py\", line 25, in\n&lt;module&gt; df = df.dropna(axis=0, thresh=4) AttributeError: 'list'\nobject has no attribute 'dropna'\n</code></pre>\n\n<p>How do I get this data into an actual dataframe, similar to what .read_csv() does?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nltk",
            "lemmatization"
        ],
        "owner": {
            "account_id": 21208294,
            "reputation": 5,
            "user_id": 15597461,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4f4d07cf4229165a32ea1583811ba2b9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Nicole",
            "link": "https://stackoverflow.com/users/15597461/nicole"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 68140312,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624695359,
        "creation_date": 1624693315,
        "last_edit_date": 1624695359,
        "question_id": 68140256,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68140256/how-can-i-lemmatize-a-tokenized-column-of-a-dataframe-in-python",
        "title": "How can I lemmatize a tokenized column of a dataframe in python?",
        "answer_body": "<p>The problem is that your &quot;tokenized&quot; column doesn't look ready to apply the lemmatization step, as it contains a string, not a list of tokens. In other words, instead of having</p>\n<pre><code>&quot; yeah simply zurich generic serving ...&quot;\n</code></pre>\n<p>you should have in your dataframe <code>tokenized</code> cell a list of tokens (generated with a tokenizer from your initial sentence), as in</p>\n<pre><code>[&quot;yeah&quot;, &quot;simply&quot;, &quot;zurich&quot;, &quot;generic&quot;, &quot;serving&quot;, ...]\n</code></pre>\n<p>If you don't have a proper list of tokens in your dataframe cell, python will iterate in your <code>apply</code>/<code>lambda</code> list comprehension character by character, which is clearly not what you want.</p>\n",
        "question_body": "<p>I try to lemmatize the column &quot;tokenized&quot; in a dataframe. One cell of the column &quot;tokenized&quot; looks as follows <em>&quot;  yeah    simply    zurich    generic    serving    think    media    bland    prepared    curry    kind    paying    well    loves    used    parboiled    oily    place    elaborate    non    tasteful    stay    underspiced    institution    vegetarian    indian    clueless    away    hiltl    anyone    served    support    veg    long    like    normal    strong    worth    insult    not    rice    kitchen    know    wont    food    cuisine    fantastic    fan    time    term    patrons  &quot;.</em></p>\n<p><strong>When I run my code it returns something like this: &quot;,,e,n,d,e,d,,,p,a,y,i&quot;</strong> which is not what i want. How can I lemmatize full words?</p>\n<p>This is my code:</p>\n<pre><code>reviews_english['tokenized_lem'] = reviews_english['tokenized'].apply(\n                    lambda lst:[lmtzr.lemmatize(word) for word in lst])\nreviews_english\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21204824,
            "reputation": 55,
            "user_id": 15594687,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9e24175ed0727389f1cee4f7af949ec7?s=128&d=identicon&r=PG&f=1",
            "display_name": "programmer987",
            "link": "https://stackoverflow.com/users/15594687/programmer987"
        },
        "is_answered": true,
        "view_count": 77,
        "accepted_answer_id": 68139012,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1624694192,
        "creation_date": 1624678591,
        "last_edit_date": 1624679298,
        "question_id": 68139000,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68139000/pandas-how-to-print-specific-elements-of-a-dataframe-when-a-column-is-true",
        "title": "Pandas - How to print specific elements of a dataframe when a column is True",
        "answer_body": "<p>Try:</p>\n<pre><code>df['Check']=df['Check'].map({'True':True,'False':False})\n#OR\ndf['Check']=pd.eval(df['Check'])\n</code></pre>\n<p>Finally:</p>\n<pre><code>out=df.loc[df['Check'],['Process_ID','Date']]\n#OR if you want to print info then:\nprint(df.loc[df['Check'],['Process_ID','Date']])\n</code></pre>\n<p>output of <code>out</code>:</p>\n<pre><code>    Process_ID  Date\n0   12345-98    2021-06-30\n3   44231-56    2023-10-02\n4   78456-00    2024-04-03\n</code></pre>\n<p><strong>Update:</strong></p>\n<p>use a custom function:</p>\n<pre><code>def printfunc():\n    df['Check']=pd.eval(df['Check'].astype(str).str.title())\n    out=df.loc[df['Check'],['Process_ID','Date']].T\n    for x in out:\n        print('Process ID:',out[x].values[0],'\\nDue Date:',out[x].values[1],'\\n')\n\nprintfunc()\n</code></pre>\n<p>output of above code:</p>\n<pre><code>Process ID: 12345-98 \nDue Date: 2021-06-30 \n\nProcess ID: 44231-56 \nDue Date: 2023-10-02 \n\nProcess ID: 78456-00 \nDue Date: 2024-04-03\n</code></pre>\n<p><strong>Note:</strong> you can skip this line inside function <code>df['Check']=pd.eval(df['Check'].astype(str).str.title())</code> if you already run this code <code>df['Check']=pd.eval(df['Check'])</code></p>\n<p><strong>OR</strong></p>\n<p>use <code>agg()</code> and <code>rename()</code>:</p>\n<pre><code>df.loc[df['Check'],['Process_ID','Date']].rename(columns={'Date':'Due Date'}).agg(','.join)\n</code></pre>\n<p>output of above code:</p>\n<pre><code>Process_ID          12345-98,44231-56,78456-00\nDue Date      2021-06-30,2023-10-02,2024-04-03\ndtype: object\n</code></pre>\n",
        "question_body": "<p>Imagine we have the following dataframe:</p>\n<pre><code>data={'Process_ID':['12345-98', '23547-75', '85763-99','44231-56','78456-00','53218-87'],\n     'Date': ['2021-06-30','2022-08-10','2021-06-15','2023-10-02','2024-04-03','2021-06-25'],\n     'Check': ['True','False','False','True','True','False']}\n\ndf=pd.DataFrame(data)\n\nprint(df)\n</code></pre>\n<p>The output is the following:</p>\n<pre><code> Process_ID        Date  Check\n0   12345-98  2021-06-30   True\n1   23547-75  2022-08-10  False\n2   85763-99  2021-06-15  False\n3   44231-56  2023-10-02   True\n4   78456-00  2024-04-03   True\n5   53218-87  2021-06-25  False\n</code></pre>\n<p>How can I print the process_IDs and the dates only for the rows where df['check']==&quot;True&quot;?</p>\n<p>I tried to do something like this, but it did not work:</p>\n<pre><code>def get_true(dataframe):\n    \n    if dataframe['new']=='True':\n        \n        print(f&quot;&quot;&quot;Process_ID: {dataframe['Process_ID']} \\n\n                  Due Date: {dataframe['Date']}&quot;&quot;&quot;)\n\nget_true(df)\n</code></pre>\n<p>I got the following error:</p>\n<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n<p>I would like the result to be a string like this:</p>\n<pre><code>Process_ID: xxxxx-xx\nDue Date: dd/mm/yyyy\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21008745,
            "reputation": 123,
            "user_id": 15437851,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/4TgHm.png?s=128&g=1",
            "display_name": "KReEd",
            "link": "https://stackoverflow.com/users/15437851/kreed"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 68139958,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1624692049,
        "creation_date": 1624690155,
        "question_id": 68139935,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68139935/create-pivot-table-and-get-count-in-pandas-dataframe",
        "title": "Create Pivot Table and get Count in Pandas Dataframe",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html\" rel=\"nofollow noreferrer\">pd.crosstab</a>:</p>\n<pre><code>df1 = pd.crosstab(df['emp_id'], df['category']).rename_axis(\n    columns=None).reset_index()\n</code></pre>\n<h5>OUTPUT:</h5>\n<pre><code>  emp_id  A  B  C\n0    033  0  0  1\n1     12  2  0  0\n2   2233  1  0  0\n3    441  0  0  3\n4   6676  1  0  1\n5     91  0  1  0\n</code></pre>\n<h5>NOTE:</h5>\n<p>If you don't need <code>0</code> in the output you can use:</p>\n<pre><code>df = pd.crosstab(df['emp_id'],  df['category']).rename_axis(\n    columns=None).reset_index().replace(0, '')\n</code></pre>\n<h5>OUTPUT:</h5>\n<pre><code>  emp_id  A  B  C\n0    033        1\n1     12  2      \n2   2233  1      \n3    441        3\n4   6676  1     1\n5     91     1   \n</code></pre>\n<h5>Updated Answer:</h5>\n<pre><code>df = (\n    df.reset_index()\n    .pivot_table(\n        index=['emp_id', df.groupby('emp_id')['year'].transform(', '.join)],\n        columns='category',\n        values='index',\n        aggfunc='count',\n        fill_value=0)\n    .rename_axis(columns=None)\n    .reset_index()\n)\n</code></pre>\n<h5>OUTPUT:</h5>\n<pre><code>  emp_id              year  A  B  C\n0    033              FY16  0  0  1\n1     12        FY18, FY14  2  0  0\n2   2233              FY21  1  0  0\n3    441  FY20, FY17, FY12  0  0  3\n4   6676        FY19, FY10  1  0  1\n5     91              FY15  0  1  0\n</code></pre>\n",
        "question_body": "<p><strong>I have my dataframe</strong> -</p>\n<pre><code>import pandas as pd\ndata = [['2233', 'A', 'FY21'], ['441', 'C', 'FY20'], ['6676', 'A', 'FY19'], ['033', 'C', 'FY16'], \n       ['12', 'A', 'FY18'], ['91', 'B', 'FY15'], ['6676', 'C', 'FY10'], ['441', 'C', 'FY17'], \n       ['12', 'A', 'FY14'], ['441', 'C', 'FY12']]\ndf = pd.DataFrame(data, columns = ('emp_id', 'category', 'year'))\ndf\n\nemp_id   category   year\n0   2233    A       FY21\n1   441     C       FY20\n2   6676    A       FY19\n3   033     C       FY16\n4   12      A       FY18\n5   91      B       FY15\n6   6676    C       FY10\n7   441     C       FY17\n8   12      A       FY14\n9   441     C       FY12\n</code></pre>\n<p>So basically I want the categories should be created as individual column i.e A, B, &amp; C and each column should contain the counts of them.</p>\n<p><strong>What I want as my output -</strong></p>\n<pre><code>emp_id    A   B   C\n0   2233  1     \n1   441           3\n2   6676  1     \n3   033           1\n4   12    2     \n5   91        1 \n6   6676          1\n</code></pre>\n<p><strong>What I was trying -</strong></p>\n<pre><code>df['count'] = df.groupby(['emp_id'])['category'].transform('count')\ndf.drop_duplicates('emp_id', inplace = True)\ndf\nemp_id  category year   count\n0   2233    A    FY21   1\n1   441     C    FY20   3\n2   6676    A    FY19   2\n3   033     C    FY16   1\n4   12      A    FY18   2\n5   91      B    FY15   1\n</code></pre>\n<p>please help me to get my desired output in python.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 22047929,
            "reputation": 11,
            "user_id": 16312629,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d5e9661db4dd1dcadf23d6f460e52449?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dan_Lee",
            "link": "https://stackoverflow.com/users/16312629/dan-lee"
        },
        "is_answered": true,
        "view_count": 52,
        "accepted_answer_id": 68139911,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624691972,
        "creation_date": 1624601705,
        "last_edit_date": 1624684541,
        "question_id": 68126303,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68126303/filtering-for-group-by-pandas",
        "title": "Filtering for group by pandas",
        "answer_body": "<p>You can apply <a href=\"https://docs.python.org/3/library/stdtypes.html#frozenset.issubset\" rel=\"nofollow noreferrer\"><code>issubset</code></a> to check if the set <code>{'Physics', 'Chemistry'}</code> is a subset of the aggregated lists:</p>\n<pre><code>subjects = {'Physics', 'Chemistry'}\ndf.groupby('Name').SubjectName.agg(list).apply(subjects.issubset)\n\n# Name\n# Albert    False\n# Alice     False\n# Jack       True\n# Jill      False\n# John       True\n# Name: SubjectId, dtype: bool\n</code></pre>\n",
        "question_body": "<p><a href=\"https://i.stack.imgur.com/7CqWG.png\" rel=\"nofollow noreferrer\">df</a></p>\n<p>I have the following df and I want to check students taking both Physics and Chemistry.</p>\n<p>I am trying to use groupby method and came up with the following coding.\nI am open for any suggestions. Thank you</p>\n<pre><code>df1.groupby('Name').SubjectName.apply(list).agg(lambda x: (x[0] in ['Physics']) &amp; (x[-1] in ['Chemistry']) )\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "account_id": 19857350,
            "reputation": 53,
            "user_id": 14546482,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgXiYq8mAwyIDH4IRd-uObd_OAQ0HtFj-bGdTwR=k-s128",
            "display_name": "aero8991",
            "link": "https://stackoverflow.com/users/14546482/aero8991"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 68138939,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624691903,
        "creation_date": 1624658334,
        "last_edit_date": 1624684808,
        "question_id": 68137444,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68137444/keep-one-columns-data-in-pandas-and-show-all-nans-from-other-columns-only",
        "title": "keep one column&#39;s data in pandas and show all NANs from other columns only",
        "answer_body": "<p>Since you require the <code>Name</code> column to be intact, you can just select other columns except <code>Name</code> and mask them, then create another data frame <code>df2</code> which removes all the <code>NaN</code> values. After that you can just drop the indexes in <code>df2</code> from <code>df</code> which you give you rows with only the <code>NaN</code> values as follows.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.mask((df.columns != 'Name') &amp; (df.notnull()), &quot;&quot;, inplace=True)\ndf2 = df.dropna()\ndf.drop(df2.index, inplace=True)\n</code></pre>\n<p>This should give you the following output.</p>\n<pre class=\"lang-py prettyprint-override\"><code>Name        Phone            Address\nJohn Doe     NAN               \nJenny Gump                     NAN\nLarry Bean    NAN \n</code></pre>\n",
        "question_body": "<p>Goal: Id like to still show who the person is so that I can display the NANs associated with them so that I can quickly find who is missing info.</p>\n<p>Consider this dataset:</p>\n<pre><code>df:\nName            Phone            Address\nJohn Doe        NAN               123 lane\nJenny Gump      222-222-2222      NAN\nLarry Bean      NAN               561 road\nHarry Smidlap   111-111-1111       555 highway\n</code></pre>\n<p>I'd like to clean the data up and show something like this (similar to an excel view when filtering for blanks):\nThen maybe populate the empty data with something that says &quot;Data exists&quot; or just leave it blank. Im open to suggestions. And also drop the rows that have all data populated.</p>\n<pre><code>df:\nName        Phone            Address\nJohn Doe     NAN               \nJenny Gump                     NAN\nLarry Bean    NAN             \n</code></pre>\n<p>I've tried:</p>\n<pre><code>df[df.isnull().any(axis=1)]\n</code></pre>\n<p>That works great but I have a big data source and I see a lot of unnecessary info that already has data. I only care about seeing the person's name and what their missing.</p>\n<p>Anyone have any ideas?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "comparison"
        ],
        "owner": {
            "account_id": 15335161,
            "reputation": 47,
            "user_id": 11063753,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/2de7944b4ea97360b0109e10a055a092?s=128&d=identicon&r=PG&f=1",
            "display_name": "Abdullah",
            "link": "https://stackoverflow.com/users/11063753/abdullah"
        },
        "is_answered": true,
        "view_count": 2263,
        "accepted_answer_id": 56003456,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1624679290,
        "creation_date": 1557137884,
        "last_edit_date": 1595024399,
        "question_id": 56003158,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/56003158/color-formatting-excel-file-row-in-python",
        "title": "Color formatting excel file row in python",
        "answer_body": "<p>Create styles in helper DataFrame and <a href=\"http://pandas.pydata.org/pandas-docs/stable/user_guide/style.html#Export-to-Excel\" rel=\"nofollow noreferrer\">export to excel</a>:</p>\n\n<pre><code>df = pd.DataFrame({'Date1':['19/3/2011','15/5/2015','18/8/2018'],\n                   'Date2':['19/3/2011','1/1/2019','18/8/2018']})\n\nprint (df)\n       Date1      Date2\n0  19/3/2011  19/3/2011\n1  15/5/2015   1/1/2019\n2  18/8/2018  18/8/2018\n\ndef highlight_diff(x): \n   c1 = 'background-color: red'\n   c2 = '' \n   m = x['Date1'] != x['Date2']\n\n   df1 = pd.DataFrame(c2, index=x.index, columns=x.columns)\n   df1.loc[m, :] = c1\n   return df1\n\n(df.style\n   .apply(highlight_diff,axis=None)\n   .to_excel('styled.xlsx', engine='openpyxl', index=False))\n</code></pre>\n\n<p><a href=\"https://i.stack.imgur.com/ljlUU.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ljlUU.png\" alt=\"pic\"></a></p>\n",
        "question_body": "<p>I have dataframe where I have 2 Date columns. I have to compare them and if they are different then whole row should be colored. Please check the picture.  <a href=\"https://i.stack.imgur.com/d8gFQ.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/d8gFQ.jpg\" alt=\"enter image description here\" /></a></p>\n<p>Please guide me how can I do that in python. Thanks in advance.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dictionary",
            "dataframe"
        ],
        "owner": {
            "account_id": 11603901,
            "reputation": 135,
            "user_id": 8500237,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-XkcqNwiWTgE/AAAAAAAAAAI/AAAAAAAAB3w/dEpnCybzu1I/photo.jpg?sz=128",
            "display_name": "aragalie",
            "link": "https://stackoverflow.com/users/8500237/aragalie"
        },
        "is_answered": true,
        "view_count": 272,
        "accepted_answer_id": 46875201,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624676477,
        "creation_date": 1508660849,
        "last_edit_date": 1508682768,
        "question_id": 46872011,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/46872011/reshaping-pandas-dataframe-for-export-in-a-nested-dict",
        "title": "reshaping pandas DataFrame for export in a nested dict",
        "answer_body": "<p>If you re-shape your dataframe into the right format, you can use the handy recursive dictionary function from the answer by @DSM to <a href=\"https://stackoverflow.com/questions/19798112/convert-pandas-dataframe-to-a-nested-dict\">this question</a>.  The goal is to get a dataframe where each row contains only one \"entry\" - a unique combination of the columns you're interested in.</p>\n\n<p>First, you need to split your country code strings into lists:</p>\n\n<pre><code>df['Country Code'] = df['Country Code'].str.split(',')\n</code></pre>\n\n<p>And then expand those lists into multiple rows (using @RomanPekar's technique from <a href=\"https://stackoverflow.com/questions/27263805/pandas-when-cell-contents-are-lists-create-a-row-for-each-element-in-the-list\">this question</a>):</p>\n\n<pre><code>s = df.apply(lambda x: pd.Series(x['Country Code']),axis=1) \\\n    .stack().reset_index(level=1, drop=True)\ns.name = 'Country Code'\ndf = df.drop('Country Code', axis=1).join(s).reset_index(drop=True)\n</code></pre>\n\n<p>Then you can reshape the <code>Spend*</code> columns into rows, where there's a row for each <code>Spend*</code> column where the value is not <code>nan</code>.</p>\n\n<pre><code>spend_cols = ['Spend1', 'Spend2', 'Spend3', 'Spend4', 'Spend5']\ndf = df.groupby('Country Code') \\\n    .apply(lambda g: g.join(pd.DataFrame(g[spend_cols].stack()) \\\n    .reset_index(level=1)['level_1'])) \\\n    .reset_index(drop=True)\n</code></pre>\n\n<p>Now you have a dataframe where each level in your nested dictionary is its own column.  So you can use this recursive dictionary function:</p>\n\n<pre><code>def recur_dictify(frame):\n    if len(frame.columns) == 1:\n        if frame.values.size == 1: return frame.values[0][0]\n        return frame.values.squeeze()\n    grouped = frame.groupby(frame.columns[0])\n    d = {k: recur_dictify(g.ix[:,1:]) for k,g in grouped}\n    return d\n</code></pre>\n\n<p>And apply it only to the columns you want to produce the nested dictionary, listed in the order in which they should nest:</p>\n\n<pre><code>cols = ['Country Code', 'Category', 'Area', 'Function', 'level_1', 'LanID', 'Last Name']\nd = recur_dictify(df[cols])\n</code></pre>\n\n<p>That should produce your desired result.</p>\n\n<hr>\n\n<p>All in one piece:</p>\n\n<pre><code>df['Country Code'] = df['Country Code'].str.split(',')\ns = df.apply(lambda x: pd.Series(x['Country Code']),axis=1) \\\n    .stack().reset_index(level=1, drop=True)\ns.name = 'Country Code'\ndf = df.drop('Country Code', axis=1).join(s).reset_index(drop=True)\n\nspend_cols = ['Spend1', 'Spend2', 'Spend3', 'Spend4', 'Spend5']\ndf = df.groupby('Country Code') \\\n    .apply(lambda g: g.join(pd.DataFrame(g[spend_cols].stack()) \\\n    .reset_index(level=1)['level_1'])) \\\n    .reset_index(drop=True)\n\ndef recur_dictify(frame):\n    if len(frame.columns) == 1:\n        if frame.values.size == 1: return frame.values[0][0]\n        return frame.values.squeeze()\n    grouped = frame.groupby(frame.columns[0])\n    d = {k: recur_dictify(g.ix[:,1:]) for k,g in grouped}\n    return d\n\ncols = ['Country Code', 'Category', 'Area', 'Function', 'level_1', 'LanID', 'Last Name']\nd = recur_dictify(df[cols])\n</code></pre>\n",
        "question_body": "<p>Given the following DataFrame: </p>\n\n<pre><code>   Category Area               Country Code Function Last Name     LanID  Spend1  Spend2  Spend3  Spend4  Spend5\n0      Bisc   EE                  RU02,UA02       Mk     Smith    df3432     1.0     NaN     NaN     NaN     NaN\n1      Bisc   EE                       RU02       Mk      Bibs    fdss34     1.0     NaN     NaN     NaN     NaN\n2      Bisc   EE               UA02,EURASIA       Mk      Crow   fdsdr43     1.0     NaN     NaN     NaN     NaN\n3      Bisc   WE                       FR31       Mk     Ellis   fdssdf3     1.0     NaN     NaN     NaN     NaN\n4      Bisc   WE                  BE32,NL31       Mk     Mower   TOZ1720     1.0     NaN     NaN     NaN     NaN\n5      Bisc   WE             FR31,BE32,NL31      LKU      Elan   SKY8851     1.0     1.0     1.0     1.0     1.0\n6      Bisc   SE                       IT31       Mk    Bobret    3dfsfg     1.0     NaN     NaN     NaN     NaN\n7      Bisc   SE                       GR31       Mk   Concept  MOSGX009     1.0     NaN     NaN     NaN     NaN\n8      Bisc   SE   RU02,IT31,GR31,PT31,ES31      LKU     Solar   MSS5723     1.0     1.0     1.0     1.0     1.0\n9      Bisc   SE        IT31,GR31,PT31,ES31       Mk      Brix    fdgd22     NaN     1.0     NaN     NaN     NaN\n10     Choc   CE   RU02,CZ31,SK31,PL31,LT31      Fin    Ocoser    43233d     NaN     1.0     NaN     NaN     NaN\n11     Choc   CE        DE31,AT31,HU31,CH31      Fin     Smuth     4rewf     NaN     1.0     NaN     NaN     NaN\n12     Choc   CE              BG31,RO31,EMA      Fin    Momocs    hgghg2     NaN     1.0     NaN     NaN     NaN\n13     Choc   WE             FR31,BE32,NL31      Fin   Bruntly    ffdd32     NaN     NaN     NaN     NaN     1.0\n14     Choc   WE             FR31,BE32,NL31       Mk      Ofer  BROGX011     NaN     1.0     1.0     NaN     NaN\n15     Choc   WE             FR31,BE32,NL31       Mk       Hem   NZJ3189     NaN     NaN     NaN     1.0     1.0\n16      G&amp;C   NE                  UA02,SE31       Mk       Cre   ORY9499     1.0     NaN     NaN     NaN     NaN\n17      G&amp;C   NE                       NO31       Mk      Qlyo   XVM7639     1.0     NaN     NaN     NaN     NaN\n18      G&amp;C   NE   GB31,NO31,SE31,IE31,FI31       Mk      Omny   LOX1512     NaN     1.0     1.0     NaN     NaN\n</code></pre>\n\n<p>I would like to get it exported into a nested Dict with the below structure:</p>\n\n<pre><code>    {RU02:  {Bisc:  {EE:    {Mkt:   {Spend1:    {df3432:    Smith}\n                                                {fdss34:     Bibs}\n            {Bisc:  {SE:    {LKU:   {Spend1:    {MSS5723:   Solar}\n                                    {Spend2:    {MSS5723:   Solar}\n                                    {Spend3:    {MSS5723:   Solar}\n                                    {Spend4:    {MSS5723:   Solar}\n                                    {Spend5:    {MSS5723:   Solar}\n            {Choc:  {CE:    {Fin:   {Spend2:    {43233d:   Ocoser}\n            .....\n\n    {UA02:  {Bisc:  {EE:    {Mkt:   {Spend1:    {df3432:    Smith}\n                                                {ffdsdr43:   Crow}\n            {G&amp;C:   {NE:    {Mkt:   {Spend1:    {ORY9499:     Cre}\n    .....\n</code></pre>\n\n<p>So essentially, in this Dict I'm trying to track for each CountryCode, what is the list of LastNames+LandIDs, per Spend category (Spend1,Spend2, etc.) and their attributes (Function, Category, Area).</p>\n\n<p>The DataFrame is not very large (less than 200rows), but it contains almost all types of combinations between Category/Area/Country Code as well as LastNames and their Spend categories (many-to-many).</p>\n\n<p>My challenge is that i'm unable to figure out how to clearly conceptualise the steps i need to do in order to prepare the DataFrame properly for export to Dict....</p>\n\n<p>What i figured so far is that i would need:</p>\n\n<ol>\n<li>a way to slice the contents of the \"Country Code\" column based on the \",\" separator: DONE</li>\n<li>create new columns based on unique Country Codes, and have 1 in each row where that column code is preset: DONE</li>\n<li>set the index of the DataFrame recursively to each of the newly added columns</li>\n<li>move into a new DataFrame each rows for each Country Code where there is data</li>\n<li>export all the new DataFrames to Dicts, and then merge them</li>\n</ol>\n\n<p>Not sure if steps 3-6 is the best way to go about this though, as i'm having difficulties still to understand how <strong>pd.DataFrame.to_dict</strong> should be configured for my case (if that's even possible)...</p>\n\n<p>Highly appreciate your help on the coding side, but also in briefly explaining your thought process for each stage.</p>\n\n<hr>\n\n<p>Here is how far i got on my own..</p>\n\n<pre><code>#keeping track of initial order of columns\ninitialOrder = list(df.columns.values)\n\n# split the Country Code by \",\"\nCCodeNoCommas= [item for items in df['Country Code'].values for item in items.split(\",\")]\n\n# add only the UNIQUE Country Codes -via set- as new columns in the DataFrame,\n#with NaN for row values\ndf = pd.concat([df,pd.DataFrame(columns=list(set(CCodeNoCommas)))])\n\n# reordering columns to have the newly added ones at the end\nreordered = initialOrder + [c for c in df.columns if c not in initialOrder]\ndf = df[reordered]\n\n\n# replace NaN with 1 in the newly added columns (Country Codes), where the same Country code\n# exists in the initial column \"Country Code\"; do this for each row\n\nCCodeUniqueOnly = set(CCodeNoCommas)\nfor c in CCodeUniqueOnly:   \n    CCodeIsPresent_rowIndex = df.index[df['Country Code'].str.contains(c)]\n\n    #print (CCodeIsPresent_rowIndex)\n    df.loc[CCodeIsPresent_rowIndex, c] = 1\n\n# no clue what do do next ??\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 21689044,
            "reputation": 33,
            "user_id": 16003919,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9e1249aeab156aabfac9d1acf24424be?s=128&d=identicon&r=PG&f=1",
            "display_name": "Eduardo Gomes",
            "link": "https://stackoverflow.com/users/16003919/eduardo-gomes"
        },
        "is_answered": true,
        "view_count": 89,
        "accepted_answer_id": 68137645,
        "answer_count": 3,
        "score": 3,
        "last_activity_date": 1624669996,
        "creation_date": 1624644425,
        "question_id": 68135422,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68135422/pandas-custom-aggregate-function-with-condition-on-group-is-it-possible",
        "title": "Pandas custom aggregate function with condition on group, is it possible?",
        "answer_body": "<p>If I'm getting it right, you want to aggregate only the groups where all of the values in the <code>asc_diff</code> column are <code>True</code>. The other groups (<code>asc_diff == False</code>), should not be changed.</p>\n<p>If that's the case, starting off from what you've done so far, the solution is straightforward. You only need to create a custom apply function that will do the work for you based on the conditions you define. The custom apply function would be like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def custom_apply(df):\n    if df['asc_diff'].all() == False:\n        df = df.set_index('asc_diff')\n        return df[['price', 'weight', 'product']]\n    \n    def wavg(x): return np.average(x, weights=df.loc[x.index, &quot;weight&quot;])\n\n    df1 = df.groupby('asc_diff').agg({'price': wavg, 'weight': 'max'})\n    df2 = df.groupby('asc_diff').agg({'product': ' '.join})\n    return pd.concat([df1, df2], axis=1)\n</code></pre>\n<p>The main tweaks of this function are the following:</p>\n<ol>\n<li>You need to check the values of the column <code>asc_diff</code>. If all of them are <code>False</code> you just return the dataframe with the columns you want.</li>\n<li>Use a custom function to calculate your weighted price (<code>wavg</code>)</li>\n<li>Calculate your aggregations and concatenate them.</li>\n</ol>\n<p>Then, you just need to apply this function in your grouped dataframe like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(g.apply(custom_apply).droplevel(1))\n</code></pre>\n<p>The result will be:</p>\n<pre><code>               price  weight           product\nasc_diff                                      \n1          19.600000       2  car apple banana\n2          27.000000       1          computer\n2         100.000000       1               toy\n3         201.071429       3   book mouse door\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe:</p>\n<pre><code>df = pd.DataFrame(\n  [{'price': 22, 'weight': 1, 'product': 'banana', },\n  {'price': 20, 'weight': 2, 'product': 'apple', },\n  {'price': 18, 'weight': 2, 'product': 'car', },\n  {'price': 100, 'weight': 1, 'product': 'toy', },\n  {'price': 27, 'weight': 1, 'product': 'computer', },\n  {'price': 200, 'weight': 1, 'product': 'book', },\n  {'price': 200.5, 'weight': 3, 'product': 'mouse', },\n  {'price': 202, 'weight': 3, 'product': 'door', },]\n)\n</code></pre>\n<p>What I have to do is to group by contiguous prices where the difference between them is less than a threshold (say 2.0) or not. After that I have to apply the following aggregations ONLY on the group 'less than threshold', otherwise the group should not be aggregated:</p>\n<ol>\n<li><code>price</code> should be the weighted average between <code>price</code> and <code>weight</code></li>\n<li><code>weight</code> should be the maximum value</li>\n<li><code>product</code> should be string concatenation</li>\n</ol>\n<p>What I did so far (step by step):</p>\n<ol>\n<li>I sorted the dataframe by prices in ascending order (to get the contiguous values)</li>\n</ol>\n<pre><code>df.sort_values(by=['price'], inplace=True)\n</code></pre>\n<pre><code>    price  weight   product\n2   18.0       2       car\n1   20.0       2     apple\n0   22.0       1    banana\n4   27.0       1  computer\n3  100.0       1       toy\n5  200.0       1      book\n6  200.5       3     mouse\n7  202.0       3      door    \n</code></pre>\n<ol start=\"2\">\n<li>Got the difference between the prices in ascesding and descending order to detect the contiguous prices</li>\n</ol>\n<pre><code>df['asc_diff'] = df['price'].diff(periods=1)\ndf['desc_diff'] = df['price'].diff(periods=-1).abs()\n</code></pre>\n<pre><code>    price  weight   product  asc_diff  desc_diff\n2   18.0       2       car       NaN        2.0\n1   20.0       2     apple       2.0        2.0\n0   22.0       1    banana       2.0        5.0\n4   27.0       1  computer       5.0       73.0\n3  100.0       1       toy      73.0      100.0\n5  200.0       1      book     100.0        0.5\n6  200.5       3     mouse       0.5        1.5\n7  202.0       3      door       1.5        NaN\n</code></pre>\n<ol start=\"3\">\n<li>Combined <code>asc_diff</code> and <code>desc_diff</code> columns to remove <code>NaN</code> and create the continous regions</li>\n</ol>\n<pre><code>df['asc_diff'] = df['asc_diff'].combine_first(df['desc_diff'])\ndf['asc_diff'] = df[['asc_diff', 'desc_diff']].min(axis=1).abs()\ndf['asc_diff'] = df['asc_diff'] &lt;= 2.0\ndf = df.drop(columns=['desc_diff'])\n</code></pre>\n<pre><code>    price  weight   product  asc_diff\n2   18.0       2       car      True\n1   20.0       2     apple      True\n0   22.0       1    banana      True\n4   27.0       1  computer     False\n3  100.0       1       toy     False\n5  200.0       1      book      True\n6  200.5       3     mouse      True\n7  202.0       3      door      True\n</code></pre>\n<ol start=\"4\">\n<li>Created the groups</li>\n</ol>\n<pre><code>g = df.groupby((df['asc_diff'].shift() != df['asc_diff']).cumsum())\nfor k, v in g:\n    print(f'[group {k}]')\n    print(v)\n</code></pre>\n<pre><code>[group 1]\n   price  weight product  asc_diff\n2   18.0       2     car      True\n1   20.0       2   apple      True\n0   22.0       1  banana      True\n[group 2]\n   price  weight   product  asc_diff\n4   27.0       1  computer     False\n3  100.0       1       toy     False\n[group 3]\n   price  weight product  asc_diff\n5  200.0       1    book      True\n6  200.5       3   mouse      True\n7  202.0       3    door      True\n</code></pre>\n<p>So far so good, but when I had to aggregate comes the problems:</p>\n<pre><code>def product_join(x):\n    return ' '.join(x)\ng.agg({'weight': 'max', 'product': product_join})\n</code></pre>\n<pre><code>           weight           product\nasc_diff                          \n1              2  car apple banana\n2              1      computer toy\n3              3   book mouse door\n</code></pre>\n<p>The problems:</p>\n<ul>\n<li>only group 1 and 3 should be aggregated (but in the code it applys to all groups)</li>\n<li>even using a custom function (e.g. product_join) I have no access to other columns values, so that I can get the weighted average prices for example.</li>\n</ul>\n<p>What I want to accomplish:</p>\n<ul>\n<li>Aggregate only groups 1 and 3 (where <code>asc_diff</code> is true) and keep group 2 intact</li>\n<li>in <code>price</code> aggregate function I needed a function to access two columns (i.e. <code>price</code> and <code>weight</code>) to get the weighted average value</li>\n</ul>\n<p>Thanks in advance!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 16450471,
            "reputation": 15,
            "user_id": 11884017,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/0886955864e9fd5cb11ac34a00dce38c?s=128&d=identicon&r=PG&f=1",
            "display_name": "poukpouk",
            "link": "https://stackoverflow.com/users/11884017/poukpouk"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 68138129,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624666948,
        "creation_date": 1624665541,
        "question_id": 68138114,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68138114/how-to-get-percentage-of-valid-data-per-column-according-to-an-indicator-in-data",
        "title": "How to get percentage of valid data per column according to an indicator in dataframe",
        "answer_body": "<p>Lets' try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html\" rel=\"nofollow noreferrer\"><code>set_index</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.notnull.html\" rel=\"nofollow noreferrer\"><code>notnull</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html\" rel=\"nofollow noreferrer\"><code>mean</code></a> on <code>level=0</code> to leverage the mathematical values of <code>True</code> and <code>False</code> to determine the sum of <code>True</code>/<code>False</code> values (<code>1</code>/<code>0</code>) divided by the length:</p>\n<pre><code>out = df.set_index('fruits').notnull().mean(level=0).mul(100)\n</code></pre>\n<pre><code>         2012  2013\nfruits             \nApple   100.0  50.0\nPeach    50.0  50.0\nCherry   50.0   0.0\n</code></pre>\n<p>Optional <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.applymap.html#pandas-dataframe-applymap\" rel=\"nofollow noreferrer\"><code>applymap</code></a> to apply format string:</p>\n<pre><code>out = (\n    df.set_index('fruits').notnull().mean(level=0)\n        .mul(100).applymap('{:.0f}%'.format)\n)\n</code></pre>\n<pre><code>        2012 2013\nfruits           \nApple   100%  50%\nPeach    50%  50%\nCherry   50%   0%\n</code></pre>\n<hr />\n<p>Or with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html#pandas-core-groupby-dataframegroupby-aggregate\" rel=\"nofollow noreferrer\"><code>groupby aggregate</code></a> with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.notnull.html\" rel=\"nofollow noreferrer\"><code>notnull</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html\" rel=\"nofollow noreferrer\"><code>mean</code></a>:</p>\n<pre><code>out = df.groupby('fruits').agg(lambda s: s.notnull().mean().mul(100))\n</code></pre>\n<p><code>out</code>:</p>\n<pre><code>         2012  2013\nfruits             \nApple   100.0  50.0\nCherry   50.0   0.0\nPeach    50.0  50.0\n</code></pre>\n<p>Optional string format and add <code>%</code> symbol with <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html#pandas-series-map\" rel=\"nofollow noreferrer\"><code>map</code></a>:</p>\n<pre><code>out = df.groupby('fruits').agg(lambda s: s.notnull().mean()\n                               .mul(100).map('{:.0f}%'.format))\n</code></pre>\n<pre><code>        2012 2013\nfruits           \nApple   100%  50%\nCherry   50%   0%\nPeach    50%  50%\n</code></pre>\n<hr />\n<p>Breakdown of steps that occurs in <code>agg</code>:</p>\n<pre><code>s = pd.DataFrame({'apple': ['10', np.nan, '20', np.nan, '30']})\n</code></pre>\n<pre><code>  apple\n0    10\n1   NaN\n2    20\n3   NaN\n4    30\n</code></pre>\n<p><code>s</code> represents an arbitrary value passed to the <code>lambda</code> in <code>agg</code>:</p>\n<pre><code>s.notnull()\n</code></pre>\n<pre><code>   apple\n0   True  # 1\n1  False  # 0\n2   True  # 1\n3  False  # 0\n4   True  # 1\n</code></pre>\n<pre><code>s.notnull().mean()\n</code></pre>\n<pre><code>apple    0.6  # (1 + 0 + 1 + 0 + 1) / 5 =&gt; 3 / 5 =&gt; .6\ndtype: float64\n</code></pre>\n<pre><code>s.notnull().mean().mul(100)\n</code></pre>\n<pre><code>apple    60.0  # 0.6 * 100\ndtype: float64\n</code></pre>\n",
        "question_body": "<p>Let's say I have this dataframe:</p>\n<pre><code>df = pd.DataFrame({'fruits': [&quot;Apple&quot;, &quot;Peach&quot;, &quot;Cherry&quot;] * 2,\n                  '2012': [&quot;10&quot;, &quot;20&quot;, &quot;30&quot;, &quot;40&quot;, np.nan, np.nan],\n                  '2013': [&quot;1&quot;, &quot;2&quot;, np.nan, np.nan, np.nan, np.nan]})\n</code></pre>\n<p>I don't manage to get a percentage for all the fruits in 2012, and 2013.</p>\n<p>Result expected are:</p>\n<ul>\n<li>Apple 100% in 2012, 50% in 2013.</li>\n<li>Cherry 50% in 2012, 0% in 2013.</li>\n</ul>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21987595,
            "reputation": 51,
            "user_id": 16261585,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5f94e4c603f799cbd63a6c0dab575576?s=128&d=identicon&r=PG&f=1",
            "display_name": "gato",
            "link": "https://stackoverflow.com/users/16261585/gato"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 68137990,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624663857,
        "creation_date": 1624646901,
        "question_id": 68135863,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68135863/complex-analysis-of-values-in-two-columns-in-python-pandas",
        "title": "Complex analysis of values in two columns in Python Pandas?",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import re\n\nbad_list = [&quot;JOHN&quot;, &quot;green&quot;]\n\nmask1 = df.apply(lambda x: x[&quot;col1&quot;] in x[&quot;description&quot;], axis=1)\ntmp = df.apply(lambda x: x[&quot;description&quot;].replace(x[&quot;col1&quot;], &quot;&quot;), axis=1)\nmask2 = tmp.str.contains(r&quot;|&quot;.join(bad_list), flags=re.I)\n\nprint(df.loc[mask1 &amp; ~mask2, &quot;col1&quot;])\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>0    John Simon\n2    John Bravo\n3     Ann Still\nName: col1, dtype: object\n</code></pre>\n",
        "question_body": "<p>I have DataFrame in Python Pandas like below:</p>\n<pre><code>col1       | description\n---------- |-----------\nJohn Simon |John Simon red\nTerry Juk  |green Terry Juk\nJohn Bravo |brown John Bravo \nAnn Still  |orange Ann Still\n\n\nbad_list = [&quot;JOHN&quot;, &quot;green&quot;]\n</code></pre>\n<p>And I need to select only these people from &quot;col1&quot; which have value from &quot;col&quot; in column &quot;description&quot; and something else (nevermind before or after), but it something else could not be from bad_list.</p>\n<p>I used code like below:</p>\n<pre><code>import re\n\nbad_list = [&quot;JOHN&quot;, &quot;green&quot;]\n\nmask1 = df[&quot;description&quot;].str.contains(r&quot;|&quot;.join(bad_list), flags=re.I)\nmask2 = df.apply(lambda x: x[&quot;col1&quot;] in x[&quot;description&quot;], axis=1)\nprint(df.loc[~mask1 &amp; mask2, &quot;col1&quot;])\n</code></pre>\n<p>Nevertheless, If I have situation like in the third (3) row that value in col1 = &quot;John Bravo&quot; and description is &quot;brown John Bravo&quot; and in bad_list I have value &quot;JOHN&quot;, above code will drop third row of this data frame, because this code will find in description sentence from bad_list (&quot;JOHN&quot;), nevertheless bad_list NOT concernes values from col1 which are in column description, bad_list concerns only &quot;something else&quot;. I want so as to code analyze like this:</p>\n<ol>\n<li>Is value + something else (before or after this value) from col1 in description?</li>\n<li>Analyse &quot;something else&quot; (for instance for third row something elese is &quot;brown&quot; because it is not in col1, but together with the value from col1 are in the description column) if it is on bad_list -&gt; drop row, if &quot;something else&quot; is not on bad_list -&gt; stay row</li>\n</ol>\n<p>So, third row &quot;John Bravo&quot; should stay because ok, John is in bad list, but checking bad list concerns only to checking &quot;something else&quot; in description column, &quot;something else&quot; is value which is concatenated with value from col1 and these values are in description column together.</p>\n<p>So, from this Data Frame should be ONLY dropped: second row Terry Juk because &quot;something else&quot; of this rows is on bad list (&quot;green&quot;).</p>\n<p>How to do that in Python Pandas?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21054173,
            "reputation": 5,
            "user_id": 15473852,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-HTi7mEjb8j4/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucllXocaVEfNx4KxYUgae_fRJAGuFw/s96-c/photo.jpg?sz=128",
            "display_name": "Sweety Vaidya",
            "link": "https://stackoverflow.com/users/15473852/sweety-vaidya"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68090968,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624662696,
        "creation_date": 1624396450,
        "last_edit_date": 1624662696,
        "question_id": 68090778,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68090778/data-frame-columns-formating",
        "title": "data frame columns formating",
        "answer_body": "<p>You can change column names by simply setting the <code>df.columns</code> attribute to some other list. In this case, it looks like your list would be <code>['ID',file1,file2,...]</code></p>\n<p>Since those filenames are included in the path in your <code>filenames</code> list, we can pull out just the filename and make a new list with those.</p>\n<pre class=\"lang-py prettyprint-override\"><code>columns = list()\nfor path in filenames: #loop through your filenames list\n    file = path.split('\\\\')[-1] #this splits the path by the '\\' character, and returns the last element, so the filename. You need the double-slash since the slash is the escape character from the string, or whatever they call it.\n    file = file[:-4] # the file has the .xls on the end. this removes the last 4 characters\n    columns.append(file)\nall_data.columns = columns #all_data is our dataframe, and all_data.columns is the attribute of the dataframe that contains the column names. Changing this object to our columns list that we made will change the column names in the dataframe.\n</code></pre>\n",
        "question_body": "<p>I have multiple data sets which is concatenated in one master file using code below.</p>\n<pre><code>all_data = pd.DataFrame()\ndfs = (pd.read_excel(f, index_col=0, skiprows=8, skipfooter=1, usecols=[0,1])\n           for f in filenames)\nall_data = pd.concat(dfs, axis=1)\n</code></pre>\n<p>The data looks in &quot;each file&quot; looks  like these:(attached)\nID VALUE</p>\n<p>Concatenated master file now has like this:</p>\n<pre><code>ID value  value value value value value value value \n</code></pre>\n<p>However, we would like to rename the each value in master  file to have the  file name such as :</p>\n<pre><code>\n</code></pre>\n<p>Basically replacing the default col in data frame(value) to file names as individual col instead.\nPlease guide.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 22037870,
            "reputation": 1,
            "user_id": 16304075,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/14071a672860e21123a589519f21db53?s=128&d=identicon&r=PG&f=1",
            "display_name": "Esther",
            "link": "https://stackoverflow.com/users/16304075/esther"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68111780,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1624661374,
        "creation_date": 1624515902,
        "last_edit_date": 1624661374,
        "question_id": 68110566,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68110566/merge-rows-based-on-similar-values-in-some-columns",
        "title": "Merge Rows based on similar values in some columns",
        "answer_body": "<p>Given <code>df</code> is the name for the <code>pandas.DataFrame</code> containing your data.</p>\n<p>To get the earliest <code>StartDate</code> and <code>EndDate</code> of each patient's resource,  you can write:</p>\n<pre class=\"lang-py prettyprint-override\"><code># Group by the 'PatientID' and 'Resource' columns\ngrouped_df =  df.groupby(['PatientID', 'Resource'])\n\n# Select Earliest `StartDate` and `EndDate` from aggregate.\ngrouped_df = grouped_df.min(['StartDate', 'EndDate'])\n\n# Remove levels from the index.\ngrouped_df.reset_index(inplace=True)\n</code></pre>\n",
        "question_body": "<p>please I need help, I am recently started learning python. Please, how do I merge rows with the same \u201cPatientID\u201d and the same \u201cResource\u201d as one with \u201cStartDate\u201d and \u201cEndDate\u201d as the average of the merged rows?</p>\n<p><img src=\"https://i.stack.imgur.com/9IyeU.png\" alt=\"enter image description here\" /></p>\n<p><a href=\"https://i.stack.imgur.com/SLrYG.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 11876131,
            "reputation": 161,
            "user_id": 8691119,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c616be063ff376fc5b414afa80d8134f?s=128&d=identicon&r=PG&f=1",
            "display_name": "issac",
            "link": "https://stackoverflow.com/users/8691119/issac"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 68136939,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624657435,
        "creation_date": 1624653850,
        "question_id": 68136875,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68136875/pandas-data-frame-group-by-multiple-cols-and-put-other-columns-contents-in-one",
        "title": "pandas data frame, group by multiple cols and put other columns&#39; contents in one",
        "answer_body": "<p>You can use <code>groupby(...)[column].agg(...)</code> since this this is a reduction/aggregation. To aggregate, you simply want to join the contents into a single string so you can use something like <code>&quot; &quot;.join</code> as your aggregation function like so:</p>\n<pre><code>new_df = (\n    df.reset_index()        # we want to operate on the &quot;id&quot; as well\n    .astype({&quot;id&quot;: str})    # &quot; &quot;.join only works on strings, so make &quot;id&quot; string dtype\n    .groupby(['name','type', 'link'], as_index=False)[[&quot;id&quot;, &quot;subs&quot;]]\n    .agg(&quot; &quot;.join)\n)\n\nprint(new_df)\n  name type  link     id            subs\n0    x   tx    10    1 2       sub1 sub2\n1    y   rx    11  3 4 5  sub3 sub4 sub5\n</code></pre>\n<p>notes:</p>\n<ul>\n<li>passing <code>as_index=False</code> to the groupby statement puts the columns that we grouped by <code>['name', 'type', 'link']</code> back into the dataframe as columns (if we didn't specify they would make up the <code>Index</code></li>\n<li>in this line <code>.groupby(['name','type', 'link'], as_index=False)[[&quot;id&quot;, &quot;subs&quot;]]</code> we don't NEED to specify the columns since they're the only remaining columns in our dataframe to work with. This snippet will run without selecting them manually, I just prefer explicit &gt; implicit</li>\n</ul>\n<p>At the end of the day, these data are mainly only useful to look at. Putting multiple values into a single string into a cell of a table makes those values very difficult to work with in the future.</p>\n<hr />\n<p>To reinstate the ordering of your original dataframe you can:</p>\n<ul>\n<li>set <code>&quot;id&quot;</code> as the index (as your original dataframe had)</li>\n<li>call <code>reindex</code> to reorder the columns of the <code>new_df</code> to be that of the original <code>df</code></li>\n<li>ALTERNATIVELY, you can simply select the columns using bracket notation with the previous <code>df.columns</code></li>\n</ul>\n<pre><code>ordered_new_df = new_df.set_index(&quot;id&quot;).reindex(df.columns, axis=&quot;columns&quot;)\n\n# alternative (both lines lead to same output)\nordered_new_df = new_df.set_index(&quot;id&quot;)[df.columns]\n\nprint(ordered_new_df)\n      name type  link            subs\nid\n1 2      x   tx    10       sub1 sub2\n3 4 5    y   rx    11  sub3 sub4 sub5\n</code></pre>\n<p>Note that you'll also get the &quot;correct&quot; ordering by just calling <code>new_df.set_index(&quot;id&quot;)</code> however the methods I showed are a more explicit.</p>\n",
        "question_body": "<p>The goal is to organize the data based on multiple columns and put contents of other columns in one cell, for example,</p>\n<p>df:</p>\n<pre><code>    name type link subs\nid\n1    x    tx   10   sub1\n2    x    tx   10   sub2\n3    y    rx   11   sub3\n4    y    rx   11   sub4 \n5    y    rx   11   sub5 \n</code></pre>\n<p>To organize the table based on name/type/link, expect to get:</p>\n<pre><code>        name type link subs\nid\n1 2      x   tx   10   sub1 sub2\n3 4 5    y   rx   11   sub3 sub4 sub5\n</code></pre>\n<p>I know it is possible to organize the data stream based on multiple columns,</p>\n<pre><code>df.groupby(['name','type', 'link']).reset_index()\n</code></pre>\n<p>But I don't know how to deal with the other two columns and put them in one cell.\nThank you for any suggestions.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22053903,
            "reputation": 27,
            "user_id": 16317755,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/cdaba52a269332e8e7d62a5e3d09c74c?s=128&d=identicon&r=PG&f=1",
            "display_name": "koler",
            "link": "https://stackoverflow.com/users/16317755/koler"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68136824,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1624653853,
        "creation_date": 1624652317,
        "question_id": 68136659,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68136659/data-frame-modification-based-on-values-in-two-columns-in-python-pandas",
        "title": "Data Frame modification based on values in two columns in Python Pandas?",
        "answer_body": "<p>Use col2 to construct an ignore-case regex, which removes that portion from the col1. Then use <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.str.title.html\" rel=\"nofollow noreferrer\"><code>Series.str.title</code></a> on col2 to construct col3 and strip the regex result for col4.</p>\n<p>Because <code>Series.str.replace</code> does not take a changing regular expression (trying it, contra Obi-Wan's warnings, throws <code>TypeError: 'Series' objects are mutable, thus they cannot be hashed</code>), I'll use apply.</p>\n<pre><code>df['col3'] = df['col2'].str.title()\ndf['col4'] = df.apply(\n    lambda r: re.sub(r['col2'], '', r['col1'], flags=re.IGNORECASE).strip(), \n    axis=1)\n</code></pre>\n<p>Note that <code>re.sub</code> takes variables in the order <code>regex</code>, <code>replacement</code>, <code>string_to_replace_in</code>. I have no idea why they decided on this unintuitive parameter order.</p>\n<p>Thus (I know the column order is wrong, just generate the columns in the order of the code above rather than the code below) ---</p>\n<pre><code>&gt;&gt;&gt; df['col4'] = df.apply(lambda r: re.sub(r['col2'], '', r['col1'], flags=re.IGNORECASE).strip(), axis=1)\n&gt;&gt;&gt; df\n            col2                 col1   col4\n0     John Simon  green John Simon     green\n1      ANN StiLL  Ann Still red          red\n2  Terry Johnson  black Terry Johnson  black\n\n&gt;&gt;&gt; df['col3'] = df['col2'].str.title()\n&gt;&gt;&gt; df\n            col2                 col1   col4           col3\n0     John Simon  green John Simon     green     John Simon\n1      ANN StiLL  Ann Still red          red      Ann Still\n2  Terry Johnson  black Terry Johnson  black  Terry Johnson\n</code></pre>\n",
        "question_body": "<p>I have Data Frame in Python Pandas like below:</p>\n<pre><code>col1               | col2\n-----------------------------------\ngreen John Simon   | John Simon\nAnn Still red      | ANN StiLL\nblack Terry Johnson| Terry Johnson\n</code></pre>\n<p>And I need to create 2 new columns (col3 and col4) based on conditions below.</p>\n<p>If value from col2 is in col1 (be aware that size of letter could be different in col1 and col2!!!) then create two new columns for this value:</p>\n<ul>\n<li>in first new col (col3) add value from col2 which is in col1</li>\n<li>in second new col (col4) add value which is in col1 together with value from col2</li>\n</ul>\n<p>So, based on Data Frame and conditions I need to create columns like below:</p>\n<pre><code>col1                | col2         | col3         | col4\n-----------------------------------------------------------\ngreen John Simon    |John Simon    |John Simon    |green \nAnn Still red       |Ann Still     |Ann Still     |red\nblack Terry Johnson |Terry Johnson |Terry Johnson |black\n</code></pre>\n<p>How to do that in Python Pandas?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 15357801,
            "reputation": 23,
            "user_id": 11079624,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/--xnDiYrbOFI/AAAAAAAAAAI/AAAAAAAAAW0/6WRYw2OWpp4/photo.jpg?sz=128",
            "display_name": "rad10wave",
            "link": "https://stackoverflow.com/users/11079624/rad10wave"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 68136191,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624649555,
        "creation_date": 1624645361,
        "question_id": 68135594,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68135594/clubbing-multiple-rows-in-a-dataframe-according-to-a-column",
        "title": "Clubbing multiple rows in a dataframe according to a column",
        "answer_body": "<p><code>df['name'].value_counts()</code> creates a <code>Series</code> that has less rows than the df has.   This does not align with the required 38973 needed to assign a column to the df.</p>\n<p>Your teacher's condition - <code>df['name'].isin([ 'Alex','Ram', 'Rishabh' ])</code> - creates a <code>Series</code> that has 38973 rows which matches the df.</p>\n",
        "question_body": "<p>In my dataframe there are various names in a row, I used <code>df['name'].value_counts()</code> to count all different names, now I want to merge the names which occurred less than 10 times and make just a one row out of them, I used the code:</p>\n<pre><code>df['name'] =np.where(df['name'].value_counts()&lt;10,'Other Persons',df['name'])\n</code></pre>\n<p>but I'm getting error: <code>operands could not be broadcast together with shapes (111,) () (38973,)</code></p>\n<p>My teacher used something similar:</p>\n<pre><code>df['name'] =np.where(df['name'].isin([ 'Alex','Ram', 'Rishabh' ]),'Other Persons',df['name'])\n</code></pre>\n<p>and it worked perfectly. Please explain the difference in both the codes if possible.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-groupby"
        ],
        "owner": {
            "account_id": 22053220,
            "reputation": 3,
            "user_id": 16317170,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gg531Tfhvt5Kp7WWpT4hcWI3EU_dmiAPvAaMpBC=k-s128",
            "display_name": "Asherz i",
            "link": "https://stackoverflow.com/users/16317170/asherz-i"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68135871,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624647014,
        "creation_date": 1624644608,
        "question_id": 68135454,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68135454/sum-python-pandas-groupby-row-with-another-groupby-value",
        "title": "Sum Python Pandas Groupby() row with another Groupby() Value?",
        "answer_body": "<p>To get the sum result of: (group a, object 6) + (group b, object 6), you can use:</p>\n<pre><code>a['Object 6'] + b['Object 6']\n</code></pre>\n<p>For now <code>a</code> and <code>b</code> are Pandas Series after the <code>.groupby()</code> and <code>.sum()</code> on the respective columns.  The original <code>Type</code> columns become row index of the series, you can access the Series elements by subscripting with the index values (i.e. <code>Object 6</code> in this case).</p>\n",
        "question_body": "<p>I am still very new to python and Pandas, but I have created three groupby dataframes to sort my spreadsheet and return totals for each type using:</p>\n<pre><code>a = (df.groupby('Type')['Income'].sum())\nb = (df.groupby('Type')['Value'].sum())\nc = (df.groupby('Type')['Price'].sum())\n</code></pre>\n<p>for each of these groups they return something like:</p>\n<pre><code>Type\nObject 1           0.00\nObject 2           -2.50\nObject 3           -30.00\nObject 4            1.30\nObject 5           -20.10\nObject 6           -7.60\nObject 7           -8.09\n</code></pre>\n<p>How would I go about now summing together for example 'Object 6' from each groupby data set? For example (group a, object 6) + (group b, object 6)?</p>\n<p>Thanks.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "account_id": 13962104,
            "reputation": 51,
            "user_id": 10082987,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/141965c0e67ba4408bdb2267fd7c04f2?s=128&d=identicon&r=PG&f=1",
            "display_name": "syedmfk",
            "link": "https://stackoverflow.com/users/10082987/syedmfk"
        },
        "is_answered": true,
        "view_count": 9216,
        "accepted_answer_id": 57033774,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1624643356,
        "creation_date": 1563168421,
        "last_edit_date": 1563169986,
        "question_id": 57033657,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
        "title": "How to Extract Month Name and Year from Date column of DataFrame",
        "answer_body": "<p>Cast you date from object to actual datetime and use dt to access what you need.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n</code></pre>\n",
        "question_body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "input_data_frames": [
            "45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n",
            "45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n"
        ],
        "output_codes": [
            "import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n"
        ],
        "ques_desc": "I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. ",
        "ans_desc": "Cast you date from object to actual datetime and use dt to access what you need. ",
        "formatted_input": {
            "qid": 57033657,
            "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
            "question": {
                "title": "How to Extract Month Name and Year from Date column of DataFrame",
                "ques_desc": "I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. "
            },
            "io": [
                "45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n",
                "45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n"
            ],
            "answer": {
                "ans_desc": "Cast you date from object to actual datetime and use dt to access what you need. ",
                "code": [
                    "import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21987595,
            "reputation": 51,
            "user_id": 16261585,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5f94e4c603f799cbd63a6c0dab575576?s=128&d=identicon&r=PG&f=1",
            "display_name": "gato",
            "link": "https://stackoverflow.com/users/16261585/gato"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 68134966,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624642553,
        "creation_date": 1624641476,
        "question_id": 68134917,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68134917/how-to-choose-values-from-col1-if-their-values-are-in-col2-but-not-on-list-in-py",
        "title": "How to choose values from col1 if their values are in col2 but not on list in Python Pandas?",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>bad_list = [&quot;red&quot;, &quot;green&quot;]\n\nmask = df[&quot;description&quot;].str.contains(r&quot;|&quot;.join(bad_list))\nprint(df.loc[~mask, &quot;col1&quot;])\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>2    John Bravo\n3     Ann Still\nName: col1, dtype: object\n</code></pre>\n<hr />\n<p>EDIT: To check if value from <code>col1</code> is in description:</p>\n<pre class=\"lang-py prettyprint-override\"><code>bad_list = [&quot;red&quot;, &quot;green&quot;]\n\nmask1 = df[&quot;description&quot;].str.contains(r&quot;|&quot;.join(bad_list))\nmask2 = df.apply(lambda x: x[&quot;col1&quot;] in x[&quot;description&quot;], axis=1)\nprint(df.loc[~mask1 &amp; mask2, &quot;col1&quot;])\n</code></pre>\n<hr />\n<p>EDIT2: To ignore case:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import re\n\nbad_list = [&quot;RED&quot;, &quot;green&quot;]\n\nmask1 = df[&quot;description&quot;].str.contains(r&quot;|&quot;.join(bad_list), flags=re.I)\nmask2 = df.apply(lambda x: x[&quot;col1&quot;] in x[&quot;description&quot;], axis=1)\nprint(df.loc[~mask1 &amp; mask2, &quot;col1&quot;])\n</code></pre>\n",
        "question_body": "<p>I have DataFrame in Python Pandas like below:</p>\n<pre><code>col1       | description\n---------- |-----------\nJohn Simon |John Simon red\nTerry Juk  |green Terry Juk\nJohn Bravo |John Bravo brown\nAnn Still  |orange Ann Still\n\nbad_list = [&quot;red&quot;, &quot;green&quot;]\n</code></pre>\n<p>And I need to select only these people from &quot;col1&quot; which have value from &quot;col&quot; in column &quot;description&quot; and something else (nevermind before or after), but it something else could not be from bad_list.</p>\n<p>So I need to select only John Bravo and Ann Still because they have their value from &quot;col1&quot; in column &quot;description&quot; and does not have words from bad_list with their name in column &quot;description&quot;.</p>\n<p>How to do that in Python Pandas?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 16501529,
            "reputation": 1760,
            "user_id": 11922765,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/ed69b624cdb86e52caf0010e274df7b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mainland",
            "link": "https://stackoverflow.com/users/11922765/mainland"
        },
        "is_answered": true,
        "view_count": 37,
        "accepted_answer_id": 68134448,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1624640858,
        "creation_date": 1624638901,
        "question_id": 68134383,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68134383/python-dataframe-logical-operations-with-two-if-statements",
        "title": "Python Dataframe logical operations with two If statements",
        "answer_body": "<p>Try via <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.select.html\" rel=\"nofollow noreferrer\"><code>np.select()</code></a>:</p>\n<pre><code>conditions=[\n    df['A'].gt(70),\n    df['A'].lt(70) &amp; df['A'].gt(40),\n    df['A'].lt(40)\n]\n\nlabels=[1,2,3]\n</code></pre>\n<p>Finally:</p>\n<pre><code>df['A_op']=np.select(conditions,labels)\n</code></pre>\n<p><strong>Note:</strong> Since you don't specify the condition for <code>df['A']==40</code> and <code>df['A']==70</code></p>\n<p>but if you wish for</p>\n<pre><code>#df['A']==40---&gt;3\n#df['A']==70---&gt;2\n</code></pre>\n<p>Then the above solution becomes:</p>\n<pre><code>conditions=[\n    df['A'].gt(70),\n    df['A'].le(70) &amp; df['A'].gt(40),\n    df['A'].le(40)\n]\n\nlabels=[1,2,3]\n\ndf['A_op']=np.select(conditions,labels)\n</code></pre>\n<p>Output of <code>df</code>:</p>\n<pre><code>    A       A_op\n0   78.5    1\n1   54.0    2\n2   48.0    2\n3   21.1    3\n</code></pre>\n",
        "question_body": "<p>I have a big data frame with float values. I want to perform two if logical operations.</p>\n<p>My code:</p>\n<pre><code>df = \n    A     \n0  78.5  \n1  54\n2  48\n3  21.1\n\n# I want to compare 'A' data with predefined limits and assign a rank.\n# Give rank 1 if &gt; 70, 2 if 70&lt; &gt; 40, 3 if &lt; 40\n# create a new df with limits\nadf = pd.DataFrame({'A_up':[70],'A_low':[40]})\n# repeat adf to match df size\nadf = adf.loc[adf.index.repeat(len(df))]\n\n# perform the logical operation\ndf['A_op'] = np.where((df['A']&gt;adf['A_up'].values),1, 2)\ndf['A_op'] = np.where((df['A']&lt;adf['A_low'].values),3, 2)\n</code></pre>\n<p>Present output</p>\n<pre><code>df = \n    A     A_op\n0  78.5   2\n1  54     2\n2  48     2\n3  21.1   3\n</code></pre>\n<p>Expected output</p>\n<pre><code>df = \n    A     A_op\n0  78.5   1\n1  54     2\n2  48     2\n3  21.1   3\n</code></pre>\n",
        "input_data_frames": [
            "df = \n    A     A_op\n0  78.5   2\n1  54     2\n2  48     2\n3  21.1   3\n",
            "df = \n    A     A_op\n0  78.5   1\n1  54     2\n2  48     2\n3  21.1   3\n"
        ],
        "output_codes": [
            "conditions=[\n    df['A'].gt(70),\n    df['A'].lt(70) & df['A'].gt(40),\n    df['A'].lt(40)\n]\n\nlabels=[1,2,3]\n",
            "conditions=[\n    df['A'].gt(70),\n    df['A'].le(70) & df['A'].gt(40),\n    df['A'].le(40)\n]\n\nlabels=[1,2,3]\n\ndf['A_op']=np.select(conditions,labels)\n"
        ],
        "ques_desc": "I have a big data frame with float values. I want to perform two if logical operations. My code: Present output Expected output ",
        "ans_desc": "Try via : Finally: Note: Since you don't specify the condition for and but if you wish for Then the above solution becomes: Output of : ",
        "formatted_input": {
            "qid": 68134383,
            "link": "https://stackoverflow.com/questions/68134383/python-dataframe-logical-operations-with-two-if-statements",
            "question": {
                "title": "Python Dataframe logical operations with two If statements",
                "ques_desc": "I have a big data frame with float values. I want to perform two if logical operations. My code: Present output Expected output "
            },
            "io": [
                "df = \n    A     A_op\n0  78.5   2\n1  54     2\n2  48     2\n3  21.1   3\n",
                "df = \n    A     A_op\n0  78.5   1\n1  54     2\n2  48     2\n3  21.1   3\n"
            ],
            "answer": {
                "ans_desc": "Try via : Finally: Note: Since you don't specify the condition for and but if you wish for Then the above solution becomes: Output of : ",
                "code": [
                    "conditions=[\n    df['A'].gt(70),\n    df['A'].lt(70) & df['A'].gt(40),\n    df['A'].lt(40)\n]\n\nlabels=[1,2,3]\n",
                    "conditions=[\n    df['A'].gt(70),\n    df['A'].le(70) & df['A'].gt(40),\n    df['A'].le(40)\n]\n\nlabels=[1,2,3]\n\ndf['A_op']=np.select(conditions,labels)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22052436,
            "reputation": 3,
            "user_id": 16316512,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJwjsI9sZbeAbn-x4mQtKWtFO6sEQs3MKCRbguCc=k-s128",
            "display_name": "FoolOnTheMoon",
            "link": "https://stackoverflow.com/users/16316512/foolonthemoon"
        },
        "is_answered": true,
        "view_count": 30,
        "accepted_answer_id": 68134710,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624640381,
        "creation_date": 1624637707,
        "question_id": 68134127,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68134127/pandas-is-either-not-finding-a-specific-row-of-data-or-is-detecting-it-as-an-emp",
        "title": "Pandas is either not finding a specific row of data or is detecting it as an empty data frame",
        "answer_body": "<p>The problem is your filter:</p>\n<pre><code>desc = info[info[&quot;IDshorttext&quot;].isin([str(u)])]\n</code></pre>\n<p>Your Dataframe contains strings and integers. However, you always cast them as strings to compare them. Hence you are saying &quot;give me the line that contains '6068871', a string.&quot; But there is only a line that contains 6068871, an integer.</p>\n<p>try:</p>\n<pre><code>desc = info[info[&quot;IDshorttext&quot;].isin([u])]\n</code></pre>\n<p>or</p>\n<pre><code>desc = info[info[&quot;IDshorttext&quot;] == u]\n</code></pre>\n<p>There is not really a reason to use &quot;isin()&quot; if you only have one value, and not an array/list.</p>\n",
        "question_body": "<p>I have a big chunk of data that needs to be ordered read and then merged using pandas, my problem is that I noticed that pandas was returning &quot;empty dataframe&quot; on specific rows.</p>\n<pre><code>info = pd.read_excel(&quot;01. US Books.xlsx&quot;)\nbook3 = load_workbook(&quot;01. US Books.xlsx&quot;,data_only=True)\nbook3sheet=book3['US Projects']\n\nfor i in range(3,10,1):\n u = book3sheet.cell(row=i,column=1).value\n print(str(u))\n desc = info[info[&quot;IDshorttext&quot;].isin([str(u)])]\n print(desc)\n</code></pre>\n<p>This is the code I've used for testing, I'm using a for loop to make it go through a X number of rows before stopping since I only want certain rows of data, when I run the code it works but it returns certain rows as &quot;empty dataframes&quot;</p>\n<p>For example my excel looks a little like this:</p>\n<pre><code>IDshorttext      X           Y           Z\n FR21AR3456    100000      234546    43434343\n\n    6068871    486512       45465      454544\n\n\nFR21AR34356 &lt;-This one is read perfectly and returns the whole row as a dataframe\n    6068871 &lt;-These ones are returned as empty dataframes\n</code></pre>\n<p>In my excel file I got a lot of values on the first column that look like the previous examples but only the ones that look like this &quot;6068871&quot; aren't being read.</p>\n<p>My question is: Is there something wrong with my code that makes those unable to be read or is the format of the excel file an issue?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 21987595,
            "reputation": 51,
            "user_id": 16261585,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5f94e4c603f799cbd63a6c0dab575576?s=128&d=identicon&r=PG&f=1",
            "display_name": "gato",
            "link": "https://stackoverflow.com/users/16261585/gato"
        },
        "is_answered": true,
        "view_count": 35,
        "closed_date": 1624637033,
        "accepted_answer_id": 68133724,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624636727,
        "creation_date": 1624635775,
        "question_id": 68133694,
        "link": "https://stackoverflow.com/questions/68133694/how-to-display-rows-where-is-more-than-2-values-in-column-in-python-pandas",
        "closed_reason": "Duplicate",
        "title": "How to display rows where is more than 2 values in column in Python Pandas?",
        "answer_body": "<p>try:</p>\n<pre><code>mask=df['col1'].str.split(' ').str.len().gt(2)\n</code></pre>\n<p><strong>OR</strong></p>\n<pre><code>mask=df['col1'].str.count(' ').ge(2)\n</code></pre>\n<p>Finally pass that mask:</p>\n<pre><code>out=df[mask]\n#OR\nprint(df[mask])\n</code></pre>\n",
        "question_body": "<p>I have DataFrame in Python Pandas like below:</p>\n<pre><code>col1\n-------\nJohn One\nJohn Kole Ole\nMike Robe Gut\nMichael Spark\n</code></pre>\n<p>How can I display only these values from column in above DataFrame, which have more than 2 values, so or example to display only John Kole Ole and Mike Robe Gut, because these values have more than 2 words?</p>\n<p>How to do that in Python Pandas?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-science"
        ],
        "owner": {
            "account_id": 22043740,
            "reputation": 33,
            "user_id": 16309118,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJybPc8vxvWIrDxCthqqokqzKWE1HAm9PjVdbU0=k-s128",
            "display_name": "user16309118",
            "link": "https://stackoverflow.com/users/16309118/user16309118"
        },
        "is_answered": true,
        "view_count": 88,
        "accepted_answer_id": 68132648,
        "answer_count": 4,
        "score": 3,
        "last_activity_date": 1624633905,
        "creation_date": 1624599756,
        "last_edit_date": 1624629921,
        "question_id": 68126025,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68126025/filter-out-rows-of-a-dataframe-containing-a-specific-string",
        "title": "filter out rows of a dataframe containing a specific string",
        "answer_body": "<p>After reproducing your Data,</p>\n<pre><code>&gt;&gt;&gt; df\n                                                         patient.drug\n0  [{'drugcharacterization': '1', 'medicinalproduct': 'PANDOL'}]\n1  [{'drugcharacterization': '2', 'medicinalproduct': 'NIFEDIPINE'}]\n2  [{'drugcharacterization': '3', 'medicinalproduct': 'SIMVASTATIN'}]\n3  [{'drugcharacterization': '3', 'medicinalproduct': 'SIMVASTATIN'}]\n4  [{'drugcharacterization': '4', 'medicinalproduct': 'NIFEDIPINE'}]\n</code></pre>\n<p>While using Your code:</p>\n<pre><code>&gt;&gt;&gt; df[df['patient.drug'].str.contains('NIFEDIPINE')]\n</code></pre>\n<p>Error:</p>\n<pre><code>    raise KeyError(f&quot;None of [{key}] are in the [{axis_name}]&quot;)\nKeyError: &quot;None of [Float64Index([nan, nan, nan, nan, nan], dtype='float64')] are in the [columns]&quot;\n</code></pre>\n<h2>Solution:</h2>\n<pre><code>    &gt;&gt;&gt; df[df['patient.drug'].astype('str').str.contains('NIFEDIPINE')]\n                                                        patient.drug\n1  [{'drugcharacterization': '2', 'medicinalproduct': 'NIFEDIPINE'}]\n4  [{'drugcharacterization': '4', 'medicinalproduct': 'NIFEDIPINE'}]\n</code></pre>\n<p>Note:</p>\n<p>This is raising issue due to <code>indexer</code> check in the pandas <code>indexer.py</code> section, which is as follows:</p>\n<p>--&gt; <code>pandas/core/indexing.py</code></p>\n<pre><code># Count missing values:\nmissing_mask = indexer &lt; 0\nmissing = (missing_mask).sum()\n\nif missing:\n    if missing == len(indexer):\n        axis_name = self.obj._get_axis_name(axis)\n        raise KeyError(f&quot;None of [{key}] are in the [{axis_name}]&quot;)\n\n    # We (temporarily) allow for some missing keys with .loc, except in\n    # some cases (e.g. setting) in which &quot;raise_missing&quot; will be False\n</code></pre>\n",
        "question_body": "<p>I have a massive dataframe. The dataframe has column patient.drug. This column contains list of dictionaries as its elements.\nI want to filter out all the rows that containn 'NIFEDIPINE' word in patient.drug column.</p>\n<p>The dataframe is very large. Here is a sample of it.</p>\n<pre><code>                                                         patient.drug\n0                       [{'drugcharacterization': '1', 'medicinalproduct': 'PANDOL'}]\n1                       [{'drugcharacterization': '2', 'medicinalproduct': 'NIFEDIPINE'}]      \n2                       [{'drugcharacterization': '3', 'medicinalproduct': 'SIMVASTATIN'}]\n3                       [{'drugcharacterization': '4', 'medicinalproduct': 'NIFEDIPINE'}]      \n</code></pre>\n<p>so far, I have tried</p>\n<pre><code>df[df['patient.drug'].str.contains('NIFEDIPINE')]\n</code></pre>\n<p>but it is giving me an error.</p>\n<pre><code> raise KeyError(f&quot;None of [{key}] are in the [{axis_name}]&quot;)\n</code></pre>\n<pre><code>KeyError: &quot;None of [Float64Index([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\\n              ...\\n              nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\\n             dtype='float64', length=12000)] are in the [columns]&quot;\n</code></pre>\n<p>I have also tried using <code>in</code> operator and iterating over rows.</p>\n<pre><code>lst=[]\nfor i in range(len(df)):\n    if 'NIFEDIPINE' in df.loc[i, &quot;patirnt.drug&quot;]:\n        lst.append(i)\nprint(lst)\n</code></pre>\n<p>Which is also giving me an error.\nWhat should I do to get it right?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime",
            "pandas-resample"
        ],
        "owner": {
            "account_id": 8994606,
            "reputation": 159,
            "user_id": 6705487,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-qMFjt-w_vdc/AAAAAAAAAAI/AAAAAAAAAIU/icMvw5STRZ4/photo.jpg?sz=128",
            "display_name": "Peter La Anguila",
            "link": "https://stackoverflow.com/users/6705487/peter-la-anguila"
        },
        "is_answered": true,
        "view_count": 50,
        "accepted_answer_id": 67736737,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1624632370,
        "creation_date": 1622195004,
        "last_edit_date": 1624632370,
        "question_id": 67736534,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67736534/how-do-i-split-a-dataframe-based-on-datetimes-differences",
        "title": "How do I split a dataframe based on datetimes differences?",
        "answer_body": "<p><strong>UPDATED ANSWER</strong>:</p>\n<ol>\n<li><p>Convert the <code>mintime</code> column to datetime via <code>pd.to_datetime</code>.</p>\n</li>\n<li><p>Evaluate the difference in seconds / fill NAN values with 0 and check if the difference is greater than 15 sec or not. Take the\n<code>cumsum</code> of the result and use <code>groupby</code> .</p>\n</li>\n</ol>\n<pre><code>df.mintime = pd.to_datetime(df.mintime)\ndf_list = [g for k,g  in df.groupby((~(df.mintime.diff().dt.total_seconds().fillna(0) &lt; 15)).cumsum())]\n</code></pre>\n<p>OUTPUT:</p>\n<pre><code>[                                 mintime check\n 1375 2020-02-18 12:17:51.275000064+00:00   GO1\n 1376 2020-02-18 12:17:56.484999936+00:00   GO1\n 1377    2020-02-18 12:18:06.020000+00:00   GO1\n 1378 2020-02-18 12:18:10.922000128+00:00  NOGO,\n                                  mintime check\n 1379 2020-02-18 14:47:48.353999872+00:00   GO2\n 1380    2020-02-18 14:47:48.768000+00:00   GO2\n 1381    2020-02-18 14:48:03.120000+00:00   GO2]\n</code></pre>\n",
        "question_body": "<p>Say I have this dataframe with datetimes separated by an unknown time interval:</p>\n<pre><code>data[0]:\n        mintime                              check\n1375    2020-02-18 12:17:51.275000064+00:00  GO1\n1376    2020-02-18 12:17:56.484999936+00:00  GO1\n1377    2020-02-18 12:18:06.020000+00:00     GO1\n1378    2020-02-18 12:18:10.922000128+00:00  NOGO\n1379    2020-02-18 14:47:48.353999872+00:00  GO2\n1380    2020-02-18 14:47:48.768000+00:00     GO2\n1381    2020-02-18 14:48:03.120000+00:00     GO2\n</code></pre>\n<p>I am trying to split the dataframe. That is, if the datetimes are separated by no more than 15 seconds, they will be grouped into a new dataframe.</p>\n<p>My attempt to do this begins with the column <code>check</code>. That column tells if the value on its row and the following value are separated within 15 seconds (GO) or more than 15 seconds (NOGO).</p>\n<p>The reason I add a number after GO is to be able to distinguish groups of GO's. And this is my attempt code:</p>\n<pre><code>databds = []\nintervalo = pd.Timedelta(seconds = 15)\np = 0\nfor x in range(0,len(data)):\n    for y in range(0,len(data[x])-1):     \n        t = pd.to_datetime(data[x]['mintime'][y][0:19])\n        tp1 = pd.to_datetime(data[x]['mintime'][y+1][0:19])\n        resta = tp1 - t\n        if resta &gt; intervalo:\n            data[x]['check'][y] = &quot;NOGO&quot;\n            p = p + 1\n        else:\n            data[x]['check'][y] = &quot;{}{}&quot;.format(&quot;GO&quot;, p)   \n    for z in range(0,p):\n        datito = data[x].loc[data[x]['check'] == &quot;{}{}&quot;.format(&quot;GO&quot;, z)]\n        databds.append(datito)\n</code></pre>\n<p>This process is long and demanding on resources. I believe there must be an easier way to do this. I have tried applying pandas resample with no luck tho.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-analysis",
            "exploratory-data-analysis"
        ],
        "owner": {
            "account_id": 10649509,
            "reputation": 11,
            "user_id": 16225006,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dff001fe5e839c6f4c17bc3791c40933?s=128&d=identicon&r=PG",
            "display_name": "Huma",
            "link": "https://stackoverflow.com/users/16225006/huma"
        },
        "is_answered": true,
        "view_count": 51,
        "accepted_answer_id": 68132415,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624630237,
        "creation_date": 1623783994,
        "last_edit_date": 1624628742,
        "question_id": 67992120,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67992120/i-want-to-apply-multiple-filters-and-change-a-column-value-accordingly-in-pandas",
        "title": "I want to apply multiple filters and change a column value accordingly in pandas [Working now]",
        "answer_body": "<p>The code is working! It had no error.<br />\nThe value that I referred to here as 'a' was a mess in my real dataset that caused the problem.</p>\n",
        "question_body": "<p>Suppose I have a dataframe like so:</p>\n<pre><code>Fil1 Fil2                    A    B   C   D \na    crossdev radio com      Act  1   23  324\nb    crossdev webapp radio   Act  4   45  343\na    Streaming webapp radio  Act  3   23  566\na    crossdev com            Act  1   12  746\n</code></pre>\n<p>The Fil1 column in the actual file is really long name that I'm filtering, but here I'm referencing it as just 'a'.</p>\n<p>The code I'm using is --</p>\n<pre><code>df.loc[(df['Fil1'] == 'a') &amp; (df['Fil2'].str.contains('com')) , 'C'] = 0\ndf.loc[(df['Fil1'] == 'a') &amp; (df['Fil2'].str.contains('com')) , 'D'] = 0\ndf.loc[(df['Fil1'] == 'a') &amp; (df['Fil2'].str.contains('com')) , 'A'] = 'Fail'\n</code></pre>\n<p>Outputting this df to excel.</p>\n<p>Desired Excel Output:</p>\n<pre><code>Fil1 Fil2                    A     B   C   D \na    crossdev radio com      Fail  1   0   0\nb    crossdev webapp radio   Act   4   45  343\na    Streaming webapp radio  Act   3   23  566\na    crossdev com            Fail  1   0   0\n</code></pre>\n<p>My code is not giving me any error but it is not even giving me desired result.\nIs there any other workaround?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 11710722,
            "reputation": 15,
            "user_id": 12467412,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/3828fc860c3fcbfba01e0a75011b05d1?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dawn.Sahil",
            "link": "https://stackoverflow.com/users/12467412/dawn-sahil"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68132293,
        "answer_count": 5,
        "score": 0,
        "last_activity_date": 1624629832,
        "creation_date": 1624628816,
        "question_id": 68132060,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68132060/drop-only-nan-values-from-a-row-in-a-dataframe",
        "title": "Drop only Nan values from a row in a dataframe",
        "answer_body": "<p>We can mask by <code>notna()</code></p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf1 = pd.DataFrame(\n    {\n        'l1': [np.nan, 5],\n        'l2': [6, np.nan],\n        'd': ['a', 'b']\n     }\n)\n\nnotna = df1[['l1', 'l2']].notna().values\nnotna_values = df1[['l1', 'l2']].values[notna]\nprint(notna_values)\n\ndf2 = pd.DataFrame(df1['d'])\ndf2['code'] = notna_values\n\nprint(df2)\n\n</code></pre>\n<p>out:</p>\n<pre><code>   d  code\n0  a   6.0\n1  b   5.0\n</code></pre>\n",
        "question_body": "<p>I have a dataframe which looks something like this:</p>\n<pre><code>Df\nlev1    lev2   lev3    lev4   lev5   description\nRD21    Nan    Nan     Nan    Nan    Oil\nNan     RD32   Nan     Nan    Nan    Oil/Canola\nNan     Nan    RD33    Nan    Nan    Oil/Canola/Wheat\nNan     Nan    RD34    Nan    Nan    Oil/Canola/Flour\nNan     Nan    Nan     RD55   Nan    Oil/Canola/Flour/Thick\nED54    Nan    Nan     Nan    Nan    Rice\nNan     ED66   Nan     Nan    Nan    Rice/White\nNan     Nan    ED88    Nan    Nan    Rice/White/Jasmine\nNan     Nan    ED89    Nan    Nan    Rice/White/Basmati\nNan     ED68   Nan     Nan    Nan    Rice/Brown\n</code></pre>\n<p>I want to remove all the NaN values and just keep the non Nan values, something like this:</p>\n<pre><code>DF2\ncode     description\nRD21     Oil\nRD32     Oil/Canola\nRD33     Oil/Canola/Wheat\nRD34     Oil/Canola/Flour\nRD55     Oil/Canola/Flour/Thick\n.\n.\n.\n</code></pre>\n<p>How do I do this? I tried using notna() method, but it returns a boolean value of the dataframe. Any help would be appreciated.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 10649509,
            "reputation": 11,
            "user_id": 16225006,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dff001fe5e839c6f4c17bc3791c40933?s=128&d=identicon&r=PG",
            "display_name": "Huma",
            "link": "https://stackoverflow.com/users/16225006/huma"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 68108035,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624628424,
        "creation_date": 1624488711,
        "last_edit_date": 1624628424,
        "question_id": 68107801,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68107801/use-one-df-column-to-filter-another-df-multiple-filter",
        "title": "Use one df column to filter another df, multiple filter",
        "answer_body": "<p>do you have an example of your data?\n<br> I'm not sure if this is what you want.. though you could try this</p>\n<pre><code>df1.loc[df1['Version \u2013 USE'].isin(df2['Version - USE']), 'Cost Total'] = 0\n</code></pre>\n",
        "question_body": "<p>I want to filter df1 based on values in the Version column of df2, and then change the Cost Total to 0 in df1. I want to change Cost of those Version which are there in df2.<br />\ndf1 is [24867 rows x 63 columns]<br />\ndf2 is [35 rows x 7 columns]</p>\n<p>The code I'm using for filtering and setting value is:</p>\n<pre><code>        df1.loc[\n            (df1['Group'] == &quot;CBSS_cq_....JZJN&quot;) &amp;\n            (df1['Version \u2013 USE'] == df2['Version - USE']),\n            df1['Cost Total']] = 0\n</code></pre>\n<p>The code is assigning Cost Total to 0 for all the 'Group', it is not filtering on my second condition for Version. giving error:</p>\n<blockquote>\n<p>raise ValueError(&quot;Can only compare identically-labeled Series objects&quot;)<br />\nValueError: Can only compare identically-labeled Series objects</p>\n</blockquote>\n<p>note that when I used <code>.values</code> :</p>\n<pre><code>    df1.loc[\n            (df1['Group'] == &quot;CBSS_.......KJZJN&quot;) &amp;\n            (df1['Version \u2013 USE'].values == df2['Version'].values),\n            df1['Cost Total']] = 0  \n</code></pre>\n<p>giving me following error:</p>\n<blockquote>\n<p>block_values = np.empty(block_shape, dtype=dtype)<br />\nValueError: array is too big; <code>arr.size * arr.dtype.itemsize</code> is larger than the maximum possible size.</p>\n</blockquote>\n<p>**********the above is sorted with .isin *************</p>\n<p>My df2 is the template files which are 24 excel files, each having 3-4 sheets. I have looped through all the files and their sheets.<br />\nIndex Template files are named like-</p>\n<p>AdDape CBS Index Template 6.3.xlsx<br />\nAdDape Midlife Index Template 5.3.xlsx</p>\n<p>And looks  like below:</p>\n<p><a href=\"https://i.stack.imgur.com/xSJml.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/xSJml.png\" alt=\"enter image description here\" /></a></p>\n<pre><code>print(&quot;\\nIndex Template Files\\n&quot;)\nos.chdir('path to my \\IndexTemplatefiles')\nFileList = glob.glob('*.xlsx')\nprint(FileList)\n\nfor fname in FileList:\n    excel = pd.ExcelFile(fname)\n\nsheets = pd.ExcelFile(fname).sheet_names  # list of sheets\nprint(fname)\n\nfor sheet in excel.sheet_names:\n\n    df2 = pd.read_excel(excel, sheet_name=sheet)\n\n    df3 = pd.read_excel(CostGroupFile, sheet_name='Sheet2')\n\n    #merging df1 and df2\n    df1 = pd.merge(df1, df2, left_on='Version', right_on='Version Market - USE', how='left')\n\n    df1.loc[(\n        (df1['Cost Group'] == &quot;CBSS_ron_rt_na_disp_JZJN&quot;) &amp;\n        (df1['Version'].isin(df2['Version Market - USE'])),\n        'Cost Total')] = (df1['Market Spend'] / df1['Sum of Impressions']) * df1['Impressions']\n\n    #deleting extra columns\n    df1 = df1.drop(columns=['..all columns that came after merging'])\n\n    df1.to_excel(writer, index=False)\n    writer.save()\n</code></pre>\n<p>This code is working and updating the Cost total values but as you can see the Cost group I have entered manually, I want that to be dynamic.</p>\n<p>If the excel file (Index Template files) name is similar to the df3[filename] and its sheet's name i.e. sheetname of df2 is similar to the df3[Sheetname] then use that corresponding cost group and use in the filter part to filter df1 and update cost total.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "date"
        ],
        "owner": {
            "account_id": 21585764,
            "reputation": 23,
            "user_id": 15916254,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/3917518878338228/picture?type=large",
            "display_name": "Damar Buana Murti",
            "link": "https://stackoverflow.com/users/15916254/damar-buana-murti"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 68131587,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624628100,
        "creation_date": 1624626500,
        "question_id": 68131500,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68131500/python-pandas-add-value-to-multiple-rows-based-on-date",
        "title": "Python Pandas Add Value to Multiple Rows Based on Date",
        "answer_body": "<p>Let's try a different approach:</p>\n<pre><code>df['date'] = df.apply(\n    lambda r: pd.date_range(r['check_in'], r['check_out'], closed='left'),\n    axis=1)\n\ndf = df.explode('date')\n\ndf['earnings'] = df['payment'] / df.groupby(level=0)['date'].transform('count')\n\ndf = df.groupby('date', as_index=False)['earnings'].agg('sum')\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>        date  earnings\n0 2020-02-27      50.0\n1 2020-02-28      72.0\n2 2020-02-29      22.0\n3 2020-03-01      22.0\n</code></pre>\n<hr />\n<p>Breakdown of steps:</p>\n<p><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\" rel=\"nofollow noreferrer\"><code>apply</code></a> <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html\" rel=\"nofollow noreferrer\"><code>pd.date_range</code></a> to each row to get the days between start and end:</p>\n<pre><code>df['date'] = df.apply(\n    lambda r: pd.date_range(r['check_in'], r['check_out'], closed='left'),\n    axis=1)\n</code></pre>\n<pre><code>    check_in  check_out  payment                                                                                         date\n0 2020-02-28 2020-03-02       66  DatetimeIndex(['2020-02-28', '2020-02-29', '2020-03-01'], dtype='datetime64[ns]', freq='D')\n1 2020-02-27 2020-02-29      100                DatetimeIndex(['2020-02-27', '2020-02-28'], dtype='datetime64[ns]', freq='D')\n</code></pre>\n<p>Then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html\" rel=\"nofollow noreferrer\"><code>explode</code></a> the <code>date</code> into rows:</p>\n<pre><code>df = df.explode('date')\n</code></pre>\n<pre><code>    check_in  check_out  payment       date\n0 2020-02-28 2020-03-02       66 2020-02-28\n0 2020-02-28 2020-03-02       66 2020-02-29\n0 2020-02-28 2020-03-02       66 2020-03-01\n1 2020-02-27 2020-02-29      100 2020-02-27\n1 2020-02-27 2020-02-29      100 2020-02-28\n</code></pre>\n<p>Then <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html\" rel=\"nofollow noreferrer\"><code>groupby transform</code></a> count <code>date</code> to get the number of dates and divide the payment by the number of days to get the daily earnings:</p>\n<pre><code>df['earnings'] = df['payment'] / df.groupby(level=0)['date'].transform('count')\n</code></pre>\n<pre><code>    check_in  check_out  payment       date  earnings\n0 2020-02-28 2020-03-02       66 2020-02-28      22.0\n0 2020-02-28 2020-03-02       66 2020-02-29      22.0\n0 2020-02-28 2020-03-02       66 2020-03-01      22.0\n1 2020-02-27 2020-02-29      100 2020-02-27      50.0\n1 2020-02-27 2020-02-29      100 2020-02-28      50.0\n</code></pre>\n<p>Then <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html\" rel=\"nofollow noreferrer\"><code>groupby agg</code></a> sum the earnings on the <code>date</code> to get the total per <code>date</code>:</p>\n<pre><code>df = df.groupby('date', as_index=False)['earnings'].agg('sum')\n</code></pre>\n<pre><code>        date  earnings\n0 2020-02-27      50.0\n1 2020-02-28      72.0\n2 2020-02-29      22.0\n3 2020-03-01      22.0\n</code></pre>\n<hr />\n<p>DataFrame and imports used:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({'check_in': {0: '2020-02-28', 1: '2020-02-27'},\n                   'check_out': {0: '2020-03-02', 1: '2020-02-29'},\n                   'payment': {0: 66, 1: 100}})\ndf['check_in'] = pd.to_datetime(df['check_in'])\ndf['check_out'] = pd.to_datetime(df['check_out'])\n</code></pre>\n",
        "question_body": "<p>I need to solve a data science problem using only <strong>Python</strong> and <strong>Pandas</strong> where the given input is the date of check in, the date of check out, and the payments of every books from a single person</p>\n<pre><code>check_in   check_out  payment\n2020-02-28 2020-03-02 66\n2020-02-27 2020-02-29 100\n</code></pre>\n<p>I need to show how much daily earnings I got from that 2 books earlier. I got the idea that I have to split it for each date like 66/3 and distribute it from 02-28 to 03-02 and 100/2 then distribute it from 02-27 to 02-29 for the second order, then I will get result like this</p>\n<pre><code>date       earnings\n2020-02-27 50\n2020-02-28 22+50\n2020-02-29 22\n2020-03-01 22\n</code></pre>\n<p>Which will be like this if I summed them up</p>\n<pre><code>date       earnings\n2020-02-27 50\n2020-02-28 72\n2020-02-29 22\n2020-03-01 22\n</code></pre>\n<p>I have already divided the payment column from input and create a new dataframe with two new columns which represent how long did the guests stay using <strong>day_stay</strong> and how much earnings did I receive with <strong>daily_earn</strong> using the code below</p>\n<pre><code>df[&quot;date_stay&quot;] = abs(df[&quot;check_in&quot;] - df[&quot;check_out&quot;]) # Get difference\ndf[&quot;date_stay&quot;] = pd.to_numeric(df[&quot;date_stay&quot;].dt.days) # Turn to days\ndf[&quot;daily_earn&quot;] = df[&quot;payment&quot;]/df[&quot;date_stay&quot;]\n</code></pre>\n<p>Then the result will be look like this</p>\n<pre><code>check_in   check_out  payment day_stay daily_earn\n2020-02-28 2020-03-02 66      3        22\n2020-02-27 2020-02-29 100     2        50\n</code></pre>\n<p>The last step is to distribute the value in <strong>daily_earn</strong> to the corresponding date from 02-27 to 03-01 but I don't have any idea to do that. I have tried to use <strong>df.groupby(&quot;check_in&quot;).sum()</strong> but it didn't give the desired output since the number of rows between the input and output is different. Any ideas?</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 22050407,
            "reputation": 1,
            "user_id": 16314795,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9866fa5fcd46188812a05e9dce69bb06?s=128&d=identicon&r=PG&f=1",
            "display_name": "Kristopher Zapata Lima",
            "link": "https://stackoverflow.com/users/16314795/kristopher-zapata-lima"
        },
        "is_answered": true,
        "view_count": 33,
        "closed_date": 1624663277,
        "accepted_answer_id": 68130378,
        "answer_count": 3,
        "score": -1,
        "last_activity_date": 1624625289,
        "creation_date": 1624620855,
        "last_edit_date": 1624624114,
        "question_id": 68130332,
        "link": "https://stackoverflow.com/questions/68130332/how-can-i-remove-values-before-a-space-in-python",
        "closed_reason": "Needs details or clarity",
        "title": "How can I remove values before a space in python",
        "answer_body": "<p>IIUC, you can try:</p>\n<pre><code>df.index = df.index.str.rsplit(' ', n=1).str[-1]\n</code></pre>\n",
        "question_body": "<p>I want to remove values before a space, since the numbers before the space don't belong there.\n<a href=\"https://i.stack.imgur.com/2tnkt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/2tnkt.png\" alt=\"enter image description here\" /></a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 6451127,
            "reputation": 5,
            "user_id": 4998017,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5bccc65883f9503f8f3fb46fb050fc4b?s=128&d=identicon&r=PG&f=1",
            "display_name": "ankl",
            "link": "https://stackoverflow.com/users/4998017/ankl"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 68130935,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624624556,
        "creation_date": 1624623139,
        "question_id": 68130801,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68130801/pandas-lookup-value-in-other-dataframe-and-document-state",
        "title": "Pandas: Lookup value in other dataframe and document state",
        "answer_body": "<p>You can do a left merge on <code>df1</code> and <code>df2</code> using <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html\" rel=\"nofollow noreferrer\"><code>.merge()</code></a> and turn on <code>indicator=</code> parameter:</p>\n<pre><code>df_out = df1.merge(df2, on='Server', how='left', indicator='MissingServer')\n</code></pre>\n<p>Then, on the resulting dataframe, you can look at the column <code>MissingServer</code> and check for those marked as <code>left_only</code>.  Those will be the servers found in <code>df1</code> but not in <code>df2</code>.</p>\n",
        "question_body": "<p>I have two dataframes... First dataframe (df1) as a list of server with further informationen... The second dataframe (df2) is almost the same but, doesn't include systems which are dismantle.</p>\n<p>I want to add a column to df1, if the server was not found in df2.</p>\n<p>df1:\n<a href=\"https://i.stack.imgur.com/8pgKj.png\" rel=\"nofollow noreferrer\">dataframe df1</a></p>\n<p>df2:\n<a href=\"https://i.stack.imgur.com/pao7M.png\" rel=\"nofollow noreferrer\">dataframe2</a></p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-science",
            "user-defined-functions"
        ],
        "owner": {
            "account_id": 22043055,
            "reputation": 53,
            "user_id": 16308567,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GiFR9NhVy6x45kWD5V3tbOXBSsw7VOBsa5QB8oAPw=k-s128",
            "display_name": "vish vas chauhan Diary",
            "link": "https://stackoverflow.com/users/16308567/vish-vas-chauhan-diary"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68120274,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1624624434,
        "creation_date": 1624554662,
        "last_edit_date": 1624568723,
        "question_id": 68120067,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68120067/how-to-find-closest-col-name-based-on-if-row-conditions-in-python-dataframe",
        "title": "How to find closest col name based on if row conditions in python dataframe?",
        "answer_body": "<p>You can do idx max and idxmin here with some changes:</p>\n<pre><code>u = df.set_index(&quot;ID&quot;).replace(0,np.nan)\nfirst_date_rank = u.idxmin(1)\nlast_date_rank= u.iloc[:,::-1].idxmax(1)\nbest_rank = u.min(1)\n\nout = u.assign(first_date_rank=first_date_rank, last_date_rank=last_date_rank,\n         best_latest_date_rank=first_date_rank,best_rank=best_rank).reset_index()\n</code></pre>\n<hr />\n<pre><code>print(out)\n\n    ID  26-01-2021  01-02-2021  01-03-2021 first_date_rank last_date_rank  \\\n0  id1         NaN          12          58      01-02-2021     01-03-2021   \n1  id2        15.0          17          17      26-01-2021     01-03-2021   \n\n  best_latest_date_rank  best_rank  \n0            01-02-2021       12.0  \n1            26-01-2021       15.0  \n</code></pre>\n",
        "question_body": "<p>All IDs contain ranks from 1-100. Purpose is to understand the flow of ranking during dates.</p>\n<p>Please Help!</p>\n<p>1.I want to find the first date from the left if value is not 0 but close to 1.</p>\n<p>2.I want to find last date for best rank from the right.</p>\n<p>3.I want to find the best rank(minimum) in all dates.</p>\n<p>Input</p>\n<pre><code>import datetime\nd = {'ID': [&quot;id1&quot;,&quot;id2&quot;], '26-01-2021': [0, 15],'01-02-2021': [12, 17],'01-03-2021': [58, 17]}\ndf = pd.DataFrame(data=d)\n\nID  26-01-2021  01-02-2021  01-03-2021\nid1     0         12          58\nid2     15        17          17\n</code></pre>\n<p>Desired_output</p>\n<pre><code>ID  26-01-2021  01-02-2021  01-03-2021  first_date_rank last_date_rank  best_latest_date_rank   best_rank\nid1   0              12           58    01-02-2021        01-03-2021            01-02-2021      12\nid2  15              17           17    26-01-2021        01-03-2021            26-01-2021      15\n</code></pre>\n<p>I tried argmin but it doesn't work</p>\n<pre><code>def get_date(row):\n    date_range = row[dft.columns[1:]]\n    closest_value_key = abs(100 - date_range).argmin()\n    closest_date = date_range[closest_value_key]\n    column_name = date_range.keys()[closest_value_key]\n    return pd.Series((closest_date, column_name))\n\ndft[['best_latest_date_rank', 'best_rank']] = dft.apply(lambda row:get_date(row), axis=1)\n</code></pre>\n<p>Please help!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 18328675,
            "reputation": 3,
            "user_id": 13347753,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-rkOycoiJ0cM/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJOWeAWEgb1857jzU1RhvoLCb9KdmA/photo.jpg?sz=128",
            "display_name": "Krusenstern",
            "link": "https://stackoverflow.com/users/13347753/krusenstern"
        },
        "is_answered": true,
        "view_count": 52,
        "accepted_answer_id": 68130464,
        "answer_count": 5,
        "score": 0,
        "last_activity_date": 1624623052,
        "creation_date": 1624621016,
        "question_id": 68130374,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68130374/converting-a-dict-to-pandas-dataframe-keeping-the-keys-in-a-row",
        "title": "Converting a dict to pandas dataframe, keeping the keys in a row",
        "answer_body": "<p>You can use:</p>\n<pre><code>df = pd.DataFrame(x).T.reset_index()\ndf.columns = ['POINT', 'X', 'Y',  'Z']\n</code></pre>\n<p>Result:</p>\n<pre><code>print(df)\n\n  POINT    X    Y    Z\n0     A  3.4  4.2  5.6\n1     B  5.6  7.8  2.3\n</code></pre>\n",
        "question_body": "<p>I have a dict like this:</p>\n<pre><code>x = { 'A': [3.4, 4.2, 5.6]\n      'B': [5.6, 7.8, 2.3]\n      etc....\n    }\n</code></pre>\n<p>I want to create a pandas dataframe that looks like this:</p>\n<pre><code>'POINT'   'X'   'Y'   'Z'\n   A      3.4   4.2   5.6\n   B      5.6   7.8   2.3\netc...\n</code></pre>\n<p>df=pd.DataFrame(list(x.items())...</p>\n<p>yields a 2 column dataframe with the key in column 0 and the value as list in column 1.</p>\n<p>df=pd.DataFrame.from_dict(x)</p>\n<p>yields a basically correct dataframe, but there is no column for POINT, as I understand it uses that as index. Is there a simple way to achieve this with dict conversion? (I know I can get there with loops through the dict and filling the dataframe cells)\nAnd sorry if this is a stupid question, I have little to no experience with pandas...</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "counter"
        ],
        "owner": {
            "account_id": 2628764,
            "reputation": 457,
            "user_id": 2275237,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://www.gravatar.com/avatar/3eb706cab04729dd1ff4a5570b6768e7?s=128&d=identicon&r=PG",
            "display_name": "DonnRK",
            "link": "https://stackoverflow.com/users/2275237/donnrk"
        },
        "is_answered": true,
        "view_count": 1992,
        "accepted_answer_id": 48251672,
        "answer_count": 5,
        "score": 6,
        "last_activity_date": 1624621547,
        "creation_date": 1515946560,
        "last_edit_date": 1515947501,
        "question_id": 48251562,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/48251562/how-can-i-get-modes-of-pandas-dataframe-object-values",
        "title": "How can I get mode(s) of pandas dataframe object values?",
        "answer_body": "<p>You can get that information directly from the <code>Counter</code> like:</p>\n\n<h3>Code:</h3>\n\n<pre><code>from collections import Counter\n\ndata = Counter({'Erk': 118, 'James': 120, 'John': 126,\n                'Michael': 122, 'Phil': 117, 'Ryan': 126})\n\nby_count = {}\nfor k, v in data.items():\n     by_count.setdefault(v, []).append(k)\nmax_value = max(by_count.keys())\nprint(by_count[max_value], len(by_count[max_value]), max_value)\n</code></pre>\n\n<h3>Results:</h3>\n\n<pre><code>['John', 'Ryan'] 2 126\n</code></pre>\n",
        "question_body": "<p>I have a <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\" rel=\"noreferrer\"><code>pandas.DataFrame</code></a> containing numerous columns. I am interested in just one of those columns ('names') whose type = 'object'. I want to answer three questions about this column:</p>\n\n<ol>\n<li><p>What value(s) appear most often excluding nan values?</p></li>\n<li><p>How many values meet that criteria (count of value in answer #1)?</p></li>\n<li><p>How often do those values appear?</p></li>\n</ol>\n\n<p>I started with a large dataframe (df). The column I am interested in is called 'names'. First, I used collection.Counter to get the number of occurrences for each unique value in the 'names' column:</p>\n\n<pre><code>In [52]: cntr = collections.Counter([r for i, r in df['names'].dropna().iteritems()])\nOut[52]: Counter({'Erk': 118,\n    'James': 120,\n    'John': 126,\n    'Michael': 129,\n    'Phil': 117,\n    'Ryan': 126})\n</code></pre>\n\n<p>Then I converted the Counter back to a dataframe:</p>\n\n<pre><code>In [53]: df1 = pd.DataFrame.from_dict(cntr, orient='index').reset_index()\nIn [54]: df1 = df1.rename(columns={'index':'names', 0:'cnt'})\n</code></pre>\n\n<p>This gave me a pandas dataframe containing:</p>\n\n<pre><code>In [55]: print (type(df1), df1)\nOut[55]: &lt;class 'pandas.core.frame.DataFrame'&gt;\n       names    cnt\n    0      Erk  118\n    1    James  120\n    2     Phil  117\n    3     John  126\n    4  Michael  122\n    5     Ryan  126\n</code></pre>\n\n<p>The next part is where I need a bit of help. My desired output in this example is:</p>\n\n<p>Answer #1 = [John, Ryan]</p>\n\n<p>Answer #2 = 2</p>\n\n<p>Answer #3 = 126</p>\n\n<p>I am not convinced using the Counter was the best option, so I am open to options that stay within the dataframe without bouncing between dataframe to counter back to dataframe.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 10301847,
            "reputation": 1639,
            "user_id": 7600334,
            "user_type": "registered",
            "accept_rate": 93,
            "profile_image": "https://www.gravatar.com/avatar/cbdcd16db25660265573fac86e88f885?s=128&d=identicon&r=PG&f=1",
            "display_name": "Mark Ginsburg",
            "link": "https://stackoverflow.com/users/7600334/mark-ginsburg"
        },
        "is_answered": true,
        "view_count": 77551,
        "accepted_answer_id": 43855492,
        "answer_count": 4,
        "score": 72,
        "last_activity_date": 1624620633,
        "creation_date": 1494270496,
        "last_edit_date": 1552095656,
        "question_id": 43855474,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/43855474/changing-sort-in-value-counts",
        "title": "changing sort in value_counts",
        "answer_body": "<p>I think you need <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sort_index.html\" rel=\"noreferrer\"><code>sort_index</code></a>, because the left column is called <code>index</code>. The full command would be <code>mt = mobile.PattLen.value_counts().sort_index()</code>. For example:</p>\n\n<pre><code>mobile = pd.DataFrame({'PattLen':[1,1,2,6,6,7,7,7,7,8]})\nprint (mobile)\n   PattLen\n0        1\n1        1\n2        2\n3        6\n4        6\n5        7\n6        7\n7        7\n8        7\n9        8\n\nprint (mobile.PattLen.value_counts())\n7    4\n6    2\n1    2\n8    1\n2    1\nName: PattLen, dtype: int64\n\n\nmt = mobile.PattLen.value_counts().sort_index()\nprint (mt)\n1    2\n2    1\n6    2\n7    4\n8    1\nName: PattLen, dtype: int64\n</code></pre>\n",
        "question_body": "<p>If I do </p>\n\n<pre><code>mt = mobile.PattLen.value_counts()   # sort True by default\n</code></pre>\n\n<p>I get</p>\n\n<pre><code>4    2831\n3    2555 \n5    1561\n[...]\n</code></pre>\n\n<p>If I do</p>\n\n<pre><code>mt = mobile.PattLen.value_counts(sort=False) \n</code></pre>\n\n<p>I get</p>\n\n<pre><code>8    225\n9    120\n2   1234 \n[...]\n</code></pre>\n\n<p>What I am trying to do is get the output in 2, 3, 4 ascending order (the left numeric column).  Can I change value_counts somehow or do I need to use a different function. </p>\n",
        "input_data_frames": [
            "4    2831\n3    2555 \n5    1561\n[...]\n",
            "8    225\n9    120\n2   1234 \n[...]\n"
        ],
        "output_codes": [
            "mobile = pd.DataFrame({'PattLen':[1,1,2,6,6,7,7,7,7,8]})\nprint (mobile)\n   PattLen\n0        1\n1        1\n2        2\n3        6\n4        6\n5        7\n6        7\n7        7\n8        7\n9        8\n\nprint (mobile.PattLen.value_counts())\n7    4\n6    2\n1    2\n8    1\n2    1\nName: PattLen, dtype: int64\n\n\nmt = mobile.PattLen.value_counts().sort_index()\nprint (mt)\n1    2\n2    1\n6    2\n7    4\n8    1\nName: PattLen, dtype: int64\n"
        ],
        "ques_desc": "If I do I get If I do I get What I am trying to do is get the output in 2, 3, 4 ascending order (the left numeric column). Can I change value_counts somehow or do I need to use a different function. ",
        "ans_desc": "I think you need , because the left column is called . The full command would be . For example: ",
        "formatted_input": {
            "qid": 43855474,
            "link": "https://stackoverflow.com/questions/43855474/changing-sort-in-value-counts",
            "question": {
                "title": "changing sort in value_counts",
                "ques_desc": "If I do I get If I do I get What I am trying to do is get the output in 2, 3, 4 ascending order (the left numeric column). Can I change value_counts somehow or do I need to use a different function. "
            },
            "io": [
                "4    2831\n3    2555 \n5    1561\n[...]\n",
                "8    225\n9    120\n2   1234 \n[...]\n"
            ],
            "answer": {
                "ans_desc": "I think you need , because the left column is called . The full command would be . For example: ",
                "code": [
                    "mobile = pd.DataFrame({'PattLen':[1,1,2,6,6,7,7,7,7,8]})\nprint (mobile)\n   PattLen\n0        1\n1        1\n2        2\n3        6\n4        6\n5        7\n6        7\n7        7\n8        7\n9        8\n\nprint (mobile.PattLen.value_counts())\n7    4\n6    2\n1    2\n8    1\n2    1\nName: PattLen, dtype: int64\n\n\nmt = mobile.PattLen.value_counts().sort_index()\nprint (mt)\n1    2\n2    1\n6    2\n7    4\n8    1\nName: PattLen, dtype: int64\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 12992596,
            "reputation": 457,
            "user_id": 9391359,
            "user_type": "registered",
            "accept_rate": 67,
            "profile_image": "https://www.gravatar.com/avatar/bf4fc6594333b2e6a047e8abb428ce5a?s=128&d=identicon&r=PG&f=1",
            "display_name": "Alex Nikitin",
            "link": "https://stackoverflow.com/users/9391359/alex-nikitin"
        },
        "is_answered": true,
        "view_count": 19,
        "accepted_answer_id": 68129414,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624616958,
        "creation_date": 1624615157,
        "last_edit_date": 1624615859,
        "question_id": 68129166,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68129166/pandas-apply-function-to-column-and-fill-2-columns-from-function-return",
        "title": "Pandas apply function to column and fill 2 columns from function return",
        "answer_body": "<p>You can return <code>pd.Series</code>. For example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>cnt = 0\n\n\ndef my_func(x):\n    global cnt\n    cnt += 10\n    return pd.Series([&quot;something {}&quot;.format(x), cnt])\n\n\ndf[[&quot;category&quot;, &quot;confidence&quot;]] = df[&quot;domain&quot;].apply(my_func)\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>            domain                   category  confidence\n0    www.test.com.    something www.test.com.          10\n1  www.someurl.com  something www.someurl.com          20\n</code></pre>\n",
        "question_body": "<p>Suppose i have a following dataframe</p>\n<pre><code>| domain           | category | confidence  \n\n| www.test.com.    |          | \n\n| www.someurl.com  |          |\n</code></pre>\n<p>I want to apply <code>my_func</code> to domain column. This function return tuple with two values, i want to fill category and confidence with those values for every row.\nSomething like  <code>df['category', 'confidence'] = df['domain'].apply(my_func)</code></p>\n<p>The result i expecting is</p>\n<pre><code>| domain           | category       | confidence  \n\n| www.test.com.    | test-category  |   0.5\n\n| www.someurl.com  |  some-category |   0.7\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 3133355,
            "reputation": 6270,
            "user_id": 2651073,
            "user_type": "registered",
            "accept_rate": 65,
            "profile_image": "https://i.stack.imgur.com/MtRZl.jpg?s=128&g=1",
            "display_name": "Ahmad",
            "link": "https://stackoverflow.com/users/2651073/ahmad"
        },
        "is_answered": true,
        "view_count": 25,
        "accepted_answer_id": 68128963,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624614311,
        "creation_date": 1624613010,
        "last_edit_date": 1624613967,
        "question_id": 68128612,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68128612/pandas-duplicated-shows-non-duplicated-rows",
        "title": "Pandas duplicated shows non-duplicated rows",
        "answer_body": "<pre><code>df.duplicated()\n</code></pre>\n<p>returns the result in the same order as the initial dataframe.\nThis means, that it is most likely that your duplicates are further down in the dataframe. Since <code>.head()</code> only shows the top 5, this might not be enough to actually see them.</p>\n<p>Also the odd number of 2877 is possible if there are duplicates with an odd amount, e.g. 3x thankful.</p>\n<p>To get a better idea if it worked, you can sort before using head:</p>\n<pre><code>dups.sort_values(by=['input_text', 'target_text']).head()\n</code></pre>\n<p>To answer the question in your title, how to get only uniques, you can invert the boolean mask you get with duplicated by using <code>~</code>:</p>\n<pre><code>df[~df.duplicated(subset=['input_text', 'target_text'], keep=False)]\n</code></pre>\n",
        "question_body": "<p>As you can see in my following code, I call <code>duplicated</code> to find duplicate rows. It says that 2877 items are duplicated, but from what I see in the results (<code>head()</code>) they are not!</p>\n<p>What's the problem?</p>\n<pre><code>&gt;&gt;&gt; df = pd.read_table('xAttr_validation_no_dups.tsv')\n&gt;&gt;&gt; dups = df[df.duplicated(subset=['input_text', 'target_text'], keep=False)]\n&gt;&gt;&gt; len(dups)\n2877\n&gt;&gt;&gt; dups.head()\n    prefix                                      input_text   target_text\n13   xAttr  PersonX \u0627\u0632 ___ \u0628\u0631\u0627\u06cc \u06a9\u0645\u06a9 \u0628\u0647 PersonY \u062a\u0634\u06a9\u0631 \u0645\u06cc \u06a9\u0646\u062f      thankful\n14   xAttr  PersonX \u0627\u0632 ___ \u0628\u0631\u0627\u06cc \u06a9\u0645\u06a9 \u0628\u0647 PersonY \u062a\u0634\u06a9\u0631 \u0645\u06cc \u06a9\u0646\u062f      grateful\n15   xAttr  PersonX \u0627\u0632 ___ \u0628\u0631\u0627\u06cc \u06a9\u0645\u06a9 \u0628\u0647 PersonY \u062a\u0634\u06a9\u0631 \u0645\u06cc \u06a9\u0646\u062f  appreciative\n36   xAttr           PersonX \u0628\u0631\u0627\u06cc \u062f\u0631\u06cc\u0627\u0641\u062a ___ \u067e\u0631\u062f\u0627\u062e\u062a \u0645\u06cc \u06a9\u0646\u062f          rich\n251  xAttr             PersonX \u0627\u0628\u062a\u062f\u0627 ___ \u0631\u0627 \u062f\u0631 \u0646\u0638\u0631 \u0645\u06cc \u06af\u06cc\u0631\u062f    thoughtful\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "account_id": 17268325,
            "reputation": 98,
            "user_id": 12504573,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/1ed57f2e7dad26a2f5ae834e38dce417?s=128&d=identicon&r=PG&f=1",
            "display_name": "jr123456jr987654321",
            "link": "https://stackoverflow.com/users/12504573/jr123456jr987654321"
        },
        "is_answered": true,
        "view_count": 17,
        "accepted_answer_id": 68128445,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624612633,
        "creation_date": 1624609239,
        "question_id": 68127758,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68127758/concatonating-2-dataframes-from-dictionary-panads",
        "title": "Concatonating 2 dataframes from dictionary panads",
        "answer_body": "<p>Use <code>add_prefix</code> method:</p>\n<pre><code>d = {'ox': dfox, 'dog': dfdog}\n\n# pd.merge, pd.concat or whatever you want\ndfcomb = pd.merge(dfox.add_prefix('ox_'), dfdog.add_prefix('dog_'), ...)\n</code></pre>\n<p>or inside the dict:</p>\n<pre><code>d1 = {k: v.add_prefix(f'{k}_') for k, v in d.items()}\n</code></pre>\n",
        "question_body": "<p>I have a dictionary which looks like this {'ox': dfox, 'dog':dfdog}. The dataframes have columns of themselves. How would I concatenate them together so that I would be able to identify what columns belong to what dataframe?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "account_id": 3337078,
            "reputation": 157,
            "user_id": 9168000,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4f8b0ca12d7cf4951fffba8f055251cb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Manglu",
            "link": "https://stackoverflow.com/users/9168000/manglu"
        },
        "is_answered": true,
        "view_count": 45,
        "closed_date": 1621621070,
        "accepted_answer_id": 67631048,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1624610654,
        "creation_date": 1621572427,
        "last_edit_date": 1624610654,
        "question_id": 67630937,
        "link": "https://stackoverflow.com/questions/67630937/dataframe-operation-when-column-values-are-column-names-of-another-dataframe",
        "closed_reason": "Needs details or clarity",
        "title": "Dataframe operation when column values are column names of another dataframe",
        "answer_body": "<p>Try:</p>\n<p>Via <code>melt()</code>,<code>merge()</code>,<code>fillna()</code> and <code>pop()</code> method:</p>\n<pre><code>df2=df2.melt(id_vars='name',var_name='Res',value_name='Flag')\ndf1=df1.merge(df2,on=['name','Res'],how='left')\ndf1['Flag']=df1.pop('Flag_y').fillna(df1.pop('Flag_x'))\n</code></pre>\n<p>Output of <code>df1</code>:</p>\n<pre><code>    name    Res     Flag\n0   abc     M       F\n1   abc     F       T\n2   pqr     F       F\n3   xyz     A       T\n</code></pre>\n",
        "question_body": "<p>Suppose I have the following two data-frames,</p>\n<pre><code>data1 = [['abc', 'M', 'T'], ['abc', 'F', 'T'], ['pqr', 'F', 'F'], ['xyz', 'A', 'F']]\ndata2 = [['abc', 'F', 'T', 'T'], ['xyz', 'T', 'F', 'T']]\n\ndf1 = pd.DataFrame(data1, columns = ['name', 'Res', 'flag'])\ndf2 = pd.DataFrame(data2, columns = ['name', 'M', 'F', 'A'])\n</code></pre>\n<p>I want to overwrite the flag column value of <code>df1 (df1['flag'])</code> based on the values given in <code>df2</code> <strong>IF REQUIRED</strong>. For example, there would not be any change for <code>pqr</code> in <code>df1['name']</code>. I have used <code>pd.mask</code> or <code>replace</code> function in pandas, but it  is not working. If anybody has any suggestions. My final output will be</p>\n<pre><code> data = [['abc', 'M', 'F'], ['abc', 'F', 'T'], ['pqr', 'F', 'F'], ['xyz', 'A', 'T']]\n df = pd.DataFrame(data, columns = ['name', 'Res', 'flag'])\n</code></pre>\n<pre><code>Out[92]: \n  name Res flag\n0  abc   M    F\n1  abc   F    T\n2  pqr   F    F\n3  xyz   A    T\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "append"
        ],
        "owner": {
            "account_id": 15028,
            "reputation": 12724,
            "user_id": 31335,
            "user_type": "registered",
            "accept_rate": 62,
            "profile_image": "https://www.gravatar.com/avatar/f469e2a39924905f0ef1563b75d3612b?s=128&d=identicon&r=PG",
            "display_name": "PhE",
            "link": "https://stackoverflow.com/users/31335/phe"
        },
        "is_answered": true,
        "view_count": 1782545,
        "protected_date": 1469170623,
        "accepted_answer_id": 24888331,
        "answer_count": 31,
        "score": 1131,
        "last_activity_date": 1624605956,
        "creation_date": 1337760751,
        "last_edit_date": 1612516764,
        "question_id": 10715965,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/10715965/create-pandas-dataframe-by-appending-one-row-at-a-time",
        "title": "Create pandas Dataframe by appending one row at a time",
        "answer_body": "<p>You can use <code>df.loc[i]</code>, where the row with index <code>i</code> will be what you specify it to be in the dataframe.</p>\n<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from numpy.random import randint\n\n&gt;&gt;&gt; df = pd.DataFrame(columns=['lib', 'qty1', 'qty2'])\n&gt;&gt;&gt; for i in range(5):\n&gt;&gt;&gt;     df.loc[i] = ['name' + str(i)] + list(randint(10, size=2))\n\n&gt;&gt;&gt; df\n     lib qty1 qty2\n0  name0    3    3\n1  name1    2    4\n2  name2    2    8\n3  name3    2    1\n4  name4    9    6\n</code></pre>\n",
        "question_body": "<p>I understand that pandas is designed to load fully populated <code>DataFrame</code> but I need to <strong>create an empty DataFrame then add rows, one by one</strong>.\nWhat is the best way to do this ?</p>\n\n<p>I successfully created an empty DataFrame with :</p>\n\n<pre><code>res = DataFrame(columns=('lib', 'qty1', 'qty2'))\n</code></pre>\n\n<p>Then I can add a new row and fill a field with :</p>\n\n<pre><code>res = res.set_value(len(res), 'qty1', 10.0)\n</code></pre>\n\n<p>It works but seems very odd :-/ (it fails for adding string value)</p>\n\n<p>How can I add a new row to my DataFrame (with different columns type) ?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 10722812,
            "reputation": 315,
            "user_id": 7891362,
            "user_type": "registered",
            "accept_rate": 75,
            "profile_image": "https://www.gravatar.com/avatar/406eb7612f7ab62216b7db4691b0f350?s=128&d=identicon&r=PG&f=1",
            "display_name": "GalacticPonderer",
            "link": "https://stackoverflow.com/users/7891362/galacticponderer"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 68124351,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1624592945,
        "creation_date": 1624584217,
        "question_id": 68124333,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68124333/pandas-non-consecutive-number-filter-dropping-0-row",
        "title": "Pandas non-consecutive number filter dropping 0 row",
        "answer_body": "<p>Try <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html#pandas-series-fillna\" rel=\"nofollow noreferrer\"><code>fillna</code></a> with a value that would make the condition true:</p>\n<pre><code>df = df[df.Test.shift().fillna(df.Test - 1) &lt; df.Test]\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>   Test\n0     1\n1     3\n2     5\n3     7\n4     9\n6    11\n8    13\n</code></pre>\n<p>Sample DataFrame that shows intermediate steps:</p>\n<pre><code>pd.DataFrame({\n    'shifted': df.Test.shift(),\n    'test': df.Test,\n    'condition': df.Test.shift() &lt; df.Test,\n    'shifted then filled': df.Test.shift().fillna(df.Test - 1),\n    'fixed condition': df.Test.shift().fillna(df.Test - 1) &lt; df.Test\n})\n</code></pre>\n<pre><code> shifted  test  condition  shifted then filled  fixed condition\n     NaN     1      False                  0.0             True\n     1.0     3       True                  1.0             True\n     3.0     5       True                  3.0             True\n     5.0     7       True                  5.0             True\n     7.0     9       True                  7.0             True\n     9.0     2      False                  9.0            False\n     2.0    11       True                  2.0             True\n    11.0     4      False                 11.0            False\n     4.0    13       True                  4.0             True\n</code></pre>\n<p>This issue is that in the first case, <code>NaN</code> is not less than 1 (<code>NaN &lt; 1</code> =&gt; <code>False</code>).</p>\n",
        "question_body": "<p>I am trying to filter a dataset in Pandas. The number must always increase, although this can be in irregular steps. I have set up a filter to ensure any values that are smaller than their predecessor are removed from the DataFrame. This is a simple example I am working with:</p>\n<pre class=\"lang-py prettyprint-override\"><code>test = {&quot;Test&quot;: [1, 3, 5, 7, 9, 2, 11, 4, 13]}\ndf = pd.DataFrame(test)\ndf = df[df.Test.shift() + 1 &lt; df.Test]\n</code></pre>\n<p>This works, with the exception that it is also dropping <code>0</code> index. i.e. the output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>    Test\n1   3\n2   5\n3   7\n4   9\n6   11\n8   13\n</code></pre>\n<p>is missing row <code>0  1</code></p>\n<p>Any ideas how to get this row in as well?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20007759,
            "reputation": 65,
            "user_id": 14665375,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GhkD8ujoLvFMdfDAtoQ9NNDHqeDBt21KHcsiLhu=k-s128",
            "display_name": "PencilBox",
            "link": "https://stackoverflow.com/users/14665375/pencilbox"
        },
        "is_answered": true,
        "view_count": 18,
        "closed_date": 1624583788,
        "accepted_answer_id": 68124305,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624583785,
        "creation_date": 1624583568,
        "question_id": 68124295,
        "link": "https://stackoverflow.com/questions/68124295/value-assignment-via-boolean-index-in-one-dataframe-changes-values-in-a-differen",
        "closed_reason": "Duplicate",
        "title": "Value assignment via boolean index in one dataframe changes values in a different dataframe",
        "answer_body": "<p>You need to add <code>copy</code></p>\n<pre><code>data_pos = data.copy()\n\ndata_neg = data.copy()\n</code></pre>\n",
        "question_body": "<p>I am not sure what I am missing... Why does <code>data</code> change when I am manipulating <code>data_pos</code> and <code>data_neg</code>? I am running 3.8.5.</p>\n<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.DataFrame(data={'A':[-3,-2,-1,0,0,1,2,3]})\n&gt;&gt;&gt; print(data)\n   A\n0 -3\n1 -2\n2 -1\n3  0\n4  0\n5  1\n6  2\n7  3\n&gt;&gt;&gt; data_pos = data\n&gt;&gt;&gt; data_neg = data\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_pos.loc[data_pos.loc[:,'A']&lt;0,'A']=0\n&gt;&gt;&gt; data_neg.loc[data_neg.loc[:,'A']&gt;0,'A']=0\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(data)\n   A\n0  0\n1  0\n2  0\n3  0\n4  0\n5  0\n6  0\n7  0\n&gt;&gt;&gt;\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 15880446,
            "reputation": 35,
            "user_id": 12104581,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/c34175b830dabab73efe861b80c61450?s=128&d=identicon&r=PG&f=1",
            "display_name": "robber",
            "link": "https://stackoverflow.com/users/12104581/robber"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 68124221,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624582622,
        "creation_date": 1624579634,
        "question_id": 68123972,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68123972/how-to-find-matching-row-based-on-a-condition-and-return-n-row-above-or-below",
        "title": "How to find matching row based on a condition and return N row above or below?",
        "answer_body": "<p>Let me know if this does the job. It chooses the subsequent indexes leaving off from the 3 PT ones, and then chooses all the rows in main_df with those index numbers.</p>\n<pre><code>final_home = home_df[home_slice]  \nprint(final_home.to_string())  # starting from where you left off\n\nsubsequent_rows = 3 # note it'll choose this value - 1, so pick 3 if you want 2\n\n# returns a list of tuples that contain the ranges of indices following the initial event\nindex_ranges = home_df[home_df['PLAYER1_TEAM_NICKNAME'] == home_df['rebounder_team']].index.map(lambda x: range(x, x + subsequent_rows))\nindex_list=[]\n# flatten the list of tuples to a list of all the index values we want\n[index_list.extend(x) for x in index_ranges]\n# go back to main_df and select all the rows with those index values\nfinal = main_df[main_df.index.isin(index_list)]\nprint(final)\n</code></pre>\n",
        "question_body": "<p>I'm trying to use Python Data Frames to process some NBA data.</p>\n<p>I have a data frame of game events and I'm trying to find when an event matches a condition (shot is a 3pointer). I then want to look at the next event in the data frame, if this event matches another condition, I would like to return the next few rows.</p>\n<p>I know this is not how to use data frames, but my first inkling is to loop and retain the values as variables.</p>\n<p>I have tried using shift to do the first part, which is getting the matching event, and in row it will have the next event. Im not sure how to get n amount of rows after when i find the correct event (in this case its same team rebound)</p>\n<p>I will chuck my current code in below, its really just trying to test the above.</p>\n<p>Thanks</p>\n<pre><code>import requests\nimport json\nimport pandas as pd\n\nurl_base = 'https://stats.nba.com/stats/playbyplayv2?EndPeriod=10&amp;EndRange=55800&amp;GameID=0022000049&amp;RangeType=2&amp;StartPeriod=1&amp;StartRange=0'\n\nheaders = {\n    'Host': 'stats.nba.com',\n    'Connection': 'keep-alive',\n    'Accept': 'application/json, text/plain, */*',\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36',\n    'Referer': 'https://stats.nba.com/',\n    &quot;x-nba-stats-origin&quot;: &quot;stats&quot;,\n    &quot;x-nba-stats-token&quot;: &quot;true&quot;,\n    'Accept-Encoding': 'gzip, deflate, br',\n    'Accept-Language': 'en-US,en;q=0.9',\n}\n\nresponse = requests.get(url_base, headers=headers)\ncontent = json.loads(response.content)\n\nresults = content[&quot;resultSets&quot;][0]\ncolumn_names = results['headers']\nrows = results['rowSet']\ndf = pd.DataFrame(rows)\ndf.columns = column_names\n\n# get a new df with just these columns\nmain_df = df[['GAME_ID', 'EVENTNUM', 'HOMEDESCRIPTION', 'VISITORDESCRIPTION', 'PLAYER1_ID', 'PLAYER1_TEAM_NICKNAME']]\nmain_df['rebounder_team'] = main_df.PLAYER1_TEAM_NICKNAME.shift(-1)\n# append the homedescription and player 1 team\nmain_df['shifted_home'] = main_df.HOMEDESCRIPTION.shift(-1)\n\n# filter and make a new df for all home descriptions that have 3pts\nhome_df = main_df[main_df['HOMEDESCRIPTION'].str.contains('3PT', na=False)]\n\n# find all rebounds that are the same team as the shooter\nhome_slice = home_df['PLAYER1_TEAM_NICKNAME'] == home_df['rebounder_team']\n\nfinal_home = home_df[home_slice]\n\nprint(final_home.to_string())\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "r",
            "pandas",
            "dataframe",
            "dplyr"
        ],
        "owner": {
            "account_id": 3690266,
            "reputation": 663,
            "user_id": 8605348,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b18cf9620ea07f375c33c770f8646fa7?s=128&d=identicon&r=PG&f=1",
            "display_name": "ardaar",
            "link": "https://stackoverflow.com/users/8605348/ardaar"
        },
        "is_answered": true,
        "view_count": 297,
        "accepted_answer_id": 60478470,
        "answer_count": 3,
        "score": 6,
        "last_activity_date": 1624577497,
        "creation_date": 1583087178,
        "question_id": 60478373,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/60478373/rearranging-columns-with-pandas-is-there-an-equivalent-to-dplyrs-select-e",
        "title": "Rearranging columns with pandas: Is there an equivalent to dplyr&#39;s select(..., everything())?",
        "answer_body": "<p>You can use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\" rel=\"noreferrer\"><code>df.drop</code></a>:</p>\n\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame({\n    \"col1\": [\"a\", \"b\", \"c\"],\n    \"id\": [1, 2, 3],\n    \"col2\": [2, 4, 6],\n    \"date\": [\"1 Feb\", \"2 Feb\", \"3 Feb\"]\n    })\n\n&gt;&gt;&gt; df\n\n  col1  id  col2   date\n0    a   1     2  1 Feb\n1    b   2     4  2 Feb\n2    c   3     6  3 Feb\n\n&gt;&gt;&gt; cols_1st = [\"id\", \"date\"]\n\n&gt;&gt;&gt; df[cols_1st + list(df.drop(cols_1st, 1))]\n\n   id   date col1  col2\n0   1  1 Feb    a     2\n1   2  2 Feb    b     4\n2   3  3 Feb    c     6\n</code></pre>\n",
        "question_body": "<p>I'm trying to rearrange columns in a DataFrame, by putting a few columns first, and then all the others after.</p>\n\n<p>With R's <code>dplyr</code>, this would look like:</p>\n\n<pre class=\"lang-r prettyprint-override\"><code>library(dplyr)\n\ndf = tibble(col1 = c(\"a\", \"b\", \"c\"),\n            id = c(1, 2, 3),\n            col2 = c(2, 4, 6),\n            date = c(\"1 Feb\", \"2 Feb\", \"3 Feb\"))\n\ndf2 = select(df,\n             id, date, everything())\n</code></pre>\n\n<p>Easy. With Python's <code>pandas</code>, here's what I've tried:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({\n    \"col1\": [\"a\", \"b\", \"c\"],\n    \"id\": [1, 2, 3],\n    \"col2\": [2, 4, 6],\n    \"date\": [\"1 Feb\", \"2 Feb\", \"3 Feb\"]\n    })\n\n# using sets\ncols = df.columns.tolist()\ncols_1st = {\"id\", \"date\"}\ncols = set(cols) - cols_1st\ncols = list(cols_1st) + list(cols)\n\n# wrong column order\ndf2 = df[cols]\n\n# using lists\ncols = df.columns.tolist()\ncols_1st = [\"id\", \"date\"]\ncols = [c for c in cols if c not in cols_1st]\ncols = cols_1st + cols\n\n# right column order, but is there a better way?\ndf3 = df[cols]\n</code></pre>\n\n<p>The <code>pandas</code> way is more tedious, but I'm fairly new to this. Is there a better way?</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dictionary",
            "dataframe",
            "pyarrow"
        ],
        "owner": {
            "account_id": 1142254,
            "reputation": 888,
            "user_id": 1125805,
            "user_type": "registered",
            "accept_rate": 91,
            "profile_image": "https://i.stack.imgur.com/llbKa.jpg?s=128&g=1",
            "display_name": "unixeO",
            "link": "https://stackoverflow.com/users/1125805/unixeo"
        },
        "is_answered": true,
        "view_count": 1071,
        "accepted_answer_id": 53640842,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624576260,
        "creation_date": 1544044025,
        "last_edit_date": 1544045604,
        "question_id": 53640738,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/53640738/preserve-index-when-loading-pyarrow-parquet-from-pandas-dataframe",
        "title": "Preserve index when loading pyarrow parquet from pandas DataFrame",
        "answer_body": "<p>If you wanted to preserve the index, then you should've specified as such; set <code>preserve_index=True</code>:</p>\n\n<pre><code>table = pa.Table.from_pandas(df, preserve_index=True)\n</code></pre>\n\n<hr>\n\n<pre><code>pq.write_table(table, 'file.parquet', flavor='spark')\npq.read_table('file.parquet').to_pandas()  # Index is preserved.\n\n     2018-12-06  2018-12-07\nKEY       250.0       234.0\n</code></pre>\n",
        "question_body": "<p>I need to convert a dict with dict values to parquet, I have data that look like this: </p>\n\n<p><code>{\"KEY\":{\"2018-12-06\":250.0,\"2018-12-07\":234.0}}</code></p>\n\n<p>I'm converting to pandas dataframe and then writing to pyarrow table: </p>\n\n<pre><code>import pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ndata = {\"KEY\":{\"2018-12-06\":250.0,\"2018-12-07\":234.0}}\ndf = pd.DataFrame.from_dict(data, orient='index')\ntable = pa.Table.from_pandas(df, preserve_index=False)\npq.write_table(table, 'file.parquet', flavor='spark')\n</code></pre>\n\n<p>I end up with data, that only have dates and values, but without the key of the dict.:</p>\n\n<pre><code>{\"2018-12-06\":250.0,\"2018-12-07\":234.0}\n</code></pre>\n\n<p>What I need is to also have the key of the data:</p>\n\n<pre><code>{\"KEY\": {\"2018-12-06\":250.0,\"2018-12-07\":234.0}}\n</code></pre>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 16182236,
            "reputation": 13,
            "user_id": 13077837,
            "user_type": "registered",
            "profile_image": "https://lh4.googleusercontent.com/-n6SVjV6yr4s/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdRrOg63lc5m3sXSGjoCI1MidKf_g/mo/photo.jpg?sz=128",
            "display_name": "bwizard123",
            "link": "https://stackoverflow.com/users/13077837/bwizard123"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68123640,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624576165,
        "creation_date": 1624574525,
        "question_id": 68123475,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68123475/is-it-possible-to-make-a-dataframe-that-only-contains-one-row-of-a-csv-at-a-time",
        "title": "Is it possible to make a dataframe that only contains one row of a csv at a time?",
        "answer_body": "<p>You can iterate through each row, convert it to a DataFrame and create a bar chart for that row</p>\n<pre><code>#Iterate through each row\nfor index, row in df.iterrows():\n   \n    #Convert row to dataframe\n    df_row = row.to_frame().T\n\n    #Proceed as before\n    df2 = df_row.set_index('ID').T\n    df2.plot(kind='bar', subplots=True, layout=(2,2), legend=False, color='g')\n</code></pre>\n",
        "question_body": "<p>I am trying to make bar graphs for each row in a csv file that I have. I have no problem making bar graphs for each row all in one go, but I want to make a png file out of each bar chart, so it would be very convenient to fill the dataframe with one row at a time.</p>\n<p>This is a snippet of code that I have that makes the bar graphs all at once from my CSV file:</p>\n<pre><code>df2 = df.set_index('ID').T\ndf2.plot(kind='bar', subplots=True, layout=(2,2), legend=False, color='g')\nplt.show()\n</code></pre>\n<p>This produces bar graphs for each row, but I want to be able to separate them and make an image file for each graph</p>\n"
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 15784124,
            "reputation": 51,
            "user_id": 11389503,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/98dc6ce5096084b52574a0df2617cb71?s=128&d=identicon&r=PG&f=1",
            "display_name": "Matt",
            "link": "https://stackoverflow.com/users/11389503/matt"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 68114983,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1624574365,
        "creation_date": 1624533013,
        "last_edit_date": 1624574365,
        "question_id": 68114631,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68114631/python-pandas-dataframe-challenge-how-do-i-avoid-iterrows-for-this-scenario",
        "title": "Python Pandas Dataframe challenge: how do I avoid Iterrows() for this scenario?",
        "answer_body": "<p>Does this work?</p>\n<pre><code>results_dict = dict()\nfor job in job_set:\n    filt = (dict_val['JobNumber'] == job)  #this creates a filter/mask with only the jobs you want\n    new_df = dict_val[filt] #this applies gives a subdataframe only with desired columns\n    temp_list = new_df['RunNumber'].tolist()\n    results_dict[job] = temp_list\n</code></pre>\n<p>I couldn't run the code as you didn't provide any example dataframe, so in can have some typo. But I hope you get the logic.</p>\n",
        "question_body": "<p><strong>Context</strong></p>\n<p>I've heard it often said &quot;you should avoid iterating through a Dataframe&quot;, or &quot;using iterrows is bad/slow/etc.&quot; or &quot;there is nothing you can do with iterrows that you can't do with apply/applymap/map&quot;. I'm trying to shed the habit of using iterrows but have found a scenario I have not as yet been able to work a way around using apply/applymap/map. I'm hoping some df guru can help me out.</p>\n<p><strong>Overview of logic/scenario</strong>\nI have a {set} of unique values that correspond to a column in my dataframe. The dataframe is transactional - I need to compare element[0] in my set to  of my dataset, where the values match: append the value of  to a list. On a complete run of the dataset (appending a list of all possible values) - pass that list as a value in dictionary (with element[0] being the key). Repeat for all {set} elements.</p>\n<p><strong>Main Challenges</strong></p>\n<ol>\n<li>I am not applying a function per element - I am comparing a {set} element to a dataframe column where it matches an arbitary number of times: append the element in a different column to a list (not df)</li>\n<li>After one complete pass of the df, take that compiled list and pass to a dictionary (again, not a df)</li>\n</ol>\n<p>All examples I've seen: apply/applymap/map work on every element in a series or dataframe - not (for example) just 2 columns out of a potential 5. Or comparing values on column 1 and 4 then appending column 10 to a structure outside of the dataframe.</p>\n<p><strong>Current Solution (looking to improve on)</strong>\n'''\n# Convert to set - make unique\njob_set = set(job_list)</p>\n<pre><code>results_dict = dict()\n# loop through set, append all runs associated per job number:\nfor job in job_set:\n    temp_list = list()\n    for idx, val in dict_val.iterrows():\n        if val['JobNumber'] == job:\n            temp_list.append(val['RunNumber'])\n\n    # append to new dict - {job number : [list of runs]}\n    results_dict[job] = temp_list\n</code></pre>\n<p>'''</p>\n<p><strong>Sample Data</strong>\n'''\nJobNumber,RunNumber,Brief Created Date\n10000,9,8/03/17\n1667,2166,5/05/18\n1667,2165,5/05/18\n1667,2153,8/04/18\n1710,3602,24/06/18\n1710,3600,22/06/18\n1710,3594,18/06/19\n1710,3589,11/06/19\n1710,3492,5/03/18\n1710,3456,27/01/18\n2265,1436,3/06/18\n2265,1429,6/05/19\n2265,1418,8/04/19\n3708,459,9/04/19\n3708,109,4/06/19\n3708,402,26/03/19\n3938,401,19/03/19\n3938,400,12/03/19\n3938,399,5/03/19\n3938,391,6/01/19\n3938,0,5/03/19\n4529,2117,24/06/19\n4529,1736,29/03/19\n4529,143,30/03/19\n4529,1158,9/03/19\n4669,374,17/05/19\n4813,30,5/01/19\n5651,62,1/06/18\n5651,61,6/04/19\n5651,60,16/03/19\n'''</p>\n<p><strong>Sample Data Added</strong></p>\n<p>Please let me know if you need more details. My challenge is simply to attempt to complete this using the 'preferred' methods of apply or applymap or map. My main intention is implimenting best practices around optimised execution times.</p>\n<p>Like I mentioned: all use cases for these methods seem to center on the dataframe/series element-level - I need a way to compare values acorss a whole dataset, then bring the result into a new data structure, rinse and repeat.</p>\n<p>Thanks in advance</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "opencv",
            "image-processing"
        ],
        "owner": {
            "account_id": 22035199,
            "reputation": 3,
            "user_id": 16301845,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/740928b32e479c7be13339c2a48298ac?s=128&d=identicon&r=PG&f=1",
            "display_name": "Apollon",
            "link": "https://stackoverflow.com/users/16301845/apollon"
        },
        "is_answered": true,
        "view_count": 53,
        "accepted_answer_id": 68107826,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1624566820,
        "creation_date": 1624487454,
        "question_id": 68107662,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68107662/how-do-i-import-images-with-filenames-corresponding-to-column-values-in-a-datafr",
        "title": "How do I import images with filenames corresponding to column values in a dataframe?",
        "answer_body": "<p>You can access all of your files using this:</p>\n<pre><code>imageList = []\n\nfor i in range(0, len(df_1)):\n    cv2.imread('./' + str(df_1['case_number'][i]) + '.bmp')\n    imageList.append('./' + str(df_1['case_number'][i]) + '.bmp')\n\nplt.imshow(imagelist[x])\n</code></pre>\n<p>This is looping through every item in the <code>case_number</code> column, the <code>./</code> shows that your file is within the current directory, using the directory path leading up to your current file. And by making everything a string and joining it you make it so that the file path is readable. The path created by joining the strings should look something like <code>./85.bmp</code>, which should open your desired file. Also, you are appending the filenames to the list so that they can be accessed by the <code>plt.imshow()</code></p>\n<p>If you would like to access the files based on their name, you can use another variable (which could be set as an input) and implement the code below</p>\n<pre><code>fileName = input('Enter Your Value: ')     \ninputFile = imageList.index('./' + fileName + '.bmp')\n</code></pre>\n<p>and from here, you could use the same <code>plt.imshow(imagelist[x])</code>, but replace the <code>x</code> with the <code>inputFile</code> variable.</p>\n",
        "question_body": "<p>I'm a doctor trying to learn some code for work, and was hoping you could help me solve a problem I have with regards to importing multiple images into python.</p>\n<p>I am working in Jupyter Notebook, where I have created a dataframe (named df_1) using pandas. In this dataframe each row represents a patient, and the first column shows the case number for each patient (e.g. 85).</p>\n<p>Now, what I want to do is import multiple images (.bmp) from a given folder(same location as the .ipynb file). There are many images in this folder, and I do not want all of them - only the ones who have filenames corresponding to the &quot;case_number&quot; column in my dataframe (e.g. 85.bmp).</p>\n<p>I already read this <a href=\"https://stackoverflow.com/questions/52485146/looping-images-in-saving-id-name-and-storing-it-correspondingly-in-dataframe?answertab=oldest#tab-top\">post</a>, but I must admit it was way to complicated for me to understand.</p>\n<p>Is there some simple loop (or something else) I could create to import all images with filenames corresponding to the values of the &quot;case number&quot; column in the dataframe?</p>\n<p>I was imagining something like the below would be possible, I just do not know how to write it.</p>\n<pre><code>for i=[(df_1['case_number'()]\n    cv2.imread('[i].bmp')\n</code></pre>\n<p>The images don't really need to be implemented in the dataframe, but I would like to be able to view them in my notebook by using e.g. plt.imshow(85) afterwards.</p>\n<p><a href=\"https://i.stack.imgur.com/xd7qy.jpg\" rel=\"nofollow noreferrer\">Here is an image of the head of my dataframe</a></p>\n<p>Thank you for helping!</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "account_id": 20509527,
            "reputation": 33,
            "user_id": 15051615,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/abbbbbfa2482f0aadc916cda9ef98d74?s=128&d=identicon&r=PG&f=1",
            "display_name": "EKLZ",
            "link": "https://stackoverflow.com/users/15051615/eklz"
        },
        "is_answered": true,
        "view_count": 55,
        "accepted_answer_id": 65918106,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1624564370,
        "creation_date": 1611677311,
        "last_edit_date": 1624564370,
        "question_id": 65904887,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/65904887/add-a-column-to-a-dataframe-based-on-another-column-dealing-with-multiple-occurr",
        "title": "Add a column to a dataframe based on another column dealing with multiple occurrence",
        "answer_body": "<p>I found an answer that is maybe not the cleanest :</p>\n<pre><code>def fun(sub_df):\n    Date = df.Date.iloc[0]\n    (sunrise, sunset) = get_sun(Date)\n    sub_df['Sunrise'] = sunrise\n    sub_df['Sunset'] = sunset\n    return sub_df\n\ndf = df.groupby('Date').apply(fun)\n</code></pre>\n<p>It's based on @Marc's answer but instead of applying my function for each rows it's being apply for each sub dataframe separated by date. I get the date by taking the first value of the date column : <code>df.Date.iloc[0]</code></p>\n",
        "question_body": "<p>I have a function that gives me the time of sunset and sunrise based on an API that I called <code>get_sun(date)</code> with date being a string of format &quot;%d/%m/%Y&quot;.</p>\n<p>I have a dataframe with a column Date containing strings of format &quot;%d/%m/%Y&quot;.</p>\n<pre><code>        Date        Time    Sky temp (C\u00b0)   Ambient temp (C\u00b0)\n0       01/01/2020  00:00:07    -13.01  8.23\n1       01/01/2020  00:01:12    -12.93  8.25\n2       01/01/2020  00:02:17    -12.91  8.19\n3       01/01/2020  00:03:22    -12.75  8.19\n4       01/01/2020  00:04:27    -12.99  8.17\n... ... ... ... ...\n349074  31/10/2020  23:54:44    8.83    8.53\n349075  31/10/2020  23:55:49    8.75    8.49\n349076  31/10/2020  23:56:54    8.65    8.47\n349077  31/10/2020  23:57:59    8.65    8.45\n349078  31/10/2020  23:59:04    8.61    8.43\n</code></pre>\n<p>I want to add to my dataframe a column 'Sunrise' and 'Sunset' but <strong>without using apply</strong>. If I use <code>dataframe.Date.apply()</code> it will iterate on every line. <strong>For a same date I have 3000 lines</strong> so it would be much quicker to call <code>get_sun</code> only once per different date.</p>\n<p>I which an output of the form :</p>\n<pre><code>        Date        Time    Sky temp (C\u00b0)   Ambient temp (C\u00b0) Sunrise Sunset\n0       01/01/2020  00:00:07    -13.01      8.23             7:58:32    18:21:39\n1       01/01/2020  00:01:12    -12.93      8.25             7:58:32    18:21:39\n2       01/01/2020  00:02:17    -12.91      8.19             7:58:32    18:21:39\n3       01/01/2020  00:03:22    -12.75      8.19             7:58:32    18:21:39\n4       01/01/2020  00:04:27    -12.99      8.17             7:58:32    18:21:39\n</code></pre>\n<p>My code is the following :</p>\n<pre><code>df['Sunrise'] = &quot;&quot;\ndf['Sunset'] = &quot;&quot;\n\nfor i in tqdm(unique(df.Date.values)):\n    (sunrise, sunset) = get_sun(i)\n    df[df.Date.apply(lambda x : x==i)]['Sunrise'].apply(lambda x : sunrise)\n    df[df.Date.apply(lambda x : x==i)]['Sunset']=sunset\n</code></pre>\n<p><code>df[df.Date.apply(lambda x : x==i)]</code> is my way to select only the lines of my dataframe where the date is equal to i. For these lines I would like to append the value of sunrise and sunset in the corresponding columns.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "string",
            "dataframe",
            "replace"
        ],
        "owner": {
            "account_id": 22044055,
            "reputation": 23,
            "user_id": 16309386,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJwcKO6c1Mr7tJ0ZJ99zWbxSK77nU_HM4loNEEpd=k-s128",
            "display_name": "KC Lee",
            "link": "https://stackoverflow.com/users/16309386/kc-lee"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 68121996,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1624564197,
        "creation_date": 1624561551,
        "question_id": 68121490,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68121490/python-dataframe-replace-values-in-column-string-with-values-from-another-data",
        "title": "Python dataframe = Replace values in column string with values from another dataframe",
        "answer_body": "<p>As @Nk03, first create a mapping dict for each Rule_ID from df2 to allow string substitution with <code>replace()</code> method:</p>\n<pre><code>params = df2.groupby('Rule_ID') \\\n            .apply(lambda x: dict(zip(x['Rule_Param'], x['Param_Value'].astype(str)))) \\\n            .to_dict()\n\nout = df1.groupby('Rule_ID') \\\n         .apply(lambda x: x['Rule_Value'].replace(params[x.name], regex=True))\n</code></pre>\n<pre><code>&gt;&gt;&gt; params\n{'R-123': {'param1': '100', 'param2': '200'}, 'R-456': {'param1': '100'}}\n\n&gt;&gt;&gt; out\nRule_ID\nR-123    0    column1 &gt; 100 and column2 &gt; 200\nR-456    1                      column1 &gt; 100\nName: Rule_Value, dtype: object\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe (RuleDF) with a column which contains a string. <br>\nInside the string are some values (parameters) which shall be replaced <br>\neg. param1 and param2</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">Rule_ID</th>\n<th style=\"text-align: left;\">Rule_Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">R-123</td>\n<td style=\"text-align: left;\">column1 &gt; param1 and column2 &gt; param2</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">R-456</td>\n<td style=\"text-align: left;\">column1 &gt; param1</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">...</td>\n<td style=\"text-align: left;\">...</td>\n</tr>\n</tbody>\n</table>\n</div><br>\nThere is another Dataframe (RuleMapDF) with the mapping:<br>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">Rule_ID</th>\n<th style=\"text-align: left;\">Rule_Param</th>\n<th style=\"text-align: left;\">Param_Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">R-123</td>\n<td style=\"text-align: left;\">param1</td>\n<td style=\"text-align: left;\">100</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">R-123</td>\n<td style=\"text-align: left;\">param2</td>\n<td style=\"text-align: left;\">200</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">R-456</td>\n<td style=\"text-align: left;\">param1</td>\n<td style=\"text-align: left;\">100</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>The result of the replacement will be something like this:<br>\nAnother option is to have a new column with the replaced string</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: left;\">Rule_ID</th>\n<th style=\"text-align: left;\">Rule_Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">R-123</td>\n<td style=\"text-align: left;\">column1 &gt; 100 and column2 &gt; 200</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">R-456</td>\n<td style=\"text-align: left;\">column1 &gt; 100</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">...</td>\n<td style=\"text-align: left;\">...</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>I appreciate any ideas.\nThank you.</p>\n"
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary",
            "data-science"
        ],
        "owner": {
            "account_id": 22043740,
            "reputation": 33,
            "user_id": 16309118,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJybPc8vxvWIrDxCthqqokqzKWE1HAm9PjVdbU0=k-s128",
            "display_name": "user16309118",
            "link": "https://stackoverflow.com/users/16309118/user16309118"
        },
        "is_answered": true,
        "view_count": 49,
        "closed_date": 1624671885,
        "accepted_answer_id": 68121468,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1624562550,
        "creation_date": 1624560998,
        "question_id": 68121372,
        "link": "https://stackoverflow.com/questions/68121372/how-to-create-a-dataframe-from-a-list-of-dictionaries",
        "closed_reason": "Needs details or clarity",
        "title": "How to create a dataframe from a list of dictionaries",
        "answer_body": "<p>Each column of your DataFrame contains a <code>list</code> with one single element, a <code>dict</code>. You can create your required DataFrame with a combination of <code>pd.concat</code> and list comprehension:</p>\n<pre><code>&gt;&gt; pd.DataFrame([x[0] for x in df[&quot;patient.reaction&quot;]])\n      reactionmeddrapt\n0           CHEST PAIN\n1       DISTURBANCE IN\n2  EXTRAMIDAL DISORDER\n</code></pre>\n",
        "question_body": "<p>I have a dataframe in which a column has dictionaries as values. I want to create a dataframe from those dictionaries.</p>\n<pre><code>\n                 patient.reaction                               patient.drug\n0           [{'reactionmeddrapt': 'CHEST PAIN'}]           [{'drugcharacterization': '1', 'medicinalprodu...\n1           [{'reactionmeddrapt': 'DISTURBANCE IN..        [{'drugcharacterization': '1', 'medi...\n2           [{'reactionmeddrapt: 'EXTRAMIDAL DISORDER'}]   [{'drugcharacterization': '1', 'medicrodu...\n</code></pre>\n<p>I want to make a dataframe from all the dictionaries that column patient.reaction has.</p>\n<p>What should be the code?</p>\n"
    }
]