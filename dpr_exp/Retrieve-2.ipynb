{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05bc30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pathlib\n",
    "import json\n",
    "import gzip\n",
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Iterator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor as T\n",
    "from torch import nn\n",
    "\n",
    "from dpr.data.qa_validation import calculate_matches_by_id\n",
    "from dpr.models import init_biencoder_components\n",
    "from dpr.options import (\n",
    "    add_encoder_params, \n",
    "    setup_args_gpu, \n",
    "    print_args, \n",
    "    set_encoder_params_from_state, \n",
    "    add_tokenizer_params, \n",
    "    add_cuda_params\n",
    ")\n",
    "from dpr.utils.data_utils import Tensorizer\n",
    "from dpr.utils.model_utils import (\n",
    "    setup_for_distributed_mode, \n",
    "    get_model_obj, \n",
    "    load_states_from_checkpoint, \n",
    "    move_to_device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165b12a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608\n",
      "{\n",
      "    \"DataFrame\": \"two-dimensional  size-mutable  potentially heterogeneous tabular data.\",\n",
      "    \"agg\": \"aggregate using one or more operations over the specified axis.\",\n",
      "    \"nunique\": \"count number of distinct elements in specified axis.\",\n",
      "    \"copy\": \"make a copy of this objects indices and data.\",\n",
      "    \"swaplevel\": \"swap levels i and j in a multiindex.\",\n",
      "    \"count\": \"count non-na cells for each column or row.\",\n",
      "    \"pivot\": \"return reshaped dataframe organized by given index / column values.\",\n",
      "    \"idxmin\": \"return index of first occurrence of minimum over requested axis.\",\n",
      "    \"drop_duplicates\": \"return dataframe with duplicate rows removed.\",\n",
      "    \"mul\": \"get multiplication of dataframe and other  element-wise (binary operator mul).\",\n",
      "    \"pct_change\": \"percentage change between the current and a prior element.\",\n",
      "    \"melt\": \"unpivot a dataframe from wide to long format  optionally leaving identifiers set.\",\n",
      "    \"to_string\": \"render a dataframe to a console-friendly tabular output.\",\n",
      "    \"dot\": \"compute the matrix multiplication between the dataframe and other.\",\n",
      "    \"reindex\": \"conform series/dataframe to new index with optional filling logic.\",\n",
      "    \"sort_values\": \"sort by the values along either axis.\",\n",
      "    \"duplicated\": \"return boolean series denoting duplicate rows.\",\n",
      "    \"rename\": \"alter axes labels.\",\n",
      "    \"stack\": \"stack the prescribed level(s) from columns to index.\",\n",
      "    \"resample\": \"resample time-series data.\",\n",
      "    \"combine_first\": \"update null elements with value in the same location in other.\",\n",
      "    \"values\": \"return a numpy representation of the dataframe.\",\n",
      "    \"head\": \"return the first n rows.\",\n",
      "    \"unstack\": \"pivot a level of the (necessarily hierarchical) index labels.\",\n",
      "    \"first\": \"select initial periods of time series data based on a date offset.\",\n",
      "    \"where\": \"replace values where the condition is false.\",\n",
      "    \"mean\": \"return the mean of the values over the requested axis.\",\n",
      "    \"merge\": \"merge dataframe or named series objects with a database-style join.\",\n",
      "    \"corr\": \"compute pairwise correlation of columns  excluding na/null values.\",\n",
      "    \"to_dict\": \"convert the dataframe to a dictionary.\",\n",
      "    \"loc\": \"access a group of rows and columns by label(s) or a boolean array.\",\n",
      "    \"sort_index\": \"sort object by labels (along an axis).\",\n",
      "    \"clip\": \"trim values at input threshold(s).\",\n",
      "    \"nlargest\": \"return the first n rows ordered by columns in descending order.\",\n",
      "    \"query\": \"query the columns of a dataframe with a boolean expression.\",\n",
      "    \"gt\": \"get greater than of dataframe and other  element-wise (binary operator gt).\",\n",
      "    \"add\": \"get addition of dataframe and other  element-wise (binary operator add).\",\n",
      "    \"lookup\": \"(deprecated) label-based fancy indexing function for dataframe.\",\n",
      "    \"max\": \"return the maximum of the values over the requested axis.\",\n",
      "    \"diff\": \"first discrete difference of element.\",\n",
      "    \"index\": \"the index (row labels) of the dataframe.\",\n",
      "    \"iat\": \"access a single value for a row/column pair by integer position.\",\n",
      "    \"rolling\": \"provide rolling window calculations.\",\n",
      "    \"ge\": \"get greater than or equal to of dataframe and other  element-wise (binary operator ge).\",\n",
      "    \"notnull\": \"detect existing (non-missing) values.\",\n",
      "    \"mask\": \"replace values where the condition is true.\",\n",
      "    \"sum\": \"return the sum of the values over the requested axis.\",\n",
      "    \"fillna\": \"fill na/nan values using the specified method.\",\n",
      "    \"quantile\": \"return values at the given quantile over requested axis.\",\n",
      "    \"insert\": \"insert column into dataframe at specified location.\",\n",
      "    \"iteritems\": \"iterate over (column name  series) pairs.\",\n",
      "    \"apply\": \"apply a function along an axis of the dataframe.\",\n",
      "    \"iterrows\": \"iterate over dataframe rows as (index  series) pairs.\",\n",
      "    \"isna\": \"detect missing values.\",\n",
      "    \"empty\": \"indicator whether dataframe is empty.\",\n",
      "    \"replace\": \"replace values given in to_replace with value.\",\n",
      "    \"ne\": \"get not equal to of dataframe and other  element-wise (binary operator ne).\",\n",
      "    \"keys\": \"get the info axis (see indexing for more).\",\n",
      "    \"pivot_table\": \"create a spreadsheet-style pivot table as a dataframe.\",\n",
      "    \"interpolate\": \"fill nan values using an interpolation method.\",\n",
      "    \"rename_axis\": \"set the name of the axis for the index or columns.\",\n",
      "    \"itertuples\": \"iterate over dataframe rows as namedtuples.\",\n",
      "    \"pop\": \"return item and drop from frame.\",\n",
      "    \"astype\": \"cast a pandas object to a specified dtype dtype.\",\n",
      "    \"round\": \"round a dataframe to a variable number of decimal places.\",\n",
      "    \"squeeze\": \"squeeze 1 dimensional axis objects into scalars.\",\n",
      "    \"transpose\": \"transpose index and columns.\",\n",
      "    \"tail\": \"return the last n rows.\",\n",
      "    \"cumprod\": \"return cumulative product over a dataframe or series axis.\",\n",
      "    \"select_dtypes\": \"return a subset of the dataframes columns based on the column dtypes.\",\n",
      "    \"to_records\": \"convert dataframe to a numpy record array.\",\n",
      "    \"at\": \"access a single value for a row/column label pair.\",\n",
      "    \"dropna\": \"remove missing values.\",\n",
      "    \"drop\": \"drop specified labels from rows or columns.\",\n",
      "    \"sample\": \"return a random sample of items from an axis of object.\",\n",
      "    \"cumsum\": \"return cumulative sum over a dataframe or series axis.\",\n",
      "    \"to_period\": \"convert dataframe from datetimeindex to periodindex.\",\n",
      "    \"groupby\": \"group dataframe using a mapper or by a series of columns.\",\n",
      "    \"le\": \"get less than or equal to of dataframe and other  element-wise (binary operator le).\",\n",
      "    \"isnull\": \"detect missing values.\",\n",
      "    \"get\": \"get item from object for given key (ex: dataframe column).\",\n",
      "    \"mode\": \"get the mode(s) of each element along the selected axis.\",\n",
      "    \"assign\": \"assign new columns to a dataframe.\",\n",
      "    \"to_json\": \"convert the object to a json string.\",\n",
      "    \"rank\": \"compute numerical data ranks (1 through n) along axis.\",\n",
      "    \"to_xarray\": \"return an xarray object from the pandas object.\",\n",
      "    \"droplevel\": \"return series/dataframe with requested index / column level(s) removed.\",\n",
      "    \"from_dict\": \"construct dataframe from dict of array-like or dicts.\",\n",
      "    \"to_csv\": \"write object to a comma-separated values (csv) file.\",\n",
      "    \"eval\": \"evaluate a string describing operations on dataframe columns.\",\n",
      "    \"lt\": \"get less than of dataframe and other  element-wise (binary operator lt).\",\n",
      "    \"size\": \"return an int representing the number of elements in this object.\",\n",
      "    \"iloc\": \"purely integer-location based indexing for selection by position.\",\n",
      "    \"append\": \"append rows of other to the end of caller  returning a new object.\",\n",
      "    \"value_counts\": \"return a series containing counts of unique rows in the dataframe.\",\n",
      "    \"update\": \"modify in place using non-na values from another dataframe.\",\n",
      "    \"columns\": \"the column labels of the dataframe.\",\n",
      "    \"any\": \"return whether any element is true  potentially over an axis.\",\n",
      "    \"set_axis\": \"assign desired index to given axis.\",\n",
      "    \"cummin\": \"return cumulative minimum over a dataframe or series axis.\",\n",
      "    \"applymap\": \"apply a function to a dataframe elementwise.\",\n",
      "    \"min\": \"return the minimum of the values over the requested axis.\",\n",
      "    \"set_index\": \"set the dataframe index using existing columns.\",\n",
      "    \"ffill\": \"synonym for dataframe.fillna() with method='ffill'.\",\n",
      "    \"join\": \"join columns of another dataframe.\",\n",
      "    \"div\": \"get floating division of dataframe and other  element-wise (binary operator truediv).\",\n",
      "    \"radd\": \"get addition of dataframe and other  element-wise (binary operator radd).\",\n",
      "    \"first_valid_index\": \"return index for first non-na value or none  if no na value is found.\",\n",
      "    \"filter\": \"subset the dataframe rows or columns according to the specified index labels.\",\n",
      "    \"to_excel\": \"write object to an excel sheet.\",\n",
      "    \"add_prefix\": \"prefix labels with string prefix.\",\n",
      "    \"std\": \"return sample standard deviation over requested axis.\",\n",
      "    \"shift\": \"shift index by desired number of periods with an optional time freq.\",\n",
      "    \"style\": \"returns a styler object.\",\n",
      "    \"explode\": \"transform each element of a list-like to a row  replicating index values.\",\n",
      "    \"reset_index\": \"reset the index  or a level of it.\",\n",
      "    \"shape\": \"return a tuple representing the dimensionality of the dataframe.\",\n",
      "    \"sub\": \"get subtraction of dataframe and other  element-wise (binary operator sub).\",\n",
      "    \"idxmax\": \"return index of first occurrence of maximum over requested axis.\",\n",
      "    \"cummax\": \"return cumulative maximum over a dataframe or series axis.\",\n",
      "    \"dtypes\": \"return the dtypes in the dataframe.\",\n",
      "    \"abs\": \"return a series/dataframe with absolute numeric value of each element.\",\n",
      "    \"eq\": \"get equal to of dataframe and other  element-wise (binary operator eq).\",\n",
      "    \"notna\": \"detect existing (non-missing) values.\",\n",
      "    \"all\": \"return whether all elements are true  potentially over an axis.\",\n",
      "    \"last\": \"select final periods of time series data based on a date offset.\",\n",
      "    \"items\": \"iterate over (column name  series) pairs.\",\n",
      "    \"isin\": \"whether each element in the dataframe is contained in values.\",\n",
      "    \"plot\": \"dataframe plotting accessor and method\",\n",
      "    \"bfill\": \"synonym for dataframe.fillna() with method='bfill'.\",\n",
      "    \"add_suffix\": \"suffix labels with string suffix.\",\n",
      "    \"transform\": \"call func on self producing a dataframe with transformed values.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk \n",
    "\n",
    "api_lists = json.load(open(\"data/api_list.json\"))\n",
    "\n",
    "def get_test_data(api_lists):\n",
    "    examples = []\n",
    "    fail_count = 0\n",
    "    example_id = 1\n",
    "    for i in range(1, 4):\n",
    "        example_file = f\"../25_K_Examples/part-{i}-output/taken_answers_with_all_details.json\"\n",
    "        data = json.load(open(example_file))\n",
    "        for e in data:\n",
    "            try:\n",
    "                ques_id = e['question_id']\n",
    "                qtitle = e['formatted_input']['question']['title']\n",
    "                qdesc = e['formatted_input']['question']['ques_desc']\n",
    "                codes = e['formatted_input']['answer']['code']\n",
    "                apis = set()\n",
    "                for c in codes:\n",
    "                    tokens = nltk.wordpunct_tokenize(c)\n",
    "                    for tidx, token in enumerate(tokens):\n",
    "                        token = token.strip()\n",
    "                        if tidx >= 0:\n",
    "                            prev_token = tokens[tidx - 1].strip()[-1]\n",
    "                            if (token in api_lists and prev_token == \".\") or token == \"DataFrame\":\n",
    "                                apis.add(token)\n",
    "                api_seq = list(sorted(apis))\n",
    "                if len(api_seq) <= 0:\n",
    "                    continue\n",
    "                examples.append({\n",
    "                    'id': ques_id,\n",
    "                    'query': qtitle.strip().lower() + \" \" + qdesc.strip().lower(),\n",
    "                    \"apis\": api_seq,\n",
    "                    'link': e['link'],\n",
    "                    \"example\": e['formatted_input']\n",
    "                })\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                fail_count += 1\n",
    "    return examples\n",
    "\n",
    "_examples = get_test_data(list(api_lists.keys()))\n",
    "\n",
    "_apis = list(sorted(api_lists.keys()))\n",
    "_api_docs = [api_lists[a] for a in _apis]\n",
    "print(len(_examples))\n",
    "print(json.dumps(api_lists, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2cd3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/bert/pandas_1/checkpoint_best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ed6281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = google/bert_uncased_L-6_H-512_A-8\n",
      "Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert\n",
      "Overriding args parameter value from checkpoint state. Param = sequence_length, value = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=32, dataset=None, device=device(type='cuda'), distributed_world_size=1, do_lower_case=False, encoder_model_type='hf_bert', fp16=False, fp16_opt_level='O1', local_rank=-1, model_file='models/bert/pandas_1/checkpoint_best.pt', n_gpu=1, no_cuda=False, pretrained_file=None, pretrained_model_cfg='google/bert_uncased_L-6_H-512_A-8', projection_dim=0, sequence_length=512, shard_size=50000)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "add_encoder_params(parser)\n",
    "add_tokenizer_params(parser)\n",
    "add_cuda_params(parser)\n",
    "parser.add_argument(\n",
    "    '--shard_size', \n",
    "    type=int, \n",
    "    default=50000, \n",
    "    help=\"Total amount of data in 1 shard\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--batch_size', \n",
    "    type=int, \n",
    "    default=32, \n",
    "    help=\"Batch size for the passage encoder forward pass\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--dataset', \n",
    "    type=str, \n",
    "    default=None, \n",
    "    help=' to build correct dataset parser '\n",
    ")\n",
    "\n",
    "args = parser.parse_args({})\n",
    "args.model_file = model_path\n",
    "setup_args_gpu(args)\n",
    "saved_state = load_states_from_checkpoint(args.model_file)\n",
    "set_encoder_params_from_state(saved_state.encoder_params, args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c76e4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorizer, encoder, _ = init_biencoder_components(args.encoder_model_type, args, inference_only=True)\n",
    "encoder.load_state_dict(saved_state.model_dict)\n",
    "\n",
    "query_model = encoder.question_model\n",
    "document_model = encoder.ctx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041fa75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3df9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([132, 512]) torch.Size([132, 512]) torch.Size([132, 512])\n",
      "torch.Size([132, 512])\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "\n",
    "api_docs = copy.deepcopy(_api_docs)\n",
    "apis = copy.deepcopy(_apis)\n",
    "document_tensors = []\n",
    "for a, d in zip(apis, api_docs):\n",
    "    tensor = tensorizer.text_to_tensor(d, title=a)\n",
    "    document_tensors.append(tensor)\n",
    "\n",
    "doc_ids = move_to_device(torch.stack(document_tensors, dim=0), args.device)\n",
    "doc_seg_batch = move_to_device(torch.zeros_like(doc_ids), args.device)\n",
    "doc_attn_mask = move_to_device(tensorizer.get_attn_mask(doc_ids), args.device)\n",
    "\n",
    "print(doc_ids.shape, doc_seg_batch.shape, doc_attn_mask.shape)\n",
    "\n",
    "document_model.to(args.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, doc_vectors, _ = document_model(doc_ids, doc_seg_batch, doc_attn_mask)\n",
    "print(doc_vectors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4edd8836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([608, 512])\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def get_query_vectors(sentences):\n",
    "    query_tensors = []\n",
    "    for ex in sentences:\n",
    "        tensor = tensorizer.text_to_tensor(ex)\n",
    "        query_tensors.append(tensor)\n",
    "\n",
    "    query_ids = move_to_device(torch.stack(query_tensors, dim=0), args.device)\n",
    "    query_seg_batch = move_to_device(torch.zeros_like(query_ids), args.device)\n",
    "    query_attn_mask = move_to_device(tensorizer.get_attn_mask(query_ids), args.device)\n",
    "\n",
    "    query_model.to(args.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, query_vectors, _ = query_model(query_ids, query_seg_batch, query_attn_mask)\n",
    "    return query_ids, query_seg_batch, query_attn_mask, query_vectors\n",
    "\n",
    "test_examples = copy.deepcopy(_examples)\n",
    "query_sentences = [ex[\"query\"] for ex in test_examples]\n",
    "query_ids, query_seg_batch, query_attn_mask, query_vectors = get_query_vectors(query_sentences)\n",
    "print(query_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b08151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(608, 132)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_results = cosine_similarity(query_vectors.cpu().numpy(), doc_vectors.cpu().numpy())\n",
    "print(similarity_results.shape)\n",
    "\n",
    "torch.save(\n",
    "    (\n",
    "        query_sentences, \n",
    "        query_ids.cpu(), \n",
    "        query_seg_batch.cpu(), \n",
    "        query_attn_mask.cpu(), \n",
    "        query_vectors.cpu(), \n",
    "        doc_vectors.cpu(), \n",
    "        similarity_results\n",
    "    ), \n",
    "    \"from_jupyter.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fafb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'query', 'apis', 'link', 'example', 'expected', 'predicted'])\n"
     ]
    }
   ],
   "source": [
    "for exid, example in enumerate(test_examples):\n",
    "    pred_similaroty = [(a, s) for a, s in zip(apis, similarity_results[exid, :].tolist())]\n",
    "    sorted_apis = sorted(pred_similaroty, key=lambda x: x[1])[::-1]\n",
    "    example[\"expected\"] = example[\"apis\"]\n",
    "    example[\"predicted\"] = sorted_apis\n",
    "    \n",
    "print(test_examples[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b6cefe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "{\n",
      "    \"id\": 63264777,\n",
      "    \"query\": \"python: append 2 columns of a dataframe together i am loading a csv file into a data frame using pandas. my dataframe looks something like this: i wish to append 2 of the columns into a new column: col4 needs to be created by appending the contents of col1 and col2 together. how can i do this in pandas/python? edit\",\n",
      "    \"link\": \"https://stackoverflow.com/questions/63264777/python-append-2-columns-of-a-dataframe-together\",\n",
      "    \"example\": {\n",
      "        \"qid\": 63264777,\n",
      "        \"link\": \"https://stackoverflow.com/questions/63264777/python-append-2-columns-of-a-dataframe-together\",\n",
      "        \"question\": {\n",
      "            \"title\": \"Python: Append 2 columns of a dataframe together\",\n",
      "            \"ques_desc\": \"I am loading a csv file into a data frame using pandas. My dataframe looks something like this: I wish to append 2 of the columns into a new column: col4 needs to be created by appending the contents of col1 and col2 together. How can I do this in pandas/python? EDIT \"\n",
      "        },\n",
      "        \"io\": [\n",
      "            \"col1       col2       col3         \\n1           4           1 \\n2           5           2\\n3           6           3\\n\",\n",
      "            \"  col1       col2        col3       col4   \\n    1           4           1         1\\n    2           5           2         2\\n    3           6           3         3\\n                                      4\\n                                      5 \\n                                      6\\n\"\n",
      "        ],\n",
      "        \"answer\": {\n",
      "            \"ans_desc\": \"First use or with for new and then add to original by or : Last for avoid converting to floats is possible use: Or convert missing values to empty strings (what should be problem if need processing df later by some numeric method): EDIT: \",\n",
      "            \"code\": [\n",
      "                \"s = df['col1'].append(df['col2'], ignore_index=True).rename('col4')\\n#alternative\\n#s = pd.concat([df['col1'], df['col2']], ignore_index=True).rename('col4')\\n\\ndf1 = df.join(s, how='outer')\\n#alternative\\n#df1 = pd.concat([df, s], axis=1)\\n\",\n",
      "                \"df = df.reset_index(drop=True)\\n\\ns1 = df['full_name'].append(df['alt_name'], ignore_index=True).rename('combined_names')\\ns2 = df['full_address'].append(df['alt_add'], ignore_index=True).rename('combined_address')\\n\\ndf1 = pd.concat([df, s1, s2], axis=1)\\n\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"expected\": [\n",
      "        \"append\",\n",
      "        \"join\",\n",
      "        \"rename\",\n",
      "        \"reset_index\"\n",
      "    ],\n",
      "    \"predicted\": [\n",
      "        [\n",
      "            \"append\",\n",
      "            0.8843740820884705\n",
      "        ],\n",
      "        [\n",
      "            \"DataFrame\",\n",
      "            0.8709995150566101\n",
      "        ],\n",
      "        [\n",
      "            \"reset_index\",\n",
      "            0.8696591258049011\n",
      "        ],\n",
      "        [\n",
      "            \"stack\",\n",
      "            0.8635185360908508\n",
      "        ],\n",
      "        [\n",
      "            \"explode\",\n",
      "            0.8600462675094604\n",
      "        ],\n",
      "        [\n",
      "            \"rename\",\n",
      "            0.8577467799186707\n",
      "        ],\n",
      "        [\n",
      "            \"itertuples\",\n",
      "            0.85721355676651\n",
      "        ],\n",
      "        [\n",
      "            \"values\",\n",
      "            0.8566570281982422\n",
      "        ],\n",
      "        [\n",
      "            \"columns\",\n",
      "            0.8565797209739685\n",
      "        ],\n",
      "        [\n",
      "            \"join\",\n",
      "            0.8562741875648499\n",
      "        ]\n",
      "    ]\n",
      "}\n",
      "====================================================================================================\n",
      "{\n",
      "    \"id\": 56020272,\n",
      "    \"query\": \"convert rows per unique id into all comma separated possibilities i have some data in the following format, at the moment this is in a pandas dataframe. what i require is all of the possible combinations of the lenders columns for each uid so the output would be something like this and the same for uid 2 and so on, apologies if this has been answered before i'm just unsure of how to approach this. thanks,\",\n",
      "    \"link\": \"https://stackoverflow.com/questions/56020272/convert-rows-per-unique-id-into-all-comma-separated-possibilities\",\n",
      "    \"example\": {\n",
      "        \"qid\": 56020272,\n",
      "        \"link\": \"https://stackoverflow.com/questions/56020272/convert-rows-per-unique-id-into-all-comma-separated-possibilities\",\n",
      "        \"question\": {\n",
      "            \"title\": \"Convert Rows per Unique Id into all comma separated possibilities\",\n",
      "            \"ques_desc\": \"i have some data in the following format, at the moment this is in a Pandas Dataframe. What i require is all of the possible combinations of the Lenders columns for each Uid so the output would be something like this And the same for Uid 2 and so on, apologies if this has been answered before i'm just unsure of how to approach this. Thanks, \"\n",
      "        },\n",
      "        \"io\": [\n",
      "            \"Row   Uid    Lender\\n1     1      HSBC\\n2     1      Lloyds\\n3     1      Barclays\\n4     2      Lloyds\\n5     2      Barclays\\n6     2      Santander\\n7     2      RBS\\n8     2      HSBC\\n\",\n",
      "            \"Row   Uid   LenderCombo\\n1     1     Barclays\\n2     1     Lloyds\\n3     1     HSBC\\n4     1     Barclays, HSBC\\n5     1     Barclays, Lloyds\\n6     1     HSBC, Lloyds \\n7     1     Barclays, HSBC, Lloyds\\n\"\n",
      "        ],\n",
      "        \"answer\": {\n",
      "            \"ans_desc\": \"Use with custom function and join tuples by : \",\n",
      "            \"code\": [\n",
      "                \"from itertools import chain, combinations\\n\\n#https://stackoverflow.com/a/5898031\\ndef all_subsets(ss):\\n    return chain(*map(lambda x: combinations(ss, x), range(1, len(ss)+1)))\\n\\ndf = (df.groupby('Uid')['Lender']\\n       .apply(lambda x: pd.Series([', '.join(y) for y in all_subsets(x)]))\\n       .reset_index()\\n       .rename(columns={'level_1':'Row'}))\\n\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"expected\": [\n",
      "        \"apply\",\n",
      "        \"groupby\",\n",
      "        \"join\",\n",
      "        \"rename\",\n",
      "        \"reset_index\"\n",
      "    ],\n",
      "    \"predicted\": [\n",
      "        [\n",
      "            \"columns\",\n",
      "            0.8913850784301758\n",
      "        ],\n",
      "        [\n",
      "            \"filter\",\n",
      "            0.888978123664856\n",
      "        ],\n",
      "        [\n",
      "            \"join\",\n",
      "            0.8820846080780029\n",
      "        ],\n",
      "        [\n",
      "            \"reset_index\",\n",
      "            0.8809670209884644\n",
      "        ],\n",
      "        [\n",
      "            \"set_index\",\n",
      "            0.879010021686554\n",
      "        ],\n",
      "        [\n",
      "            \"values\",\n",
      "            0.8757408261299133\n",
      "        ],\n",
      "        [\n",
      "            \"groupby\",\n",
      "            0.8756147027015686\n",
      "        ],\n",
      "        [\n",
      "            \"rename\",\n",
      "            0.8716951608657837\n",
      "        ],\n",
      "        [\n",
      "            \"apply\",\n",
      "            0.8689320683479309\n",
      "        ],\n",
      "        [\n",
      "            \"unstack\",\n",
      "            0.8678364753723145\n",
      "        ]\n",
      "    ]\n",
      "}\n",
      "====================================================================================================\n",
      "113 113\n"
     ]
    }
   ],
   "source": [
    "def analyze_performance(top_k):\n",
    "    result = []\n",
    "    percentage = []\n",
    "    count = []\n",
    "    full_correct = []\n",
    "    for example in test_examples:\n",
    "        match_found = 0\n",
    "        prediction_apis = [x[0] for x in example[\"predicted\"][:top_k]]\n",
    "#         example[\"predicted\"] = example[\"predicted\"][:top_k]\n",
    "        expected = set(example['expected'])\n",
    "        num_apis = len(expected)\n",
    "        for api in expected:\n",
    "            if api in prediction_apis:\n",
    "                match_found += 1\n",
    "        result.append({\n",
    "            'example': example,\n",
    "            'num_apis': num_apis,\n",
    "            'match_found': match_found,\n",
    "            'percentage': match_found/float(num_apis),\n",
    "            'full_match': 1 if match_found == len(expected) else 0\n",
    "        })\n",
    "        if match_found == len(expected):\n",
    "            _ex = copy.deepcopy(example)\n",
    "            _ex[\"predicted\"] = _ex[\"predicted\"][:top_k]\n",
    "            _ex.pop(\"apis\", None)\n",
    "            full_correct.append(_ex)\n",
    "        count.append(match_found)\n",
    "        percentage.append(match_found/float(num_apis) * 100)\n",
    "    return result, percentage, count, full_correct\n",
    "\n",
    "\n",
    "def show_performance(k, max_apis=8):\n",
    "    top_k_res, top_k_per, top_k_count, _ = analyze_performance(k)\n",
    "\n",
    "    count_to_percentage = {}\n",
    "    for res in top_k_res:\n",
    "        if res['num_apis'] not in count_to_percentage:\n",
    "            count_to_percentage[res['num_apis']] = []\n",
    "        count_to_percentage[res['num_apis']].append(res)\n",
    "        pass\n",
    "    print(\"|\" + (\"=\" * 55) + \"|\")\n",
    "    print(\"|\" + (\" \" * 25) + (f\"Top %2d\" % k) + (\" \" * 24) + \"|\")\n",
    "    print(\"|\" + (\"-\" * 55) + \"|\")\n",
    "    print(\"| #APIs\\t| #Examples\\t| Full\\t| Half\\t| 1/3\\t| 1/4\\t|\")\n",
    "    print(\"|\" + (\"-\" * 55) + \"|\")\n",
    "    full, half, one_third, one_fourth, total_nums = 0, 0, 0, 0, 0\n",
    "    for num_actual_api in sorted(count_to_percentage.keys()):\n",
    "        if num_actual_api > max_apis:\n",
    "            break\n",
    "        results = count_to_percentage[num_actual_api]\n",
    "        total_full_correct = sum([1 if r['percentage'] > 0.999 else 0 for r in results])\n",
    "        full += total_full_correct\n",
    "        total_half_correct = sum([1 if r['percentage'] >= 0.499 else 0 for r in results])\n",
    "        half += total_half_correct\n",
    "        total_one_third_correct = sum([1 if r['percentage'] >= 0.33 else 0 for r in results])\n",
    "        one_third += total_one_third_correct\n",
    "        total_one_fourth_correct = sum([1 if r['percentage'] >= 0.2499 else 0 for r in results])\n",
    "        one_fourth += total_one_fourth_correct\n",
    "        total_nums += len(results)\n",
    "        print(\n",
    "            \"| %d\\t| %d\\t\\t| %d\\t| %d\\t| %d\\t| %d\\t|\" % (\n",
    "                num_actual_api, \n",
    "                len(results),\n",
    "                total_full_correct, \n",
    "                total_half_correct, \n",
    "                total_one_third_correct, \n",
    "                total_one_fourth_correct, \n",
    "            )\n",
    "        )\n",
    "        pass\n",
    "    print(\"|\" + (\"=\" * 55) + \"|\")\n",
    "    print(\n",
    "            \"| %s\\t| %d\\t\\t| %d\\t| %d\\t| %d\\t| %d\\t|\" % (\n",
    "                \"total\", \n",
    "                total_nums,\n",
    "                full, \n",
    "                half, \n",
    "                one_third, \n",
    "                one_fourth, \n",
    "            )\n",
    "        )\n",
    "    print(\"|\" + (\"=\" * 55) + \"|\")\n",
    "    return top_k_res, count_to_percentage\n",
    "    pass\n",
    "\n",
    "top_k_res, top_k_per, top_k_count, full_correct = analyze_performance(10)\n",
    "print(len(full_correct))\n",
    "taken_ids = set()\n",
    "for f in full_correct:\n",
    "    if f['id'] != f['example']['qid']:\n",
    "        print(f['id'], f['example']['qid'])\n",
    "    taken_ids.add(f['id'])\n",
    "    if len(f['expected']) >= 4:\n",
    "        print(json.dumps(f, indent=4))\n",
    "        print(\"=\" * 100)\n",
    "# print(json.dumps(full_correct[6:15], indent=4))\n",
    "print(len(full_correct), len(taken_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ed7a349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"id\": 67257898,\n",
      "        \"query\": \"how to add a value to a new column by referencing the values in a column i have a dataframe like this: the xy column must be filled with the value of the column names in the reason column. let's look at the first row. the reason column shows our value x1. so our value in column xy, will be the value of x1 column in the first row. like this: is there a way to do this?\",\n",
      "        \"link\": \"https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column\",\n",
      "        \"example\": {\n",
      "            \"qid\": 67257898,\n",
      "            \"link\": \"https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column\",\n",
      "            \"question\": {\n",
      "                \"title\": \"How to add a value to a new column by referencing the values in a column\",\n",
      "                \"ques_desc\": \"I have a dataframe like this: The xy column must be filled with the value of the column names in the reason column. Let's look at the first row. The reason column shows our value x1. So our value in column xy, will be the value of x1 column in the first row. Like this: Is there a way to do this? \"\n",
      "            },\n",
      "            \"io\": [\n",
      "                \"id  reason  x1    x2   x3   x4   x5 \\n 1  x1      100   15   10   20   25\\n 2  x1      15    16   14   10   10\\n 3  x4      10    50   40   30   25\\n 4  x3      12    15   60   5    1\\n 5  x1      80    15   10   20   25\\n 6  x1      15    19   84   10   10\\n 7  x4      90    40   90   30   25\\n 8  x4      12    85   60   50   10\\n\",\n",
      "                \"id  reason  x1    x2   x3   x4   x5   xy\\n 1  x1      100   15   10   20   25   100\\n 2  x1      15    16   14   10   10   15\\n 3  x4      10    50   40   30   25   30\\n 4  x3      12    15   60   5    1    60\\n 5  x1      80    15   10   20   25   80\\n 6  x1      15    19   84   10   10   15\\n 7  x4      90    40   90   30   25   30\\n 8  x4      12    85   60   50   10   50\\n\"\n",
      "            ],\n",
      "            \"answer\": {\n",
      "                \"ans_desc\": \" Prints: \",\n",
      "                \"code\": [\n",
      "                    \"df[\\\"xy\\\"] = df.apply(lambda x: x[x[\\\"reason\\\"]], axis=1)\\nprint(df)\\n\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"expected\": [\n",
      "            \"apply\"\n",
      "        ],\n",
      "        \"predicted\": [\n",
      "            [\n",
      "                \"astype\",\n",
      "                0.9048309326171875\n",
      "            ],\n",
      "            [\n",
      "                \"where\",\n",
      "                0.9040565490722656\n",
      "            ],\n",
      "            [\n",
      "                \"iat\",\n",
      "                0.9031678438186646\n",
      "            ],\n",
      "            [\n",
      "                \"values\",\n",
      "                0.9017855525016785\n",
      "            ],\n",
      "            [\n",
      "                \"apply\",\n",
      "                0.9003180861473083\n",
      "            ],\n",
      "            [\n",
      "                \"applymap\",\n",
      "                0.8976870775222778\n",
      "            ],\n",
      "            [\n",
      "                \"loc\",\n",
      "                0.8966784477233887\n",
      "            ],\n",
      "            [\n",
      "                \"eval\",\n",
      "                0.8945940136909485\n",
      "            ],\n",
      "            [\n",
      "                \"lookup\",\n",
      "                0.8939566612243652\n",
      "            ],\n",
      "            [\n",
      "                \"transform\",\n",
      "                0.8932572603225708\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 67243081,\n",
      "        \"query\": \"best way to change column data for all rows over multiple dataframes in pandas? consider dataframes , , and . and have an column, and has a and column. i need to iterate over all rows of , and replace and with new unique randomly generated uuids, and then update those in and where (before the change to uuid). i originally wanted to iterate over all rows of and simply check both and if they contain the original or inside the column before replacing both, but i found that iterating over pandas rows is a bad idea and slow. i'm not sure how i can apply the other mentioned methods in that post to this problem since i'm not applying a simple function or calculating anything, and i think the way i had intended to do it would be too slow for big dataframes. my current method that i believe to be slow and inefficient: here and are above mentioned and , and is example example : example :\",\n",
      "        \"link\": \"https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas\",\n",
      "        \"example\": {\n",
      "            \"qid\": 67243081,\n",
      "            \"link\": \"https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas\",\n",
      "            \"question\": {\n",
      "                \"title\": \"Best way to change column data for all rows over multiple dataframes in pandas?\",\n",
      "                \"ques_desc\": \"Consider dataframes , , and . and have an column, and has a and column. I need to iterate over all rows of , and replace and with new unique randomly generated UUIDs, and then update those in and where (before the change to UUID). I originally wanted to iterate over all rows of and simply check both and if they contain the original or inside the column before replacing both, but I found that iterating over pandas rows is a bad idea and slow. I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes. My current method that I believe to be slow and inefficient: Here and are above mentioned and , and is Example Example : Example : \"\n",
      "            },\n",
      "            \"io\": [\n",
      "                \"+---+----+\\n|   | id |\\n+---+----+\\n| 1 | a1 |\\n+---+----+\\n| 2 | c1 |\\n+---+----+\\n\",\n",
      "                \"+---+----+\\n|   | id |\\n+---+----+\\n| 1 | b1 |\\n+---+----+\\n\"\n",
      "            ],\n",
      "            \"answer\": {\n",
      "                \"ans_desc\": \"A very simple approach: \",\n",
      "                \"code\": [\n",
      "                    \"import itertools\\nimport uuid\\n\\ndef rand_uuid():\\n    return uuid.uuid4()\\n\\nrep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}\\n\\ndf3.replace(rep_dict, inplace=True)\\ndf3.id = df3.id.map(lambda x: rand_uuid())\\n\\ndf1.replace(rep_dict, inplace=True)\\ndf2.replace(rep_dict, inplace=True)\\n\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"expected\": [\n",
      "            \"replace\"\n",
      "        ],\n",
      "        \"predicted\": [\n",
      "            [\n",
      "                \"where\",\n",
      "                0.8666212558746338\n",
      "            ],\n",
      "            [\n",
      "                \"replace\",\n",
      "                0.856270432472229\n",
      "            ],\n",
      "            [\n",
      "                \"isin\",\n",
      "                0.8547900319099426\n",
      "            ],\n",
      "            [\n",
      "                \"loc\",\n",
      "                0.8534621596336365\n",
      "            ],\n",
      "            [\n",
      "                \"transform\",\n",
      "                0.8520407676696777\n",
      "            ],\n",
      "            [\n",
      "                \"index\",\n",
      "                0.8511112928390503\n",
      "            ],\n",
      "            [\n",
      "                \"any\",\n",
      "                0.8506150245666504\n",
      "            ],\n",
      "            [\n",
      "                \"update\",\n",
      "                0.850396990776062\n",
      "            ],\n",
      "            [\n",
      "                \"fillna\",\n",
      "                0.8482847809791565\n",
      "            ],\n",
      "            [\n",
      "                \"iteritems\",\n",
      "                0.8473182320594788\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 67027649,\n",
      "        \"query\": \"how to unlist a list with one value inside a pandas columns? i have a pandas data frame: is possible to convert the data frame into another data frame that look like this? i tried with this way but only get the . thanks for your time!\",\n",
      "        \"link\": \"https://stackoverflow.com/questions/67027649/how-to-unlist-a-list-with-one-value-inside-a-pandas-columns\",\n",
      "        \"example\": {\n",
      "            \"qid\": 67027649,\n",
      "            \"link\": \"https://stackoverflow.com/questions/67027649/how-to-unlist-a-list-with-one-value-inside-a-pandas-columns\",\n",
      "            \"question\": {\n",
      "                \"title\": \"How to unlist a list with one value inside a pandas columns?\",\n",
      "                \"ques_desc\": \"I have a pandas data frame: Is possible to convert the data frame into another data frame that look like this? I tried with this way but only get the . Thanks for your time! \"\n",
      "            },\n",
      "            \"io\": [\n",
      "                \"Id       Col1\\n1     ['string']\\n2     ['string2']\\n\",\n",
      "                \"Id     Col1\\n1     string\\n2     string2\\n\"\n",
      "            ],\n",
      "            \"answer\": {\n",
      "                \"ans_desc\": \" I tried with this way but only get the [. Then this means they are ings, not s. You can convert them to s by ing and then : to get \",\n",
      "                \"code\": [\n",
      "                    \"import ast\\n\\ndf.Col1 = df.Col1.apply(ast.literal_eval).explode()\\n\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"expected\": [\n",
      "            \"apply\",\n",
      "            \"explode\"\n",
      "        ],\n",
      "        \"predicted\": [\n",
      "            [\n",
      "                \"values\",\n",
      "                0.8779488205909729\n",
      "            ],\n",
      "            [\n",
      "                \"join\",\n",
      "                0.8738932013511658\n",
      "            ],\n",
      "            [\n",
      "                \"explode\",\n",
      "                0.8706566095352173\n",
      "            ],\n",
      "            [\n",
      "                \"stack\",\n",
      "                0.8683419823646545\n",
      "            ],\n",
      "            [\n",
      "                \"DataFrame\",\n",
      "                0.8675342798233032\n",
      "            ],\n",
      "            [\n",
      "                \"apply\",\n",
      "                0.8670063018798828\n",
      "            ],\n",
      "            [\n",
      "                \"columns\",\n",
      "                0.8664469122886658\n",
      "            ],\n",
      "            [\n",
      "                \"eval\",\n",
      "                0.8660154342651367\n",
      "            ],\n",
      "            [\n",
      "                \"applymap\",\n",
      "                0.8641992807388306\n",
      "            ],\n",
      "            [\n",
      "                \"index\",\n",
      "                0.8618522882461548\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 66940820,\n",
      "        \"query\": \"python: how to pass dataframe columns as parameters to a function? i have a dataframe with 2 columns of text embeddings namely and . i want to create a third column in named which should contain the cosine_similarity between every row of and . but when i try to implement this using the following code i get a . how to fix it? dataframe code to calculate cosine similarity error required dataframe\",\n",
      "        \"link\": \"https://stackoverflow.com/questions/66940820/python-how-to-pass-dataframe-columns-as-parameters-to-a-function\",\n",
      "        \"example\": {\n",
      "            \"qid\": 66940820,\n",
      "            \"link\": \"https://stackoverflow.com/questions/66940820/python-how-to-pass-dataframe-columns-as-parameters-to-a-function\",\n",
      "            \"question\": {\n",
      "                \"title\": \"Python: How to pass Dataframe Columns as parameters to a function?\",\n",
      "                \"ques_desc\": \"I have a dataframe with 2 columns of text embeddings namely and . I want to create a third column in named which should contain the cosine_similarity between every row of and . But when I try to implement this using the following code I get a . How to fix it? Dataframe Code to Calculate Cosine Similarity Error Required Dataframe \"\n",
      "            },\n",
      "            \"io\": [\n",
      "                \"           embedding_1              |            embedding_2                                 \\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]\\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]\\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]\\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]\\n [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]\\n\",\n",
      "                \"       embedding_1              |            embedding_2                 |  distances                        \\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]   |    0.427\\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]   |    0.673\\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]  |    0.882\\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]     |    0.665\\n [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]    |    0.312\\n\"\n",
      "            ],\n",
      "            \"answer\": {\n",
      "                \"ans_desc\": \"You can use to use on each row: or one liner \",\n",
      "                \"code\": [\n",
      "                    \"def cal_cosine_similarity(row):\\n    return cosine_similarity(row['embeddings_1'], row['embeddings_2'])\\n\\ndf['distances'] = df.apply(cal_cosine_similarity, axis=1)\\n\",\n",
      "                    \"df['distances'] = df.apply(lambda row: cosine_similarity(row['embeddings_1'], row['embeddings_2']), axis=1)\\n\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"expected\": [\n",
      "            \"apply\"\n",
      "        ],\n",
      "        \"predicted\": [\n",
      "            [\n",
      "                \"apply\",\n",
      "                0.8923725485801697\n",
      "            ],\n",
      "            [\n",
      "                \"eval\",\n",
      "                0.8860245943069458\n",
      "            ],\n",
      "            [\n",
      "                \"applymap\",\n",
      "                0.883025586605072\n",
      "            ],\n",
      "            [\n",
      "                \"rename\",\n",
      "                0.879647970199585\n",
      "            ],\n",
      "            [\n",
      "                \"join\",\n",
      "                0.8780505061149597\n",
      "            ],\n",
      "            [\n",
      "                \"astype\",\n",
      "                0.8767362833023071\n",
      "            ],\n",
      "            [\n",
      "                \"iteritems\",\n",
      "                0.8723489046096802\n",
      "            ],\n",
      "            [\n",
      "                \"loc\",\n",
      "                0.8711830377578735\n",
      "            ],\n",
      "            [\n",
      "                \"add\",\n",
      "                0.8694084286689758\n",
      "            ],\n",
      "            [\n",
      "                \"columns\",\n",
      "                0.8686243891716003\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 20025882,\n",
      "        \"query\": \"add a string prefix to each value in a string column using pandas i would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly). i already figured out how to kind-of do this and i am currently using: this seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or nan)? in case this is yet unclear, i would like to turn: into:\",\n",
      "        \"link\": \"https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas\",\n",
      "        \"example\": {\n",
      "            \"qid\": 20025882,\n",
      "            \"link\": \"https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas\",\n",
      "            \"question\": {\n",
      "                \"title\": \"add a string prefix to each value in a string column using Pandas\",\n",
      "                \"ques_desc\": \"I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly). I already figured out how to kind-of do this and I am currently using: This seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)? In case this is yet unclear, I would like to turn: into: \"\n",
      "            },\n",
      "            \"io\": [\n",
      "                \"    col \\n1     a\\n2     0\\n\",\n",
      "                \"       col \\n1     stra\\n2     str0\\n\"\n",
      "            ],\n",
      "            \"answer\": {\n",
      "                \"ans_desc\": \" Example: \",\n",
      "                \"code\": [\n",
      "                    \"df['col'] = 'str' + df['col'].astype(str)\\n\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"expected\": [\n",
      "            \"astype\"\n",
      "        ],\n",
      "        \"predicted\": [\n",
      "            [\n",
      "                \"apply\",\n",
      "                0.887987494468689\n",
      "            ],\n",
      "            [\n",
      "                \"transform\",\n",
      "                0.884873628616333\n",
      "            ],\n",
      "            [\n",
      "                \"astype\",\n",
      "                0.8827208280563354\n",
      "            ],\n",
      "            [\n",
      "                \"where\",\n",
      "                0.8814499378204346\n",
      "            ],\n",
      "            [\n",
      "                \"values\",\n",
      "                0.8785062432289124\n",
      "            ],\n",
      "            [\n",
      "                \"eval\",\n",
      "                0.8773069381713867\n",
      "            ],\n",
      "            [\n",
      "                \"reset_index\",\n",
      "                0.8771543502807617\n",
      "            ],\n",
      "            [\n",
      "                \"join\",\n",
      "                0.8770843744277954\n",
      "            ],\n",
      "            [\n",
      "                \"index\",\n",
      "                0.8764394521713257\n",
      "            ],\n",
      "            [\n",
      "                \"replace\",\n",
      "                0.8739581108093262\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 66567933,\n",
      "        \"query\": \"can i shift specific values in one data column to another column while keeping the other values unchanged? here is an example dataset that i have: i want to take all the values that have \\\"1\\\" in them in the column \\\"c2\\\" and shift them to replace the adjacent values in column \\\"c1\\\". so the output should look like: alternatively, i could create a new column with these values replaced. main point is, that i need all the \\\"1s\\\" in c2 to replace the nan values in c1. i can't do find all nan and replace with 1, because there are some nan values that should stay in c1. is there a way to do this? thanks for the help in advance.\",\n",
      "        \"link\": \"https://stackoverflow.com/questions/66567933/can-i-shift-specific-values-in-one-data-column-to-another-column-while-keeping-t\",\n",
      "        \"example\": {\n",
      "            \"qid\": 66567933,\n",
      "            \"link\": \"https://stackoverflow.com/questions/66567933/can-i-shift-specific-values-in-one-data-column-to-another-column-while-keeping-t\",\n",
      "            \"question\": {\n",
      "                \"title\": \"Can I shift specific values in one data column to another column while keeping the other values unchanged?\",\n",
      "                \"ques_desc\": \"Here is an example dataset that I have: I want to take all the values that have \\\"1\\\" in them in the Column \\\"C2\\\" and shift them to replace the adjacent values in column \\\"C1\\\". So the output should look like: Alternatively, I could create a new column with these values replaced. Main point is, that I need all the \\\"1s\\\" in C2 TO replace the NaN values in C1. I can't do find all NaN and replace with 1, because there are some NaN values that should stay in C1. Is there a way to do this? Thanks for the help in advance. \"\n",
      "            },\n",
      "            \"io\": [\n",
      "                \"C1      C2\\n 1       1\\nNaN      1\\n 2       0\\nNaN      0\\nNaN      1\\n 1       1\\n 2       2\\n 2       2\\nNaN      1\\n\",\n",
      "                \"C1      C2\\n 1       1\\n 1       1\\n 2       0\\n NaN     0\\n 1       1\\n 1       1\\n 2       2\\n 2       2\\n 1       1\\n\\n\"\n",
      "            ],\n",
      "            \"answer\": {\n",
      "                \"ans_desc\": \"You could use the api to apply values from one column to another in rows where some condition is true (e.g. ==1). \",\n",
      "                \"code\": [\n",
      "                    \"value = 1\\nsource_col = 1\\ntarget_col = 0\\n\\ncondition = df[source_col] == value\\n\\ndf[target_col] = df[target_col].mask(condition,\\n                                     df[source_col])\\n\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"expected\": [\n",
      "            \"mask\"\n",
      "        ],\n",
      "        \"predicted\": [\n",
      "            [\n",
      "                \"fillna\",\n",
      "                0.8677147626876831\n",
      "            ],\n",
      "            [\n",
      "                \"where\",\n",
      "                0.8646842837333679\n",
      "            ],\n",
      "            [\n",
      "                \"isna\",\n",
      "                0.8549478054046631\n",
      "            ],\n",
      "            [\n",
      "                \"isnull\",\n",
      "                0.8547688722610474\n",
      "            ],\n",
      "            [\n",
      "                \"replace\",\n",
      "                0.8543781042098999\n",
      "            ],\n",
      "            [\n",
      "                \"dropna\",\n",
      "                0.8527646660804749\n",
      "            ],\n",
      "            [\n",
      "                \"notna\",\n",
      "                0.8515973091125488\n",
      "            ],\n",
      "            [\n",
      "                \"notnull\",\n",
      "                0.8503559827804565\n",
      "            ],\n",
      "            [\n",
      "                \"mask\",\n",
      "                0.8496589660644531\n",
      "            ],\n",
      "            [\n",
      "                \"transform\",\n",
      "                0.8491277694702148\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 66496528,\n",
      "        \"query\": \"repeating single dataframe with changing datetimeindex let's say i have very simple dataframe like this: output: i would like to take this dataframe and create longer that would append dataframe itself with changing year of index. something like this: it's still the same dataframe, repeating again and again, and year is incrementally changed. i could do something like this (example for 3 years): i have mainly two questions: is there a way how to do this in a single command? what is the best way how to deal with leap-year?\",\n",
      "        \"link\": \"https://stackoverflow.com/questions/66496528/repeating-single-dataframe-with-changing-datetimeindex\",\n",
      "        \"example\": {\n",
      "            \"qid\": 66496528,\n",
      "            \"link\": \"https://stackoverflow.com/questions/66496528/repeating-single-dataframe-with-changing-datetimeindex\",\n",
      "            \"question\": {\n",
      "                \"title\": \"Repeating single DataFrame with changing DateTimeIndex\",\n",
      "                \"ques_desc\": \"Let's say I have very simple DataFrame like this: Output: I would like to take this DataFrame and create longer that would append DataFrame itself with changing year of index. Something like this: It's still the same DataFrame, repeating again and again, and year is incrementally changed. I could do something like this (example for 3 years): I have mainly two questions: Is there a way how to do this in a single command? What is the best way how to deal with leap-year? \"\n",
      "            },\n",
      "            \"io\": [\n",
      "                \"             A  B   C   D\\n2010-01-31   6  0   8  10\\n2010-02-28   7  8  10   3\\n2010-03-31  10  5   8  10\\n2010-04-30   4  4   9   7\\n2010-05-31   2  3   0  11\\n2010-06-30   8  7  10   8\\n2010-07-31  11  9   0   4\\n2010-08-31   0  3   8   6\\n2010-09-30   4  6   7   9\\n2010-10-31   1  0  11   9\\n2010-11-30   5  4   8   4\\n2010-12-31   1  4   5   1\\n\",\n",
      "                \"             A  B   C   D\\n2010-01-31   6  0   8  10\\n2010-02-28   7  8  10   3\\n2010-03-31  10  5   8  10\\n2010-04-30   4  4   9   7\\n2010-05-31   2  3   0  11\\n2010-06-30   8  7  10   8\\n2010-07-31  11  9   0   4\\n2010-08-31   0  3   8   6\\n2010-09-30   4  6   7   9\\n2010-10-31   1  0  11   9\\n2010-11-30   5  4   8   4\\n2010-12-31   1  4   5   1\\n2011-01-31   6  0   8  10\\n2011-02-28   7  8  10   3\\n2011-03-31  10  5   8  10\\n2011-04-30   4  4   9   7\\n2011-05-31   2  3   0  11\\n2011-06-30   8  7  10   8\\n2011-07-31  11  9   0   4\\n2011-08-31   0  3   8   6\\n2011-09-30   4  6   7   9\\n2011-10-31   1  0  11   9\\n2011-11-30   5  4   8   4\\n2011-12-31   1  4   5   1\\n2012-01-31   6  0   8  10\\n2012-02-28   7  8  10   3\\n2012-03-31  10  5   8  10\\n2012-04-30   4  4   9   7\\n2012-05-31   2  3   0  11\\n2012-06-30   8  7  10   8\\n2012-07-31  11  9   0   4\\n2012-08-31   0  3   8   6\\n2012-09-30   4  6   7   9\\n2012-10-31   1  0  11   9\\n2012-11-30   5  4   8   4\\n2012-12-31   1  4   5   1\\n\"\n",
      "            ],\n",
      "            \"answer\": {\n",
      "                \"ans_desc\": \"Here's an option with applying to the original index in list comprehension: \",\n",
      "                \"code\": [\n",
      "                    \"data_new = pd.concat([\\n    df.set_index(df.index + pd.DateOffset(year=x)) for x in [2010, 2011, 2012]])\\n\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"expected\": [\n",
      "            \"index\",\n",
      "            \"set_index\"\n",
      "        ],\n",
      "        \"predicted\": [\n",
      "            [\n",
      "                \"reset_index\",\n",
      "                0.8790493011474609\n",
      "            ],\n",
      "            [\n",
      "                \"index\",\n",
      "                0.8786312937736511\n",
      "            ],\n",
      "            [\n",
      "                \"unstack\",\n",
      "                0.8755730986595154\n",
      "            ],\n",
      "            [\n",
      "                \"pivot_table\",\n",
      "                0.8718231320381165\n",
      "            ],\n",
      "            [\n",
      "                \"set_index\",\n",
      "                0.8711591958999634\n",
      "            ],\n",
      "            [\n",
      "                \"sort_index\",\n",
      "                0.8669191002845764\n",
      "            ],\n",
      "            [\n",
      "                \"lookup\",\n",
      "                0.8633547425270081\n",
      "            ],\n",
      "            [\n",
      "                \"reindex\",\n",
      "                0.862734854221344\n",
      "            ],\n",
      "            [\n",
      "                \"iloc\",\n",
      "                0.8627194166183472\n",
      "            ],\n",
      "            [\n",
      "                \"groupby\",\n",
      "                0.8615778684616089\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 66422239,\n",
      "        \"query\": \"python transposing multiple dataframes in a list i have a few dataframes which are similar (in terms of number of rows and columns) to the 2 dataframes listed below my desired output is to have multiple dataframes with the email as column header and the factor or item as rows i am able to get the result by transposing each dataframe individually using this but i'd like to create a for loop as i have several dataframes to transpose wrote something like this but the dataframes do not get transposed. would like to directly change the dataframes in the list of dataframes (somewhere along the lines of inplace=true). was wondering if there is something i am missing, appreciate any form of help, thank you.\",\n",
      "        \"link\": \"https://stackoverflow.com/questions/66422239/python-transposing-multiple-dataframes-in-a-list\",\n",
      "        \"example\": {\n",
      "            \"qid\": 66422239,\n",
      "            \"link\": \"https://stackoverflow.com/questions/66422239/python-transposing-multiple-dataframes-in-a-list\",\n",
      "            \"question\": {\n",
      "                \"title\": \"Python transposing multiple dataframes in a list\",\n",
      "                \"ques_desc\": \"I have a few dataframes which are similar (in terms of number of rows and columns) to the 2 dataframes listed below my desired output is to have multiple dataframes with the email as column header and the factor or item as rows I am able to get the result by transposing each dataframe individually using this but i'd like to create a for loop as i have several dataframes to transpose wrote something like this but the dataframes do not get transposed. Would like to directly change the dataframes in the list of dataframes (somewhere along the lines of inplace=True). Was wondering if there is something i am missing, appreciate any form of help, thank you. \"\n",
      "            },\n",
      "            \"io\": [\n",
      "                \"0    email           factor1_final   factor2_final   factor3_final\\n1    john@abc.com    85%             90%             50%\\n2    peter@abc.com   80%             60%             60%\\n3    shelby@abc.com  50%             70%             60%\\n4    jess@abc.com    60%             65%             50% \\n5    mark@abc.com    98%             50%             60%\\n\",\n",
      "                \"\\nemail     john@abc.com   peter@abc.com   shelby@abc.com   jess@abc.com   mark@abc.com\\nfactor1     85%          80%             50%              60%            98%\\nfactor2     90%          60%             70%              65%            50% \\nfactor3     50%          60%             60%              50%            60%\\n\"\n",
      "            ],\n",
      "            \"answer\": {\n",
      "                \"ans_desc\": \"For me second solution working, here is small alternative: Solution with create new list of DataFrames: \",\n",
      "                \"code\": [\n",
      "                    \"dfs = [df.set_index('email').T for df in df_list]\\n\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"expected\": [\n",
      "            \"set_index\"\n",
      "        ],\n",
      "        \"predicted\": [\n",
      "            [\n",
      "                \"reset_index\",\n",
      "                0.8918575048446655\n",
      "            ],\n",
      "            [\n",
      "                \"columns\",\n",
      "                0.8902212381362915\n",
      "            ],\n",
      "            [\n",
      "                \"rename\",\n",
      "                0.8881272673606873\n",
      "            ],\n",
      "            [\n",
      "                \"groupby\",\n",
      "                0.8837826251983643\n",
      "            ],\n",
      "            [\n",
      "                \"unstack\",\n",
      "                0.8810451626777649\n",
      "            ],\n",
      "            [\n",
      "                \"set_index\",\n",
      "                0.8794878721237183\n",
      "            ],\n",
      "            [\n",
      "                \"transpose\",\n",
      "                0.877932071685791\n",
      "            ],\n",
      "            [\n",
      "                \"agg\",\n",
      "                0.8773115277290344\n",
      "            ],\n",
      "            [\n",
      "                \"pivot\",\n",
      "                0.877085268497467\n",
      "            ],\n",
      "            [\n",
      "                \"index\",\n",
      "                0.8769875764846802\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": 66357650,\n",
      "        \"query\": \"transform a pandas dataframe in a pandas with multicolumns i have the following pandas dataframe, where the columna is the dataframe index and i want to convert this datframe in to a multi column data frame, that looks like this i've tried transforming my old pandas dataframe in to a dict this way: but i had no success, can someone give me tips and advices on how to do that? any help is more than welcome.\",\n",
      "        \"link\": \"https://stackoverflow.com/questions/66357650/transform-a-pandas-dataframe-in-a-pandas-with-multicolumns\",\n",
      "        \"example\": {\n",
      "            \"qid\": 66357650,\n",
      "            \"link\": \"https://stackoverflow.com/questions/66357650/transform-a-pandas-dataframe-in-a-pandas-with-multicolumns\",\n",
      "            \"question\": {\n",
      "                \"title\": \"transform a pandas dataframe in a pandas with multicolumns\",\n",
      "                \"ques_desc\": \"I have the following pandas dataframe, where the columna is the dataframe index And i want to convert this datframe in to a multi column data frame, that looks like this I've tried transforming my old pandas dataframe in to a dict this way: But i had no success, can someone give me tips and advices on how to do that? Any help is more than welcome. \"\n",
      "            },\n",
      "            \"io\": [\n",
      "                \"+----+-----------+------------+-----------+------------+\\n|    |   price_A |   amount_A |   price_B |   amount_b |\\n|----+-----------+------------+-----------+------------|\\n|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |\\n|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |\\n|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |\\n|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |\\n|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |\\n|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |\\n|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |\\n|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |\\n|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |\\n|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |\\n+----+-----------+------------+-----------+------------+\\n\\n\",\n",
      "                \"\\n+----+-----------+------------+-----------+------------+\\n|    |           A            |           B            |\\n+----+-----------+------------+-----------+------------+\\n| id |   price   |   amount   |   price   |   amount   |\\n|----+-----------+------------+-----------+------------|\\n|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |\\n|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |\\n|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |\\n|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |\\n|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |\\n|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |\\n|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |\\n|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |\\n|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |\\n|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |\\n+----+-----------+------------+-----------+------------+\\n\\n\"\n",
      "            ],\n",
      "            \"answer\": {\n",
      "                \"ans_desc\": \"Try renaming columns manually: Output: \",\n",
      "                \"code\": [\n",
      "                    \"df.columns=pd.MultiIndex.from_tuples([x.split('_')[::-1] for x in df.columns])\\ndf.index.name='id'\\n\"\n",
      "                ]\n",
      "            }\n",
      "        },\n",
      "        \"expected\": [\n",
      "            \"columns\",\n",
      "            \"index\"\n",
      "        ],\n",
      "        \"predicted\": [\n",
      "            [\n",
      "                \"index\",\n",
      "                0.8727530241012573\n",
      "            ],\n",
      "            [\n",
      "                \"set_index\",\n",
      "                0.8665201663970947\n",
      "            ],\n",
      "            [\n",
      "                \"columns\",\n",
      "                0.8565488457679749\n",
      "            ],\n",
      "            [\n",
      "                \"reset_index\",\n",
      "                0.8553952574729919\n",
      "            ],\n",
      "            [\n",
      "                \"droplevel\",\n",
      "                0.8515306115150452\n",
      "            ],\n",
      "            [\n",
      "                \"rename\",\n",
      "                0.8492177724838257\n",
      "            ],\n",
      "            [\n",
      "                \"rename_axis\",\n",
      "                0.8474821448326111\n",
      "            ],\n",
      "            [\n",
      "                \"lookup\",\n",
      "                0.8470704555511475\n",
      "            ],\n",
      "            [\n",
      "                \"stack\",\n",
      "                0.846551775932312\n",
      "            ],\n",
      "            [\n",
      "                \"pivot_table\",\n",
      "                0.8459517955780029\n",
      "            ]\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(full_correct[6:15], indent=4))\n",
    "fp = open(\"full_correct_results_top_10.json\", \"w\")\n",
    "json.dump(full_correct, fp, indent=4)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "517105d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\"\"\n",
    "#                    No Negative Samples\n",
    "# |=======================================================|\n",
    "# |                         Top  1                        |\n",
    "# |-------------------------------------------------------|\n",
    "# | #APIs\t| #Examples\t| Full\t| Half\t| 1/3\t| 1/4\t|\n",
    "# |-------------------------------------------------------|\n",
    "# | 1\t| 111\t\t| 13\t| 13\t| 13\t| 13\t|\n",
    "# | 2\t| 111\t\t| 0\t| 23\t| 23\t| 23\t|\n",
    "# | 3\t| 110\t\t| 0\t| 0\t| 28\t| 28\t|\n",
    "# | 4\t| 81\t\t| 0\t| 0\t| 0\t| 18\t|\n",
    "# | 5\t| 66\t\t| 0\t| 0\t| 0\t| 0\t|\n",
    "# | 6\t| 52\t\t| 0\t| 0\t| 0\t| 0\t|\n",
    "# | 7\t| 19\t\t| 0\t| 0\t| 0\t| 0\t|\n",
    "# | 8\t| 14\t\t| 0\t| 0\t| 0\t| 0\t|\n",
    "# |=======================================================|\n",
    "# | total\t| 564\t\t| 13\t| 36\t| 64\t| 82\t|\n",
    "# |=======================================================|\n",
    "\n",
    "# |=======================================================|\n",
    "# |                         Top  2                        |\n",
    "# |-------------------------------------------------------|\n",
    "# | #APIs\t| #Examples\t| Full\t| Half\t| 1/3\t| 1/4\t|\n",
    "# |-------------------------------------------------------|\n",
    "# | 1\t| 111\t\t| 23\t| 23\t| 23\t| 23\t|\n",
    "# | 2\t| 111\t\t| 2\t| 33\t| 33\t| 33\t|\n",
    "# | 3\t| 110\t\t| 0\t| 4\t| 38\t| 38\t|\n",
    "# | 4\t| 81\t\t| 0\t| 4\t| 4\t| 30\t|\n",
    "# | 5\t| 66\t\t| 0\t| 0\t| 4\t| 4\t|\n",
    "# | 6\t| 52\t\t| 0\t| 0\t| 2\t| 2\t|\n",
    "# | 7\t| 19\t\t| 0\t| 0\t| 0\t| 1\t|\n",
    "# | 8\t| 14\t\t| 0\t| 0\t| 0\t| 0\t|\n",
    "# |=======================================================|\n",
    "# | total\t| 564\t\t| 25\t| 64\t| 104\t| 131\t|\n",
    "# |=======================================================|\n",
    "\n",
    "# |=======================================================|\n",
    "# |                         Top  5                        |\n",
    "# |-------------------------------------------------------|\n",
    "# | #APIs\t| #Examples\t| Full\t| Half\t| 1/3\t| 1/4\t|\n",
    "# |-------------------------------------------------------|\n",
    "# | 1\t| 111\t\t| 31\t| 31\t| 31\t| 31\t|\n",
    "# | 2\t| 111\t\t| 6\t| 53\t| 53\t| 53\t|\n",
    "# | 3\t| 110\t\t| 0\t| 13\t| 57\t| 57\t|\n",
    "# | 4\t| 81\t\t| 0\t| 11\t| 11\t| 44\t|\n",
    "# | 5\t| 66\t\t| 0\t| 0\t| 12\t| 12\t|\n",
    "# | 6\t| 52\t\t| 0\t| 1\t| 7\t| 7\t|\n",
    "# | 7\t| 19\t\t| 0\t| 0\t| 0\t| 4\t|\n",
    "# | 8\t| 14\t\t| 0\t| 0\t| 3\t| 6\t|\n",
    "# |=======================================================|\n",
    "# | total\t| 564\t\t| 37\t| 109\t| 174\t| 214\t|\n",
    "# |=======================================================|\n",
    "\n",
    "# |=======================================================|\n",
    "# |                         Top 10                        |\n",
    "# |-------------------------------------------------------|\n",
    "# | #APIs\t| #Examples\t| Full\t| Half\t| 1/3\t| 1/4\t|\n",
    "# |-------------------------------------------------------|\n",
    "# | 1\t| 111\t\t| 43\t| 43\t| 43\t| 43\t|\n",
    "# | 2\t| 111\t\t| 16\t| 64\t| 64\t| 64\t|\n",
    "# | 3\t| 110\t\t| 3\t| 24\t| 70\t| 70\t|\n",
    "# | 4\t| 81\t\t| 1\t| 25\t| 25\t| 62\t|\n",
    "# | 5\t| 66\t\t| 0\t| 6\t| 21\t| 21\t|\n",
    "# | 6\t| 52\t\t| 0\t| 3\t| 16\t| 16\t|\n",
    "# | 7\t| 19\t\t| 0\t| 0\t| 2\t| 6\t|\n",
    "# | 8\t| 14\t\t| 0\t| 3\t| 4\t| 8\t|\n",
    "# |=======================================================|\n",
    "# | total\t| 564\t\t| 63\t| 168\t| 245\t| 290\t|\n",
    "# |=======================================================|\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f36c6214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=======================================================|\n",
      "|                         Top  1                        |\n",
      "|-------------------------------------------------------|\n",
      "| #APIs\t| #Examples\t| Full\t| Half\t| 1/3\t| 1/4\t|\n",
      "|-------------------------------------------------------|\n",
      "| 1\t| 127\t\t| 20\t| 20\t| 20\t| 20\t|\n",
      "| 2\t| 143\t\t| 0\t| 45\t| 45\t| 45\t|\n",
      "| 3\t| 108\t\t| 0\t| 0\t| 34\t| 34\t|\n",
      "| 4\t| 74\t\t| 0\t| 0\t| 0\t| 25\t|\n",
      "| 5\t| 64\t\t| 0\t| 0\t| 0\t| 0\t|\n",
      "| 6\t| 25\t\t| 0\t| 0\t| 0\t| 0\t|\n",
      "| 7\t| 20\t\t| 0\t| 0\t| 0\t| 0\t|\n",
      "| 8\t| 23\t\t| 0\t| 0\t| 0\t| 0\t|\n",
      "|=======================================================|\n",
      "| total\t| 584\t\t| 20\t| 65\t| 99\t| 124\t|\n",
      "|=======================================================|\n"
     ]
    }
   ],
   "source": [
    "show_performance(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10f3e71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=======================================================|\n",
      "|                         Top  2                        |\n",
      "|-------------------------------------------------------|\n",
      "| #APIs\t| #Examples\t| Full\t| Half\t| 1/3\t| 1/4\t|\n",
      "|-------------------------------------------------------|\n",
      "| 1\t| 127\t\t| 32\t| 32\t| 32\t| 32\t|\n",
      "| 2\t| 143\t\t| 7\t| 65\t| 65\t| 65\t|\n",
      "| 3\t| 108\t\t| 0\t| 4\t| 48\t| 48\t|\n",
      "| 4\t| 74\t\t| 0\t| 4\t| 4\t| 34\t|\n",
      "| 5\t| 64\t\t| 0\t| 0\t| 5\t| 5\t|\n",
      "| 6\t| 25\t\t| 0\t| 0\t| 2\t| 2\t|\n",
      "| 7\t| 20\t\t| 0\t| 0\t| 0\t| 1\t|\n",
      "| 8\t| 23\t\t| 0\t| 0\t| 0\t| 2\t|\n",
      "|=======================================================|\n",
      "| total\t| 584\t\t| 39\t| 105\t| 156\t| 189\t|\n",
      "|=======================================================|\n"
     ]
    }
   ],
   "source": [
    "show_performance(2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e467cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=======================================================|\n",
      "|                         Top  5                        |\n",
      "|-------------------------------------------------------|\n",
      "| #APIs\t| #Examples\t| Full\t| Half\t| 1/3\t| 1/4\t|\n",
      "|-------------------------------------------------------|\n",
      "| 1\t| 127\t\t| 49\t| 49\t| 49\t| 49\t|\n",
      "| 2\t| 143\t\t| 21\t| 96\t| 96\t| 96\t|\n",
      "| 3\t| 108\t\t| 1\t| 13\t| 75\t| 75\t|\n",
      "| 4\t| 74\t\t| 0\t| 20\t| 20\t| 51\t|\n",
      "| 5\t| 64\t\t| 0\t| 3\t| 21\t| 21\t|\n",
      "| 6\t| 25\t\t| 0\t| 1\t| 7\t| 7\t|\n",
      "| 7\t| 20\t\t| 0\t| 0\t| 3\t| 7\t|\n",
      "| 8\t| 23\t\t| 0\t| 0\t| 3\t| 8\t|\n",
      "|=======================================================|\n",
      "| total\t| 584\t\t| 71\t| 182\t| 274\t| 314\t|\n",
      "|=======================================================|\n"
     ]
    }
   ],
   "source": [
    "show_performance(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d84e8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|=======================================================|\n",
      "|                         Top 10                        |\n",
      "|-------------------------------------------------------|\n",
      "| #APIs\t| #Examples\t| Full\t| Half\t| 1/3\t| 1/4\t|\n",
      "|-------------------------------------------------------|\n",
      "| 1\t| 127\t\t| 68\t| 68\t| 68\t| 68\t|\n",
      "| 2\t| 143\t\t| 40\t| 114\t| 114\t| 114\t|\n",
      "| 3\t| 108\t\t| 3\t| 36\t| 93\t| 93\t|\n",
      "| 4\t| 74\t\t| 1\t| 33\t| 33\t| 64\t|\n",
      "| 5\t| 64\t\t| 1\t| 8\t| 35\t| 35\t|\n",
      "| 6\t| 25\t\t| 0\t| 4\t| 12\t| 12\t|\n",
      "| 7\t| 20\t\t| 0\t| 2\t| 4\t| 14\t|\n",
      "| 8\t| 23\t\t| 0\t| 1\t| 7\t| 16\t|\n",
      "|=======================================================|\n",
      "| total\t| 584\t\t| 113\t| 266\t| 366\t| 416\t|\n",
      "|=======================================================|\n"
     ]
    }
   ],
   "source": [
    "result, count_to_p = show_performance(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee59a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ad3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
