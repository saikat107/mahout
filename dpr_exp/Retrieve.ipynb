{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05bc30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pathlib\n",
    "import json\n",
    "import gzip\n",
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Iterator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor as T\n",
    "from torch import nn\n",
    "\n",
    "from dpr.data.qa_validation import calculate_matches_by_id\n",
    "from dpr.models import init_biencoder_components\n",
    "from dpr.options import (\n",
    "    add_encoder_params, \n",
    "    setup_args_gpu, \n",
    "    print_args, \n",
    "    set_encoder_params_from_state, \n",
    "    add_tokenizer_params, \n",
    "    add_cuda_params\n",
    ")\n",
    "from dpr.utils.data_utils import Tensorizer\n",
    "from dpr.utils.model_utils import (\n",
    "    setup_for_distributed_mode, \n",
    "    get_model_obj, \n",
    "    load_states_from_checkpoint, \n",
    "    move_to_device\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk \n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import argparse\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06b214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_lists = json.load(open(\"data/api_list.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a3193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    }
   ],
   "source": [
    "print(len(api_lists.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4edd8836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "def get_test_data(api_lists):\n",
    "    examples = []\n",
    "    fail_count = 0\n",
    "    example_id = 1\n",
    "    for i in range(1, 4):\n",
    "        example_file = f\"../25_K_Examples/part-{i}-output/taken_answers_with_all_details.json\"\n",
    "        data = json.load(open(example_file))\n",
    "        for e in data:\n",
    "            try:\n",
    "                ques_id = e['question_id']\n",
    "                qtitle = e['formatted_input']['question']['title']\n",
    "                qdesc = e['formatted_input']['question']['ques_desc']\n",
    "                codes = e['formatted_input']['answer']['code']\n",
    "                apis = set()\n",
    "                api_list = []\n",
    "                for c in codes:\n",
    "                    tokens = nltk.wordpunct_tokenize(c)\n",
    "                    for tidx, token in enumerate(tokens):\n",
    "                        token = token.strip()\n",
    "                        if tidx >= 0:\n",
    "                            prev_token = tokens[tidx - 1].strip()[-1]\n",
    "                            if token == \"T\":\n",
    "                                token = \"transpose\"\n",
    "                            if (token in api_lists and prev_token == \".\"):\n",
    "                                apis.add(token)\n",
    "                                api_list.append(token)\n",
    "                api_seq = list(sorted(apis))\n",
    "                if len(api_seq) <= 0:\n",
    "                    continue\n",
    "                examples.append({\n",
    "                    'id': ques_id,\n",
    "                    'query': qtitle.strip().lower() + \" \" + qdesc.strip().lower(),\n",
    "                    \"apis\": api_seq,\n",
    "                    \"api_list\": api_list,\n",
    "                    'link': e['link'],\n",
    "                    \"example\": e['formatted_input']\n",
    "                })\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                fail_count += 1\n",
    "    return examples\n",
    "\n",
    "test_examples = get_test_data(list(api_lists.keys()))\n",
    "print(len(test_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe89a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGramModel:\n",
    "    def __init__(\n",
    "        self, \n",
    "        train_data=\"/home/saikatc/HDD_4TB/from_server/StackOverFlow-Pandas/dpr_exp/data/ngram/train.txt\",\n",
    "        alpha=0.5, beta=0.5,\n",
    "        size_avg=True\n",
    "    ):\n",
    "        self.a = alpha\n",
    "        self.b = beta\n",
    "        self.size_avg = size_avg\n",
    "        self.bigram_freq = {}\n",
    "        self.conditional_freq = {}\n",
    "        with open(train_data) as fp:\n",
    "            for line in fp:\n",
    "                line = line.strip()\n",
    "                words = [\"<s>\"] + line.split() + [\"</s>\"]\n",
    "                l = len(words)\n",
    "                for i in range(l - 1):\n",
    "                    t0 = words[i]\n",
    "                    t1 = words[i + 1]\n",
    "                    if t0 not in self.conditional_freq.keys():\n",
    "                        self.conditional_freq[t0] = {}\n",
    "                    if t1 not in self.conditional_freq[t0].keys():\n",
    "                        self.conditional_freq[t0][t1] = 0\n",
    "                    self.conditional_freq[t0][t1] += 1\n",
    "                    bigram_tuple = (words[i], words[i + 1])\n",
    "                    if bigram_tuple not in self.bigram_freq:\n",
    "                        self.bigram_freq[bigram_tuple] = 0\n",
    "                    self.bigram_freq[bigram_tuple] += 1\n",
    "        self.conditional_prob = {}\n",
    "        for t0 in self.conditional_freq:\n",
    "            frequencies = self.conditional_freq[t0]\n",
    "            total = sum([self.conditional_freq[t0][t1] for t1 in frequencies])\n",
    "            if total == 0:\n",
    "                total = 100000000\n",
    "            self.conditional_prob[t0] = {\n",
    "                t1: self.conditional_freq[t0][t1] / total for t1 in self.conditional_freq[t0].keys()\n",
    "            }\n",
    "\n",
    "    def calculate_probs(self, tokens):\n",
    "        if tokens[0] != \"<s>\":\n",
    "            tokens = [\"<s>\"] + tokens\n",
    "        if tokens[-1] != \"</s>\":\n",
    "            tokens = tokens + [\"</s>\"]\n",
    "        l = len(tokens)\n",
    "        prob = 0.\n",
    "        for i in range(l - 1):\n",
    "            t0 = tokens[i]\n",
    "            t1 = tokens[i + 1]\n",
    "            if t0 not in self.conditional_prob.keys():\n",
    "                p = 1e-9\n",
    "            elif t1 not in self.conditional_prob[t0].keys():\n",
    "                p = 1e-9\n",
    "            else:\n",
    "                p = self.conditional_prob[t0][t1]\n",
    "                if p == 0:\n",
    "                    p = 1e-9\n",
    "            prob += np.log(p)\n",
    "        if self.size_avg:\n",
    "            prob = prob / l\n",
    "        return prob\n",
    "\n",
    "    def get_top_tokens(self, token, mask):\n",
    "        if token not in self.conditional_prob:\n",
    "            return [\"</s>\"]\n",
    "        probabilities = copy.copy(self.conditional_prob[token])\n",
    "        mask_token_probs = []\n",
    "        for t, prior in mask:\n",
    "            if t not in probabilities:\n",
    "                p = self.a * np.log(1e-9) + self.b * np.log(prior)\n",
    "            else:\n",
    "                p = self.a * np.log(probabilities[t]) + self.b * np.log(prior)\n",
    "            mask_token_probs.append((t, p))\n",
    "        mask_token_probs = sorted(mask_token_probs, key=lambda x: x[1], reverse=True)\n",
    "        return mask_token_probs\n",
    "\n",
    "    def beam_search(self, mask, beam_size=20, min_len=1, max_len=3):\n",
    "        if isinstance(mask[0], str):\n",
    "            mask = [(m, 1.0) for m in mask]\n",
    "        complete_beams = []\n",
    "        beam = [\n",
    "            ([\"<s>\"], 0)\n",
    "        ]\n",
    "        while len(complete_beams) < beam_size and len(beam) > 0:\n",
    "            new_beam = []\n",
    "            for cand_sent, score in beam:\n",
    "                last_token = cand_sent[-1]\n",
    "                current_length = len(cand_sent) - 1\n",
    "                if current_length >= max_len:\n",
    "                    current_mask = [(\"</s>\", 1.)]\n",
    "                elif current_length >= min_len:\n",
    "                    current_mask = mask + [(\"</s>\", 1.)]\n",
    "                else:\n",
    "                    current_mask = mask\n",
    "                top_toks_with_score = self.get_top_tokens(token=last_token, mask=current_mask)\n",
    "                for t, s in top_toks_with_score:\n",
    "                    if self.size_avg:\n",
    "                        new_score = (score * len(cand_sent) + s) / (len(cand_sent) + 1)\n",
    "                    else:\n",
    "                        new_score = (score + s)\n",
    "                    new_beam.append(\n",
    "                        (cand_sent + [t], new_score)\n",
    "                    )\n",
    "            new_beam = sorted(new_beam, key=lambda x: x[1], reverse=True)\n",
    "            beam = []\n",
    "            for cand_sent, score in new_beam:\n",
    "                if cand_sent[-1] == \"</s>\":\n",
    "                    complete_beams.append((cand_sent, score))\n",
    "                else:\n",
    "                    beam.append((cand_sent, score))\n",
    "                if len(beam) == beam_size:\n",
    "                    break\n",
    "        complete_beams = sorted(complete_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        final_sequences = [(cand_sent[1:-1], score) for cand_sent, score in complete_beams]\n",
    "        return final_sequences\n",
    "        pass\n",
    "    \n",
    "    def update_param(self, alpha, beta, size_avg):\n",
    "        self.a = alpha\n",
    "        self.b = beta\n",
    "        self.size_avg = size_avg\n",
    "\n",
    "\n",
    "bgmodel = BiGramModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd7cc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class RetrieverModel:\n",
    "    def __init__(self, model_path, batch_size=64, quiet=False, no_cuda=False):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        add_encoder_params(parser)\n",
    "        add_tokenizer_params(parser)\n",
    "        add_cuda_params(parser)\n",
    "        parser.add_argument(\n",
    "            '--shard_size', \n",
    "            type=int, \n",
    "            default=50000, \n",
    "            help=\"Total amount of data in 1 shard\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--batch_size', \n",
    "            type=int, \n",
    "            default=32, \n",
    "            help=\"Batch size for the passage encoder forward pass\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--dataset', \n",
    "            type=str, \n",
    "            default=None, \n",
    "            help=' to build correct dataset parser '\n",
    "        )\n",
    "\n",
    "        self.args = parser.parse_args({})\n",
    "        self.quiet = quiet\n",
    "        self.args.model_file = model_path\n",
    "        setup_args_gpu(self.args)\n",
    "        if no_cuda:\n",
    "            self.args.device = torch.device(\"cpu\")\n",
    "        saved_state = load_states_from_checkpoint(self.args.model_file)\n",
    "        set_encoder_params_from_state(\n",
    "            saved_state.encoder_params, \n",
    "            self.args,\n",
    "            quiet=self.quiet\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.tensorizer, self.encoder, _ = init_biencoder_components(\n",
    "            self.args.encoder_model_type, \n",
    "            self.args, \n",
    "            inference_only=True\n",
    "        )\n",
    "        self.encoder.load_state_dict(saved_state.model_dict)\n",
    "        self.query_model = self.encoder.question_model\n",
    "        self.document_model = self.encoder.ctx_model\n",
    "        \n",
    "        self.api_lists = json.load(open(\"data/api_list.json\"))\n",
    "\n",
    "        self.apis = list(sorted(self.api_lists.keys()))\n",
    "        self.api_docs = [self.api_lists[a] for a in self.apis]\n",
    "\n",
    "        _, _, _, self.doc_vectors = self.generate_query_vectors()\n",
    "    \n",
    "    def generate_query_vectors(self):\n",
    "        return self.generate_vectors(\n",
    "            model=self.document_model, \n",
    "            sentences=self.api_docs,\n",
    "            batch_size=self.batch_size,\n",
    "            task='\"API_VECTORS\"'\n",
    "        )\n",
    "    \n",
    "    def generate_vectors(self, model, sentences, batch_size, task):\n",
    "        if not self.quiet:\n",
    "            print(\n",
    "                \"Generating vectors for %d sentences using %s task model\" % (\n",
    "                    len(sentences), \n",
    "                    task\n",
    "                )\n",
    "            )\n",
    "        tensors = []\n",
    "        for ex in sentences:\n",
    "            tensor = self.tensorizer.text_to_tensor(ex)\n",
    "            tensors.append(tensor)\n",
    "        ids = torch.stack(tensors, dim=0)\n",
    "        seg_batch = torch.zeros_like(ids)\n",
    "        attn_mask = self.tensorizer.get_attn_mask(ids)\n",
    "        model.to(self.args.device)\n",
    "        l = ids.size(0)\n",
    "        start_idx = 0\n",
    "        vectors = [] * l\n",
    "        num_batches = math.ceil(l / batch_size)\n",
    "        with torch.no_grad():\n",
    "            batches = range(num_batches) if self.quiet else tqdm(range(num_batches))\n",
    "            for _ in batches:\n",
    "                end_idx = start_idx + batch_size\n",
    "                if end_idx > l:\n",
    "                    end_idx = l\n",
    "                _ids = move_to_device(ids[start_idx:end_idx, :], self.args.device)\n",
    "                _seg_batch = move_to_device(seg_batch[start_idx:end_idx, :], self.args.device)\n",
    "                _attn_mask = move_to_device(attn_mask[start_idx:end_idx, :], self.args.device)\n",
    "                _, _vectors, _ = model(_ids, _seg_batch, _attn_mask)\n",
    "                vectors.append(_vectors)\n",
    "                start_idx = end_idx\n",
    "        vectors = torch.cat(vectors, dim=0)\n",
    "        return ids, seg_batch, attn_mask, vectors\n",
    "    \n",
    "    def retrieve_apis(\n",
    "        self, \n",
    "        examples, \n",
    "        top_k_apis=10, \n",
    "        use_score=True, \n",
    "        top_k_seq=None,\n",
    "        beam_size=None,\n",
    "        min_length=1,\n",
    "        max_length=4,\n",
    "    ):\n",
    "        retrieval_necessary = True\n",
    "        if top_k_apis == -1:\n",
    "            top_k_apis = len(self.apis)\n",
    "            retrieval_necessary = False\n",
    "        if top_k_seq is None:\n",
    "            top_k_seq = top_k_apis\n",
    "        if beam_size is None:\n",
    "            beam_size = top_k_seq\n",
    "        if retrieval_necessary:\n",
    "            query_sentences = [ex[\"query\"] for ex in examples]\n",
    "            _, _, _, query_vectors = self.generate_vectors(\n",
    "                model=self.query_model, \n",
    "                sentences=query_sentences, \n",
    "                batch_size=self.batch_size,\n",
    "                task='\"QUESTION_VECTORS\"'\n",
    "            )\n",
    "            similarity_results = cosine_similarity(\n",
    "                query_vectors.cpu().numpy(), \n",
    "                self.doc_vectors.cpu().numpy()\n",
    "            )\n",
    "        singled_out = []\n",
    "        return_examples = []\n",
    "        singled_out_seq = []\n",
    "        indices = []\n",
    "        for exid, ex in enumerate(examples):\n",
    "            example = copy.deepcopy(ex)\n",
    "            if retrieval_necessary:\n",
    "                pred_similaroty = [(a, s) for a, s in zip(self.apis, similarity_results[exid, :].tolist())]\n",
    "                sorted_apis = sorted(pred_similaroty, key=lambda x: x[1])[::-1]\n",
    "            else:\n",
    "                sorted_apis = [(a, 1.) for a in self.apis]\n",
    "            example[\"expected\"] = example[\"apis\"]\n",
    "            example[\"predicted\"] = sorted_apis\n",
    "            predictions = set([e[0] for e in sorted_apis[:top_k_apis]])\n",
    "            if len(set(example[\"expected\"]).difference(predictions)) == 0:\n",
    "                new_example = copy.deepcopy(example)\n",
    "                singled_out.append(new_example)\n",
    "            if use_score:\n",
    "                mask = [\n",
    "                    (a, (1.0/(position + 1))) for position, (a, _) in enumerate(sorted_apis[:top_k_apis])\n",
    "                ]\n",
    "            else:\n",
    "                mask = [(e[0], 1.) for e in sorted_apis[:top_k_apis]]\n",
    "            # mask = self.apis\n",
    "            beam_candidates = bgmodel.beam_search(\n",
    "                mask=mask, \n",
    "                beam_size=beam_size, \n",
    "                min_len=min_length,\n",
    "                max_len=max_length,\n",
    "            )\n",
    "            if \"api_list\" not in example:\n",
    "                example[\"api_list\"] = []\n",
    "            example.pop(\"apis\", None)\n",
    "            example[\"predicted\"] = sorted_apis[:top_k_apis]\n",
    "            example[\"expected_api_seq\"] = copy.deepcopy(example[\"api_list\"])\n",
    "            example.pop(\"api_list\", None)\n",
    "            example[\"predicted_api_seq\"] = beam_candidates[:top_k_seq]\n",
    "            expected_api_sent = \" \".join(example[\"expected_api_seq\"])\n",
    "            generated_api_sents = [\" \".join(c[0]) for c in beam_candidates[:top_k_seq]]\n",
    "            if expected_api_sent in generated_api_sents:\n",
    "                index = generated_api_sents.index(expected_api_sent) + 1\n",
    "                new_example = copy.deepcopy(example)\n",
    "                singled_out_seq.append(new_example)\n",
    "                indices.append(1.0/index)\n",
    "            else:\n",
    "                indices.append(0.0)\n",
    "            return_examples.append(example)\n",
    "            pass\n",
    "        return return_examples, singled_out, singled_out_seq, indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b9adc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import math\n",
    "\n",
    "# x, y, z = [], [], []\n",
    "# retriever = RetrieverModel(\n",
    "#     model_path=f\"models/bert/pandas_2/check\", \n",
    "#     batch_size=128, \n",
    "#     quiet=True,\n",
    "#     no_cuda=False\n",
    "# )\n",
    "\n",
    "# x, y, z = [], [], []\n",
    "# yt, zt = [], []\n",
    "\n",
    "# bgmodel = BiGramModel()\n",
    "\n",
    "# for tk in list(range(5, 51, 5)):\n",
    "#     x.append(tk)\n",
    "#     start = datetime.now()\n",
    "#     bgmodel.update_param(alpha=1.0, beta=0, size_avg=False)\n",
    "#     _, _, singled_out_seq_ngram_only, indices_ng = retriever.retrieve_apis(\n",
    "#         test_examples, top_k_apis=-1, use_score=False, top_k_seq=tk, max_length=4\n",
    "#     )\n",
    "#     y.append(len(singled_out_seq_ngram_only))\n",
    "#     spent_ng = (datetime.now() - start).total_seconds()\n",
    "#     yt.append(spent_ng)\n",
    "#     start = datetime.now()\n",
    "#     bgmodel.update_param(alpha=0.6, beta=0.4, size_avg=True)\n",
    "#     _, _, singled_out_seq_retr_ngram, indices_retr = retriever.retrieve_apis(\n",
    "#         test_examples, top_k_apis=min(tk, 100), top_k_seq=tk, use_score=True, max_length=4\n",
    "#     )\n",
    "#     z.append(len(singled_out_seq_retr_ngram))\n",
    "#     spent_retr = (datetime.now() - start).total_seconds() \n",
    "#     zt.append(spent_retr)\n",
    "#     print(\n",
    "#         tk, len(singled_out_seq_ngram_only), round(spent_ng,2),\n",
    "#         len(singled_out_seq_retr_ngram), round(spent_retr, 2), \n",
    "#         round(np.mean(indices_ng).item(), 4),  \n",
    "#         round(np.mean(indices_retr).item(), 4), sep=\"\\t\"\n",
    "#     )\n",
    "#     print(\"-\" * 100)\n",
    "#     print(singled_out_seq_retr_ngram[0]['predicted'])\n",
    "#     print(\"-\" * 100)\n",
    "#     print(singled_out_seq_retr_ngram[0]['predicted_api_seq'])\n",
    "#     print(\"=\" * 100)\n",
    "    \n",
    "    \n",
    "# plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "# plt.figure(figsize=(16,8))\n",
    "# plt.plot(x, y, label=\"NGram Only\")\n",
    "# plt.plot(x, z, label=\"Retrieval + NGram\")\n",
    "# plt.xlabel(\"Beam Size\")\n",
    "# plt.ylabel(\"Number of Correct Examples\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Correct Sequences\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(16,8))\n",
    "# plt.plot(x, yt, label=\"NGram Only\")\n",
    "# plt.plot(x, zt, label=\"Retrieval + NGram\")\n",
    "# plt.xlabel(\"Beam Size\")\n",
    "# plt.ylabel(\"Time Required\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Time\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a68a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# x, y, z = [], [], []\n",
    "# retriever = RetrieverModel(\n",
    "#     model_path=f\"models/bert/pandas_2/checkpoint_best.pt\", \n",
    "#     batch_size=128, \n",
    "#     quiet=True,\n",
    "#     no_cuda=False\n",
    "# )\n",
    "\n",
    "# x, y, z = [], [], []\n",
    "# yt, zt = [], []\n",
    "# yr, zr = [], []\n",
    "\n",
    "# for tk in list(range(50, 56, 5)):\n",
    "#     x.append(tk)\n",
    "#     start = datetime.now()\n",
    "#     _, _, singled_out_seq_ngram_only, indices_ng = retriever.retrieve_apis(\n",
    "#         test_examples, top_k_apis=130, use_score=False, top_k_seq=tk\n",
    "#     )\n",
    "#     y.append(len(singled_out_seq_ngram_only))\n",
    "#     spent_ng = (datetime.now() - start).total_seconds() - 5.2\n",
    "#     yr.append(np.mean(indices_ng).item())\n",
    "#     yt.append(spent_ng)\n",
    "#     start = datetime.now()\n",
    "#     _, _, singled_out_seq_retr_ngram, indices_retr = retriever.retrieve_apis(\n",
    "#         test_examples, top_k_apis=tk, use_score=True\n",
    "#     )\n",
    "#     z.append(len(singled_out_seq_retr_ngram))\n",
    "#     spent_retr = (datetime.now() - start).total_seconds() \n",
    "#     zt.append(spent_retr)\n",
    "#     zr.append(np.mean(indices_retr).item())\n",
    "#     print(\n",
    "#         tk, len(singled_out_seq_ngram_only), spent_ng,\n",
    "#         len(singled_out_seq_retr_ngram), spent_retr, \n",
    "#         np.mean(indices_ng).item(),  \n",
    "#         np.mean(indices_retr).item(), sep=\"\\t\"\n",
    "#     )\n",
    "    \n",
    "# plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea7a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# x, y, z = [], [], []\n",
    "# retriever = RetrieverModel(\n",
    "#     model_path=f\"models/bert/pandas_2/dpr_biencoder.4.2528\", \n",
    "#     batch_size=128, \n",
    "#     quiet=True,\n",
    "#     no_cuda=False\n",
    "# )\n",
    "# for i in [1]:\n",
    "#     for k in [\n",
    "#         2, 3, 5, 8, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, \n",
    "#         70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135\n",
    "#     ]:\n",
    "#         _, singles, singled_out_seq_no_score = retriever.retrieve_apis(\n",
    "#             test_examples, top_k=k, use_score=False\n",
    "#         )\n",
    "#         _, _, singled_out_seq_score = retriever.retrieve_apis(\n",
    "#             test_examples, top_k=k, use_score=True\n",
    "#         )\n",
    "#         x.append(k)\n",
    "#         y.append(len(singled_out_seq_score))\n",
    "#         z.append(len(singled_out_seq_no_score))\n",
    "#         print(k, len(singles), len(singled_out_seq_score), len(singled_out_seq_no_score), sep=\"\\t\")\n",
    "#         pass\n",
    "#     plt.figure()\n",
    "#     plt.plot(x, y, label=\"With Score\")\n",
    "#     plt.plot(x, z, label=\"Without Score\")\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cbe126",
   "metadata": {},
   "source": [
    "## Autopandas Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a30eca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fc54191eca4c4dac6301b238c9dc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNum APIS : 35\t27\tNum Seqs : 190\tCorrect APIs: 19\tCorrect Seqs: 15\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "autopandas_examples = json.load(open(\"autopandas_test.json\"))\n",
    "import os \n",
    "from tqdm.notebook import tqdm\n",
    "os.makedirs(\"autopandas_result\", exist_ok=True)\n",
    "\n",
    "# files = os.listdir(\"models/bert/pandas_2/\")\n",
    "files = [\"checkpoint_best.pt\"]\n",
    "\n",
    "results = {}\n",
    "for f in files:\n",
    "    retriever = RetrieverModel(\n",
    "        model_path=f\"models/bert/pandas_2/\" + f, \n",
    "        batch_size=128, \n",
    "        quiet=True,\n",
    "        no_cuda=False\n",
    "    )\n",
    "    for num_apis in tqdm(list(range(35, 36)) + []):\n",
    "        for num_seqs in range(190, 191, 10):\n",
    "            predictions, apis, seqs, c = retriever.retrieve_apis(\n",
    "                autopandas_examples,\n",
    "                top_k_apis=num_apis, use_score=True, top_k_seq=num_seqs, max_length=4\n",
    "            )\n",
    "            fp = open(f\"autopandas_result/27-ex-top-{num_apis}_apis-top-{num_seqs}-seqs.json\", \"w\")\n",
    "            json.dump(predictions, fp, indent=4)\n",
    "            fp.close()\n",
    "            print(f\"\"\"\\tNum APIS : {num_apis}\\t{len(predictions)}\\tNum Seqs : {num_seqs}\\tCorrect APIs: {len(apis)}\\tCorrect Seqs: {len(seqs)}\"\"\", \"=\" * 100, sep=\"\\n\")\n",
    "            results[(num_apis, num_seqs)] = (len(apis), len(seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b72bebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "18 13\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))\n",
    "# expected, predicted, expected_api_seq, predicted_api_seq\n",
    "correct_apis, correct_seq = 0, 0\n",
    "for p in predictions:\n",
    "    expected = set(p['expected'])\n",
    "    predicted = set([x[0] for x in p['predicted']])\n",
    "    if len(expected.difference(predicted)) == 0:\n",
    "        correct_apis += 1\n",
    "    expected_seq = \" \".join(p['expected_api_seq'])\n",
    "    predicted_seq = [\" \".join(x[0]) for x in p['predicted_api_seq']]\n",
    "    if expected_seq in predicted_seq:\n",
    "        correct_seq += 1\n",
    "        pass\n",
    "    pass\n",
    "print(correct_apis, correct_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a10108f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(5, 50)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-b32d7f485472>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmeshgrid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mapis_z\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0mseqs_z\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseqs_z\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-b32d7f485472>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmeshgrid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mapis_z\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0mseqs_z\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseqs_z\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-b32d7f485472>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmeshgrid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mapis_z\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0mseqs_z\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseqs_z\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: (5, 50)"
     ]
    }
   ],
   "source": [
    "# %matplotlib widget\n",
    "x = list(range(5, 131, 5)) + [131, 132]\n",
    "y = list(range(50, 251, 10))\n",
    "x, y = np.meshgrid(x, y)\n",
    "m, n = x.shape\n",
    "apis_z = np.array([[results[(x[j, i], y[j, i])][0] for i in range(n)] for j in range(m)])\n",
    "seqs_z = np.array([[results[(x[j, i], y[j, i])][1] for i in range(n)] for j in range(m)])\n",
    "print(np.argmax(seqs_z))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(15, 15))\n",
    "ax.plot_surface(x, y, seqs_z)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_example_file = open(\"Correct_solutions.json\", \"w\")\n",
    "# json.dump(singles, correct_example_file, indent=4)\n",
    "# correct_example_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_file_num(num_str):\n",
    "#     if \".\" in num_str:\n",
    "#         parts = num_str.split(\".\")\n",
    "#         full = parts[0].strip()\n",
    "#         frac = parts[1].strip()\n",
    "#         if len(frac) < 4:\n",
    "#             frac = ('0' * (4-len(frac))) + frac\n",
    "#         elif len(frac) > 4:\n",
    "#             return None\n",
    "#         num_str = full + \".\" + frac\n",
    "#     return float(num_str)\n",
    "# # print(process_file_num(\"dpr_biencoder.2.108\"[14:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cd3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "\n",
    "# directories = [5]\n",
    "# all_results = {}\n",
    "\n",
    "\n",
    "# output_file = open(\"all_outputs.tsv\", 'a')\n",
    "\n",
    "# for d in directories:\n",
    "#     results = {}\n",
    "#     files = os.listdir(os.path.join(\"models/bert\", \"pandas_\" + str(d)))\n",
    "#     taken_files = [f for f in files if f.startswith(\"dpr_biencoder\")]\n",
    "#     points = []\n",
    "#     for f in taken_files:\n",
    "#         v = process_file_num(f[14:])\n",
    "#         if v is not None:\n",
    "#             points.append((f, v))\n",
    "#     points = sorted(points, key=lambda x: x[1])\n",
    "#     for i, (f, e) in enumerate(tqdm(points, total=len(points))):\n",
    "#         model_path = os.path.join(\"models/bert/\", \"pandas_\" + str(d), f)\n",
    "#         retriever = RetrieverModel(\n",
    "#             model_path=model_path, \n",
    "#             batch_size=128, \n",
    "#             quiet=True\n",
    "#         )\n",
    "#         _, singles = retriever.retrieve_apis(test_examples)\n",
    "#         results[f] = len(singles)\n",
    "#         print(\n",
    "#             d, e, len(singles), os.path.join(\"models/bert/\", \"pandas_\" + str(d), f), \n",
    "#             sep=\"\\t\", \n",
    "#             file=output_file, \n",
    "#             flush=True\n",
    "#         )\n",
    "#         if i % 1 == 0:\n",
    "#             print(\n",
    "#                 d, e, len(singles), os.path.join(\"models/bert/\", \"pandas_\" + str(d), f), \n",
    "#                 sep=\"\\t\", \n",
    "#             )\n",
    "#         pass\n",
    "#         del retriever\n",
    "#     print(\"=\" * 100)\n",
    "#     all_results[d] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5efd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in [0, 1, 2, 5]:\n",
    "#     retriever = RetrieverModel(\n",
    "#         model_path=f\"models/bert/pandas_{p}/checkpoint_best.pt\", \n",
    "#         batch_size=128, \n",
    "#         quiet=False\n",
    "#     )\n",
    "#     _, singles = retriever.retrieve_apis(test_examples, top_k=10)\n",
    "#     print(p, len(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ed827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}