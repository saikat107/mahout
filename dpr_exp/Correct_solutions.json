[
    {
        "id": 67845362,
        "query": "sort pandas df subset of rows (within a group) by specific column i have the following dataframe let\u2019s say: df and i would like to sort it based on col d for each sub row (that has for example same cols a,b and c in this case) the expected output would be: df any help for this kind of operation?",
        "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
        "example": {
            "qid": 67845362,
            "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
            "question": {
                "title": "Sort pandas df subset of rows (within a group) by specific column",
                "ques_desc": "I have the following dataframe let\u2019s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? "
            },
            "io": [
                "\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n",
                "\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n"
            ],
            "answer": {
                "ans_desc": "I think it should be as simple as this: ",
                "code": [
                    "df = df.sort_values([\"A\", \"B\", \"C\", \"D\"])\n"
                ]
            }
        },
        "expected": [
            "sort_values"
        ],
        "predicted": [
            [
                "loc",
                0.836526095867157
            ],
            [
                "groupby",
                0.8240469694137573
            ],
            [
                "sort_index",
                0.8012261986732483
            ],
            [
                "sort_values",
                0.8001962304115295
            ],
            [
                "filter",
                0.7986167073249817
            ],
            [
                "reset_index",
                0.7958390712738037
            ],
            [
                "iloc",
                0.7957351207733154
            ],
            [
                "values",
                0.7928802967071533
            ],
            [
                "query",
                0.792574405670166
            ],
            [
                "agg",
                0.792296290397644
            ]
        ]
    },
    {
        "id": 67696814,
        "query": "convert dictionary with sub-list of dictionaries into pandas dataframe i have this code with a dictionary \"dict\": the result is: but what i want is: i would like to obtain this, without using loops in python, and by using pandas. can anyone help me out? thanks in advance!",
        "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
        "example": {
            "qid": 67696814,
            "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
            "question": {
                "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
                "ques_desc": "I have this code with a dictionary \"dict\": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! "
            },
            "io": [
                "                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n",
                "        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n"
            ],
            "answer": {
                "ans_desc": "Tr this. This would depend on how large your data is. ",
                "code": [
                    "\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "items"
        ],
        "predicted": [
            [
                "explode",
                0.8834835290908813
            ],
            [
                "to_dict",
                0.8807324171066284
            ],
            [
                "append",
                0.8800475597381592
            ],
            [
                "DataFrame",
                0.879502534866333
            ],
            [
                "from_dict",
                0.8794682025909424
            ],
            [
                "itertuples",
                0.8777250051498413
            ],
            [
                "iterrows",
                0.8765648603439331
            ],
            [
                "iteritems",
                0.8760921955108643
            ],
            [
                "items",
                0.8760921955108643
            ],
            [
                "join",
                0.8755698204040527
            ]
        ]
    },
    {
        "id": 67595888,
        "query": "how to extract each numbers from pandas string column to list? how to do that? i have pandas dataframe looks like: i need to transfer this each row to separated list:",
        "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
        "example": {
            "qid": 67595888,
            "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
            "question": {
                "title": "how to extract each numbers from pandas string column to list?",
                "ques_desc": "How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: "
            },
            "io": [
                "Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n",
                "listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n"
            ],
            "answer": {
                "ans_desc": "You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: ",
                "code": [
                    "df['Column_A'].apply(lambda x: re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n",
                    "listA, listB, listC, listD = df.Column_A.str.findall(r\"[-+]?\\d*\\.\\d+|\\d+\")\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "join",
                0.9010043740272522
            ],
            [
                "explode",
                0.8940094709396362
            ],
            [
                "apply",
                0.8935996294021606
            ],
            [
                "DataFrame",
                0.8882400989532471
            ],
            [
                "values",
                0.8873914480209351
            ],
            [
                "to_string",
                0.8870333433151245
            ],
            [
                "applymap",
                0.8847289681434631
            ],
            [
                "from_dict",
                0.8823223114013672
            ],
            [
                "shape",
                0.8820246458053589
            ],
            [
                "append",
                0.8820088505744934
            ]
        ]
    },
    {
        "id": 67561501,
        "query": "splitting list in dataframe columns to separate columns my data frame looks like as follows i need to make it look like: my code so far i have tried using apply(pd.series) and iterating through a for loop to reassign the values and have not had success",
        "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
        "example": {
            "qid": 67561501,
            "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
            "question": {
                "title": "splitting list in dataframe columns to separate columns",
                "ques_desc": "my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success "
            },
            "io": [
                "    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n",
                "   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n"
            ],
            "answer": {
                "ans_desc": "Here is a way using and : ",
                "code": [
                    "df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n"
                ]
            }
        },
        "expected": [
            "applymap",
            "assign"
        ],
        "predicted": [
            [
                "apply",
                0.8569146394729614
            ],
            [
                "values",
                0.8400300741195679
            ],
            [
                "from_dict",
                0.8392795920372009
            ],
            [
                "assign",
                0.8389506936073303
            ],
            [
                "loc",
                0.8386296033859253
            ],
            [
                "update",
                0.8376967310905457
            ],
            [
                "applymap",
                0.8374828100204468
            ],
            [
                "eval",
                0.8361482620239258
            ],
            [
                "DataFrame",
                0.8355123996734619
            ],
            [
                "sort_index",
                0.8338254690170288
            ]
        ]
    },
    {
        "id": 67361824,
        "query": "convert dataframe objects to float by iterating over columns i want to convert data in pandas.series by iterating over series dataframe df looks like '%' and '-' only values should be removed. desired result: if i call it works. but if i try to iterate it does not: thanks in advance",
        "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
        "example": {
            "qid": 67361824,
            "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
            "question": {
                "title": "Convert dataframe objects to float by iterating over columns",
                "ques_desc": "I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance "
            },
            "io": [
                "   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n",
                "   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n"
            ],
            "answer": {
                "ans_desc": "EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. ",
                "code": [
                    "# Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n"
                ]
            }
        },
        "expected": [
            "replace"
        ],
        "predicted": [
            [
                "astype",
                0.8513514995574951
            ],
            [
                "replace",
                0.8485844135284424
            ],
            [
                "applymap",
                0.8457849025726318
            ],
            [
                "itertuples",
                0.8432376980781555
            ],
            [
                "rename",
                0.8413880467414856
            ],
            [
                "from_dict",
                0.8401776552200317
            ],
            [
                "values",
                0.8399680852890015
            ],
            [
                "iterrows",
                0.8391250967979431
            ],
            [
                "apply",
                0.8389530181884766
            ],
            [
                "DataFrame",
                0.8386911153793335
            ]
        ]
    },
    {
        "id": 26837998,
        "query": "pandas replace nan with blank/empty string i have a pandas dataframe as shown below: i want to remove the nan values with an empty string so that it looks like so:",
        "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string",
        "example": {
            "qid": 26837998,
            "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string",
            "question": {
                "title": "Pandas Replace NaN with blank/empty string",
                "ques_desc": "I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so: "
            },
            "io": [
                "    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n",
                "    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n"
            ],
            "answer": {
                "ans_desc": " This might help. It will replace all NaNs with an empty string. ",
                "code": [
                    "import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n"
                ]
            }
        },
        "expected": [
            "replace"
        ],
        "predicted": [
            [
                "fillna",
                0.8772585988044739
            ],
            [
                "notnull",
                0.8738058805465698
            ],
            [
                "notna",
                0.8738058805465698
            ],
            [
                "replace",
                0.8707874417304993
            ],
            [
                "where",
                0.8684812784194946
            ],
            [
                "mask",
                0.8676301836967468
            ],
            [
                "dropna",
                0.8672060370445251
            ],
            [
                "isnull",
                0.8608222603797913
            ],
            [
                "isna",
                0.8608222603797913
            ],
            [
                "ffill",
                0.8579850792884827
            ]
        ]
    },
    {
        "id": 67257898,
        "query": "how to add a value to a new column by referencing the values in a column i have a dataframe like this: the xy column must be filled with the value of the column names in the reason column. let's look at the first row. the reason column shows our value x1. so our value in column xy, will be the value of x1 column in the first row. like this: is there a way to do this?",
        "link": "https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column",
        "example": {
            "qid": 67257898,
            "link": "https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column",
            "question": {
                "title": "How to add a value to a new column by referencing the values in a column",
                "ques_desc": "I have a dataframe like this: The xy column must be filled with the value of the column names in the reason column. Let's look at the first row. The reason column shows our value x1. So our value in column xy, will be the value of x1 column in the first row. Like this: Is there a way to do this? "
            },
            "io": [
                "id  reason  x1    x2   x3   x4   x5 \n 1  x1      100   15   10   20   25\n 2  x1      15    16   14   10   10\n 3  x4      10    50   40   30   25\n 4  x3      12    15   60   5    1\n 5  x1      80    15   10   20   25\n 6  x1      15    19   84   10   10\n 7  x4      90    40   90   30   25\n 8  x4      12    85   60   50   10\n",
                "id  reason  x1    x2   x3   x4   x5   xy\n 1  x1      100   15   10   20   25   100\n 2  x1      15    16   14   10   10   15\n 3  x4      10    50   40   30   25   30\n 4  x3      12    15   60   5    1    60\n 5  x1      80    15   10   20   25   80\n 6  x1      15    19   84   10   10   15\n 7  x4      90    40   90   30   25   30\n 8  x4      12    85   60   50   10   50\n"
            ],
            "answer": {
                "ans_desc": " Prints: ",
                "code": [
                    "df[\"xy\"] = df.apply(lambda x: x[x[\"reason\"]], axis=1)\nprint(df)\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "columns",
                0.899443507194519
            ],
            [
                "index",
                0.8953673839569092
            ],
            [
                "apply",
                0.8937037587165833
            ],
            [
                "values",
                0.8932081460952759
            ],
            [
                "rename_axis",
                0.8897669911384583
            ],
            [
                "shape",
                0.8895226120948792
            ],
            [
                "iterrows",
                0.8893981575965881
            ],
            [
                "transpose",
                0.888500452041626
            ],
            [
                "sort_index",
                0.8884154558181763
            ],
            [
                "rename",
                0.887365996837616
            ]
        ]
    },
    {
        "id": 67213950,
        "query": "how can i compare each row from a dataframe against every row from another dataframe and see the difference between values? i have two dataframes: df1 df2 df1 acts like a dictionary, from which i can get the respective number for each item by checking their code. there are, however, unregistered codes, and in case i find an unregistered code, i'm supposed to look for the codes that look the most like them. so, the outcome should to be: abd123 = 1 (because it has 1 different character from abc123) dea456 = 4 (because it has 1 different character from dea456, and 2 from def456, so it chooses the closest one) ghi789 = 3 (because it has an equivalent at df1) i know how to check for the differences of each code individually and save the \"length\" of characters that differ, but i don't know how to apply this code as i don't know how to compare each row from df2 against all rows from df1. is there a way?",
        "link": "https://stackoverflow.com/questions/67213950/how-can-i-compare-each-row-from-a-dataframe-against-every-row-from-another-dataf",
        "example": {
            "qid": 67213950,
            "link": "https://stackoverflow.com/questions/67213950/how-can-i-compare-each-row-from-a-dataframe-against-every-row-from-another-dataf",
            "question": {
                "title": "How can I compare each row from a dataframe against every row from another dataframe and see the difference between values?",
                "ques_desc": "I have two dataframes: df1 df2 df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be: ABD123 = 1 (because it has 1 different character from ABC123) DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one) GHI789 = 3 (because it has an equivalent at df1) I know how to check for the differences of each code individually and save the \"length\" of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way? "
            },
            "io": [
                "     Code     Number\n0   ABC123      1\n1   DEF456      2\n2   GHI789      3\n3   DEA456      4\n",
                "     Code \n0   ABD123\n1   DEA458\n2   GHI789\n"
            ],
            "answer": {
                "ans_desc": " don't know how to compare each row from df2 against all rows from df1. Nested loops will work. If you had a function named it would look like this... Nested loops are usually not ideal when working with Pandas or Numpy but they do work. There may be better solutions. DataFrame.iterrows() ",
                "code": [
                    "for index2, row2 in df2.iterrows():\n    for index1, row1 in df1.iterrows():\n        difference = compare(row2,row1)\n        #do something with the difference.\n"
                ]
            }
        },
        "expected": [
            "iterrows"
        ],
        "predicted": [
            [
                "merge",
                0.8738445043563843
            ],
            [
                "rename",
                0.8680089712142944
            ],
            [
                "DataFrame",
                0.8652855753898621
            ],
            [
                "drop_duplicates",
                0.8648254871368408
            ],
            [
                "iterrows",
                0.8635987043380737
            ],
            [
                "filter",
                0.8629974126815796
            ],
            [
                "duplicated",
                0.8626834750175476
            ],
            [
                "index",
                0.8623223900794983
            ],
            [
                "agg",
                0.8622242212295532
            ],
            [
                "loc",
                0.8622172474861145
            ]
        ]
    },
    {
        "id": 66991966,
        "query": "how to convert this dataframe into json i have this with 2 columns when i try to convert it into it goes wrong: i don't even know ehere the numbers come from. my desired :",
        "link": "https://stackoverflow.com/questions/66991966/how-to-convert-this-dataframe-into-json",
        "example": {
            "qid": 66991966,
            "link": "https://stackoverflow.com/questions/66991966/how-to-convert-this-dataframe-into-json",
            "question": {
                "title": "How to convert this DataFrame into Json",
                "ques_desc": "I have this with 2 columns when I try to convert it into it goes wrong: I don't even know ehere the numbers come from. My desired : "
            },
            "io": [
                "print(df)\n\n     a                b\n\n     10          {'A': 'foo', ...}\n     20          {'B': 'faa', ...}\n     30          {'C': 'fee', ...}\n     40          {'D': 'fii', ...}\n     50          {'E': 'foo', ...}\n\n",
                "[{\n   'a': 10,\n   'b': {\n         'A': 'foo', \n         ...\n        }, \n    ...\n    'a': 50,\n    'b': {\n         'E': 'foo', \n         ...\n        }\n}\n]\n\n"
            ],
            "answer": {
                "ans_desc": "You could try the following: This should give you your desired output. If you want to convert this into a JSON file then you can do the following: ",
                "code": [
                    "data = []\nfor i in df:\n    data.append({'a': df[i[0]], 'b': df(i[1])})\n",
                    "with open(\"myjson.json\", \"w\") as f:\n    json.dump(data, f, indent=4)\n"
                ]
            }
        },
        "expected": [
            "append"
        ],
        "predicted": [
            [
                "to_dict",
                0.8830084800720215
            ],
            [
                "from_dict",
                0.8728833794593811
            ],
            [
                "join",
                0.8715981841087341
            ],
            [
                "apply",
                0.8667629957199097
            ],
            [
                "to_period",
                0.864472508430481
            ],
            [
                "DataFrame",
                0.8644299507141113
            ],
            [
                "reindex",
                0.8635793924331665
            ],
            [
                "shape",
                0.8628946542739868
            ],
            [
                "merge",
                0.861977756023407
            ],
            [
                "append",
                0.8599321842193604
            ]
        ]
    },
    {
        "id": 66940820,
        "query": "python: how to pass dataframe columns as parameters to a function? i have a dataframe with 2 columns of text embeddings namely and . i want to create a third column in named which should contain the cosine_similarity between every row of and . but when i try to implement this using the following code i get a . how to fix it? dataframe code to calculate cosine similarity error required dataframe",
        "link": "https://stackoverflow.com/questions/66940820/python-how-to-pass-dataframe-columns-as-parameters-to-a-function",
        "example": {
            "qid": 66940820,
            "link": "https://stackoverflow.com/questions/66940820/python-how-to-pass-dataframe-columns-as-parameters-to-a-function",
            "question": {
                "title": "Python: How to pass Dataframe Columns as parameters to a function?",
                "ques_desc": "I have a dataframe with 2 columns of text embeddings namely and . I want to create a third column in named which should contain the cosine_similarity between every row of and . But when I try to implement this using the following code I get a . How to fix it? Dataframe Code to Calculate Cosine Similarity Error Required Dataframe "
            },
            "io": [
                "           embedding_1              |            embedding_2                                 \n [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]\n [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]\n",
                "       embedding_1              |            embedding_2                 |  distances                        \n [[-0.28876397, -0.6367827, ...]]   |  [[-0.49163356, -0.4877703,...]]   |    0.427\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.06686627, -0.75147504...]]   |    0.673\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.42776933, -0.88310856,...]]  |    0.882\n [[-0.28876397, -0.6367827, ...]]   |  [[-0.6520882, -1.049325,...]]     |    0.665\n [[-0.28876397, -0.6367827, ...]]   |  [[-1.4216679, -0.8930428,...]]    |    0.312\n"
            ],
            "answer": {
                "ans_desc": "You can use to use on each row: or one liner ",
                "code": [
                    "def cal_cosine_similarity(row):\n    return cosine_similarity(row['embeddings_1'], row['embeddings_2'])\n\ndf['distances'] = df.apply(cal_cosine_similarity, axis=1)\n",
                    "df['distances'] = df.apply(lambda row: cosine_similarity(row['embeddings_1'], row['embeddings_2']), axis=1)\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "apply",
                0.8889824748039246
            ],
            [
                "join",
                0.885391354560852
            ],
            [
                "DataFrame",
                0.8779939413070679
            ],
            [
                "query",
                0.8774340152740479
            ],
            [
                "rename",
                0.876046359539032
            ],
            [
                "sort_index",
                0.8756147623062134
            ],
            [
                "eval",
                0.8747957348823547
            ],
            [
                "values",
                0.8746257424354553
            ],
            [
                "applymap",
                0.8738197088241577
            ],
            [
                "loc",
                0.8733600378036499
            ]
        ]
    },
    {
        "id": 66782284,
        "query": "python pandas give comma separated values into columns with &quot;title&quot; i have some comma-separated data in the same column and i wish to separate each value into different columns. i have done separation using below and the output i got is but i need something like below (has to give some titles for each column) how would i do that?",
        "link": "https://stackoverflow.com/questions/66782284/python-pandas-give-comma-separated-values-into-columns-with-title",
        "example": {
            "qid": 66782284,
            "link": "https://stackoverflow.com/questions/66782284/python-pandas-give-comma-separated-values-into-columns-with-title",
            "question": {
                "title": "python pandas give comma separated values into columns with &quot;title&quot;",
                "ques_desc": "I have some comma-separated data in the same column and I wish to separate each value into different columns. I have done separation using below and the output I got is but I need something like below (has to give some titles for each column) How would I do that? "
            },
            "io": [
                "0          13.4119837, 42.082885, 13.4119837, 42.082885\n1        11.6285463, 42.4193742, 11.6285463, 42.4193742\n2            -3.606772, 39.460299, -3.606772, 39.460299\n3            -0.515639, 38.988847, -0.515639, 38.988847\n4            -2.403309, 37.241792, -2.403309, 37.241792\n",
                "     0           1           2           3\n0   13.4119837  42.082885   13.4119837  42.082885\n1   11.6285463  42.4193742  11.6285463  42.4193742\n2   -3.606772   39.460299   -3.606772   39.460299\n3   -0.515639   38.988847   -0.515639   38.988847\n4   -2.403309   37.241792   -2.403309   37.241792\n"
            ],
            "answer": {
                "ans_desc": "Use : ",
                "code": [
                    "data = data['column_name'].str.split(\",\", n = 3, expand = True)\ndata.columns = ['minLat', 'maxLat', 'minLong', 'maxLong']\n"
                ]
            }
        },
        "expected": [
            "columns"
        ],
        "predicted": [
            [
                "join",
                0.8822118639945984
            ],
            [
                "apply",
                0.8816473484039307
            ],
            [
                "DataFrame",
                0.8779950141906738
            ],
            [
                "applymap",
                0.8773882389068604
            ],
            [
                "rename",
                0.8747094869613647
            ],
            [
                "drop",
                0.873667299747467
            ],
            [
                "explode",
                0.8719507455825806
            ],
            [
                "itertuples",
                0.8717380166053772
            ],
            [
                "columns",
                0.8711817264556885
            ],
            [
                "stack",
                0.8708388209342957
            ]
        ]
    },
    {
        "id": 66567933,
        "query": "can i shift specific values in one data column to another column while keeping the other values unchanged? here is an example dataset that i have: i want to take all the values that have \"1\" in them in the column \"c2\" and shift them to replace the adjacent values in column \"c1\". so the output should look like: alternatively, i could create a new column with these values replaced. main point is, that i need all the \"1s\" in c2 to replace the nan values in c1. i can't do find all nan and replace with 1, because there are some nan values that should stay in c1. is there a way to do this? thanks for the help in advance.",
        "link": "https://stackoverflow.com/questions/66567933/can-i-shift-specific-values-in-one-data-column-to-another-column-while-keeping-t",
        "example": {
            "qid": 66567933,
            "link": "https://stackoverflow.com/questions/66567933/can-i-shift-specific-values-in-one-data-column-to-another-column-while-keeping-t",
            "question": {
                "title": "Can I shift specific values in one data column to another column while keeping the other values unchanged?",
                "ques_desc": "Here is an example dataset that I have: I want to take all the values that have \"1\" in them in the Column \"C2\" and shift them to replace the adjacent values in column \"C1\". So the output should look like: Alternatively, I could create a new column with these values replaced. Main point is, that I need all the \"1s\" in C2 TO replace the NaN values in C1. I can't do find all NaN and replace with 1, because there are some NaN values that should stay in C1. Is there a way to do this? Thanks for the help in advance. "
            },
            "io": [
                "C1      C2\n 1       1\nNaN      1\n 2       0\nNaN      0\nNaN      1\n 1       1\n 2       2\n 2       2\nNaN      1\n",
                "C1      C2\n 1       1\n 1       1\n 2       0\n NaN     0\n 1       1\n 1       1\n 2       2\n 2       2\n 1       1\n\n"
            ],
            "answer": {
                "ans_desc": "You could use the api to apply values from one column to another in rows where some condition is true (e.g. ==1). ",
                "code": [
                    "value = 1\nsource_col = 1\ntarget_col = 0\n\ncondition = df[source_col] == value\n\ndf[target_col] = df[target_col].mask(condition,\n                                     df[source_col])\n"
                ]
            }
        },
        "expected": [
            "mask"
        ],
        "predicted": [
            [
                "fillna",
                0.8661568760871887
            ],
            [
                "notnull",
                0.8508012294769287
            ],
            [
                "notna",
                0.8508012294769287
            ],
            [
                "transform",
                0.8477768301963806
            ],
            [
                "dropna",
                0.847236156463623
            ],
            [
                "where",
                0.8465253114700317
            ],
            [
                "mask",
                0.8450599908828735
            ],
            [
                "interpolate",
                0.8427770137786865
            ],
            [
                "isnull",
                0.842170774936676
            ],
            [
                "isna",
                0.842170774936676
            ]
        ]
    },
    {
        "id": 66496528,
        "query": "repeating single dataframe with changing datetimeindex let's say i have very simple dataframe like this: output: i would like to take this dataframe and create longer that would append dataframe itself with changing year of index. something like this: it's still the same dataframe, repeating again and again, and year is incrementally changed. i could do something like this (example for 3 years): i have mainly two questions: is there a way how to do this in a single command? what is the best way how to deal with leap-year?",
        "link": "https://stackoverflow.com/questions/66496528/repeating-single-dataframe-with-changing-datetimeindex",
        "example": {
            "qid": 66496528,
            "link": "https://stackoverflow.com/questions/66496528/repeating-single-dataframe-with-changing-datetimeindex",
            "question": {
                "title": "Repeating single DataFrame with changing DateTimeIndex",
                "ques_desc": "Let's say I have very simple DataFrame like this: Output: I would like to take this DataFrame and create longer that would append DataFrame itself with changing year of index. Something like this: It's still the same DataFrame, repeating again and again, and year is incrementally changed. I could do something like this (example for 3 years): I have mainly two questions: Is there a way how to do this in a single command? What is the best way how to deal with leap-year? "
            },
            "io": [
                "             A  B   C   D\n2010-01-31   6  0   8  10\n2010-02-28   7  8  10   3\n2010-03-31  10  5   8  10\n2010-04-30   4  4   9   7\n2010-05-31   2  3   0  11\n2010-06-30   8  7  10   8\n2010-07-31  11  9   0   4\n2010-08-31   0  3   8   6\n2010-09-30   4  6   7   9\n2010-10-31   1  0  11   9\n2010-11-30   5  4   8   4\n2010-12-31   1  4   5   1\n",
                "             A  B   C   D\n2010-01-31   6  0   8  10\n2010-02-28   7  8  10   3\n2010-03-31  10  5   8  10\n2010-04-30   4  4   9   7\n2010-05-31   2  3   0  11\n2010-06-30   8  7  10   8\n2010-07-31  11  9   0   4\n2010-08-31   0  3   8   6\n2010-09-30   4  6   7   9\n2010-10-31   1  0  11   9\n2010-11-30   5  4   8   4\n2010-12-31   1  4   5   1\n2011-01-31   6  0   8  10\n2011-02-28   7  8  10   3\n2011-03-31  10  5   8  10\n2011-04-30   4  4   9   7\n2011-05-31   2  3   0  11\n2011-06-30   8  7  10   8\n2011-07-31  11  9   0   4\n2011-08-31   0  3   8   6\n2011-09-30   4  6   7   9\n2011-10-31   1  0  11   9\n2011-11-30   5  4   8   4\n2011-12-31   1  4   5   1\n2012-01-31   6  0   8  10\n2012-02-28   7  8  10   3\n2012-03-31  10  5   8  10\n2012-04-30   4  4   9   7\n2012-05-31   2  3   0  11\n2012-06-30   8  7  10   8\n2012-07-31  11  9   0   4\n2012-08-31   0  3   8   6\n2012-09-30   4  6   7   9\n2012-10-31   1  0  11   9\n2012-11-30   5  4   8   4\n2012-12-31   1  4   5   1\n"
            ],
            "answer": {
                "ans_desc": "Here's an option with applying to the original index in list comprehension: ",
                "code": [
                    "data_new = pd.concat([\n    df.set_index(df.index + pd.DateOffset(year=x)) for x in [2010, 2011, 2012]])\n"
                ]
            }
        },
        "expected": [
            "index",
            "set_index"
        ],
        "predicted": [
            [
                "shape",
                0.8834635019302368
            ],
            [
                "index",
                0.8830075263977051
            ],
            [
                "DataFrame",
                0.8815581798553467
            ],
            [
                "set_index",
                0.8768843412399292
            ],
            [
                "reset_index",
                0.8768802285194397
            ],
            [
                "values",
                0.8763217926025391
            ],
            [
                "iloc",
                0.8762235641479492
            ],
            [
                "pivot_table",
                0.8760902285575867
            ],
            [
                "unstack",
                0.8751288652420044
            ],
            [
                "sort_index",
                0.8750174045562744
            ]
        ]
    },
    {
        "id": 66422239,
        "query": "python transposing multiple dataframes in a list i have a few dataframes which are similar (in terms of number of rows and columns) to the 2 dataframes listed below my desired output is to have multiple dataframes with the email as column header and the factor or item as rows i am able to get the result by transposing each dataframe individually using this but i'd like to create a for loop as i have several dataframes to transpose wrote something like this but the dataframes do not get transposed. would like to directly change the dataframes in the list of dataframes (somewhere along the lines of inplace=true). was wondering if there is something i am missing, appreciate any form of help, thank you.",
        "link": "https://stackoverflow.com/questions/66422239/python-transposing-multiple-dataframes-in-a-list",
        "example": {
            "qid": 66422239,
            "link": "https://stackoverflow.com/questions/66422239/python-transposing-multiple-dataframes-in-a-list",
            "question": {
                "title": "Python transposing multiple dataframes in a list",
                "ques_desc": "I have a few dataframes which are similar (in terms of number of rows and columns) to the 2 dataframes listed below my desired output is to have multiple dataframes with the email as column header and the factor or item as rows I am able to get the result by transposing each dataframe individually using this but i'd like to create a for loop as i have several dataframes to transpose wrote something like this but the dataframes do not get transposed. Would like to directly change the dataframes in the list of dataframes (somewhere along the lines of inplace=True). Was wondering if there is something i am missing, appreciate any form of help, thank you. "
            },
            "io": [
                "0    email           factor1_final   factor2_final   factor3_final\n1    john@abc.com    85%             90%             50%\n2    peter@abc.com   80%             60%             60%\n3    shelby@abc.com  50%             70%             60%\n4    jess@abc.com    60%             65%             50% \n5    mark@abc.com    98%             50%             60%\n",
                "\nemail     john@abc.com   peter@abc.com   shelby@abc.com   jess@abc.com   mark@abc.com\nfactor1     85%          80%             50%              60%            98%\nfactor2     90%          60%             70%              65%            50% \nfactor3     50%          60%             60%              50%            60%\n"
            ],
            "answer": {
                "ans_desc": "For me second solution working, here is small alternative: Solution with create new list of DataFrames: ",
                "code": [
                    "dfs = [df.set_index('email').T for df in df_list]\n"
                ]
            }
        },
        "expected": [
            "set_index"
        ],
        "predicted": [
            [
                "transpose",
                0.8794631958007812
            ],
            [
                "rename",
                0.8727134466171265
            ],
            [
                "columns",
                0.8659223914146423
            ],
            [
                "append",
                0.8653271794319153
            ],
            [
                "DataFrame",
                0.8652817010879517
            ],
            [
                "set_index",
                0.8641124963760376
            ],
            [
                "reset_index",
                0.8636143803596497
            ],
            [
                "rename_axis",
                0.8630592823028564
            ],
            [
                "index",
                0.861667275428772
            ],
            [
                "shape",
                0.8594627976417542
            ]
        ]
    },
    {
        "id": 66357650,
        "query": "transform a pandas dataframe in a pandas with multicolumns i have the following pandas dataframe, where the columna is the dataframe index and i want to convert this datframe in to a multi column data frame, that looks like this i've tried transforming my old pandas dataframe in to a dict this way: but i had no success, can someone give me tips and advices on how to do that? any help is more than welcome.",
        "link": "https://stackoverflow.com/questions/66357650/transform-a-pandas-dataframe-in-a-pandas-with-multicolumns",
        "example": {
            "qid": 66357650,
            "link": "https://stackoverflow.com/questions/66357650/transform-a-pandas-dataframe-in-a-pandas-with-multicolumns",
            "question": {
                "title": "transform a pandas dataframe in a pandas with multicolumns",
                "ques_desc": "I have the following pandas dataframe, where the columna is the dataframe index And i want to convert this datframe in to a multi column data frame, that looks like this I've tried transforming my old pandas dataframe in to a dict this way: But i had no success, can someone give me tips and advices on how to do that? Any help is more than welcome. "
            },
            "io": [
                "+----+-----------+------------+-----------+------------+\n|    |   price_A |   amount_A |   price_B |   amount_b |\n|----+-----------+------------+-----------+------------|\n|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |\n|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |\n|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |\n|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |\n|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |\n|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |\n|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |\n|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |\n|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |\n|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |\n+----+-----------+------------+-----------+------------+\n\n",
                "\n+----+-----------+------------+-----------+------------+\n|    |           A            |           B            |\n+----+-----------+------------+-----------+------------+\n| id |   price   |   amount   |   price   |   amount   |\n|----+-----------+------------+-----------+------------|\n|  0 | 0.652826  |  0.941421  |  0.823048 |  0.728427  |\n|  1 | 0.400078  |  0.600585  |  0.194912 |  0.269842  |\n|  2 | 0.223524  |  0.146675  |  0.375459 |  0.177165  |\n|  3 | 0.330626  |  0.214981  |  0.389855 |  0.541666  |\n|  4 | 0.578132  |  0.30478   |  0.789573 |  0.268851  |\n|  5 | 0.0943601 |  0.514878  |  0.419333 |  0.0170096 |\n|  6 | 0.279122  |  0.401132  |  0.722363 |  0.337094  |\n|  7 | 0.444977  |  0.333254  |  0.643878 |  0.371528  |\n|  8 | 0.724673  |  0.0632807 |  0.345225 |  0.935403  |\n|  9 | 0.905482  |  0.8465    |  0.585653 |  0.364495  |\n+----+-----------+------------+-----------+------------+\n\n"
            ],
            "answer": {
                "ans_desc": "Try renaming columns manually: Output: ",
                "code": [
                    "df.columns=pd.MultiIndex.from_tuples([x.split('_')[::-1] for x in df.columns])\ndf.index.name='id'\n"
                ]
            }
        },
        "expected": [
            "columns",
            "index"
        ],
        "predicted": [
            [
                "transpose",
                0.8701788783073425
            ],
            [
                "shape",
                0.8675963878631592
            ],
            [
                "DataFrame",
                0.8652389049530029
            ],
            [
                "index",
                0.8627429008483887
            ],
            [
                "columns",
                0.8569331765174866
            ],
            [
                "values",
                0.8566398620605469
            ],
            [
                "rename_axis",
                0.8557980060577393
            ],
            [
                "explode",
                0.8554813861846924
            ],
            [
                "set_index",
                0.8552137017250061
            ],
            [
                "rename",
                0.8527869582176208
            ]
        ]
    },
    {
        "id": 26716616,
        "query": "convert a pandas dataframe to a dictionary i have a dataframe with four columns. i want to convert this dataframe to a python dictionary. i want the elements of first column be and the elements of other columns in same row be . dataframe: output should be like this: dictionary:",
        "link": "https://stackoverflow.com/questions/26716616/convert-a-pandas-dataframe-to-a-dictionary",
        "example": {
            "qid": 26716616,
            "link": "https://stackoverflow.com/questions/26716616/convert-a-pandas-dataframe-to-a-dictionary",
            "question": {
                "title": "Convert a Pandas DataFrame to a dictionary",
                "ques_desc": "I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be and the elements of other columns in same row be . DataFrame: Output should be like this: Dictionary: "
            },
            "io": [
                "    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n",
                "{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n"
            ],
            "answer": {
                "ans_desc": "The method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this. also accepts an 'orient' argument which you'll need in order to output a list of values for each column. Otherwise, a dictionary of the form will be returned for each column. These steps can be done with the following line: In case a different dictionary format is needed, here are examples of the possible orient arguments. Consider the following simple DataFrame: Then the options are as follows. dict - the default: column names are keys, values are dictionaries of index:data pairs list - keys are column names, values are lists of column data series - like 'list', but values are Series split - splits columns/data/index as keys with values being column names, data values by row and index labels respectively records - each row becomes a dictionary where key is column name and value is the data in the cell index - like 'records', but a dictionary of dictionaries with keys as index labels (rather than a list) ",
                "code": [
                    ">>> df.to_dict('split')\n{'columns': ['a', 'b'],\n 'data': [['red', 0.5], ['yellow', 0.25], ['blue', 0.125]],\n 'index': [0, 1, 2]}\n",
                    ">>> df.to_dict('records')\n[{'a': 'red', 'b': 0.5}, \n {'a': 'yellow', 'b': 0.25}, \n {'a': 'blue', 'b': 0.125}]\n",
                    ">>> df.to_dict('index')\n{0: {'a': 'red', 'b': 0.5},\n 1: {'a': 'yellow', 'b': 0.25},\n 2: {'a': 'blue', 'b': 0.125}}\n"
                ]
            }
        },
        "expected": [
            "to_dict"
        ],
        "predicted": [
            [
                "to_dict",
                0.8688710927963257
            ],
            [
                "explode",
                0.8561611771583557
            ],
            [
                "DataFrame",
                0.854378879070282
            ],
            [
                "to_string",
                0.8517342209815979
            ],
            [
                "apply",
                0.8482462763786316
            ],
            [
                "values",
                0.8478108048439026
            ],
            [
                "shape",
                0.8474128246307373
            ],
            [
                "from_dict",
                0.8466327786445618
            ],
            [
                "stack",
                0.8464726209640503
            ],
            [
                "applymap",
                0.845433235168457
            ]
        ]
    },
    {
        "id": 66255574,
        "query": "reverse columns of dataframe based on the column name i have a dataframe: i would like to reverse the columns that their column names have their 1st and 2nd letters reversed and their 3rd and 4th as-is. i.e. 1st col: 1000 \u2192 2nd col: 0100 3rd col: 0010 \u2192 5th col: 1110 4th col: 0001 \u2192 6th col: 1101 7th col: 1011 \u2192 8th col: 0111 i would like to have a dataframe like this: this is what i have for the reversion:",
        "link": "https://stackoverflow.com/questions/66255574/reverse-columns-of-dataframe-based-on-the-column-name",
        "example": {
            "qid": 66255574,
            "link": "https://stackoverflow.com/questions/66255574/reverse-columns-of-dataframe-based-on-the-column-name",
            "question": {
                "title": "Reverse columns of dataframe based on the column name",
                "ques_desc": "I have a dataframe: I would like to reverse the columns that their column names have their 1st and 2nd letters reversed and their 3rd and 4th as-is. i.e. 1st col: 1000 \u2192 2nd col: 0100 3rd col: 0010 \u2192 5th col: 1110 4th col: 0001 \u2192 6th col: 1101 7th col: 1011 \u2192 8th col: 0111 I would like to have a dataframe like this: This is what I have for the reversion: "
            },
            "io": [
                "     '1000'    '0100'    '0010'    '0001'    '1110'    '1101'    '1011'    '0111'\n0        0         1         2         3         4         5         6         7\n1       00        11        22        33        44        55        66        77\n",
                "     '0100'    '1000'    '1110'    '1101'    '0010'    '0001'    '1011'    '0111'\n0        1         0         4         5         2         3         7         6\n1       11        00        44        55        22        33        77        66\n"
            ],
            "answer": {
                "ans_desc": "Use your function in list comprehenion with add next 2 values and then change order of columns by original ordering in subset: EDIT: More general solution is convert values to 2d numpy array filled by boolean: And then for inverting use indexing, here means all rows and columns positions form list and invert mask: Last join back to list of strings: ",
                "code": [
                    "df.columns = df.columns.str.strip(\"'\")\ncols = df.columns\n\narr = np.array([[bool(int(y)) for y in x] for x in df.columns])\nprint (arr)\n[[ True False False False]\n [False  True False False]\n [False False  True False]\n [False False False  True]\n [ True  True  True False]\n [ True  True False  True]\n [ True False  True  True]\n [False  True  True  True]]\n"
                ]
            }
        },
        "expected": [
            "columns"
        ],
        "predicted": [
            [
                "transpose",
                0.8389755487442017
            ],
            [
                "rename",
                0.8389595746994019
            ],
            [
                "columns",
                0.8353990912437439
            ],
            [
                "drop",
                0.8320572376251221
            ],
            [
                "apply",
                0.8317517042160034
            ],
            [
                "rename_axis",
                0.8307570219039917
            ],
            [
                "join",
                0.8290790915489197
            ],
            [
                "loc",
                0.8264009356498718
            ],
            [
                "sort_index",
                0.8263287544250488
            ],
            [
                "iloc",
                0.8249563574790955
            ]
        ]
    },
    {
        "id": 66194236,
        "query": "dataframe - pandas - how ploting same columns of two dataframe for visualising the differences there are two dataframes and i would like to have a plot that shows the both price columns on x axis and sum on the y axis to see how are the difference between these two dataframes. i tried the below but does nothing. what is the best way to show the differences between the two patterns of the price in these two dataframes? i thought of something like this, but if there is a better way to show the differences please mention it.",
        "link": "https://stackoverflow.com/questions/66194236/dataframe-pandas-how-ploting-same-columns-of-two-dataframe-for-visualising-t",
        "example": {
            "qid": 66194236,
            "link": "https://stackoverflow.com/questions/66194236/dataframe-pandas-how-ploting-same-columns-of-two-dataframe-for-visualising-t",
            "question": {
                "title": "Dataframe - Pandas - How ploting same columns of two dataframe for visualising the differences",
                "ques_desc": "There are two dataframes and I would like to have a plot that shows the both price columns on X axis and sum on the Y axis to see how are the difference between these two dataframes. I tried the below but does nothing. What is the best way to show the differences between the two patterns of the price in these two dataframes? I thought of something like this, but if there is a better way to show the differences please mention it. "
            },
            "io": [
                "df1\n+-----+-----+-------+\n|     | id  | price |\n+-----+-----+-------+\n| 1   | 1   | 5    |\n+-----+-----+-------+\n| 2   | 2   | 12    |\n+-----+-----+-------+\n| 3   | 3   | 34    |\n+-----+-----+-------+\n| 4   | 4   | 62    |\n+-----+-----+-------+\n| ... | ... | ...   |\n+-----+-----+-------+\n| 125 | 125 | 90    |\n+-----+-----+-------+\n",
                "df2\n+-----+-----+-------+\n|     | id  | price |\n+-----+-----+-------+\n| 1   | 1   | 14    |\n+-----+-----+-------+\n| 2   | 2   | 15    |\n+-----+-----+-------+\n| 3   | 3   | 45    |\n+-----+-----+-------+\n| 4   | 4   | 62    |\n+-----+-----+-------+\n| ... | ... | ...   |\n+-----+-----+-------+\n| 125 | 125 | 31    |\n+-----+-----+-------+\n"
            ],
            "answer": {
                "ans_desc": "If you take first column from df1, and second column from df2, there will be no couple of lines, only one. For qualitatively compartion you can use matplotlib in simple way, because it automaticly creates a figure. Here is the result: For every arg on x it shows point on df1[col1] and df2[col2]. But with this plot you can not compare it quantitatively. PS: Here is logic you tried to realize but with seaborn. Result: Optional Quantitative comparison. Result: ",
                "code": [
                    "import pandas as pd \nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\n\ndf1 = pd.DataFrame({\n    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])\n                  })\ndf2 = pd.DataFrame({\n    'col1': range(0,5), 'col2': sorted([round(random.uniform(100, 2000)) for i in range(0,5)])\n                  })\n\nplt.plot(df1['col2'], label='first')\nplt.plot(df2['col2'], label='second')\nplt.legend()\n",
                    "df3 = pd.merge(df1,df2, on='col1')\nsns.lineplot(x='col2_x', y='col2_y', data=df3)\n",
                    "df3['dif'] = abs(df3['col2_x'] - df3['col2_y'])\n\nsns.lineplot(x='col1', y='dif', data=df3)\nplt.xticks(df3['col1'])\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "merge",
            "plot"
        ],
        "predicted": [
            [
                "set_index",
                0.8485668301582336
            ],
            [
                "filter",
                0.8472500443458557
            ],
            [
                "sum",
                0.8468520045280457
            ],
            [
                "plot",
                0.8448403477668762
            ],
            [
                "columns",
                0.8445854187011719
            ],
            [
                "pivot",
                0.8444874882698059
            ],
            [
                "agg",
                0.8425431847572327
            ],
            [
                "sort_values",
                0.842380940914154
            ],
            [
                "DataFrame",
                0.842129111289978
            ],
            [
                "merge",
                0.8421146273612976
            ]
        ]
    },
    {
        "id": 66138245,
        "query": "pandas regex comprehension - isolate single result i have a dataframe that's been extracted from an sql server, and i used regex to extract a string of three dimensions. i need all three dimensions in three new columns so i have used a regular expression for a number optionally separated by a period, and created a column from this findall result. but the result shows as a list and i am unable to index the second dimension. due to the urgency i have been able to temporarily solve this with a lookaround. but how can i directly extract dimension column extract - not all are in this format code extract for finding dims sample output using the findall result i would need a column for 1493.4 and a second column for 204.2 - i can do the first one but how would i create a column for specific indexes in the regex results. i have tried lambda, list comprehension, and everything else i can think of. so far i cannot find a similar question online and i know it should be simple - but its taken me 2 days! many thanks for all your help edit: to confirm, the initial regex results are not always in the same format, sometimes as zz.zmm x zz.zmm x zmm, sometimes as zz x zz mm, there are many cases where it is preferable to extract a list of the numbers only, not with a strong, specific regex pattern. additionally, my focus is on obtaining only list item n to a new column and not every item in the list",
        "link": "https://stackoverflow.com/questions/66138245/pandas-regex-comprehension-isolate-single-result",
        "example": {
            "qid": 66138245,
            "link": "https://stackoverflow.com/questions/66138245/pandas-regex-comprehension-isolate-single-result",
            "question": {
                "title": "Pandas regex comprehension - isolate single result",
                "ques_desc": "I have a dataframe that's been extracted from an SQL server, and I used regex to extract a string of three dimensions. I need all three dimensions in three new columns so I have used a regular expression for a number optionally separated by a period, and created a column from this findall result. But the result shows as a list and I am unable to index the second dimension. Due to the urgency I have been able to temporarily solve this with a lookaround. But how can I directly extract dimension column extract - not all are in this format code extract for finding dims sample output using the findall result I would need a column for 1493.4 and a second column for 204.2 - I can do the first one but how would I create a column for specific indexes in the regex results. I have tried lambda, list comprehension, and everything else I can think of. So far I cannot find a similar question online and I know it should be simple - but its taken me 2 days! Many thanks for all your help EDIT: To confirm, the initial regex results are not always in the same format, sometimes as zz.zmm x zz.zmm x zmm, sometimes as zz x zz mm, there are many cases where it is preferable to extract a list of the numbers only, not with a strong, specific regex pattern. Additionally, my focus is on obtaining only list item n to a new column and not every item in the list "
            },
            "io": [
                "[1103.5 x 308.8 x 25.4 mm]\n[33.3 x 13 x 9.5mm]\n[136.5 x 15 x 12.7 mm]\n",
                "[1493.4, 204.2, 25.4, 0013, 900, 4]\n[136.5, 15, 12.7, 001, 900, 2]\n"
            ],
            "answer": {
                "ans_desc": "If I understand correctly you are looking for way of expanding column holding lists, assuming that number of matches is equal in all record, you might do: output: If only n-th element is required you might use following way: ",
                "code": [
                    "import pandas as pd\ndf = pd.DataFrame({'dim':['10x10','12x10','10x12']})\ndf['dim_list'] = df.dim.str.findall(r'\\d+')\ndf[['width','height']] = df.dim_list.apply(pd.Series)\nprint(df)\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "apply"
        ],
        "predicted": [
            [
                "shape",
                0.8880380392074585
            ],
            [
                "values",
                0.8859575986862183
            ],
            [
                "apply",
                0.885668158531189
            ],
            [
                "transpose",
                0.8829876184463501
            ],
            [
                "loc",
                0.8828998804092407
            ],
            [
                "append",
                0.8824234008789062
            ],
            [
                "explode",
                0.882155179977417
            ],
            [
                "index",
                0.8791894316673279
            ],
            [
                "itertuples",
                0.8790353536605835
            ],
            [
                "DataFrame",
                0.8787304162979126
            ]
        ]
    },
    {
        "id": 66088290,
        "query": "use a categorical column to order the dataframe according to an array i have an array like this: i also have a dataframe like this: i want to use to order the column dataframe according to the array. the expected output is:",
        "link": "https://stackoverflow.com/questions/66088290/use-a-categorical-column-to-order-the-dataframe-according-to-an-array",
        "example": {
            "qid": 66088290,
            "link": "https://stackoverflow.com/questions/66088290/use-a-categorical-column-to-order-the-dataframe-according-to-an-array",
            "question": {
                "title": "Use a categorical column to order the dataframe according to an array",
                "ques_desc": "I have an array like this: I also have a dataframe like this: I want to use to order the column dataframe according to the array. The expected output is: "
            },
            "io": [
                "BIN      CA      SUM\n100       B      B 100\n300       A      A 300\n300       B      B 300\n400       B      B 400\n400       A      A 400\n200       B      B 200\n100       A      A 100\n200       A      A 200\n",
                "BIN      CA      SUM\n100       A      A 100\n200       A      A 200\n300       A      A 300\n400       A      A 400\n100       B      B 100\n200       B      B 200\n300       B      B 300\n400       B      B 400\n"
            ],
            "answer": {
                "ans_desc": "You can use to convert the column to categorical column having order, then the values: Alternatively you can create a dictionary that maps the items in to their sorting order then this dictionary on column and use to get the indices that would sort the dataframe: ",
                "code": [
                    "df['SUM'] = pd.Categorical(df['SUM'], categories=arr, ordered=True)\ndf.sort_values('SUM')\n",
                    "dct = {v: i for i, v in enumerate(arr)}\ndf.iloc[np.argsort(df['SUM'].map(dct))]\n"
                ]
            }
        },
        "expected": [
            "iloc",
            "sort_values"
        ],
        "predicted": [
            [
                "DataFrame",
                0.8879797458648682
            ],
            [
                "sort_index",
                0.8862607479095459
            ],
            [
                "apply",
                0.8856474757194519
            ],
            [
                "iloc",
                0.8851556777954102
            ],
            [
                "loc",
                0.8834023475646973
            ],
            [
                "shape",
                0.8832244277000427
            ],
            [
                "values",
                0.8804608583450317
            ],
            [
                "query",
                0.8784454464912415
            ],
            [
                "sort_values",
                0.8782917857170105
            ],
            [
                "from_dict",
                0.8751342296600342
            ]
        ]
    },
    {
        "id": 66075388,
        "query": "create df columns based on second ddf i have 2 dataframes with different columns: i would like to add the missing columns for the 2 dataframes - so each one will have each own columns + the other dfs columns (without column \"number\"). and the new columns will have initial number for our choice (let's say 0). so the final output: what's the best way to achieve this result? i've got messed up with getting the columns and trying to create new ones. thank!",
        "link": "https://stackoverflow.com/questions/66075388/create-df-columns-based-on-second-ddf",
        "example": {
            "qid": 66075388,
            "link": "https://stackoverflow.com/questions/66075388/create-df-columns-based-on-second-ddf",
            "question": {
                "title": "Create DF Columns Based on Second DDF",
                "ques_desc": "I have 2 dataframes with different columns: I would like to add the missing columns for the 2 dataframes - so each one will have each own columns + the other DFs columns (without column \"number\"). And the new columns will have initial number for our choice (let's say 0). So the final output: What's the best way to achieve this result? I've got messed up with getting the columns and trying to create new ones. Thank! "
            },
            "io": [
                "DF A -                   DF B - \nnumber | a  | b  | c  |||| a  | c  | d  | e  | f\n1      | 12 | 13 | 15 |||| 22 | 33 | 44 | 55 | 77\n",
                "    DF A -                    \n    number | a  | b  | c  | d  | e  | f \n    1      | 12 | 13 | 15 | 0  | 0  | 0 \n    DF B -\n    a  | b  | c  | d  | e  | f\n    22 | 0  | 33 | 44 | 55 | 77\n"
            ],
            "answer": {
                "ans_desc": "First, you need to create a superset of all the columns which are present in both the dataframes. This you can do using the below code. Then for each dataframes, you need to check which columns are missing that you can do using the below code. Then add the missing columns in both the dataframes Hope this solves your query! ",
                "code": [
                    "all_columns = list(set(A.columns.to_list() + B.columns.to_list()))\n",
                    "col_missing_from_A = [col for col in all_columns if col not in A.columns]\ncol_missing_from_B = [col for col in all_columns if col not in B.columns]\n"
                ]
            }
        },
        "expected": [
            "columns"
        ],
        "predicted": [
            [
                "join",
                0.8840097784996033
            ],
            [
                "set_index",
                0.880952000617981
            ],
            [
                "rename",
                0.8799252510070801
            ],
            [
                "columns",
                0.879880428314209
            ],
            [
                "DataFrame",
                0.8762134909629822
            ],
            [
                "drop",
                0.8750748634338379
            ],
            [
                "pivot",
                0.8740907311439514
            ],
            [
                "transpose",
                0.8734502792358398
            ],
            [
                "query",
                0.8712027072906494
            ],
            [
                "stack",
                0.8683297634124756
            ]
        ]
    },
    {
        "id": 65893903,
        "query": "how can i use split() in a string when broadcasting a dataframe&#39;s column? take the following dataframe: result: i need to create a 3rd column (broadcasting), using a condition on , and splitting the string on . this is ok to do: result: but i need to specify dynamic indexes to split the string on , instead of (5, 8). when i try to run the following code it does not work, because is treated as a : i'm spending a huge time trying to solve this without needing to iterate the dataframe.",
        "link": "https://stackoverflow.com/questions/65893903/how-can-i-use-split-in-a-string-when-broadcasting-a-dataframes-column",
        "example": {
            "qid": 65893903,
            "link": "https://stackoverflow.com/questions/65893903/how-can-i-use-split-in-a-string-when-broadcasting-a-dataframes-column",
            "question": {
                "title": "How can I use split() in a string when broadcasting a dataframe&#39;s column?",
                "ques_desc": "Take the following dataframe: Result: I need to create a 3rd column (broadcasting), using a condition on , and splitting the string on . This is ok to do: Result: But I need to specify dynamic indexes to split the string on , instead of (5, 8). When I try to run the following code it does not work, because is treated as a : I'm spending a huge time trying to solve this without needing to iterate the dataframe. "
            },
            "io": [
                "   col_1     col_2\n0      0  here 123\n1      1  here 456\n",
                "   col_1     col_2 col_3\n0      0  here 123   NaN\n1      1  here 456   456\n"
            ],
            "answer": {
                "ans_desc": "This one liner does the trick. ",
                "code": [
                    "df['col_3']=[y.split(' ')[1] if x==1 else float('nan') for x,y in df[['col_1','col_2']].values]\n"
                ]
            }
        },
        "expected": [
            "values"
        ],
        "predicted": [
            [
                "apply",
                0.851647675037384
            ],
            [
                "shape",
                0.8469465970993042
            ],
            [
                "values",
                0.8429808616638184
            ],
            [
                "to_string",
                0.8417230844497681
            ],
            [
                "where",
                0.8401130437850952
            ],
            [
                "transpose",
                0.8397025465965271
            ],
            [
                "DataFrame",
                0.8385563492774963
            ],
            [
                "sort_index",
                0.8371601104736328
            ],
            [
                "iterrows",
                0.8360508680343628
            ],
            [
                "rename",
                0.8357108235359192
            ]
        ]
    },
    {
        "id": 65750044,
        "query": "adding value from one pandas dataframe to another dataframe by matching a variable suppose i have a pandas dataframe with 2 columns a second dataframe, contains c1, c2 and a few other columns. my goal is to replace the empty values for c1 in df2, with those in df, corresponding to the values in c2, so the first five values for c1 in df2, should be v5,v2,v1,v2 and v3 respectively. what is the best way to do this?",
        "link": "https://stackoverflow.com/questions/65750044/adding-value-from-one-pandas-dataframe-to-another-dataframe-by-matching-a-variab",
        "example": {
            "qid": 65750044,
            "link": "https://stackoverflow.com/questions/65750044/adding-value-from-one-pandas-dataframe-to-another-dataframe-by-matching-a-variab",
            "question": {
                "title": "Adding value from one pandas dataframe to another dataframe by matching a variable",
                "ques_desc": "Suppose I have a pandas dataframe with 2 columns A second dataframe, contains c1, c2 and a few other columns. My goal is to replace the empty values for c1 in df2, with those in df, corresponding to the values in c2, so the first five values for c1 in df2, should be v5,v2,v1,v2 and v3 respectively. What is the best way to do this? "
            },
            "io": [
                "           c1            c2\n    0      v1            b1\n    1      v2            b2\n    2      v3            b3\n    3      v4            b4\n    4      v5            b5\n",
                "   c1 c2 c3  c4\n0  \"\" b5 500 3\n1  \"\" b2 420 7\n2  \"\" b1 380 5\n3  \"\" b2 470 9\n4  \"\" b3 290 2\n"
            ],
            "answer": {
                "ans_desc": "One easy way you can do is to use the merge of pandas based on similar column. ",
                "code": [
                    "main_df = pd.merge(df2, df, on=\"c2\", how=\"left\")"
                ]
            }
        },
        "expected": [
            "merge"
        ],
        "predicted": [
            [
                "merge",
                0.8669178485870361
            ],
            [
                "fillna",
                0.8650673627853394
            ],
            [
                "to_string",
                0.858980119228363
            ],
            [
                "agg",
                0.8586997389793396
            ],
            [
                "shape",
                0.8572006225585938
            ],
            [
                "update",
                0.8571727275848389
            ],
            [
                "assign",
                0.8570307493209839
            ],
            [
                "interpolate",
                0.8568698167800903
            ],
            [
                "reindex",
                0.8561095595359802
            ],
            [
                "values",
                0.8558946251869202
            ]
        ]
    },
    {
        "id": 65692969,
        "query": "how to spread a key-value pair across multiple columns and flatten the matrix based on another column? using pandas 1.2.0, i want to transform this dataframe where column 'a' contains the groups, while 'b' and 'c' represent the key and value respectively: into: my attempt: what should i do next to flatten the diagonals of these sub-matrices and group by 'a'?",
        "link": "https://stackoverflow.com/questions/65692969/how-to-spread-a-key-value-pair-across-multiple-columns-and-flatten-the-matrix-ba",
        "example": {
            "qid": 65692969,
            "link": "https://stackoverflow.com/questions/65692969/how-to-spread-a-key-value-pair-across-multiple-columns-and-flatten-the-matrix-ba",
            "question": {
                "title": "How to spread a key-value pair across multiple columns and flatten the matrix based on another column?",
                "ques_desc": "Using Pandas 1.2.0, I want to transform this dataframe where column 'a' contains the groups, while 'b' and 'c' represent the key and value respectively: into: My attempt: What should I do next to flatten the diagonals of these sub-matrices and group by 'a'? "
            },
            "io": [
                "     a  b     c\n0  x_1  1  6.00\n1  x_1  2  3.00\n2  x_1  3  0.00\n3  x_1  4  1.00\n4  x_1  5  3.40\n5  j_2  1  4.50\n6  j_2  2  0.10\n7  j_2  3  0.20\n8  j_2  5  0.88\n",
                "     a    1    2    3    4     5\n0  x_1  6.0  3.0  0.0  1.0  3.40\n1  j_2  4.5  0.1  0.2  NaN  0.88\n"
            ],
            "answer": {
                "ans_desc": "EDIT: A typo in the sample data made not work because of duplicates. should work as well or : Tested on your specific version. ",
                "code": [
                    "df = df.pivot(index='a', columns='b', values='c')",
                    "df = df.pivot('a', 'b', 'c')"
                ]
            }
        },
        "expected": [
            "pivot"
        ],
        "predicted": [
            [
                "groupby",
                0.8806589841842651
            ],
            [
                "loc",
                0.8602742552757263
            ],
            [
                "pivot",
                0.8583722114562988
            ],
            [
                "unstack",
                0.855868399143219
            ],
            [
                "pivot_table",
                0.8552398681640625
            ],
            [
                "apply",
                0.8543700575828552
            ],
            [
                "values",
                0.8534138202667236
            ],
            [
                "shape",
                0.8522233366966248
            ],
            [
                "rename_axis",
                0.8514329195022583
            ],
            [
                "sort_index",
                0.8500874638557434
            ]
        ]
    },
    {
        "id": 65550028,
        "query": "count the number of specific values in multiple columns pandas i have a data frame: i want to count the number of times 'buy' appears in each row. intended result: i have tried the following but it simply gives 0 for all the rows: note that buy can only appear in b, c, d, e columns. i tried to find the solution online but shockingly found none. little help will be appreciated. thanks!",
        "link": "https://stackoverflow.com/questions/65550028/count-the-number-of-specific-values-in-multiple-columns-pandas",
        "example": {
            "qid": 65550028,
            "link": "https://stackoverflow.com/questions/65550028/count-the-number-of-specific-values-in-multiple-columns-pandas",
            "question": {
                "title": "Count the number of specific values in multiple columns pandas",
                "ques_desc": "I have a data frame: I want to count the number of times 'BUY' appears in each row. Intended result: I have tried the following but it simply gives 0 for all the rows: Note that BUY can only appear in B, C, D, E columns. I tried to find the solution online but shockingly found none. Little help will be appreciated. THANKS! "
            },
            "io": [
                "A    B    C    D     E\n\n12  4.5  6.1   BUY  NaN\n12  BUY  BUY   5.6  NaN\nBUY  4.5  6.1  BUY  NaN\n12  4.5  6.1   0    NaN \n",
                "A    B    C    D     E   score\n\n12  4.5  6.1   BUY  NaN    1\n12  BUY  BUY   5.6  NaN    2\n15  4.5  6.1  BUY   NaN    1\n12  4.5  6.1   0    NaN    0\n"
            ],
            "answer": {
                "ans_desc": "Or you could use with : Output: ",
                "code": [
                    "df['score'] = df.apply(lambda x: x.tolist().count('BUY'), axis=1)\nprint(df)\n"
                ]
            }
        },
        "expected": [
            "apply",
            "count"
        ],
        "predicted": [
            [
                "value_counts",
                0.8909294605255127
            ],
            [
                "size",
                0.8842671513557434
            ],
            [
                "sum",
                0.8796630501747131
            ],
            [
                "loc",
                0.8762293457984924
            ],
            [
                "shape",
                0.876191258430481
            ],
            [
                "values",
                0.8732710480690002
            ],
            [
                "index",
                0.8720080852508545
            ],
            [
                "apply",
                0.8704898953437805
            ],
            [
                "count",
                0.8695535063743591
            ],
            [
                "iat",
                0.8694579005241394
            ]
        ]
    },
    {
        "id": 65530626,
        "query": "append loop output in column pandas python i am working with the code below to append output to empty dataframe i am getting output as below but i want what i want the output to be how can i make 3 rows to 3 columns every time the loop repeats.",
        "link": "https://stackoverflow.com/questions/65530626/append-loop-output-in-column-pandas-python",
        "example": {
            "qid": 65530626,
            "link": "https://stackoverflow.com/questions/65530626/append-loop-output-in-column-pandas-python",
            "question": {
                "title": "Append loop output in column pandas python",
                "ques_desc": "I am working with the code below to append output to empty dataframe i am getting output as below but i want What i want the output to be How can i make 3 rows to 3 columns every time the loop repeats. "
            },
            "io": [
                "        0\n0   30708\n1      15\n2    1800\n0   19200\n1      50\n2    1180\n",
                "        0    1       2\n0   30708   15    1800\n1   19200   50    1180\n"
            ],
            "answer": {
                "ans_desc": "It perplexes me when you write without doing any manipulation on . Seems like a redundant operation. Another problem with your code is that is slow since it has to copy the backing memory. Repeatedly calling it in a loop is a guarantee of performance bottleneck. Try this instead: ",
                "code": [
                    "# image_data starts as a list\nimage_data = []\n\nfor i in words:\n    y = re.findall('{} ([^ ]*)'.format(re.escape(i)), data)\n    image_data.append(y)\n\n# And it ends as a DataFrame\nimage_data = pd.DataFrame(image_data)\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "append"
        ],
        "predicted": [
            [
                "append",
                0.9025250673294067
            ],
            [
                "DataFrame",
                0.8865764141082764
            ],
            [
                "transpose",
                0.8847835659980774
            ],
            [
                "shape",
                0.8769644498825073
            ],
            [
                "itertuples",
                0.8765702247619629
            ],
            [
                "to_string",
                0.8762638568878174
            ],
            [
                "rename",
                0.8753456473350525
            ],
            [
                "drop",
                0.8751676082611084
            ],
            [
                "reset_index",
                0.8747361898422241
            ],
            [
                "to_csv",
                0.8722512722015381
            ]
        ]
    },
    {
        "id": 65262268,
        "query": "how do i turn categorical column values into different column names? i'm not sure how to approach this problem since i'm a beginner with pandas. i have this dataframe: and i want to turn it into a dataframe or a matrix like this: how should i approach this in python?",
        "link": "https://stackoverflow.com/questions/65262268/how-do-i-turn-categorical-column-values-into-different-column-names",
        "example": {
            "qid": 65262268,
            "link": "https://stackoverflow.com/questions/65262268/how-do-i-turn-categorical-column-values-into-different-column-names",
            "question": {
                "title": "How do I turn categorical column values into different column names?",
                "ques_desc": "I'm not sure how to approach this problem since I'm a beginner with pandas. I have this dataframe: and I want to turn it into a dataframe or a matrix like this: How should I approach this in Python? "
            },
            "io": [
                "  col1 col2\n0    a    1 \n1    a    2 \n2    a    3 \n3    b    4\n4    b    5\n5    b    6\n6    c    7\n7    c    8\n8    c    9\n",
                "   cola colb  colc\n0    1    4    7\n1    2    5    8\n2    3    6    9\n"
            ],
            "answer": {
                "ans_desc": "Let's the dataframe on and create key-value pairs inside comprehension: Alternatively, you can use + to create a sequential counter to distinguish different rows per group in , then use + to reshape: Another approach with with + : Result: ",
                "code": [
                    "pd.DataFrame({k: [*g['col2']] for k, g in df.groupby('col1')})\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "groupby"
        ],
        "predicted": [
            [
                "groupby",
                0.8811808824539185
            ],
            [
                "filter",
                0.8759607672691345
            ],
            [
                "pivot",
                0.8752604722976685
            ],
            [
                "shape",
                0.8730422258377075
            ],
            [
                "select_dtypes",
                0.8726050853729248
            ],
            [
                "query",
                0.8724897503852844
            ],
            [
                "DataFrame",
                0.8714161515235901
            ],
            [
                "apply",
                0.8707365393638611
            ],
            [
                "sort_index",
                0.8702852725982666
            ],
            [
                "values",
                0.8693413138389587
            ]
        ]
    },
    {
        "id": 65223498,
        "query": "unprecise values when using pd.dataframe.values.tolist when converting a pd.dataframe to a nested list, some values are unprecise. pd.dataframe examplary row: pd.dataframe.values.tolist() of this row: how can this be explained and avoided?",
        "link": "https://stackoverflow.com/questions/65223498/unprecise-values-when-using-pd-dataframe-values-tolist",
        "example": {
            "qid": 65223498,
            "link": "https://stackoverflow.com/questions/65223498/unprecise-values-when-using-pd-dataframe-values-tolist",
            "question": {
                "title": "Unprecise values when using pd.Dataframe.values.tolist",
                "ques_desc": "When converting a pd.DataFrame to a nested list, some values are unprecise. pd.DataFrame examplary row: pd.DataFrame.values.tolist() of this row: How can this be explained and avoided? "
            },
            "io": [
                "1.0 -3.0 -3.0 0.01 -3.0 -1.0\n",
                "[1.0, -3.0, -3.0, 0.010000000000000009, -3.0, -1.0]\n"
            ],
            "answer": {
                "ans_desc": "This is because this is the original value. When you display the pd.DataFrame it gets rounded: So it is not tolist()'s problem. It is pd.DataFrame that is rounding the numbers. Use to set display precision for DataFrame. ",
                "code": [
                    "df.values.tolist()\n# [[1.0], [-3.0], [-3.0], [0.010000000000000009], [-3.0], [-1.0]]\n"
                ]
            }
        },
        "expected": [
            "values"
        ],
        "predicted": [
            [
                "DataFrame",
                0.8898484110832214
            ],
            [
                "values",
                0.8846283555030823
            ],
            [
                "join",
                0.8813430070877075
            ],
            [
                "to_string",
                0.8788542151451111
            ],
            [
                "query",
                0.8785393834114075
            ],
            [
                "explode",
                0.8784295320510864
            ],
            [
                "apply",
                0.8766549825668335
            ],
            [
                "shape",
                0.8760700821876526
            ],
            [
                "append",
                0.8756933212280273
            ],
            [
                "from_dict",
                0.875529408454895
            ]
        ]
    },
    {
        "id": 65222196,
        "query": "efficiently relocate elements conditionally in a panda.dataframe i am trying to sort the values of my data.frame in the following way: it is working, however it is very slow for my +40k rows. how can i do this more efficiently and more elegantly? i would prefer a solution that directly manipulates the original df, if possible. example data: desired output:",
        "link": "https://stackoverflow.com/questions/65222196/efficiently-relocate-elements-conditionally-in-a-panda-dataframe",
        "example": {
            "qid": 65222196,
            "link": "https://stackoverflow.com/questions/65222196/efficiently-relocate-elements-conditionally-in-a-panda-dataframe",
            "question": {
                "title": "Efficiently relocate elements conditionally in a panda.Dataframe",
                "ques_desc": "I am trying to sort the values of my data.frame in the following way: It is working, however it is very slow for my +40k rows. How can I do this more efficiently and more elegantly? I would prefer a solution that directly manipulates the original df, if possible. Example data: Desired output: "
            },
            "io": [
                "x1  p1 x2   p2\n1  0.4  2  0.6\n2  0.2  1  0.8\n",
                "x1  p1 x2   p2\n1  0.4  2  0.6\n1  0.8  2  0.2\n"
            ],
            "answer": {
                "ans_desc": "Here's one way that use a selection of the rows and then does a swap of the values using that selection ",
                "code": [
                    "check = df[\"x1\"] > df[\"x2\"]\ndf.loc[check, [\"x2\", \"x1\", \"p2\", \"p1\"]] = df.loc[check, [\"x1\", \"x2\", \"p1\", \"p2\"]].values\n"
                ]
            }
        },
        "expected": [
            "loc",
            "values"
        ],
        "predicted": [
            [
                "iloc",
                0.8686789274215698
            ],
            [
                "all",
                0.8649542927742004
            ],
            [
                "values",
                0.8630716800689697
            ],
            [
                "query",
                0.8626716136932373
            ],
            [
                "shape",
                0.8618748784065247
            ],
            [
                "loc",
                0.8614093065261841
            ],
            [
                "quantile",
                0.8596878051757812
            ],
            [
                "sort_index",
                0.8594585061073303
            ],
            [
                "any",
                0.8578220009803772
            ],
            [
                "apply",
                0.8569768071174622
            ]
        ]
    },
    {
        "id": 64958453,
        "query": "how to insert values of a dataframe to others dataframe this is the first dataframe: and in the other side i have other dataframes (csv files) that have the same content. here is two exemple: i want to add new column to each dataframe and to add each element of the first dataset to the others. expected output: nb: i read csv files as pandas so here the multiple dataframe are multiple csv files and i'm inserting new colum to each file and this column contain an element from the first dataframe.df1. this is my code it add the last element of df1 to all other df like that:",
        "link": "https://stackoverflow.com/questions/64958453/how-to-insert-values-of-a-dataframe-to-others-dataframe",
        "example": {
            "qid": 64958453,
            "link": "https://stackoverflow.com/questions/64958453/how-to-insert-values-of-a-dataframe-to-others-dataframe",
            "question": {
                "title": "How to insert values of a dataframe to others dataframe",
                "ques_desc": "This is the first dataframe: and in the other side I have other dataframes (CSV files) that have the same content. Here is two exemple: I want to add new column to each dataframe and to add each element of the first dataset to the others. Expected output: NB: I read CSV files as pandas so here the multiple dataframe are multiple csv files and I'm inserting new colum to each file and this column contain an element from the first dataframe.df1. This is my code it add the last element of df1 to all other df like that: "
            },
            "io": [
                "\ndf3=      df4=\nA B C     A B C \n1 2 3     1 2 3\n4 5 6     4 5 6\n",
                "df3=      df4=\nA B C D    A B C D\n1 2 3 a    1 2 3 b\n4 5 6 a    4 5 6 b\n"
            ],
            "answer": {
                "ans_desc": "You don't need to iterate through both and . That is where the problem originates. You are overriding each file multiple times, keeping only the last value from (where multiple is equal to ). Drop the outer loop and use to index into the instead. ",
                "code": [
                    "list1=[]\ndf=pd.read_csv('C:/dataframe_1.csv', sep=',')\nfor elem in df['Name']:\n    list1.append(elem)\n\nos.chdir('C:/New')\nextension = 'csv'\nall_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n\nfor i, file in enumerate(all_filenames):\n    df1 = pd.read_csv(file, sep=',')\n    df1['new_column'] = list1[i]\n    \ndf1.to_csv(file, index=False, na_rep='NaN')\n"
                ]
            }
        },
        "expected": [
            "append",
            "to_csv"
        ],
        "predicted": [
            [
                "append",
                0.8853676319122314
            ],
            [
                "join",
                0.8820847272872925
            ],
            [
                "values",
                0.8787571787834167
            ],
            [
                "DataFrame",
                0.8785209655761719
            ],
            [
                "to_string",
                0.8784210681915283
            ],
            [
                "explode",
                0.8776162266731262
            ],
            [
                "transpose",
                0.876999020576477
            ],
            [
                "shape",
                0.8764456510543823
            ],
            [
                "to_csv",
                0.876006543636322
            ],
            [
                "itertuples",
                0.8728838562965393
            ]
        ]
    },
    {
        "id": 64828120,
        "query": "pandas: fill gaps in a series with mean given df i want to replace the nans with the inbetween mean expected output: i have seen this_answer but it's for a grouping which isn't my case and i couldn't find anything else.",
        "link": "https://stackoverflow.com/questions/64828120/pandas-fill-gaps-in-a-series-with-mean",
        "example": {
            "qid": 64828120,
            "link": "https://stackoverflow.com/questions/64828120/pandas-fill-gaps-in-a-series-with-mean",
            "question": {
                "title": "Pandas: Fill gaps in a series with mean",
                "ques_desc": "Given df I want to replace the nans with the inbetween mean Expected output: I have seen this_answer but it's for a grouping which isn't my case and I couldn't find anything else. "
            },
            "io": [
                "   distance\n0       0.0\n1       1.0\n2       2.0\n3       NaN\n4       3.0\n5       4.0\n6       5.0\n7       NaN\n8       NaN\n9       6.0\n",
                "   distance\n0       0.0\n1       1.0\n2       2.0\n3       2.5\n4       3.0\n5       4.0\n6       5.0\n7       5.5\n8       5.5\n9       6.0\n"
            ],
            "answer": {
                "ans_desc": "If you don't want you can compute the mean of the surrounding values manually with and Out: ",
                "code": [
                    "(df.ffill() + df.bfill()) / 2\n"
                ]
            }
        },
        "expected": [
            "bfill",
            "ffill"
        ],
        "predicted": [
            [
                "fillna",
                0.8598061800003052
            ],
            [
                "groupby",
                0.8592498302459717
            ],
            [
                "ffill",
                0.8563960194587708
            ],
            [
                "bfill",
                0.8549754619598389
            ],
            [
                "interpolate",
                0.8549476861953735
            ],
            [
                "loc",
                0.8533890247344971
            ],
            [
                "reset_index",
                0.8529731035232544
            ],
            [
                "sort_values",
                0.8493879437446594
            ],
            [
                "sort_index",
                0.8488330841064453
            ],
            [
                "iterrows",
                0.8466923832893372
            ]
        ]
    },
    {
        "id": 64747375,
        "query": "pandas remove characters from index i have the following dataframe: i want to remove the '-' character with the upper value in the index so i end up with the following dataframe: how do i do this?",
        "link": "https://stackoverflow.com/questions/64747375/pandas-remove-characters-from-index",
        "example": {
            "qid": 64747375,
            "link": "https://stackoverflow.com/questions/64747375/pandas-remove-characters-from-index",
            "question": {
                "title": "Pandas remove characters from index",
                "ques_desc": "I have the following dataframe: I want to remove the '-' character with the upper value in the index so I end up with the following dataframe: How do I do this? "
            },
            "io": [
                "           A\n0-1.5      1\n1.5-3.3    2\n3.3-5.4    3\n5.4-7.9    4\n",
                "     A\n0    1\n1.5  2\n3.3  3\n5.4  4\n"
            ],
            "answer": {
                "ans_desc": "You can use with seelct first lists by indexing: Or use with lambda function: ",
                "code": [
                    "df = df.rename(lambda x: x.split('-')[0])\n"
                ]
            }
        },
        "expected": [
            "rename"
        ],
        "predicted": [
            [
                "transpose",
                0.8790533542633057
            ],
            [
                "index",
                0.8738835453987122
            ],
            [
                "rename",
                0.8718319535255432
            ],
            [
                "add_suffix",
                0.8702734112739563
            ],
            [
                "iterrows",
                0.8667387366294861
            ],
            [
                "shape",
                0.8657640218734741
            ],
            [
                "values",
                0.8641054034233093
            ],
            [
                "apply",
                0.862057626247406
            ],
            [
                "add_prefix",
                0.8611880540847778
            ],
            [
                "lookup",
                0.8602124452590942
            ]
        ]
    },
    {
        "id": 64722958,
        "query": "concatenation of two dataframe after onehotencoding let us consider following code result of this code is following ( i am writing final dataframe) all others works fine, they are so my point is to remove commas in header part of the final dataframe, please help me",
        "link": "https://stackoverflow.com/questions/64722958/concatenation-of-two-dataframe-after-onehotencoding",
        "example": {
            "qid": 64722958,
            "link": "https://stackoverflow.com/questions/64722958/concatenation-of-two-dataframe-after-onehotencoding",
            "question": {
                "title": "Concatenation of two dataframe after onehotencoding",
                "ques_desc": "Let us consider following code result of this code is following ( i am writing final dataframe) all others works fine, they are so my point is to remove commas in header part of the final dataframe, please help me "
            },
            "io": [
                " Alphabet  (A,)  (B,)  (C,)\n0        A   1.0   0.0   0.0\n1        B   0.0   1.0   0.0\n2        C   0.0   0.0   1.0\n3        A   1.0   0.0   0.0\n4        B   0.0   1.0   0.0\n",
                "   A    B    C\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0\n3  1.0  0.0  0.0\n4  0.0  1.0  0.0\n"
            ],
            "answer": {
                "ans_desc": "Try adding: Your columns are a multi-index in . If you write: , you will get and see what I mean, so you need to change the column names with what I have suggested, by joining the multiple levels with an empty string, so that you have one string as a column name instead of a tuple. Also, you could use as an alternative to : Full Code: ",
                "code": [
                    "Encoded_Dataframe.columns = [''.join(col) for col in Encoded_Dataframe.columns]\n"
                ]
            }
        },
        "expected": [
            "columns",
            "join"
        ],
        "predicted": [
            [
                "rename",
                0.860688328742981
            ],
            [
                "join",
                0.8522183299064636
            ],
            [
                "transpose",
                0.8516615629196167
            ],
            [
                "itertuples",
                0.8507118225097656
            ],
            [
                "columns",
                0.8481494188308716
            ],
            [
                "drop",
                0.8473842144012451
            ],
            [
                "append",
                0.8472627401351929
            ],
            [
                "iterrows",
                0.8456484079360962
            ],
            [
                "sort_index",
                0.845473051071167
            ],
            [
                "add_suffix",
                0.8452343940734863
            ]
        ]
    },
    {
        "id": 64715870,
        "query": ".max() for dataframe converts object type to float64 have 3 columns namely whenever i do , it ends up giving a float value. something like output: i wanted tried using but no luck.",
        "link": "https://stackoverflow.com/questions/64715870/max-for-dataframe-converts-object-type-to-float64",
        "example": {
            "qid": 64715870,
            "link": "https://stackoverflow.com/questions/64715870/max-for-dataframe-converts-object-type-to-float64",
            "question": {
                "title": ".max() for dataframe converts object type to float64",
                "ques_desc": "Have 3 columns namely Whenever I do , it ends up giving a float value. Something like Output: I wanted Tried using but no luck. "
            },
            "io": [
                "   \"A\",         \"B\"\n -7.480000e+01,-1.480000e+01\n-7.410000e+01,-1.410000e+01\n-7.370000e+01,-1.370000e+01\n-7.360000e+01,-1.360000e+01\n-7.370000e+01,-1.370000e+01\n-7.390000e+01,-1.390000e+01\n",
                "       \"C\"     \n  -7.480000e+01,\n    -7.410000e+01\n    -7.370000e+01,\n    -7.360000e+01,\n    -7.370000e+01\n    -7.390000e+01\n"
            ],
            "answer": {
                "ans_desc": " If values of 'A' and 'B' are strings: ",
                "code": [
                    "df.apply(lambda x: '{:e}'.format(max(x['A'], x['B'])), axis=1)\n",
                    "df.apply(lambda x: '{:e}'.format(max(float(x['A']), float(x['B']))), axis=1)\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "astype",
                0.8582477569580078
            ],
            [
                "to_string",
                0.8479111194610596
            ],
            [
                "shape",
                0.8475385904312134
            ],
            [
                "values",
                0.8466488718986511
            ],
            [
                "dtypes",
                0.8393785953521729
            ],
            [
                "max",
                0.8331315517425537
            ],
            [
                "apply",
                0.8310837745666504
            ],
            [
                "columns",
                0.829690158367157
            ],
            [
                "DataFrame",
                0.8294035196304321
            ],
            [
                "join",
                0.8293720483779907
            ]
        ]
    },
    {
        "id": 64599227,
        "query": "is there a way to combine 9,12 or 15 columns from a single df into just 3? i'm trying to convert a df that has the data divided every 3 columns into just three. an example is from this: to this:",
        "link": "https://stackoverflow.com/questions/64599227/is-there-a-way-to-combine-9-12-or-15-columns-from-a-single-df-into-just-3",
        "example": {
            "qid": 64599227,
            "link": "https://stackoverflow.com/questions/64599227/is-there-a-way-to-combine-9-12-or-15-columns-from-a-single-df-into-just-3",
            "question": {
                "title": "Is there a way to combine 9,12 or 15 columns from a single df into just 3?",
                "ques_desc": "I'm trying to convert a df that has the data divided every 3 columns into just three. An example is from this: To this: "
            },
            "io": [
                "C1 C2 C3 C4 C5 C6  C7  C8  C9 \n1  6   9  A  D  G  1A  6A  9A\n2  7  10  B  E  H  2A  7A  10A\n3  8  11  C  F  I  3A  8A  11A\n",
                "C1 C2 C3\n1  6   9\n2  7  10\n3  8  11\nC4 C5 C6\nA  D  G\nB  E  H\nC  F  I\nC7 C8 C9\n1A 6A 9A\n2A 7A 10A\n3A 8A 11A\n"
            ],
            "answer": {
                "ans_desc": "You can use : Output: ",
                "code": [
                    "arr = np.hsplit(np.vstack([df.columns.values, df.values]), 3)\npd.DataFrame(np.vstack(arr))\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "columns",
            "values"
        ],
        "predicted": [
            [
                "join",
                0.855728268623352
            ],
            [
                "DataFrame",
                0.8549404740333557
            ],
            [
                "stack",
                0.8511226177215576
            ],
            [
                "columns",
                0.850716233253479
            ],
            [
                "unstack",
                0.8503592014312744
            ],
            [
                "values",
                0.8502826690673828
            ],
            [
                "shape",
                0.8499702215194702
            ],
            [
                "pivot",
                0.8484755158424377
            ],
            [
                "iloc",
                0.8483254909515381
            ],
            [
                "melt",
                0.8482810258865356
            ]
        ]
    },
    {
        "id": 64326826,
        "query": "plotting a data frame of error bars onto a data frame in matplotlib python i've got a pandas data frame () of values as follows: i also have a data frame () with the error of each of those values: i have successfully been able to plot with matplotlib as i desired: however, i am struggling to get the error bars onto this graph. my code for plotting is currently as follows: as you can see, i am trying to pass the data frame into the flag, but it does not do anything, and i am getting the error: i have had a look online but it seems not many people are trying to add so many error bars like i am trying to. what do i need to change to allow this to work?",
        "link": "https://stackoverflow.com/questions/64326826/plotting-a-data-frame-of-error-bars-onto-a-data-frame-in-matplotlib-python",
        "example": {
            "qid": 64326826,
            "link": "https://stackoverflow.com/questions/64326826/plotting-a-data-frame-of-error-bars-onto-a-data-frame-in-matplotlib-python",
            "question": {
                "title": "Plotting a data frame of error bars onto a data frame in matplotlib Python",
                "ques_desc": "I've got a pandas data frame () of values as follows: I also have a data frame () with the error of each of those values: I have successfully been able to plot with matplotlib as I desired: However, I am struggling to get the error bars onto this graph. My code for plotting is currently as follows: As you can see, I am trying to pass the data frame into the flag, but it does not do anything, and I am getting the error: I have had a look online but it seems not many people are trying to add so many error bars like I am trying to. What do I need to change to allow this to work? "
            },
            "io": [
                "            0           1           2\n0  100.000000  100.000000  100.000000\n1    0.412497    0.668880  136.019498\n2    5.144450   77.323610  163.496773\n3   31.078457   78.151325  146.772621\n",
                "          0         1         2\n0  0.083579  0.048520  0.082328\n1  0.005855  0.005904  0.046494\n2  0.009907  0.080799  0.083671\n3  0.045831  0.075932  0.044581\n"
            ],
            "answer": {
                "ans_desc": "try casting deviation to list in (and multiply by 100 to see anything) ",
                "code": [
                    "ax = df.T.plot(kind='bar', yerr=list(deviation.values*100) color=['C0', 'C3', 'C1', 'C2'])\n"
                ]
            }
        },
        "expected": [
            "plot",
            "values"
        ],
        "predicted": [
            [
                "plot",
                0.8512789011001587
            ],
            [
                "append",
                0.8486160039901733
            ],
            [
                "columns",
                0.8441494703292847
            ],
            [
                "query",
                0.8441030979156494
            ],
            [
                "itertuples",
                0.8440690040588379
            ],
            [
                "DataFrame",
                0.8430855870246887
            ],
            [
                "transpose",
                0.84237140417099
            ],
            [
                "shape",
                0.8421151041984558
            ],
            [
                "eval",
                0.8414627909660339
            ],
            [
                "values",
                0.8391971588134766
            ]
        ]
    },
    {
        "id": 64242508,
        "query": "how to create new dataframe by combining some columns together of existing one? i am having a dataframe df like shown: where the explanation of the columns as the following: the first digit is a group number and the second is part of it or subgroup in our example we have groups 1,2,3,4,5 and group 1 consists of 1-1,1-2,1-3. i would like to create a new dataframe that have only the groups 1,2,3,4,5 without subgroups and choose for each row the max number in the subgroup and be flexible for any new modifications or increasing the groups or subgroups. the new dataframe i need is like the shown:",
        "link": "https://stackoverflow.com/questions/64242508/how-to-create-new-dataframe-by-combining-some-columns-together-of-existing-one",
        "example": {
            "qid": 64242508,
            "link": "https://stackoverflow.com/questions/64242508/how-to-create-new-dataframe-by-combining-some-columns-together-of-existing-one",
            "question": {
                "title": "how to create new dataframe by combining some columns together of existing one?",
                "ques_desc": "I am having a dataframe df like shown: where the explanation of the columns as the following: the first digit is a group number and the second is part of it or subgroup in our example we have groups 1,2,3,4,5 and group 1 consists of 1-1,1-2,1-3. I would like to create a new dataframe that have only the groups 1,2,3,4,5 without subgroups and choose for each row the max number in the subgroup and be flexible for any new modifications or increasing the groups or subgroups. The new dataframe I need is like the shown: "
            },
            "io": [
                "1-1    1-2    1-3    2-1    2-2    3-1    3-2    4-1    5-1\n10      3      9      1     3       9      33     10     11\n21      31     3      22    21      13     11     7      13\n33      22     61     31    35      34     8      10     16\n6       9      32     5      4      8      9      6      8\n",
                "1    2    3    4    5\n10   3    33   10   11\n31   22   13   7    13\n61   35   34   10   16\n32   5    9    6    8\n"
            ],
            "answer": {
                "ans_desc": "You can aggregate by columns with and lambda function for split and select first values with and : This working correct if numbers of groups contains 2 or more digits. Alternative is pass splitted columns names: ",
                "code": [
                    "df1 = df.groupby(lambda x: x.split('-')[0], axis=1).max()\n"
                ]
            }
        },
        "expected": [
            "groupby",
            "max"
        ],
        "predicted": [
            [
                "groupby",
                0.8910290598869324
            ],
            [
                "loc",
                0.8763762712478638
            ],
            [
                "size",
                0.8752763271331787
            ],
            [
                "sort_values",
                0.8745269775390625
            ],
            [
                "max",
                0.8733497858047485
            ],
            [
                "agg",
                0.8728662729263306
            ],
            [
                "reset_index",
                0.8724403381347656
            ],
            [
                "sort_index",
                0.8708315491676331
            ],
            [
                "transform",
                0.8689380288124084
            ],
            [
                "iterrows",
                0.8668146133422852
            ]
        ]
    },
    {
        "id": 64230159,
        "query": "shifting and reverting multiple rows in pandas dataframe i have the following dataframe and wish to shift over the 0 values to the right and then revert each row: this is the result i would like to get: i've tried varius shift and apply combinations without any success. is there a simple way of achieving this?",
        "link": "https://stackoverflow.com/questions/64230159/shifting-and-reverting-multiple-rows-in-pandas-dataframe",
        "example": {
            "qid": 64230159,
            "link": "https://stackoverflow.com/questions/64230159/shifting-and-reverting-multiple-rows-in-pandas-dataframe",
            "question": {
                "title": "Shifting and reverting multiple rows in pandas dataframe",
                "ques_desc": "I have the following dataframe and wish to shift over the 0 values to the right and then revert each row: This is the result I would like to get: I've tried varius shift and apply combinations without any success. Is there a simple way of achieving this? "
            },
            "io": [
                "    H00 H01 H02 H03 H04 H05 H06\nNR                          \n1   33  28  98  97  0   0   0\n2   29  24  22  98  97  0   0\n3   78  76  98  97  0   0   0\n4   16  15  98  97  0   0   0\n5   81  72  70  98  97  0   0\n",
                "    H00 H01 H02 H03 H04 H05 H06\nNR                          \n1   97  98  28  33  0   0   0\n2   97  98  22  24  29  0   0\n3   97  98  76  78  0   0   0\n4   97  98  15  16  0   0   0\n5   97  98  70  72  81  0   0\n"
            ],
            "answer": {
                "ans_desc": "You can reverse the values that are greater than zero Out: ",
                "code": [
                    "def reverse_part(series):\n  series[series > 0] = series[series > 0][::-1]\n  return series\n\ndf.apply(reverse_part, axis=1, raw=True)\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "shift",
                0.8553764224052429
            ],
            [
                "rename",
                0.8533713817596436
            ],
            [
                "values",
                0.8510894775390625
            ],
            [
                "shape",
                0.8499382734298706
            ],
            [
                "index",
                0.8490848541259766
            ],
            [
                "columns",
                0.8483115434646606
            ],
            [
                "apply",
                0.8450406789779663
            ],
            [
                "iloc",
                0.8444668650627136
            ],
            [
                "drop",
                0.8434991240501404
            ],
            [
                "iterrows",
                0.8432165384292603
            ]
        ]
    },
    {
        "id": 41428539,
        "query": "data frame to file.txt python i have this dataframe i want to save it as a text file with this format i tried this code but is not working:",
        "link": "https://stackoverflow.com/questions/41428539/data-frame-to-file-txt-python",
        "example": {
            "qid": 41428539,
            "link": "https://stackoverflow.com/questions/41428539/data-frame-to-file-txt-python",
            "question": {
                "title": "data frame to file.txt python",
                "ques_desc": "I have this dataframe I want to save it as a text file with this format I tried this code but is not working: "
            },
            "io": [
                "       X    Y  Z    Value \n0      18   55  1      70   \n1      18   55  2      67 \n2      18   57  2      75     \n3      18   58  1      35  \n4      19   54  2      70   \n",
                "   X    Y  Z    Value \n   18   55  1      70   \n   18   55  2      67 \n   18   57  2      75     \n   18   58  1      35  \n   19   54  2      70   \n"
            ],
            "answer": {
                "ans_desc": "This is an almost exact duplicate of the following: Python, Pandas : write content of DataFrame into text File I report again here the answer from the cited SO question with some very small modifications to fit this case. You can use two methods. np.savetxt(), in which case you should have something like the following: assuming is the dataframe. Of course you can change the delimiter you want (tab, comma, space,etc.). The other option, as mentioned in the answer I attached and in the answer here from @MYGz, is to use the to_csv method, i.e.: ",
                "code": [
                    "np.savetxt('xgboost.txt', a.values, fmt='%d', delimiter=\"\\t\", header=\"X\\tY\\tZ\\tValue\")  \n",
                    "a.to_csv('xgboost.txt', header=True, index=False, sep='\\t', mode='a')\n"
                ]
            }
        },
        "expected": [
            "to_csv",
            "values"
        ],
        "predicted": [
            [
                "DataFrame",
                0.9006792306900024
            ],
            [
                "append",
                0.8970631957054138
            ],
            [
                "to_csv",
                0.8911872506141663
            ],
            [
                "to_string",
                0.8876795768737793
            ],
            [
                "join",
                0.8850302696228027
            ],
            [
                "transpose",
                0.8848097920417786
            ],
            [
                "values",
                0.8844413757324219
            ],
            [
                "shape",
                0.8827065825462341
            ],
            [
                "itertuples",
                0.8822550773620605
            ],
            [
                "stack",
                0.8805218935012817
            ]
        ]
    },
    {
        "id": 63674555,
        "query": "pandas drop diffrent rows with differing column values i have dataframe that looks like input: i would like to remove rows from \"col1\" that share a common value in \"col2\" except values that are the same i.e. letter \"e\". i would like it to be where only one value in \"col1\" can = a unique one in \"col2\" the expected output would look something like... output: what would be the process of doing this?",
        "link": "https://stackoverflow.com/questions/63674555/pandas-drop-diffrent-rows-with-differing-column-values",
        "example": {
            "qid": 63674555,
            "link": "https://stackoverflow.com/questions/63674555/pandas-drop-diffrent-rows-with-differing-column-values",
            "question": {
                "title": "pandas drop diffrent rows with differing column values",
                "ques_desc": "I have DataFrame that looks like Input: I would like to remove rows from \"col1\" that share a common value in \"col2\" except values that are the same i.e. letter \"e\". I would like it to be where only one value in \"col1\" can = a unique one in \"col2\" The expected output would look something like... Output: What would be the process of doing this? "
            },
            "io": [
                "     col1 col2 col3\n    0   a   1   1\n    1   b   3   2\n    2   c   3   3\n    3   d   2   4\n    4   e   6   5\n    5   e   6   6\n",
                "     col1 col2 col3\n    0   a   1   1\n    3   d   2   4\n    4   e   6   5\n    5   e   6   6\n"
            ],
            "answer": {
                "ans_desc": "Based on what you described, I understood as follows: If two rows have same values in , they both are dropped. If two rows have same value in but have same values in , you want to keep them. All other rows which do not fall in above two categories, you want to keep. This can be achieved as: ",
                "code": [
                    "df[np.logical_or(~df.duplicated('col2', keep = False),df.duplicated('col1', keep = False)) ]\n"
                ]
            }
        },
        "expected": [
            "duplicated"
        ],
        "predicted": [
            [
                "eq",
                0.8682679533958435
            ],
            [
                "ne",
                0.8671665191650391
            ],
            [
                "loc",
                0.8637481331825256
            ],
            [
                "applymap",
                0.8605040311813354
            ],
            [
                "drop",
                0.8600134253501892
            ],
            [
                "apply",
                0.8589323163032532
            ],
            [
                "all",
                0.8571035861968994
            ],
            [
                "any",
                0.8568366765975952
            ],
            [
                "duplicated",
                0.8565724492073059
            ],
            [
                "join",
                0.8564497232437134
            ]
        ]
    },
    {
        "id": 63479071,
        "query": "how to convert a pandas dataframe column from string to an array of floats? i have a dataframe where a column is an array of floats. when i am reading the csv file as a pandas dataframe, the particular column is recognized as a string as follows: i want to convert this long character string into an array of floats like this: is there a way to do that?",
        "link": "https://stackoverflow.com/questions/63479071/how-to-convert-a-pandas-dataframe-column-from-string-to-an-array-of-floats",
        "example": {
            "qid": 63479071,
            "link": "https://stackoverflow.com/questions/63479071/how-to-convert-a-pandas-dataframe-column-from-string-to-an-array-of-floats",
            "question": {
                "title": "How to convert a pandas dataframe column from string to an array of floats?",
                "ques_desc": "I have a dataframe where a column is an array of floats. When I am reading the csv file as a pandas dataframe, the particular column is recognized as a string as follows: I want to convert this long character string into an array of floats like this: Is there a way to do that? "
            },
            "io": [
                "'[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0]', \n'[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0]',....\n",
                "[4816.0, 20422.0, 2015.0, 2020.0, 2025.0, 5799.0, 2000.0, 1996.0, 3949.0, 3488.0],\n[13047.0, 7388.0, 16437.0, 2096.0, 13618.0, 2000.0, 1996.0, 23828.0, 6466.0, 1996.0],...\n"
            ],
            "answer": {
                "ans_desc": "If possible, it would be best to read the csv correctly (as a list of floats) rather than casting them from the strings. You can however use or to cast this to a list of floats: ",
                "code": [
                    "from ast import literal_eval    \ndf[\"a\"] = df[\"a\"].apply(lambda x: literal_eval(x))\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "astype",
                0.8657698631286621
            ],
            [
                "shape",
                0.864315390586853
            ],
            [
                "values",
                0.8634060025215149
            ],
            [
                "explode",
                0.8569000363349915
            ],
            [
                "reset_index",
                0.8568702340126038
            ],
            [
                "DataFrame",
                0.8543384671211243
            ],
            [
                "to_string",
                0.8533087372779846
            ],
            [
                "join",
                0.8528972864151001
            ],
            [
                "apply",
                0.8526289463043213
            ],
            [
                "to_xarray",
                0.8510099649429321
            ]
        ]
    },
    {
        "id": 63241112,
        "query": "how to to fuzzy merge items from a list that repeat many times python pandas i have a one column df called ```logos''' consisting of the following list: (note i have searched for similar questions on stackoverflow to no avail i would like to merge with the following df that consists of each item, minus the .png filename i would like to merge in a way that the item from the list matches accordingly every time each team is listed in the df i am wondering how i should go about this considering the and aren't identical, and item in the df i would like to merge with is listed multiple times. is there such thing as a fuzzy join in python like in r? thanks in advance for any help.",
        "link": "https://stackoverflow.com/questions/63241112/how-to-to-fuzzy-merge-items-from-a-list-that-repeat-many-times-python-pandas",
        "example": {
            "qid": 63241112,
            "link": "https://stackoverflow.com/questions/63241112/how-to-to-fuzzy-merge-items-from-a-list-that-repeat-many-times-python-pandas",
            "question": {
                "title": "How to to fuzzy merge items from a list that repeat many times python pandas",
                "ques_desc": "I have a one column df called ```logos''' consisting of the following list: (note I have searched for similar questions on stackoverflow to no avail I would like to merge with the following df that consists of each item, minus the .png filename I would like to merge in a way that the item from the list matches accordingly every time each team is listed in the df I am wondering how I should go about this considering the and aren't identical, and item in the df I would like to merge with is listed multiple times. Is there such thing as a fuzzy join in python like in R? Thanks in advance for any help. "
            },
            "io": [
                "0   ARI\n1   ARI\n2   ARI\n3   DEN\n4   DEN\n5   DEN\n",
                "0   ARI ARI.png\n1   ARI ARI.png\n2   ARI ARI.png\n3   DEN DEN.png\n4   DEN DEN.png\n5   DEN DEN.png\n"
            ],
            "answer": {
                "ans_desc": "AFIK there is no option for 'fuzzy' merge. You can make a new column in logos with and then merge with df Edit Pay atention to the parameter in merge. If ommited it will default to inner. Then if you encounter a row in df that has no corresponding filename in logos it will be excluded from the merged result. ",
                "code": [
                    "df = df.merge(logos, how='left', left_on='column_name', right_on='no_ext')\n"
                ]
            }
        },
        "expected": [
            "merge"
        ],
        "predicted": [
            [
                "merge",
                0.8836650252342224
            ],
            [
                "rename",
                0.8803600668907166
            ],
            [
                "join",
                0.8695375323295593
            ],
            [
                "reset_index",
                0.8664535880088806
            ],
            [
                "append",
                0.8656861782073975
            ],
            [
                "drop_duplicates",
                0.8650681376457214
            ],
            [
                "set_index",
                0.8642231822013855
            ],
            [
                "update",
                0.8639988303184509
            ],
            [
                "agg",
                0.8631470203399658
            ],
            [
                "DataFrame",
                0.8631160855293274
            ]
        ]
    },
    {
        "id": 62283545,
        "query": "way to show multiple spaces in pandas dataframe on jupyter notebook when displaying pandas dataframe object on notebook, multiple spaces are shown as single space. and i cannot copy multiple spaces. i would like to show all spaces as they are and copy a string of them. any way to do so? actual my expectation",
        "link": "https://stackoverflow.com/questions/62283545/way-to-show-multiple-spaces-in-pandas-dataframe-on-jupyter-notebook",
        "example": {
            "qid": 62283545,
            "link": "https://stackoverflow.com/questions/62283545/way-to-show-multiple-spaces-in-pandas-dataframe-on-jupyter-notebook",
            "question": {
                "title": "Way to show multiple spaces in Pandas Dataframe on Jupyter Notebook",
                "ques_desc": "When displaying Pandas Dataframe object on notebook, multiple spaces are shown as single space. And I cannot copy multiple spaces. I would like to show all spaces as they are and copy a string of them. Any way to do so? Actual My expectation "
            },
            "io": [
                "    0     1   \n0   ab c  ab c\n1   ab c  ab c\n",
                "    0       1     \n0   ab c    ab  c\n1   ab   c  ab    c\n"
            ],
            "answer": {
                "ans_desc": "This issue has been answered in a different context: keep the extra whitespaces in display of pandas dataframe in jupyter notebook ",
                "code": [
                    "def print_df(df):\n    return df.style.set_properties(**{'white-space': 'pre'})\n\nprint_df(df)\n"
                ]
            }
        },
        "expected": [
            "style"
        ],
        "predicted": [
            [
                "DataFrame",
                0.8761165738105774
            ],
            [
                "values",
                0.8601159453392029
            ],
            [
                "shape",
                0.85843825340271
            ],
            [
                "to_string",
                0.8512645959854126
            ],
            [
                "from_dict",
                0.8470326066017151
            ],
            [
                "iloc",
                0.8466273546218872
            ],
            [
                "sort_index",
                0.8434664011001587
            ],
            [
                "apply",
                0.8431463241577148
            ],
            [
                "style",
                0.8419790863990784
            ],
            [
                "any",
                0.8402273058891296
            ]
        ]
    },
    {
        "id": 62935487,
        "query": "trying to append the sum of an existing dataframe into a new excel sheet so i have been trying to append the of an existing dataframe into a new dataframe for a new the below example: i want this to be appended to a new excel as: following is the code where i am merging the files in a particular location i need help with the summing part.",
        "link": "https://stackoverflow.com/questions/62935487/trying-to-append-the-sum-of-an-existing-dataframe-into-a-new-excel-sheet",
        "example": {
            "qid": 62935487,
            "link": "https://stackoverflow.com/questions/62935487/trying-to-append-the-sum-of-an-existing-dataframe-into-a-new-excel-sheet",
            "question": {
                "title": "Trying to append the sum of an existing dataframe into a new excel sheet",
                "ques_desc": "So I have been trying to append the of an existing dataframe into a new dataframe for a new the below example: I want this to be appended to a new excel as: Following is the code where I am merging the files in a particular location I need help with the summing part. "
            },
            "io": [
                "A   B     C     \n10  10   10\n10  10   10\n10  10   10\n",
                "A   B   C\n30  30  30\n"
            ],
            "answer": {
                "ans_desc": "Try Result: when you do: then you want to make series to df ",
                "code": [
                    "df.sum()\nA    30\nB    30\nC    30\ndtype: int64\n",
                    "df.sum().to_frame()\n\n0 \nA 30\nB 30\nC 30\n \n"
                ]
            }
        },
        "expected": [
            "sum"
        ],
        "predicted": [
            [
                "sum",
                0.8821409344673157
            ],
            [
                "cumsum",
                0.8731734156608582
            ],
            [
                "agg",
                0.8703814744949341
            ],
            [
                "shape",
                0.8621879816055298
            ],
            [
                "append",
                0.8607606291770935
            ],
            [
                "index",
                0.8590303659439087
            ],
            [
                "explode",
                0.8588365912437439
            ],
            [
                "resample",
                0.8585611581802368
            ],
            [
                "rolling",
                0.8571677803993225
            ],
            [
                "to_dict",
                0.8571234345436096
            ]
        ]
    },
    {
        "id": 62912823,
        "query": "merge two columns of a dataframe into an already existing column of dictionaries as a key value pair if we have 3 columns of a dataframe as : i want the column3 to be something like : i have tried a few things from using lambda functions with apply to iterating over rows but all were unsuccessful.",
        "link": "https://stackoverflow.com/questions/62912823/merge-two-columns-of-a-dataframe-into-an-already-existing-column-of-dictionaries",
        "example": {
            "qid": 62912823,
            "link": "https://stackoverflow.com/questions/62912823/merge-two-columns-of-a-dataframe-into-an-already-existing-column-of-dictionaries",
            "question": {
                "title": "Merge two columns of a dataframe into an already existing column of dictionaries as a key value pair",
                "ques_desc": "If we have 3 columns of a dataframe as : I want the column3 to be something like : I have tried a few things from using lambda functions with apply to iterating over rows but all were unsuccessful. "
            },
            "io": [
                "column1 : ['A','A','B','C']\ncolumn2 : [12,13,14,15]\ncolumn3 : [{\"key1\":\"val1\"},{\"key2\":\"val2\"},{\"key3\":\"val3\"},{\"key4\":\"val4\"}]\n",
                "column3 : [{\"key1\":\"val1\", \"A\":12},{\"key2\":\"val2\", \"A\":13},{\"key3\":\"val3\", \"B\":14},{\"key4\":\"val4\", \"C\":15}]\n"
            ],
            "answer": {
                "ans_desc": "You could use a list comprehension and unpacking as: Input data - ",
                "code": [
                    "df['col3'] = [{**d, k:v} for k,v,d in df.values.tolist()]\n"
                ]
            }
        },
        "expected": [
            "values"
        ],
        "predicted": [
            [
                "apply",
                0.8849961161613464
            ],
            [
                "applymap",
                0.8726586103439331
            ],
            [
                "values",
                0.8713778257369995
            ],
            [
                "explode",
                0.8676422238349915
            ],
            [
                "astype",
                0.8666535019874573
            ],
            [
                "DataFrame",
                0.8657837510108948
            ],
            [
                "from_dict",
                0.8656492829322815
            ],
            [
                "update",
                0.8651894330978394
            ],
            [
                "join",
                0.8650957345962524
            ],
            [
                "shape",
                0.864494800567627
            ]
        ]
    },
    {
        "id": 62875221,
        "query": "keep a single element in dataframe of lists given the following dataframe: how can i remove all but the first element in each column and then unlist so the dataframe becomes like this:",
        "link": "https://stackoverflow.com/questions/62875221/keep-a-single-element-in-dataframe-of-lists",
        "example": {
            "qid": 62875221,
            "link": "https://stackoverflow.com/questions/62875221/keep-a-single-element-in-dataframe-of-lists",
            "question": {
                "title": "Keep a single element in dataframe of lists",
                "ques_desc": "Given the following dataframe: How can I remove all but the first element in each column and then unlist so the dataframe becomes like this: "
            },
            "io": [
                "  Movement Distance     Speed   Delay    Loss\n0   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n1   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n2   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n3   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n4   [1, 1]   [1, 1]  [25, 25]  [0, 0]  [0, 0]\n",
                "  Movement Distance     Speed   Delay    Loss\n0   1      1            25      0        0\n1   1      1            25      0        0\n2   1      1            25      0        0\n3   1      1            25      0        0\n4   1      1            25      0        0\n"
            ],
            "answer": {
                "ans_desc": "You can with or indexing equivalently as: Or if the data is already clean, we have new in pandas 1.0 (thanks cs95): ",
                "code": [
                    "df.apply(lambda x: pd.to_numeric(x.str[0], downcast='integer', errors='ignore'))\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "values",
                0.8620081543922424
            ],
            [
                "join",
                0.8560463190078735
            ],
            [
                "apply",
                0.8556101322174072
            ],
            [
                "applymap",
                0.8509582281112671
            ],
            [
                "dropna",
                0.8488022685050964
            ],
            [
                "DataFrame",
                0.8475825786590576
            ],
            [
                "notnull",
                0.8470562696456909
            ],
            [
                "notna",
                0.8470562696456909
            ],
            [
                "shape",
                0.8469840288162231
            ],
            [
                "columns",
                0.844931960105896
            ]
        ]
    },
    {
        "id": 56044461,
        "query": "position or move pandas column to a specific column index i have a df and it has multiple columns (over 75 columns) with default numeric index: i need to arrange/change position to as follows: i can get the index of using: but i don't seem to be able to figure out how to swap, without manually listing all columns and then manually rearrange in a list.",
        "link": "https://stackoverflow.com/questions/56044461/position-or-move-pandas-column-to-a-specific-column-index",
        "example": {
            "qid": 56044461,
            "link": "https://stackoverflow.com/questions/56044461/position-or-move-pandas-column-to-a-specific-column-index",
            "question": {
                "title": "position or move pandas column to a specific column index",
                "ques_desc": "I have a DF and it has multiple columns (over 75 columns) with default numeric index: I need to arrange/change position to as follows: I can get the index of using: but I don't seem to be able to figure out how to swap, without manually listing all columns and then manually rearrange in a list. "
            },
            "io": [
                "Col1 Col2 Col3 ... Coln\n",
                "Col1 Col3 Col2 ... Coln \n"
            ],
            "answer": {
                "ans_desc": "How to proceed: store the names of columns in a list; swap the names in that list; apply the new order on the dataframe. code: ",
                "code": [
                    "l = list(df)\n\ni1, i2 = l.index('Col2'), l.index('Col3')\nl[i2], l[i1] = l[i1], l[i2]\n\ndf = df[l]\n"
                ]
            }
        },
        "expected": [
            "index"
        ],
        "predicted": [
            [
                "iloc",
                0.8723074793815613
            ],
            [
                "loc",
                0.8622710704803467
            ],
            [
                "values",
                0.8556234836578369
            ],
            [
                "transpose",
                0.8545162677764893
            ],
            [
                "shape",
                0.8542043566703796
            ],
            [
                "to_string",
                0.8536601066589355
            ],
            [
                "index",
                0.8534449338912964
            ],
            [
                "DataFrame",
                0.8528485894203186
            ],
            [
                "rename",
                0.8520591259002686
            ],
            [
                "sort_index",
                0.8492918610572815
            ]
        ]
    },
    {
        "id": 62811834,
        "query": "rename columns in a pandas dataframe with values form dictionary i have a pandas data frame read from an excel file. note: the column names remain the same but the position of the column might vary in the excel file. df i have a list of dictionaries that should be used to change the column names, which is as below field_map i could convert the column keys for each row in the dataframe separately in this way and using the for further operations. this method is taking too long when my file is large. i want to change the column headers of the data frame before processing the entries further, this will reduce a lot of processing time for me. kindly help me with this. i'm expecting the data frame to be something like this expected df thanks in advance",
        "link": "https://stackoverflow.com/questions/62811834/rename-columns-in-a-pandas-dataframe-with-values-form-dictionary",
        "example": {
            "qid": 62811834,
            "link": "https://stackoverflow.com/questions/62811834/rename-columns-in-a-pandas-dataframe-with-values-form-dictionary",
            "question": {
                "title": "Rename Columns in a Pandas Dataframe with values form dictionary",
                "ques_desc": "I have a pandas data frame read from an excel file. Note: the column names remain the same but the position of the column might vary in the excel file. df I have a list of dictionaries that should be used to change the column names, which is as below field_map I could convert the column keys for each row in the DataFrame separately in this way and using the for further operations. This method is taking too long when my file is large. I want to change the column headers of the data Frame before processing the entries further, this will reduce a lot of processing time for me. Kindly help me with this. I'm expecting the data frame to be something like this Expected df Thanks in Advance "
            },
            "io": [
                "    colA    colB    colC   ...\n0   val11   val12   val13  ... \n1   val21   val22   val23  ...\n... ... ...\n",
                "    tab1    tab2    tab3   ...\n0   val11   val12   val13  ... \n1   val21   val22   val23  ...\n... ... ...\n"
            ],
            "answer": {
                "ans_desc": "Just use the function in your existing dataframe : You would need to modify the dictionary a bit: ",
                "code": [
                    "df = df.rename(columns={\"colA\":\"tab1\", \"colB\":\"tab2\", \"colB\":\"tab3\"})\n",
                    "col_rename_dict = {el[\"file_field\"]:el[\"table_field\"] for el in field_map}\ndf = df.rename(columns=col_rename_dict)\n"
                ]
            }
        },
        "expected": [
            "rename"
        ],
        "predicted": [
            [
                "columns",
                0.8849616646766663
            ],
            [
                "rename",
                0.8701934814453125
            ],
            [
                "transpose",
                0.8701645731925964
            ],
            [
                "itertuples",
                0.8602237105369568
            ],
            [
                "drop",
                0.8593643307685852
            ],
            [
                "join",
                0.8565266728401184
            ],
            [
                "index",
                0.8556956052780151
            ],
            [
                "iteritems",
                0.8540627360343933
            ],
            [
                "items",
                0.8540627360343933
            ],
            [
                "set_index",
                0.8513842821121216
            ]
        ]
    },
    {
        "id": 62139372,
        "query": "read csv and iterate through 10 row blocks i am trying to read a csv file and iterate through 10-row blocks. the data is quite unusual, with two columns and 10-row blocks. 57485 rows x 2 columns in the format below: every 10 rows consist of a grid reference and two records x/y ref. the grid reference and x value is in column 1, the y value is in column 2, and then 9 rows with 12 columns, in column one. the code below reads 10 rows, but keeps repeating the first row in all following 10-row blocks?? i don't understand why it keeps repeating the first row?? any suggestions to resolve this would be appreciated.. the first two block:",
        "link": "https://stackoverflow.com/questions/62139372/read-csv-and-iterate-through-10-row-blocks",
        "example": {
            "qid": 62139372,
            "link": "https://stackoverflow.com/questions/62139372/read-csv-and-iterate-through-10-row-blocks",
            "question": {
                "title": "read csv and Iterate through 10 row blocks",
                "ques_desc": "I am trying to read a CSV file and Iterate through 10-row blocks. The data is quite unusual, with two columns and 10-row blocks. 57485 rows x 2 columns in the format below: Every 10 rows consist of a grid reference and two records X/Y ref. The grid reference and X value is in column 1, the Y value is in column 2, and then 9 rows with 12 columns, in column one. The code below reads 10 rows, but keeps repeating the first row in all following 10-row blocks?? I don't understand why it keeps repeating the first row?? Any suggestions to resolve this would be appreciated.. The first two block: "
            },
            "io": [
                "Grid-ref=   1,148\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n 3020 2820 3040 2880 1740 1360  980  990 1410 1770 2580 2630\n\nGrid-ref=   1,311\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n  490  290  280  230  200  250  440  530  460  420  530  450\n\nGrid-ref=   1,312\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n  460  280  260  220  190  240  430  520  450  400  520  410\n",
                "                                      Grid-ref=   1   148\n0   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n1   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n2   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n3   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n4   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n5   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n6   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n7   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n8   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n9   3020 2820 3040 2880 1740 1360  980  990 1410 ...   NaN\n                                        Grid-ref=   1    148\n10                                      Grid-ref=   1  311.0\n11    490  290  280  230  200  250  440  530  460 ...    NaN\n12    490  290  280  230  200  250  440  530  460 ...    NaN\n13    490  290  280  230  200  250  440  530  460 ...    NaN\n14    490  290  280  230  200  250  440  530  460 ...    NaN\n15    490  290  280  230  200  250  440  530  460 ...    NaN\n16    490  290  280  230  200  250  440  530  460 ...    NaN\n17    490  290  280  230  200  250  440  530  460 ...    NaN\n18    490  290  280  230  200  250  440  530  460 ...    NaN\n19    490  290  280  230  200  250  440  530  460 ...    NaN\n"
            ],
            "answer": {
                "ans_desc": "Pandas is good for uniform columnar data. If your input isn't uniform, you can preprocess it and then load the dataframe. This one is easy, all you need to do is scan for grid headers and remove them. Since the data itself is numeric, separated by whitespace, a simple split will parse it. This example creates a list but if the dataset is large, it may be reasonable to write to an intermediate file instead. ",
                "code": [
                    "import csv\nimport re\nimport pandas as  pd\n\ngrid_re = re.compile(r\"Grid-ref=\\s*(\\d+),(\\d+)\")\n\nwith open('test.csv') as fobj:\n    table = []\n    try:\n        while True:\n            # find next block\n            for line in fobj:\n                match = grid_re.search(line)\n                if match:\n                    grid_xy = list(match.groups())\n                    break\n            else:\n                raise StopIteration()\n            for _ in range(10):\n                line = next(fobj)\n                # add row plus grid columns\n                table.append(line.strip().split() + grid_xy)\n    except StopIteration:\n        pass\n\ndf = pd.DataFrame(table)\nprint(df)\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "append"
        ],
        "predicted": [
            [
                "itertuples",
                0.8655900955200195
            ],
            [
                "append",
                0.8631008267402649
            ],
            [
                "iterrows",
                0.8606468439102173
            ],
            [
                "iteritems",
                0.8575146794319153
            ],
            [
                "items",
                0.8575146794319153
            ],
            [
                "transpose",
                0.8563600182533264
            ],
            [
                "rename",
                0.8558756709098816
            ],
            [
                "columns",
                0.8557881116867065
            ],
            [
                "DataFrame",
                0.8546202182769775
            ],
            [
                "shape",
                0.8516464829444885
            ]
        ]
    },
    {
        "id": 62053788,
        "query": "how to sum single row to multiple rows in pandas dataframe using multiindex? my dataframe with quarter and week as multiindex: i am trying to add the last row in q1 (q1-w04) to all the rows in q2 (q2-w15 through q2-w18). this is what i would like the dataframe to look like: when i try to only specify the level 0 index and sumthe specific row, all q2 values go to nan. i have figured out that if i specify both the level 0 and level 1 index, there is no problem. is there a way to sum the specific row to all the rows within the q2 level 0 index without having to call out each row individually by its level 1 index? any insight/guidance would be greatly appreciated. thank you.",
        "link": "https://stackoverflow.com/questions/62053788/how-to-sum-single-row-to-multiple-rows-in-pandas-dataframe-using-multiindex",
        "example": {
            "qid": 62053788,
            "link": "https://stackoverflow.com/questions/62053788/how-to-sum-single-row-to-multiple-rows-in-pandas-dataframe-using-multiindex",
            "question": {
                "title": "How to sum single row to multiple rows in pandas dataframe using multiindex?",
                "ques_desc": "My dataframe with Quarter and Week as MultiIndex: I am trying to add the last row in Q1 (Q1-W04) to all the rows in Q2 (Q2-W15 through Q2-W18). This is what I would like the dataframe to look like: When I try to only specify the level 0 index and sumthe specific row, all Q2 values go to NaN. I have figured out that if I specify both the level 0 and level 1 index, there is no problem. Is there a way to sum the specific row to all the rows within the Q2 Level 0 index without having to call out each row individually by its level 1 index? Any insight/guidance would be greatly appreciated. Thank you. "
            },
            "io": [
                "Quarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    15  15  15\n          Q2-W16    16  16  16\n          Q2-W17    17  17  17\n          Q2-W18    18  18  18\n",
                "Quarter   Week      X   Y   Z\nQ1        Q1-W01    1   1   1\n          Q1-W02    2   2   2\n          Q1-W03    3   3   3\n          Q1-W04    4   4   4\nQ2        Q2-W15    19  19  19\n          Q2-W16    20  20  20\n          Q2-W17    21  21  21\n          Q2-W18    22  22  22\n"
            ],
            "answer": {
                "ans_desc": "try this df.loc returns a DataFrame, to set the value it looks for the list or array. Hence the above. ",
                "code": [
                    "df.loc['Q2'] = (df.loc['Q2'] + df.loc['Q1', 'Q1-W04']).values.tolist()\n"
                ]
            }
        },
        "expected": [
            "loc",
            "values"
        ],
        "predicted": [
            [
                "sum",
                0.8463441133499146
            ],
            [
                "index",
                0.837114691734314
            ],
            [
                "agg",
                0.8367610573768616
            ],
            [
                "columns",
                0.8318910598754883
            ],
            [
                "values",
                0.8314862251281738
            ],
            [
                "transpose",
                0.8295502662658691
            ],
            [
                "shape",
                0.8284372687339783
            ],
            [
                "append",
                0.8268455266952515
            ],
            [
                "loc",
                0.8238108158111572
            ],
            [
                "reset_index",
                0.8214662075042725
            ]
        ]
    },
    {
        "id": 62011520,
        "query": "how to pick some values of a column and make another one with them? this is a table similar to the one i'm working with and what i'm trying to do is take some values of the column a that follow a certain pattern and create another column with such values. for example, the column c would have only the values from a that are bigger than 12, and column d the ones smaller or equal: i've tried making a list for each group of values, but i can't merge them back with the original table, since there are some numbers that repeat and the number of columns grow. i think there's an esasier way to do that, but i can't seem to find it. how can i do that?",
        "link": "https://stackoverflow.com/questions/62011520/how-to-pick-some-values-of-a-column-and-make-another-one-with-them",
        "example": {
            "qid": 62011520,
            "link": "https://stackoverflow.com/questions/62011520/how-to-pick-some-values-of-a-column-and-make-another-one-with-them",
            "question": {
                "title": "How to pick some values of a column and make another one with them?",
                "ques_desc": "This is a table similar to the one I'm working with And what I'm trying to do is take some values of the column A that follow a certain pattern and create another column with such values. For example, the column C would have only the values from A that are bigger than 12, and column D the ones smaller or equal: I've tried making a list for each group of values, but I can't merge them back with the original table, since there are some numbers that repeat and the number of columns grow. I think there's an esasier way to do that, but I can't seem to find it. How can I do that? "
            },
            "io": [
                "      A     B   \n0   12.2   43\n1   10.1   32\n2    3.4   34\n3   12.0   55\n4   40.6   31\n",
                "      A     B      C     D \n0   12.2   43    12.2   NaN \n1   10.1   32     NaN   10.1\n2    3.4   34     NaN   3.4 \n3   12.0   55     NaN   12.0\n4   40.6   31    40.6   NaN\n"
            ],
            "answer": {
                "ans_desc": "And another version: Prints: ",
                "code": [
                    "df['C'] = df.loc[df.A > 12, 'A']\ndf['D'] = df.loc[df.A <= 12, 'A']\n\nprint(df)\n"
                ]
            }
        },
        "expected": [
            "loc"
        ],
        "predicted": [
            [
                "groupby",
                0.8874836564064026
            ],
            [
                "loc",
                0.8753875494003296
            ],
            [
                "reset_index",
                0.8717204928398132
            ],
            [
                "unstack",
                0.8676913380622864
            ],
            [
                "filter",
                0.8669390678405762
            ],
            [
                "agg",
                0.8663473725318909
            ],
            [
                "query",
                0.8657404780387878
            ],
            [
                "sort_index",
                0.8657214641571045
            ],
            [
                "size",
                0.8642741441726685
            ],
            [
                "apply",
                0.8641097545623779
            ]
        ]
    },
    {
        "id": 61678886,
        "query": "filter series/dataframe by another dataframe let's suppose i have a series (or dataframe) , for example list of all universities and colleges in the usa: and another series (od dataframe) , for example list of all cities in the usa: and my desired output (bascially an intersection of and ): the thing is: i'd like to create a series that consists of cities but only these, that have a university/college. my very first thought was to remove \"university\" or \"college\" parts from the , but it turns out that it is not enough, as in case of . then i thought of leaving only the first word, but that excludes . finally, i got a series of all the cities and now i'm trying to use it as a filter (something similiar to or ), so if a string (uni name) contains any of the elements of (city name), then return only the city name. my question is: how to do it in a neat way?",
        "link": "https://stackoverflow.com/questions/61678886/filter-series-dataframe-by-another-dataframe",
        "example": {
            "qid": 61678886,
            "link": "https://stackoverflow.com/questions/61678886/filter-series-dataframe-by-another-dataframe",
            "question": {
                "title": "Filter Series/DataFrame by another DataFrame",
                "ques_desc": "Let's suppose I have a Series (or DataFrame) , for example list of all Universities and Colleges in the USA: And another Series (od DataFrame) , for example list of all cities in the USA: And my desired output (bascially an intersection of and ): The thing is: I'd like to create a Series that consists of cities but only these, that have a university/college. My very first thought was to remove \"University\" or \"College\" parts from the , but it turns out that it is not enough, as in case of . Then I thought of leaving only the first word, but that excludes . Finally, I got a series of all the cities and now I'm trying to use it as a filter (something similiar to or ), so if a string (Uni name) contains any of the elements of (city name), then return only the city name. My question is: how to do it in a neat way? "
            },
            "io": [
                "      City\n0    Searcy\n1    Angwin \n2   New York \n3   Ann Arbor \n",
                "     Uni City\n0     Searcy\n1     Angwin \n2    Fairbanks \n3    Ann Arbor \n"
            ],
            "answer": {
                "ans_desc": "I would try to build a list comprehension of cities that are contained in at least one university name: With your example data it gives: ",
                "code": [
                    "pd.Series([i for i in s2 if s1.str.contains(i).any()], name='Uni City')\n"
                ]
            }
        },
        "expected": [
            "any"
        ],
        "predicted": [
            [
                "isin",
                0.8355221748352051
            ],
            [
                "loc",
                0.8313696384429932
            ],
            [
                "sort_index",
                0.8290981650352478
            ],
            [
                "iterrows",
                0.8275309801101685
            ],
            [
                "groupby",
                0.8268061876296997
            ],
            [
                "sort_values",
                0.823329746723175
            ],
            [
                "any",
                0.8230842351913452
            ],
            [
                "apply",
                0.8217178583145142
            ],
            [
                "merge",
                0.8204078674316406
            ],
            [
                "all",
                0.8198546767234802
            ]
        ]
    },
    {
        "id": 61556827,
        "query": "parsing a list of lists to a data frame in pandas i have list of lists. below is how my list looks like, i want to parse it into a data frame with continuation of values with columns = a,b,c the expected data frame is as below really appreciate the help.",
        "link": "https://stackoverflow.com/questions/61556827/parsing-a-list-of-lists-to-a-data-frame-in-pandas",
        "example": {
            "qid": 61556827,
            "link": "https://stackoverflow.com/questions/61556827/parsing-a-list-of-lists-to-a-data-frame-in-pandas",
            "question": {
                "title": "Parsing a list of lists to a data frame in pandas",
                "ques_desc": "I have list of lists. Below is how my list looks like, I want to parse it into a data frame with continuation of values with columns = A,B,C The expected data frame is as below Really appreciate the help. "
            },
            "io": [
                "[     A  B  C\n  0   1  2  3\n  1   1  2  3\n  2   1  2  3\n  3   1  2  3\n\n      A  B  C\n  0   4  5  6\n  1   4  5  6\n  2   4  5  6\n  3   4  5  6\n]\n\n",
                "     A  B  C\n  0   1  2  3\n  1   1  2  3\n  2   1  2  3\n  3   1  2  3\n  4   4  5  6\n  5   4  5  6\n  6   4  5  6\n  7   4  5  6\n\n"
            ],
            "answer": {
                "ans_desc": "Try this: : List which contains sub-lists : First it takes sub-lists then append it to : Final list which contains all sub-lists as one separate list : Pandas DataFrame ",
                "code": [
                    "import pandas as pd\nlist1=[]\ncount=0\nwhile count<len(myList):\n    list2= myList[count]\n    list1.append(list2)\n    #print(list2)\n    count+=1\ndf = pd.concat(list1)\nprint(df)\n"
                ]
            }
        },
        "expected": [
            "append"
        ],
        "predicted": [
            [
                "values",
                0.8666176795959473
            ],
            [
                "apply",
                0.8658741116523743
            ],
            [
                "append",
                0.8634532690048218
            ],
            [
                "explode",
                0.8619117736816406
            ],
            [
                "DataFrame",
                0.8614122867584229
            ],
            [
                "shape",
                0.8605758547782898
            ],
            [
                "to_string",
                0.8566930890083313
            ],
            [
                "join",
                0.8528950214385986
            ],
            [
                "from_dict",
                0.8507720828056335
            ],
            [
                "itertuples",
                0.8500155210494995
            ]
        ]
    },
    {
        "id": 60862810,
        "query": "how to break/pop a nested dictionary inside a list, inside a pandas dataframe? i have a dataframe which has a dictionary inside a nested list and i want to split the column 'c' : expected output :",
        "link": "https://stackoverflow.com/questions/60862810/how-to-break-pop-a-nested-dictionary-inside-a-list-inside-a-pandas-dataframe",
        "example": {
            "qid": 60862810,
            "link": "https://stackoverflow.com/questions/60862810/how-to-break-pop-a-nested-dictionary-inside-a-list-inside-a-pandas-dataframe",
            "question": {
                "title": "How to break/pop a nested Dictionary inside a list, inside a pandas dataframe?",
                "ques_desc": "I have a dataframe which has a dictionary inside a nested list and i want to split the column 'C' : expected output : "
            },
            "io": [
                "A B     C     \n1 a    [ {\"id\":2,\"Col\":{\"x\":3,\"y\":4}}]\n2 b    [ {\"id\":5,\"Col\":{\"x\":6,\"y\":7}}]\n",
                "A B C_id Col_x Col_y\n1 a  2    3     4 \n2 b  5    6     7\n"
            ],
            "answer": {
                "ans_desc": "From the comments, might help you. After extracting and columns with: You can explode the dictionary in with and use concat to merge with existing dataframe: Also, use drop to remove old columns. Full code: ",
                "code": [
                    "df[[\"Col\", \"id\"]] = df[\"C\"].apply(lambda x: pd.Series(x[0]))\n",
                    "df = pd.concat([df, json_normalize(df.Col)], axis=1)\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "explode",
                0.8758324980735779
            ],
            [
                "DataFrame",
                0.8722272515296936
            ],
            [
                "append",
                0.8708024024963379
            ],
            [
                "join",
                0.8704813718795776
            ],
            [
                "to_string",
                0.8703746199607849
            ],
            [
                "apply",
                0.8678945302963257
            ],
            [
                "to_dict",
                0.8677279949188232
            ],
            [
                "values",
                0.8654197454452515
            ],
            [
                "itertuples",
                0.8632859587669373
            ],
            [
                "transpose",
                0.8626766204833984
            ]
        ]
    },
    {
        "id": 60989914,
        "query": "add id found in list to new column in pandas dataframe say i have the following dataframe (a column of integers and a column with a list of integers)... and also a separate list of ids... given that, and ignoring the column and any index, i want to see if any of the ids in the list are mentioned in the column. the code i have so far is: this works but only if the list is longer than the dataframe and for the real dataset the list is going to be a lot shorter than the dataframe. if i set the list to only two elements... i get a very popular error (i have read many questions with the same error)... i have tried converting the list to a series (no change in the error). i have also tried adding the new column and setting all values to before doing the comprehension line (again no change in the error). two questions: how do i get my code (below) to work for a list that is shorter than a dataframe? how would i get the code to write the actual id found back to the column (more useful than true/false)? expected output for : ideal output for (id(s) are written to a new column or columns): code:",
        "link": "https://stackoverflow.com/questions/60989914/add-id-found-in-list-to-new-column-in-pandas-dataframe",
        "example": {
            "qid": 60989914,
            "link": "https://stackoverflow.com/questions/60989914/add-id-found-in-list-to-new-column-in-pandas-dataframe",
            "question": {
                "title": "Add ID found in list to new column in pandas dataframe",
                "ques_desc": "Say I have the following dataframe (a column of integers and a column with a list of integers)... And also a separate list of IDs... Given that, and ignoring the column and any index, I want to see if any of the IDs in the list are mentioned in the column. The code I have so far is: This works but only if the list is longer than the dataframe and for the real dataset the list is going to be a lot shorter than the dataframe. If I set the list to only two elements... I get a very popular error (I have read many questions with the same error)... I have tried converting the list to a series (no change in the error). I have also tried adding the new column and setting all values to before doing the comprehension line (again no change in the error). Two questions: How do I get my code (below) to work for a list that is shorter than a dataframe? How would I get the code to write the actual ID found back to the column (more useful than True/False)? Expected output for : Ideal output for (ID(s) are written to a new column or columns): Code: "
            },
            "io": [
                "      ID                   Found_IDs\n0  12345        [15443, 15533, 3433]\n1  15533  [2234, 16608, 12002, 7654]\n2   6789      [43322, 876544, 36789]\n",
                "bad_ids = [15533, 876544, 36789, 11111]\n"
            ],
            "answer": {
                "ans_desc": "Using to get the intersect of the two lists: Or with just vanilla python using intersect of : ",
                "code": [
                    "bad_ids_set = set(bad_ids)\ndf['Found_IDs'].apply(lambda x: list(set(x) & bad_ids_set))\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "apply",
                0.8806233406066895
            ],
            [
                "any",
                0.8757718801498413
            ],
            [
                "join",
                0.875687837600708
            ],
            [
                "values",
                0.8744431138038635
            ],
            [
                "explode",
                0.8735570907592773
            ],
            [
                "applymap",
                0.8725614547729492
            ],
            [
                "all",
                0.8724671006202698
            ],
            [
                "shape",
                0.8671539425849915
            ],
            [
                "DataFrame",
                0.867114245891571
            ],
            [
                "rename",
                0.8659840226173401
            ]
        ]
    },
    {
        "id": 46488530,
        "query": "python pandas.dataframe: make whole row nan according to condition i want to make the whole row nan according to a condition, based on a column. for example, if , i want to make the whole row nan. unprocessed data frame looks like this: make whole row nan, if : thank you.",
        "link": "https://stackoverflow.com/questions/46488530/python-pandas-dataframe-make-whole-row-nan-according-to-condition",
        "example": {
            "qid": 46488530,
            "link": "https://stackoverflow.com/questions/46488530/python-pandas-dataframe-make-whole-row-nan-according-to-condition",
            "question": {
                "title": "Python pandas.DataFrame: Make whole row NaN according to condition",
                "ques_desc": "I want to make the whole row NaN according to a condition, based on a column. For example, if , I want to make the whole row NaN. Unprocessed data frame looks like this: Make whole row NaN, if : Thank you. "
            },
            "io": [
                "   A  B\n0  1  4\n1  3  5\n2  4  6\n3  8  7\n",
                "     A    B\n0  1.0  4.0\n1  3.0  5.0\n2  NaN  NaN\n3  NaN  NaN\n"
            ],
            "answer": {
                "ans_desc": "You can also use Example in human language can be translated to: assign to any column () of the dataframe ( ) where the condition is valid. ",
                "code": [
                    "df.loc[df.B > 5, :] = np.nan",
                    "df.loc[df.B > 5, :] = np.nan"
                ]
            }
        },
        "expected": [
            "loc"
        ],
        "predicted": [
            [
                "loc",
                0.8569066524505615
            ],
            [
                "transform",
                0.855828046798706
            ],
            [
                "all",
                0.855159342288971
            ],
            [
                "any",
                0.8548706769943237
            ],
            [
                "where",
                0.8461633920669556
            ],
            [
                "fillna",
                0.8458935618400574
            ],
            [
                "shape",
                0.8452535271644592
            ],
            [
                "max",
                0.8405486345291138
            ],
            [
                "mask",
                0.8405061960220337
            ],
            [
                "eq",
                0.8404671549797058
            ]
        ]
    },
    {
        "id": 24412510,
        "query": "transpose pandas dataframe how do i convert a list of lists to a panda dataframe? it is not in the form of coloumns but instead in the form of rows. for example: i want it to be shown as rows and not coloumns. currently it shows somethign like this i want the rows and coloumns to be switched. moreover, how do i make it for all 5 main lists? this is how i want the output to look like with other coloumns also filled in. however. won't help.",
        "link": "https://stackoverflow.com/questions/24412510/transpose-pandas-dataframe",
        "example": {
            "qid": 24412510,
            "link": "https://stackoverflow.com/questions/24412510/transpose-pandas-dataframe",
            "question": {
                "title": "Transpose pandas dataframe",
                "ques_desc": "How do I convert a list of lists to a panda dataframe? it is not in the form of coloumns but instead in the form of rows. for example: I want it to be shown as rows and not coloumns. currently it shows somethign like this I want the rows and coloumns to be switched. Moreover, How do I make it for all 5 main lists? This is how I want the output to look like with other coloumns also filled in. However. won't help. "
            },
            "io": [
                "     B   P   F   I  FP  BP   2   M   3   1   I   L\n0   64  73  76  64  61  32  36  94  81  49  94  48\n1   57  58  69  46  34  66  15  24  20  49  25  98\n2   99  61  73  69  21  33  78  31  16  11  77  71\n3   41   1  55  34  97  64  98   9  42  77  95  41\n4   36  50  54  27  74   0   8  59  27  54   6  90\n5   74  72  75  30  62  42  90  26  13  49  74   9\n6   41  92  11  38  24  48  34  74  50  10  42   9\n7   77   9  77  63  23   5  50  66  49   5  66  98\n8   90  66  97  16  39  55  38   4  33  52  64   5\n9   18  14  62  87  54  38  29  10  66  18  15  86\n10  60  89  57  28  18  68  11  29  94  34  37  59\n11  78  67  93  18  14  28  64  11  77  79  94  66\n",
                "     B   P   F   I  FP  BP   2   M   3   1   I   L\n0    64 \n1    73  \n1    76  \n2    64  \n3    61  \n4    32  \n5    36  \n6    94  \n7    81  \n8    49  \n9    94  \n10   48\n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "import numpy\n\ndf = pandas.DataFrame(numpy.asarray(data[x]).T.tolist(),\n                      columns=['B','P','F','I','FP','BP','2','M','3','1','I','L'])\n"
                ]
            }
        },
        "expected": [
            "DataFrame"
        ],
        "predicted": [
            [
                "apply",
                0.8502697944641113
            ],
            [
                "join",
                0.8494740128517151
            ],
            [
                "rename",
                0.8472059965133667
            ],
            [
                "sub",
                0.8452343940734863
            ],
            [
                "itertuples",
                0.8449744582176208
            ],
            [
                "values",
                0.8441446423530579
            ],
            [
                "DataFrame",
                0.8420118689537048
            ],
            [
                "pivot",
                0.8415499925613403
            ],
            [
                "applymap",
                0.8407093286514282
            ],
            [
                "from_dict",
                0.840269148349762
            ]
        ]
    },
    {
        "id": 60502832,
        "query": "how to drop pandas consecutive column by column name simultaneously? here's my data the output i expected, what i did but this is not efficient, how to do this effectively?",
        "link": "https://stackoverflow.com/questions/60502832/how-to-drop-pandas-consecutive-column-by-column-name-simultaneously",
        "example": {
            "qid": 60502832,
            "link": "https://stackoverflow.com/questions/60502832/how-to-drop-pandas-consecutive-column-by-column-name-simultaneously",
            "question": {
                "title": "How to drop pandas consecutive column by column name simultaneously?",
                "ques_desc": "Here's my data The Output I expected, What I did But this is not efficient, how to do this effectively? "
            },
            "io": [
                "Id   Column1  Column2  Column3  Column4 ....  Column112  Column113 ... Column143\n1         67       89       86       43              56         72            67\n",
                "Id   Column1  Column113 ... Column143\n1         67         72            67\n"
            ],
            "answer": {
                "ans_desc": "Use: ",
                "code": [
                    "df1 = df.drop(df.loc[:, 'Column2':'Column112'].columns, axis=1)\n"
                ]
            }
        },
        "expected": [
            "columns",
            "drop",
            "loc"
        ],
        "predicted": [
            [
                "rename",
                0.8601289391517639
            ],
            [
                "columns",
                0.8556076288223267
            ],
            [
                "groupby",
                0.8547437787055969
            ],
            [
                "sort_index",
                0.8520302176475525
            ],
            [
                "set_index",
                0.8509461283683777
            ],
            [
                "agg",
                0.8506379723548889
            ],
            [
                "drop",
                0.8505336046218872
            ],
            [
                "rename_axis",
                0.8486318588256836
            ],
            [
                "loc",
                0.8473278284072876
            ],
            [
                "droplevel",
                0.847027063369751
            ]
        ]
    },
    {
        "id": 15112234,
        "query": "how can i convert columns of a pandas dataframe into a list of lists? i have a pandas dataframe with multiple columns. what i want to do is to convert this into a list like following 2u 2s 4r 4n 4m 7h 7v are column headings. it will change in different situations, so don't bother about it.",
        "link": "https://stackoverflow.com/questions/15112234/how-can-i-convert-columns-of-a-pandas-dataframe-into-a-list-of-lists",
        "example": {
            "qid": 15112234,
            "link": "https://stackoverflow.com/questions/15112234/how-can-i-convert-columns-of-a-pandas-dataframe-into-a-list-of-lists",
            "question": {
                "title": "How can I convert columns of a pandas DataFrame into a list of lists?",
                "ques_desc": "I have a pandas DataFrame with multiple columns. What I want to do is to convert this into a list like following 2u 2s 4r 4n 4m 7h 7v are column headings. It will change in different situations, so don't bother about it. "
            },
            "io": [
                "2u    2s    4r     4n     4m   7h   7v\n0     1     1      0      0     0    1\n0     1     0      1      0     0    1\n1     0     0      1      0     1    0\n1     0     0      0      1     1    0\n1     0     1      0      0     1    0\n0     1     1      0      0     0    1\n",
                "X = [\n     [0, 0, 1, 1, 1, 0],\n     [1, 1, 0, 0, 0, 1],\n     [1, 0, 0, 0, 1, 1],\n     [0, 1, 1, 0, 0, 0],\n     [0, 0, 0, 1, 0, 0],\n     [0, 0, 1, 1, 1, 0],\n     [1, 1, 0, 0, 0, 1]\n    ]\n"
            ],
            "answer": {
                "ans_desc": "It looks like a transposed matrix: ",
                "code": [
                    "[list(l) for l in zip(*df.values)]\n"
                ]
            }
        },
        "expected": [
            "values"
        ],
        "predicted": [
            [
                "DataFrame",
                0.8968266248703003
            ],
            [
                "join",
                0.8963663578033447
            ],
            [
                "rename",
                0.89111328125
            ],
            [
                "shape",
                0.8897379636764526
            ],
            [
                "apply",
                0.8893688321113586
            ],
            [
                "values",
                0.8889048099517822
            ],
            [
                "itertuples",
                0.8885319828987122
            ],
            [
                "explode",
                0.8878946900367737
            ],
            [
                "from_dict",
                0.8874989748001099
            ],
            [
                "drop",
                0.8873090147972107
            ]
        ]
    },
    {
        "id": 39903090,
        "query": "efficiently replace values from a column to another column pandas dataframe i have a pandas dataframe like this: i want to replace the values with the values in the second column () only if values are equal to 0, and after (for the zero values remaining), do it again but with the third column (). the desired result is the next one: i did it using the function, but it seems too slow.. i think must be a faster way to accomplish that. is there a faster way to do that?, using some other function instead of the function?",
        "link": "https://stackoverflow.com/questions/39903090/efficiently-replace-values-from-a-column-to-another-column-pandas-dataframe",
        "example": {
            "qid": 39903090,
            "link": "https://stackoverflow.com/questions/39903090/efficiently-replace-values-from-a-column-to-another-column-pandas-dataframe",
            "question": {
                "title": "Efficiently replace values from a column to another column Pandas DataFrame",
                "ques_desc": "I have a Pandas DataFrame like this: I want to replace the values with the values in the second column () only if values are equal to 0, and after (for the zero values remaining), do it again but with the third column (). The Desired Result is the next one: I did it using the function, but it seems too slow.. I think must be a faster way to accomplish that. is there a faster way to do that?, using some other function instead of the function? "
            },
            "io": [
                "   col1 col2 col3\n1   0.2  0.3  0.3\n2   0.2  0.3  0.3\n3     0  0.4  0.4\n4     0    0  0.3\n5     0    0    0\n6   0.1  0.4  0.4\n",
                "   col1 col2 col3\n1   0.2  0.3  0.3\n2   0.2  0.3  0.3\n3   0.4  0.4  0.4\n4   0.3    0  0.3\n5     0    0    0\n6   0.1  0.4  0.4\n"
            ],
            "answer": {
                "ans_desc": "Using is faster. Using a similar pattern as you used with : However, using a nested is slightly faster: Timings Using the following setup to produce a larger sample DataFrame and timing functions: I get the following timings: I tried timing your method, but it's been running for multiple minutes without completing. As a comparison, timing your method on just the 6 row example DataFrame (not the much larger one tested above) took 12.8 ms. ",
                "code": [
                    "df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])\ndf['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])\n",
                    "df['col1'] = np.where(df['col1'] == 0, \n                      np.where(df['col2'] == 0, df['col3'], df['col2']),\n                      df['col1'])\n"
                ]
            }
        },
        "expected": [
            "where"
        ],
        "predicted": [
            [
                "apply",
                0.8744956851005554
            ],
            [
                "shape",
                0.869231104850769
            ],
            [
                "values",
                0.8691657185554504
            ],
            [
                "where",
                0.8661274909973145
            ],
            [
                "mask",
                0.8638678193092346
            ],
            [
                "quantile",
                0.8637863993644714
            ],
            [
                "fillna",
                0.8623605966567993
            ],
            [
                "applymap",
                0.8617802858352661
            ],
            [
                "interpolate",
                0.8595470190048218
            ],
            [
                "to_string",
                0.8564104437828064
            ]
        ]
    },
    {
        "id": 59673066,
        "query": "most efficient way to multiply every column of a large pandas dataframe with every other column of the same dataframe suppose i have a dataset that looks something like: i want to get a dataframe that looks like the following, with the original columns, and all possible interactions between columns: my actual datasets are pretty large (~100 columns). what is the fastest way to achieve this? i could, of course, do a nested loop, or similar, to achieve this but i was hoping there is a more efficient way.",
        "link": "https://stackoverflow.com/questions/59673066/most-efficient-way-to-multiply-every-column-of-a-large-pandas-dataframe-with-eve",
        "example": {
            "qid": 59673066,
            "link": "https://stackoverflow.com/questions/59673066/most-efficient-way-to-multiply-every-column-of-a-large-pandas-dataframe-with-eve",
            "question": {
                "title": "Most efficient way to multiply every column of a large pandas dataframe with every other column of the same dataframe",
                "ques_desc": "Suppose I have a dataset that looks something like: I want to get a dataframe that looks like the following, with the original columns, and all possible interactions between columns: My actual datasets are pretty large (~100 columns). What is the fastest way to achieve this? I could, of course, do a nested loop, or similar, to achieve this but I was hoping there is a more efficient way. "
            },
            "io": [
                "INDEX   A   B   C\n    1   1   1   0.75\n    2   1   1   1\n    3   1   0   0.35\n    4   0   0   1\n    5   1   1   0\n",
                "INDEX   A   B   C       A_B     A_C     B_C\n    1   1   1   0.75    1       0.75    0.75\n    2   1   1   1       1       1       1\n    3   1   0   0.35    0       0.35    0\n    4   0   0   1       0       0       0\n    5   1   1   0       1       0       0\n"
            ],
            "answer": {
                "ans_desc": "You could use itertools.combinations for this: If you need to vectorize an arbitrary function on the pairs of columns you could use: ",
                "code": [
                    "import numpy as np\n\ndef fx(x, y):\n    return np.multiply(x, y)\n\nfor col1, col2 in combinations(df.columns, 2):\n    df[f\"{col1}_{col2}\"] = np.vectorize(fx)(df[col1], df[col2])\n"
                ]
            }
        },
        "expected": [
            "columns"
        ],
        "predicted": [
            [
                "shape",
                0.8801303505897522
            ],
            [
                "agg",
                0.8801295161247253
            ],
            [
                "query",
                0.877790093421936
            ],
            [
                "columns",
                0.8772299885749817
            ],
            [
                "set_index",
                0.8756380677223206
            ],
            [
                "size",
                0.874737024307251
            ],
            [
                "join",
                0.8744474649429321
            ],
            [
                "values",
                0.8741701245307922
            ],
            [
                "DataFrame",
                0.8737319707870483
            ],
            [
                "pivot",
                0.8735165596008301
            ]
        ]
    },
    {
        "id": 59505475,
        "query": "pandas: swap rows between columns some rows were input in the wrong columns so now i need to swap them. my current approach expected output it works but this is only possible because it was 4 columns. is there a better way to do this? note the dtypes can cause issues with sorting is maybe there is something like",
        "link": "https://stackoverflow.com/questions/59505475/pandas-swap-rows-between-columns",
        "example": {
            "qid": 59505475,
            "link": "https://stackoverflow.com/questions/59505475/pandas-swap-rows-between-columns",
            "question": {
                "title": "Pandas: Swap rows between columns",
                "ques_desc": "Some rows were input in the wrong columns so now I need to swap them. My current approach Expected output It works but this is only possible because it was 4 columns. Is there a better way to do this? Note the dtypes can cause issues with sorting is Maybe there is something like "
            },
            "io": [
                "          a         b         c         d\n0         0        10  22:58:00  23:27:00\n1        10        17  23:03:00  23:39:00\n2  22:58:00  23:27:00         0        10\n3  23:03:00  23:39:00        10        17\n",
                "    a   b         c         d\n0   0  10  22:58:00  23:27:00\n1  10  17  23:03:00  23:39:00\n2   0  10  22:58:00  23:27:00\n3  10  17  23:03:00  23:39:00\n"
            ],
            "answer": {
                "ans_desc": "For swapping and separating and in your sample, you may use , and numpy indexing (Note: your numbers in sample are in string format, so I check type ) If you get replace with ",
                "code": [
                    "AttributeError: 'DataFrame' object has no attribute 'to_numpy'"
                ]
            }
        },
        "expected": [
            "DataFrame"
        ],
        "predicted": [
            [
                "rename",
                0.8797072172164917
            ],
            [
                "dtypes",
                0.8786417841911316
            ],
            [
                "drop",
                0.8753107786178589
            ],
            [
                "query",
                0.8746932744979858
            ],
            [
                "join",
                0.8746586441993713
            ],
            [
                "sort_index",
                0.8738067746162415
            ],
            [
                "select_dtypes",
                0.8732823133468628
            ],
            [
                "columns",
                0.873110830783844
            ],
            [
                "itertuples",
                0.8727246522903442
            ],
            [
                "DataFrame",
                0.8720740079879761
            ]
        ]
    },
    {
        "id": 59482469,
        "query": "python: convert two columns of dataframe into one interposed list how can i convert two columns in a dataframe into an interposed list? ex: i want to do something like closest i've found is but that returns a bunch of tuples in the list like this:",
        "link": "https://stackoverflow.com/questions/59482469/python-convert-two-columns-of-dataframe-into-one-interposed-list",
        "example": {
            "qid": 59482469,
            "link": "https://stackoverflow.com/questions/59482469/python-convert-two-columns-of-dataframe-into-one-interposed-list",
            "question": {
                "title": "Python: Convert two columns of dataframe into one interposed list",
                "ques_desc": "How can I convert two columns in a dataframe into an interposed list? ex: I want to do something like Closest I've found is but that returns a bunch of tuples in the list like this: "
            },
            "io": [
                ">>> [1, 2, 3, 4, 5, 6, 0, -1]",
                "[(1, 2), (3, 4), (5, 6), (0, -1)]"
            ],
            "answer": {
                "ans_desc": "Try this: ",
                "code": [
                    "df.values.flatten()                                                                                                                                                                  \n# array([ 1,  2,  3,  4,  5,  6,  0, -1])\n"
                ]
            }
        },
        "expected": [
            "values"
        ],
        "predicted": [
            [
                "DataFrame",
                0.8615269064903259
            ],
            [
                "shape",
                0.8529677391052246
            ],
            [
                "transpose",
                0.8502835035324097
            ],
            [
                "to_string",
                0.8436787128448486
            ],
            [
                "values",
                0.8426885008811951
            ],
            [
                "append",
                0.8413234949111938
            ],
            [
                "explode",
                0.8390305638313293
            ],
            [
                "reset_index",
                0.8377145528793335
            ],
            [
                "join",
                0.837554931640625
            ],
            [
                "rename",
                0.8374988436698914
            ]
        ]
    },
    {
        "id": 59284049,
        "query": "how to concat 2 columns in single column with column value check i want to concat two column from data frame where column1 not equals to any: dataframe : as a result i want dataframe as follows any is variable, could represent null, emptystring, string, number. thanks.",
        "link": "https://stackoverflow.com/questions/59284049/how-to-concat-2-columns-in-single-column-with-column-value-check",
        "example": {
            "qid": 59284049,
            "link": "https://stackoverflow.com/questions/59284049/how-to-concat-2-columns-in-single-column-with-column-value-check",
            "question": {
                "title": "How to Concat 2 columns in single column with column value check",
                "ques_desc": "I want to concat two column from data frame where Column1 not equals to ANY: DataFrame : as a result I want dataframe as follows ANY is variable, could represent Null, EmptyString, String, Number. Thanks. "
            },
            "io": [
                "   COLUMN1 | COLUMN2\n0     A    |   FOO\n1     B    |   BAR  \n2    ANY   |   FOO\n3    ANY   |   BAR\n4     C    |   FOO\n",
                "   COLUMN1 | COLUMN2\n0     A    |  FOO_A\n1     B    |  BAR_B\n2    ANY   |  FOO\n3    ANY   |  BAR  \n4     C    |  FOO_C\n"
            ],
            "answer": {
                "ans_desc": "You can do ",
                "code": [
                    "df['COLUMN2']=df.apply(lambda row:row['COLUMN2']+'_'+row['COLUMN1'] if row['COLUMN1']!='ANY' else row['COLUMN2'],axis=1)\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "join",
                0.8684747219085693
            ],
            [
                "drop",
                0.8615045547485352
            ],
            [
                "rename",
                0.8595421314239502
            ],
            [
                "apply",
                0.8580915927886963
            ],
            [
                "set_index",
                0.857880711555481
            ],
            [
                "columns",
                0.8547661900520325
            ],
            [
                "sort_index",
                0.8501048684120178
            ],
            [
                "eval",
                0.8494604825973511
            ],
            [
                "reset_index",
                0.848329484462738
            ],
            [
                "values",
                0.8481662273406982
            ]
        ]
    },
    {
        "id": 59266300,
        "query": "how to drop the rows if two columns cells are empty? this is my df i want to compare the columns b and c then i have to check both are null after that i want to remove that rows from df. output looks like,this then i need to check again both columns of b and c like whether the values or same or not , if same i need to create one column say validation_results and print y and if not same print n. i am new to python so anybody here tell me how can i do this with minimum lines of code.",
        "link": "https://stackoverflow.com/questions/59266300/how-to-drop-the-rows-if-two-columns-cells-are-empty",
        "example": {
            "qid": 59266300,
            "link": "https://stackoverflow.com/questions/59266300/how-to-drop-the-rows-if-two-columns-cells-are-empty",
            "question": {
                "title": "How to drop the rows if two columns cells are empty?",
                "ques_desc": "This is my DF i want to compare the columns B and C then i have to check both are null after that i want to remove that rows from DF. Output looks like,this Then i need to check again both columns of B and C like whether the values or same or not , if same i need to create one column say validation_results and print Y and if not same print N. I am new to python so anybody here tell me how can i do this with minimum lines of code. "
            },
            "io": [
                "A  B  C\n1  10 10\n2  \n3  12 12\n4      \n5  21 22\n",
                "A  B  C\n1  10 10\n3  12 12     \n5  21 22\n"
            ],
            "answer": {
                "ans_desc": "Solution if no values are missing values: Use with new column created by : Solution if no values are empty strings: Details: First compare both columns by for not equal - empty string: And then test if both values in row are s by : And filter them by : ",
                "code": [
                    "print (df[['B','C']].ne(''))\n       B      C\n0   True   True\n1  False  False\n2   True   True\n3  False  False\n4   True   True\n",
                    "print (df[['B','C']].ne('').all(axis=1))\n0     True\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n"
                ]
            }
        },
        "expected": [
            "all",
            "ne"
        ],
        "predicted": [
            [
                "all",
                0.8716801404953003
            ],
            [
                "any",
                0.8632036447525024
            ],
            [
                "drop",
                0.8623906970024109
            ],
            [
                "filter",
                0.8607481122016907
            ],
            [
                "query",
                0.8597488403320312
            ],
            [
                "where",
                0.8594841361045837
            ],
            [
                "transform",
                0.8576030135154724
            ],
            [
                "ne",
                0.8571233153343201
            ],
            [
                "rename",
                0.8569764494895935
            ],
            [
                "eq",
                0.8568778038024902
            ]
        ]
    },
    {
        "id": 59250863,
        "query": "python: sort subsection of columns suppose we have the following dataframe: which can be computed as follows i was wondering whether it's possible to sort the dataframe based on the dates labels of the last three columns. i would want the end result to look as",
        "link": "https://stackoverflow.com/questions/59250863/python-sort-subsection-of-columns",
        "example": {
            "qid": 59250863,
            "link": "https://stackoverflow.com/questions/59250863/python-sort-subsection-of-columns",
            "question": {
                "title": "Python: Sort subsection of columns",
                "ques_desc": "Suppose we have the following dataframe: Which can be computed as follows I was wondering whether it's possible to sort the dataframe based on the dates labels of the last three columns. I would want the end result to look as "
            },
            "io": [
                "Label1  2016-03-31  2016-05-31  2016-04-30\n0   A   A1              1            6\n1   B   B1              3            4\n2   C   C2              5            7\n3   D   D1              7            2\n4   E   E4              9            4\n5   F   F1              11           6\n",
                "Label1  2016-03-31  2016-04-30 2016-05-31   \n0   A   A1              6            1\n1   B   B1              4            3\n2   C   C2              7            5\n3   D   D1              2            7\n4   E   E4              4            9\n5   F   F1              6            11\n"
            ],
            "answer": {
                "ans_desc": "This should work: Output: ",
                "code": [
                    "new_cols = list(df.columns[:-3]) + list(df.columns[-3:].sort_values())\n\ndf[new_cols]\n"
                ]
            }
        },
        "expected": [
            "columns",
            "sort_values"
        ],
        "predicted": [
            [
                "columns",
                0.8503199219703674
            ],
            [
                "sort_index",
                0.8480737209320068
            ],
            [
                "rename",
                0.8446391224861145
            ],
            [
                "sort_values",
                0.8420131802558899
            ],
            [
                "index",
                0.8408606648445129
            ],
            [
                "shape",
                0.8377732038497925
            ],
            [
                "DataFrame",
                0.8374511003494263
            ],
            [
                "eval",
                0.8338290452957153
            ],
            [
                "sub",
                0.8338084816932678
            ],
            [
                "values",
                0.8335091471672058
            ]
        ]
    },
    {
        "id": 58942218,
        "query": "pandas documentation example for append does not work (pandas.dataframe.append) i copied the example from the pandas documentation for the append method, but it isn't working for me. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.dataframe.append.html outputs: and not: what are possible reasons for this? where is my mistake? thanks for your help.",
        "link": "https://stackoverflow.com/questions/58942218/pandas-documentation-example-for-append-does-not-work-pandas-dataframe-append",
        "example": {
            "qid": 58942218,
            "link": "https://stackoverflow.com/questions/58942218/pandas-documentation-example-for-append-does-not-work-pandas-dataframe-append",
            "question": {
                "title": "pandas documentation example for append does not work (pandas.DataFrame.append)",
                "ques_desc": "I copied the example from the pandas documentation for the append method, but it isn't working for me. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html outputs: and not: What are possible reasons for this? Where is my mistake? Thanks for your help. "
            },
            "io": [
                "   A  B\n0  1  2\n1  3  4\n",
                "   A  B\n0  1  2\n1  3  4\n0  5  6\n1  7  8\n"
            ],
            "answer": {
                "ans_desc": "Pandas method returns a concatenated dataframe, it does not concatenate the two dataframes inplace. This means does not modify the original but instead returns a new dataframe. The pandas example is in IPython and is correct. This should work: This will print out ",
                "code": [
                    "df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n\ndf2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n\ndf = df.append(df2)\n\nprint(df)\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "append"
        ],
        "predicted": [
            [
                "DataFrame",
                0.8765945434570312
            ],
            [
                "append",
                0.8697150945663452
            ],
            [
                "itertuples",
                0.8596344590187073
            ],
            [
                "values",
                0.8563835620880127
            ],
            [
                "shape",
                0.8550513982772827
            ],
            [
                "to_csv",
                0.8548638820648193
            ],
            [
                "to_string",
                0.8537368774414062
            ],
            [
                "loc",
                0.852940559387207
            ],
            [
                "transpose",
                0.8514123558998108
            ],
            [
                "iloc",
                0.850802481174469
            ]
        ]
    },
    {
        "id": 28990256,
        "query": "python pandas time series year extraction i have a df containing timestamps: i would like to extract the year from each timestamp, creating additional column in the df that would look like: obviously i can go over all df entries stripping off the first 4 characters of the date. which is very slow. i wonder if there is a fast python-way to do this. i saw that it's possible to convert the column into the datetime format by df = pd.to_datetime(df,'%y-%m-%d %h:%m:%s') but when i try to then apply datetime.datetime.year(df) it doesn't work. i will also need to parse the timestamps to months and combinations of years-months and so on... help please. thanks.",
        "link": "https://stackoverflow.com/questions/28990256/python-pandas-time-series-year-extraction",
        "example": {
            "qid": 28990256,
            "link": "https://stackoverflow.com/questions/28990256/python-pandas-time-series-year-extraction",
            "question": {
                "title": "python pandas time series year extraction",
                "ques_desc": "I have a DF containing timestamps: I would like to extract the year from each timestamp, creating additional column in the DF that would look like: Obviously I can go over all DF entries stripping off the first 4 characters of the date. Which is very slow. I wonder if there is a fast python-way to do this. I saw that it's possible to convert the column into the datetime format by DF = pd.to_datetime(DF,'%Y-%m-%d %H:%M:%S') but when I try to then apply datetime.datetime.year(DF) it doesn't work. I will also need to parse the timestamps to months and combinations of years-months and so on... Help please. Thanks. "
            },
            "io": [
                "0     2005-08-31 16:39:40\n1     2005-12-28 16:00:34\n2     2005-10-21 17:52:10\n3     2014-01-28 12:23:15\n4     2014-01-28 12:23:15\n5     2011-02-04 18:32:34\n6     2011-02-04 18:32:34\n7     2011-02-04 18:32:34\n",
                "0     2005-08-31 16:39:40 2005\n1     2005-12-28 16:00:34 2005\n2     2005-10-21 17:52:10 2005\n3     2014-01-28 12:23:15 2014\n4     2014-01-28 12:23:15 2014\n5     2011-02-04 18:32:34 2011\n6     2011-02-04 18:32:34 2011\n7     2011-02-04 18:32:34 2011\n"
            ],
            "answer": {
                "ans_desc": "No need to apply a function for each row there is a new datetime accessor you can call to access the year property: If your timestamps are str then you can convert to datetime64 using : You can access the months and other attributes using like the above. For version prior to you can perform the following: ",
                "code": [
                    "df1['year'] = df1['timestamp'].apply(lambda x: x.year)\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "apply",
                0.8896020650863647
            ],
            [
                "sort_index",
                0.8836008906364441
            ],
            [
                "iterrows",
                0.8827416896820068
            ],
            [
                "values",
                0.8808373212814331
            ],
            [
                "explode",
                0.8797588348388672
            ],
            [
                "from_dict",
                0.8793966770172119
            ],
            [
                "DataFrame",
                0.8780634999275208
            ],
            [
                "groupby",
                0.8770190477371216
            ],
            [
                "itertuples",
                0.876794695854187
            ],
            [
                "eval",
                0.8766101598739624
            ]
        ]
    },
    {
        "id": 58820383,
        "query": "drop values in specific condition in pandas dataframe i have a dataframe like below: now if column a > 7, i want to drop column b and c like below: how can i achieve that?",
        "link": "https://stackoverflow.com/questions/58820383/drop-values-in-specific-condition-in-pandas-dataframe",
        "example": {
            "qid": 58820383,
            "link": "https://stackoverflow.com/questions/58820383/drop-values-in-specific-condition-in-pandas-dataframe",
            "question": {
                "title": "Drop values in specific condition in pandas dataframe",
                "ques_desc": "I have a dataframe like below: now If Column A > 7, I want to drop Column B and C like below: How can I achieve that? "
            },
            "io": [
                "A       B       C\n4.43    NaN     1.11\n3.70    0.48    0.79\n2.78   -0.29    1.26\n1.78    2.90    1.13\n40.70  -0.03    0.55\n51.75   0.29    1.45\n3.65    1.74    0.37\n2.93    1.56    1.64\n3.43    NaN     NaN\n2.93    NaN     NaN\n10.37   NaN     NaN\n",
                "A       B       C\n4.43    NaN     1.11\n3.70    0.48    0.79\n2.78   -0.29    1.26\n1.78    2.90    1.13\n40.70   NaN     NaN\n51.75   NaN     NaN\n3.65    1.74    0.37\n2.93    1.56    1.64\n3.43    NaN     NaN\n2.93    NaN     NaN\n10.37   NaN     NaN\n"
            ],
            "answer": {
                "ans_desc": "Use with default value for replace by mask: Or with specify : ",
                "code": [
                    "df[['B','C']] = df[['B','C']].mask(df.A > 7)\n",
                    "df.loc[df.A > 7, ['B','C']] = np.nan\n"
                ]
            }
        },
        "expected": [
            "loc",
            "mask"
        ],
        "predicted": [
            [
                "loc",
                0.8748445510864258
            ],
            [
                "all",
                0.8653779029846191
            ],
            [
                "where",
                0.8648907542228699
            ],
            [
                "eq",
                0.864784300327301
            ],
            [
                "any",
                0.8645047545433044
            ],
            [
                "drop",
                0.8625121712684631
            ],
            [
                "ne",
                0.8620437383651733
            ],
            [
                "gt",
                0.8617998957633972
            ],
            [
                "mask",
                0.8614650964736938
            ],
            [
                "transform",
                0.8593096137046814
            ]
        ]
    },
    {
        "id": 58811263,
        "query": "how to update the index of df with a new index? i am currently having one df which has an incomplete index. like this: i have the complete . would like to how to supplement the complete index into the incomplete df. the can be \"\", the purpose is for me to identify which case i am currently missing. it's the first time i ask question on stackover, apology for the poor formatting.",
        "link": "https://stackoverflow.com/questions/58811263/how-to-update-the-index-of-df-with-a-new-index",
        "example": {
            "qid": 58811263,
            "link": "https://stackoverflow.com/questions/58811263/how-to-update-the-index-of-df-with-a-new-index",
            "question": {
                "title": "How to update the Index of df with a new Index?",
                "ques_desc": "I am currently having one df which has an incomplete Index. like this: I have the complete . Would like to how to supplement the complete Index into the incomplete df. The can be \"\", the purpose is for me to identify which case I am currently missing. It's the first time I ask question on stackover, apology for the poor formatting. "
            },
            "io": [
                "Idx  bar  baz  zoo\n001   A    1    x\n003   B    2    y\n005   C    3    z\n007   A    4    q\n008   B    5    w\n009   C    6    t\n",
                "Idx  bar  baz  zoo\n001   A    1    x\n002  nan  nan  nan\n003   B    2    y\n004  nan  nan  nan\n005   C    3    z\n006  nan  nan  nan\n007   A    4    q\n008   B    5    w\n009   C    6    t \n010  nan  nan  nan\n"
            ],
            "answer": {
                "ans_desc": "you can do this easily by using the pandas df reindex method.. df.reindex all you have to do is supply a list to be used as the new index i.e. then pass this into the reindex method like this: the method will automatically put nan values into the rows with indices that were not in the original index... e.g.: output: ",
                "code": [
                    "df = pd.DataFrame({'bar':['A','B','C','A','B','C'],'baz':[1,2,3,4,5,6],'zoo':['x','y','z','q','w','t']}, index = ['001','003','005','007','008','009']) #your original df\nfull_index = ['001','002','003','004','005','006','007','008','009','010']  \ndf = df.reindex(full_index)\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "reindex"
        ],
        "predicted": [
            [
                "index",
                0.8623767495155334
            ],
            [
                "ffill",
                0.8604816794395447
            ],
            [
                "bfill",
                0.8590877056121826
            ],
            [
                "DataFrame",
                0.8578439950942993
            ],
            [
                "transpose",
                0.8538535833358765
            ],
            [
                "fillna",
                0.8537899255752563
            ],
            [
                "notnull",
                0.8537288904190063
            ],
            [
                "notna",
                0.8537288904190063
            ],
            [
                "set_index",
                0.8534244894981384
            ],
            [
                "reindex",
                0.8521866202354431
            ]
        ]
    },
    {
        "id": 46451284,
        "query": "groupby and perform row-wise calculation using a custom function following on from this question: python - group by and add new row which is calculation of other rows i have a pandas dataframe as follows: and i want to, for each value in col_1, apply a function with the values in col_3 and col_4 (and many more columns) that correspond to x and z from col_2 and create a new row with these values. so the output would be as below: where are the outputs of the function. original question (which only requires a simple addition) was answered with: i'm now looking for a way to use a custom function, such as or , rather than . how can i modify this code to work with my new requirements?",
        "link": "https://stackoverflow.com/questions/46451284/groupby-and-perform-row-wise-calculation-using-a-custom-function",
        "example": {
            "qid": 46451284,
            "link": "https://stackoverflow.com/questions/46451284/groupby-and-perform-row-wise-calculation-using-a-custom-function",
            "question": {
                "title": "Groupby and perform row-wise calculation using a custom function",
                "ques_desc": "Following on from this question: python - Group by and add new row which is calculation of other rows I have a pandas dataframe as follows: And I want to, for each value in col_1, apply a function with the values in col_3 and col_4 (and many more columns) that correspond to X and Z from col_2 and create a new row with these values. So the output would be as below: Where are the outputs of the function. Original question (which only requires a simple addition) was answered with: I'm now looking for a way to use a custom function, such as or , rather than . How can I modify this code to work with my new requirements? "
            },
            "io": [
                "col_1   col_2   col_3  col_4\na       X        5      1\na       Y        3      2\na       Z        6      4\nb       X        7      8\nb       Y        4      3\nb       Z        6      5\n",
                "col_1   col_2   col_3  col_4 \na       X        5      1\na       Y        3      2\na       Z        6      4\na       NEW      *      *\nb       X        7      8\nb       Y        4      3\nb       Z        6      5\nb       NEW      *      *\n"
            ],
            "answer": {
                "ans_desc": "I'm not sure if this is what you're looking for, but here goes: And, the change to is: I'm taking a leap of faith in and assuming those columns are sorted before they hit the function. If this isn't the case, an additional call is needed: Should do the trick. ",
                "code": [
                    "def f(x):\n    y = x.values\n    return y[0] / y[1] # replace with your function\n"
                ]
            }
        },
        "expected": [
            "values"
        ],
        "predicted": [
            [
                "groupby",
                0.8879454135894775
            ],
            [
                "loc",
                0.8682331442832947
            ],
            [
                "agg",
                0.8582695722579956
            ],
            [
                "unstack",
                0.8561087250709534
            ],
            [
                "apply",
                0.8539330959320068
            ],
            [
                "values",
                0.8534818887710571
            ],
            [
                "reset_index",
                0.8531466722488403
            ],
            [
                "pivot",
                0.8520380854606628
            ],
            [
                "mean",
                0.8511768579483032
            ],
            [
                "sort_index",
                0.8505831360816956
            ]
        ]
    },
    {
        "id": 58649009,
        "query": "write pandas dataframe to_csv in columns with trailing zeros i have a pandas dataframe of floats and wish to write out to_csv, setting whitespace as the delimeter, and with trailing zeros to pad so it is still readable (i.e with equally spaced columns). the complicating factor is i also want each column to be rounded to different number of decimals (some need much higher accuracy). to reproduce: current result for out.txt: desired:",
        "link": "https://stackoverflow.com/questions/58649009/write-pandas-dataframe-to-csv-in-columns-with-trailing-zeros",
        "example": {
            "qid": 58649009,
            "link": "https://stackoverflow.com/questions/58649009/write-pandas-dataframe-to-csv-in-columns-with-trailing-zeros",
            "question": {
                "title": "Write pandas dataframe to_csv in columns with trailing zeros",
                "ques_desc": "I have a pandas dataframe of floats and wish to write out to_csv, setting whitespace as the delimeter, and with trailing zeros to pad so it is still readable (i.e with equally spaced columns). The complicating factor is I also want each column to be rounded to different number of decimals (some need much higher accuracy). To reproduce: Current result for out.txt: Desired: "
            },
            "io": [
                "0 1.0 3.0 5.0\n1 1.5 3.455 5.45454\n\n",
                "0 1.0 3.000 5.00000\n1 1.5 3.455 5.45454\n"
            ],
            "answer": {
                "ans_desc": "You can get the string representation of the dataframe using (docs). Then simply write this string to a text file. This method also has parameter to further adjust the spacing between columns. Example: Outputs: There is pandas (more info) but it applies the formatting to values in all columns. Whereas, in your case, you need different formatting for each column. ",
                "code": [
                    "with open('out.txt', 'w') as file:\n    file.writelines(df_rounded.to_string(header=False))\n"
                ]
            }
        },
        "expected": [
            "to_string"
        ],
        "predicted": [
            [
                "to_string",
                0.8818283081054688
            ],
            [
                "to_csv",
                0.878311276435852
            ],
            [
                "replace",
                0.874951958656311
            ],
            [
                "astype",
                0.8715109825134277
            ],
            [
                "values",
                0.8666149377822876
            ],
            [
                "transpose",
                0.8640884757041931
            ],
            [
                "DataFrame",
                0.8637746572494507
            ],
            [
                "add_suffix",
                0.8624346852302551
            ],
            [
                "iterrows",
                0.8621610403060913
            ],
            [
                "to_excel",
                0.8617525100708008
            ]
        ]
    },
    {
        "id": 58490071,
        "query": "pandas: drop duplicates based on row value i have a dataframe and i want to drop duplicates based on different conditions.... i want to drop all the duplicates from column a except rows with \"-\". after this, i want to drop duplicates from column a with \"-\" as a value based on their column b value. given the input dataframe, this should return the following:- i have the following code but it's not very efficient for very large amounts of data, how can i improve this....",
        "link": "https://stackoverflow.com/questions/58490071/pandas-drop-duplicates-based-on-row-value",
        "example": {
            "qid": 58490071,
            "link": "https://stackoverflow.com/questions/58490071/pandas-drop-duplicates-based-on-row-value",
            "question": {
                "title": "Pandas: Drop duplicates based on row value",
                "ques_desc": "I have a dataframe and I want to drop duplicates based on different conditions.... I want to drop all the duplicates from column A except rows with \"-\". After this, I want to drop duplicates from column A with \"-\" as a value based on their column B value. Given the input dataframe, this should return the following:- I have the following code but it's not very efficient for very large amounts of data, how can I improve this.... "
            },
            "io": [
                "        A      B\n  0     1     1.0\n  1     1     1.0\n  2     2     2.0\n  3     2     2.0\n  4     3     3.0\n  5     4     4.0\n  6     5     5.0\n  7     -     5.1\n  8     -     5.1\n  9     -     5.3\n",
                "        A      B\n  0     1     1.0\n  2     2     2.0\n  4     3     3.0\n  5     4     4.0\n  6     5     5.0\n  7     -     5.1\n  9     -     5.3\n"
            ],
            "answer": {
                "ans_desc": " and : Output: ",
                "code": [
                    "df[~df.duplicated('A')            # keep those not duplicates in A\n   | (df['A'].eq('-')             # or those '-' in A\n      & ~df['B'].duplicated())]   # which are not duplicates in B\n"
                ]
            }
        },
        "expected": [
            "duplicated",
            "eq"
        ],
        "predicted": [
            [
                "ne",
                0.8147863149642944
            ],
            [
                "duplicated",
                0.813105583190918
            ],
            [
                "eq",
                0.8116123080253601
            ],
            [
                "drop_duplicates",
                0.8100526928901672
            ],
            [
                "reset_index",
                0.8087338209152222
            ],
            [
                "groupby",
                0.8018733263015747
            ],
            [
                "radd",
                0.8012118935585022
            ],
            [
                "ge",
                0.8007664084434509
            ],
            [
                "add",
                0.798770546913147
            ],
            [
                "le",
                0.7982628345489502
            ]
        ]
    },
    {
        "id": 58448671,
        "query": "python list to pandas dataframe with number &amp; strings if i have a following list (this list need the separator for each comma); and also another list; how can i get this desire output with python? could you please help me about this?",
        "link": "https://stackoverflow.com/questions/58448671/python-list-to-pandas-dataframe-with-number-strings",
        "example": {
            "qid": 58448671,
            "link": "https://stackoverflow.com/questions/58448671/python-list-to-pandas-dataframe-with-number-strings",
            "question": {
                "title": "Python List to Pandas DataFrame with Number &amp; Strings",
                "ques_desc": "If I have a following list (this list need the separator for each comma); And also another list; How can i get this desire output with python? Could you please help me about this? "
            },
            "io": [
                "[(5461, '1.20', 'A', 'BR SK-EL 7 EP', '146', 'E', 52, 0)]\n",
                "A      B     C   D              E    F   G   H\n5461   1.20  A   BR SK-EL 7 EP  146  E   52  0\n"
            ],
            "answer": {
                "ans_desc": "Solution if data are list of tuples: EDIT: Or: ",
                "code": [
                    "df = pd.DataFrame([list(x) for x in data], columns=cols)\n",
                    "df = pd.DataFrame([[x for x in data]], columns=cols)\n"
                ]
            }
        },
        "expected": [
            "DataFrame"
        ],
        "predicted": [
            [
                "to_string",
                0.8704060912132263
            ],
            [
                "DataFrame",
                0.8694539070129395
            ],
            [
                "values",
                0.8656712174415588
            ],
            [
                "join",
                0.8632993698120117
            ],
            [
                "loc",
                0.8618660569190979
            ],
            [
                "rename",
                0.8613917827606201
            ],
            [
                "shape",
                0.8607615232467651
            ],
            [
                "explode",
                0.859641432762146
            ],
            [
                "sort_index",
                0.858833909034729
            ],
            [
                "transpose",
                0.8581601977348328
            ]
        ]
    },
    {
        "id": 58431148,
        "query": "how to make a deepcopy of a dataframe with dataframes within it? (python) i want a copy of a dataframe which contains a dataframe. when i change something in the nested dataframe, it shouldn't change in the original dataframe. i have a dataframe like this: generated with the next code: when i make a deepcopy of the hole dataframe and the nested dataframe and change something in the nested dataframe in the copy, the value also changes in the original. output: but i want: what is the fix for this problem?",
        "link": "https://stackoverflow.com/questions/58431148/how-to-make-a-deepcopy-of-a-dataframe-with-dataframes-within-it-python",
        "example": {
            "qid": 58431148,
            "link": "https://stackoverflow.com/questions/58431148/how-to-make-a-deepcopy-of-a-dataframe-with-dataframes-within-it-python",
            "question": {
                "title": "How to make a deepcopy of a dataframe with dataframes within it? (python)",
                "ques_desc": "I want a copy of a dataframe which contains a dataframe. When I change something in the nested dataframe, it shouldn't change in the original dataframe. I have a dataframe like this: Generated with the next code: When I make a deepcopy of the hole dataframe and the nested dataframe and change something in the nested dataframe in the copy, the value also changes in the original. output: but I want: What is the fix for this problem? "
            },
            "io": [
                "   0  1     2\n0  1  2  doei\n1  4  5     6\n\n   0  1     2\n0  1  2  doei\n1  4  5     6\n",
                "   0  1     2\n0  1  2  doei\n1  4  5     6\n\n   0  1     2\n0  1  2     3\n1  4  5     6\n"
            ],
            "answer": {
                "ans_desc": "This is the fix: output: This solves the problem! ",
                "code": [
                    "import pickle\ndeepCopy = pickle.loads(pickle.dumps(df))\n\ndeepCopy.iloc[0,2].dfCombinations.iloc[0,2] = \"doei\"\n\nprint(deepCopy.iloc[0,2].dfCombinations)\nprint(\" \")\nprint(df.iloc[0,2].dfCombinations)\n"
                ]
            }
        },
        "expected": [
            "iloc"
        ],
        "predicted": [
            [
                "copy",
                0.8649888634681702
            ],
            [
                "index",
                0.8587355017662048
            ],
            [
                "columns",
                0.8527264595031738
            ],
            [
                "DataFrame",
                0.8526036739349365
            ],
            [
                "set_index",
                0.852012574672699
            ],
            [
                "rename",
                0.8481548428535461
            ],
            [
                "iloc",
                0.8447751998901367
            ],
            [
                "shape",
                0.8431645631790161
            ],
            [
                "loc",
                0.8423484563827515
            ],
            [
                "query",
                0.8422147631645203
            ]
        ]
    },
    {
        "id": 57864979,
        "query": "print dataframe without float precision i would like to print a dataframe in my console without displaying any float decimal precision. the output i got after printing is: whereas what i expected is: there seems to be an issue to display the tuple without float precision. any idea why ? cheers",
        "link": "https://stackoverflow.com/questions/57864979/print-dataframe-without-float-precision",
        "example": {
            "qid": 57864979,
            "link": "https://stackoverflow.com/questions/57864979/print-dataframe-without-float-precision",
            "question": {
                "title": "Print dataframe without float precision",
                "ques_desc": "I would like to print a dataframe in my console without displaying any float decimal precision. The output I got after printing is: Whereas what I expected is: There seems to be an issue to display the tuple without float precision. Any idea why ? Cheers "
            },
            "io": [
                "0   (a, 2.01212121212)\n\n1                1.12\n\n2                8.12\n",
                "0            (a, 2.01)\n\n1                1.12\n\n2                8.12\n"
            ],
            "answer": {
                "ans_desc": "You can do like this (very specific to this problem): Output: ",
                "code": [
                    "pd_tmp = pd.DataFrame()\npd_tmp[\"new_column\"] = [(\"a\", 2.01212121212), 1.123123, 8.1212]\n\n\ndef foo(pd_col):\n    pd_new_col = []\n    for i in pd_col:\n        if not isinstance(i, float):\n            lst = []\n            for j in i:\n                if not isinstance(j, float):\n                    k = j\n                    lst.append(k)\n                else:\n                    k = round(j, 2)\n                    lst.append(k)\n            pd_new_col.append(tuple(lst))\n\n        else:\n            pd_new_col.append(round(i, 2))\n\n   return pd_new_col\n\n\npd_tmp[\"new_column\"] = foo(pd_tmp[\"new_column\"])\nprint(pd_tmp)\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "append"
        ],
        "predicted": [
            [
                "to_string",
                0.869490385055542
            ],
            [
                "shape",
                0.8691920638084412
            ],
            [
                "values",
                0.8674799799919128
            ],
            [
                "DataFrame",
                0.858450174331665
            ],
            [
                "append",
                0.8560932278633118
            ],
            [
                "apply",
                0.8543060421943665
            ],
            [
                "transpose",
                0.8542286157608032
            ],
            [
                "join",
                0.8538416028022766
            ],
            [
                "astype",
                0.8534050583839417
            ],
            [
                "columns",
                0.8529049754142761
            ]
        ]
    },
    {
        "id": 57589538,
        "query": "identical dictionaries in dataframe all changing at once i'm working on a project and currently experiencing an issue with populating some dictionaries within a dataframe. the problem is more complicated but essentially the main issue can be simplified as follows: i have a dataframe of dictionaries, all of which initially empty, say when i attempt to add a (key, value) item to one dictionary at position [0][0], all dictionaries that are identical to the one i'm attempting to change will experience the same behaviour, i.e. add an entry of key 'char' and value 'a': i'm assuming this behaviour is caused by using in my dataframe initialization, but i'm not familiar enough with python to understand why. is python creating one dictionary and passing references to it to populate the dataframe? if so, how could i initialise it to create individual dictionaries? i found that i can create a deep copy of each dictionary before processing them, i.e. , but i'm curious if there is a way of doing it without resorting to that.",
        "link": "https://stackoverflow.com/questions/57589538/identical-dictionaries-in-dataframe-all-changing-at-once",
        "example": {
            "qid": 57589538,
            "link": "https://stackoverflow.com/questions/57589538/identical-dictionaries-in-dataframe-all-changing-at-once",
            "question": {
                "title": "Identical dictionaries in DataFrame all changing at once",
                "ques_desc": "I'm working on a project and currently experiencing an issue with populating some dictionaries within a DataFrame. The problem is more complicated but essentially the main issue can be simplified as follows: I have a DataFrame of dictionaries, all of which initially empty, say When I attempt to add a (key, value) item to one dictionary at position [0][0], all dictionaries that are identical to the one I'm attempting to change will experience the same behaviour, i.e. add an entry of key 'char' and value 'a': I'm assuming this behaviour is caused by using in my DataFrame initialization, but I'm not familiar enough with Python to understand why. Is Python creating one dictionary and passing references to it to populate the DataFrame? If so, how could I initialise it to create individual dictionaries? I found that I can create a deep copy of each dictionary before processing them, i.e. , but I'm curious if there is a way of doing it without resorting to that. "
            },
            "io": [
                "    0   1   \n\n0   {}  {}  \n1   {}  {}  \n2   {}  {}  \n",
                "          0               1         \n0   {'char': 'a'}   {'char': 'a'}   \n1   {'char': 'a'}   {'char': 'a'}\n2   {'char': 'a'}   {'char': 'a'}   \n"
            ],
            "answer": {
                "ans_desc": " I have a DataFrame of dictionaries, all of which initially empty If you multiply a list with an integer, you do not make a deep copy, you just copy the references to a new list. This thus means that you made a single dictionary, not two identical ones. So you have one dictionary, and all the six cells in the dataframe refer to the same dictionary. If you modify that dictionary, of course all the cells see that change. You can use list comprehension to make different dictionaries, for example: or for an arbitrary length rows and columns: ",
                "code": [
                    "example = pd.DataFrame([[{} for _ in range(2)] for _ in range(3)], index=range(0, 3))",
                    "example = pd.DataFrame([[{} for _ in range(n)] for _ in range(m)])"
                ]
            }
        },
        "expected": [
            "DataFrame"
        ],
        "predicted": [
            [
                "rename",
                0.8758594989776611
            ],
            [
                "update",
                0.8659605979919434
            ],
            [
                "itertuples",
                0.863239586353302
            ],
            [
                "drop_duplicates",
                0.8628376126289368
            ],
            [
                "iterrows",
                0.8611330389976501
            ],
            [
                "explode",
                0.8611012101173401
            ],
            [
                "join",
                0.8605217933654785
            ],
            [
                "append",
                0.8600612878799438
            ],
            [
                "loc",
                0.8591790795326233
            ],
            [
                "DataFrame",
                0.857205867767334
            ]
        ]
    },
    {
        "id": 35682719,
        "query": "drop rows with a &#39;question mark&#39; value in any column in a pandas dataframe i want to remove all rows (or take all rows without) a question mark symbol in any column. i also want to change the elements to float type. input: output: preferably using pandas dataframe operations.",
        "link": "https://stackoverflow.com/questions/35682719/drop-rows-with-a-question-mark-value-in-any-column-in-a-pandas-dataframe",
        "example": {
            "qid": 35682719,
            "link": "https://stackoverflow.com/questions/35682719/drop-rows-with-a-question-mark-value-in-any-column-in-a-pandas-dataframe",
            "question": {
                "title": "Drop rows with a &#39;question mark&#39; value in any column in a pandas dataframe",
                "ques_desc": "I want to remove all rows (or take all rows without) a question mark symbol in any column. I also want to change the elements to float type. Input: Output: Preferably using pandas dataframe operations. "
            },
            "io": [
                "X Y Z\n0 1 ?\n1 2 3\n? ? 4\n4 4 4\n? 2 5\n",
                "X Y Z\n1 2 3\n4 4 4\n"
            ],
            "answer": {
                "ans_desc": "You can try first find string in columns, create boolean mask and last filter rows - use boolean indexing. If you need convert columns to , use : Or you can try: Better is use: Or: ",
                "code": [
                    "df = df[(df != '?').all(axis=1)]\n",
                    "df = df[~(df == '?').any(axis=1)]\n"
                ]
            }
        },
        "expected": [
            "all",
            "any"
        ],
        "predicted": [
            [
                "astype",
                0.8456911444664001
            ],
            [
                "any",
                0.8404858708381653
            ],
            [
                "all",
                0.838165819644928
            ],
            [
                "apply",
                0.8362436294555664
            ],
            [
                "values",
                0.8333964347839355
            ],
            [
                "applymap",
                0.833217442035675
            ],
            [
                "transform",
                0.8287949562072754
            ],
            [
                "shape",
                0.8269954919815063
            ],
            [
                "dtypes",
                0.8246133923530579
            ],
            [
                "join",
                0.8234900832176208
            ]
        ]
    },
    {
        "id": 57234137,
        "query": "group rows (dates) and summarize serveral columns (several measured values for eacht date) in python pandas i use python pandas and load a table like this from postgres: i want to group the date rows using pandas and summarize the values. the result should look like this i can group the dates and summarize the values separately, but not in one view. my results are and thats the code:",
        "link": "https://stackoverflow.com/questions/57234137/group-rows-dates-and-summarize-serveral-columns-several-measured-values-for-e",
        "example": {
            "qid": 57234137,
            "link": "https://stackoverflow.com/questions/57234137/group-rows-dates-and-summarize-serveral-columns-several-measured-values-for-e",
            "question": {
                "title": "group rows (dates) and summarize serveral columns (several measured values for eacht date) in Python Pandas",
                "ques_desc": "I use Python Pandas and load a table like this from Postgres: I want to group the Date rows using Pandas and summarize the values. The result should look like this I can group the dates and summarize the values separately, but not in one view. My results are And Thats the Code: "
            },
            "io": [
                "2001-01-01 00:00:00   500\n2001-02-01 00:00:00   160\n",
                "1 220\n2 280\n3 160\n"
            ],
            "answer": {
                "ans_desc": "If is column first aggregate sum and then per : Or create by column, then sum all rows and last sum per index (aggregate sum per index): If is index solution above is simplify: ",
                "code": [
                    "df1 = df.sum(axis=1).sum(level=0).reset_index(name='sum')\n\ndf1 = df.sum(level=0).sum(axis=1).reset_index(name='sum')\n"
                ]
            }
        },
        "expected": [
            "reset_index",
            "sum"
        ],
        "predicted": [
            [
                "groupby",
                0.8844965696334839
            ],
            [
                "agg",
                0.8694338202476501
            ],
            [
                "pivot",
                0.8684406876564026
            ],
            [
                "reset_index",
                0.8582663536071777
            ],
            [
                "pivot_table",
                0.8562715649604797
            ],
            [
                "sum",
                0.8561198115348816
            ],
            [
                "shape",
                0.8552078604698181
            ],
            [
                "values",
                0.8538997173309326
            ],
            [
                "from_dict",
                0.8535870313644409
            ],
            [
                "value_counts",
                0.8527911305427551
            ]
        ]
    },
    {
        "id": 56929277,
        "query": "can not make desired pandas dataframe i am trying to make a pandas dataframe using 2 paramters as columns. but it makes a dataframe transpose of what i need. i have and as column parameters as follows: this gives the following dataframe: however, i want the dataframe as:",
        "link": "https://stackoverflow.com/questions/56929277/can-not-make-desired-pandas-dataframe",
        "example": {
            "qid": 56929277,
            "link": "https://stackoverflow.com/questions/56929277/can-not-make-desired-pandas-dataframe",
            "question": {
                "title": "Can not make desired pandas dataframe",
                "ques_desc": "I am trying to make a pandas dataframe using 2 paramters as columns. But it makes a dataframe transpose of what I need. I have and as column parameters as follows: This gives the following dataframe: However, I want the dataframe as: "
            },
            "io": [
                "\n    0   1   2   3   4 \n0   1   2   3   4   5\n1   11  22  33  44  55\n",
                "    0   1 \n0   1   11\n1   2   22\n2   3   33\n3   4   44\n4   5   55\n"
            ],
            "answer": {
                "ans_desc": "One solution is transpose by : Or use : Also is possible specify column names by dictionary: ",
                "code": [
                    "df = pd.DataFrame(list(zip(a, b)))\n#pandas 0.24+\n#df = pd.DataFrame(zip(a, b))\n"
                ]
            }
        },
        "expected": [
            "DataFrame"
        ],
        "predicted": [
            [
                "transpose",
                0.8929093480110168
            ],
            [
                "set_index",
                0.8917399644851685
            ],
            [
                "rename_axis",
                0.8879262208938599
            ],
            [
                "reset_index",
                0.8878610730171204
            ],
            [
                "DataFrame",
                0.8871580362319946
            ],
            [
                "rename",
                0.8850791454315186
            ],
            [
                "columns",
                0.8830452561378479
            ],
            [
                "shape",
                0.8823290467262268
            ],
            [
                "index",
                0.8798809051513672
            ],
            [
                "pivot",
                0.8786067962646484
            ]
        ]
    },
    {
        "id": 48864923,
        "query": "python[pandas]: select certain rows by index of another dataframe i have a dataframe and i would select only rows that contain index value into df1.index. for example: and these indexes i would like this output: thanks",
        "link": "https://stackoverflow.com/questions/48864923/pythonpandas-select-certain-rows-by-index-of-another-dataframe",
        "example": {
            "qid": 48864923,
            "link": "https://stackoverflow.com/questions/48864923/pythonpandas-select-certain-rows-by-index-of-another-dataframe",
            "question": {
                "title": "Python[pandas]: Select certain rows by index of another dataframe",
                "ques_desc": "I have a dataframe and I would select only rows that contain index value into df1.index. for Example: and these indexes I would like this output: thanks "
            },
            "io": [
                "In [96]: df\nOut[96]:\n   A  B  C  D\n1  1  4  9  1\n2  4  5  0  2\n3  5  5  1  0\n22 1  3  9  6\n",
                "In [96]: df\nOut[96]:\n   A  B  C  D\n1  1  4  9  1\n3  5  5  1  0\n22 1  3  9  6\n"
            ],
            "answer": {
                "ans_desc": "Use : Or get all intersectioned indices and select by : EDIT: I tried solution: df = df.loc[df1.index]. Do you think that this solution is correct? Solution is incorrect: ",
                "code": [
                    "df = df.loc[df.index & df1.index]\ndf = df.loc[np.intersect1d(df.index, df1.index)]\ndf = df.loc[df.index.intersection(df1.index)]\n"
                ]
            }
        },
        "expected": [
            "index",
            "loc"
        ],
        "predicted": [
            [
                "index",
                0.8684046268463135
            ],
            [
                "iloc",
                0.8511545062065125
            ],
            [
                "transpose",
                0.8495292663574219
            ],
            [
                "loc",
                0.8481270670890808
            ],
            [
                "set_index",
                0.8475740551948547
            ],
            [
                "add_suffix",
                0.8474246859550476
            ],
            [
                "rename",
                0.8453801870346069
            ],
            [
                "iterrows",
                0.8440450429916382
            ],
            [
                "duplicated",
                0.8404354453086853
            ],
            [
                "filter",
                0.8397558927536011
            ]
        ]
    },
    {
        "id": 56572543,
        "query": "combine pandas dataframes eliminating common columns with python i have 3 dataframes: i want to combine them together to get the following results: when i try to combine them, i keep getting: the common column (a) is duplicated once for each dataframe used in the concat call. i have tried various combinations on: some variations have been disastrous while some keep giving the undesired result. any suggestions would be much appreciated. thanks.",
        "link": "https://stackoverflow.com/questions/56572543/combine-pandas-dataframes-eliminating-common-columns-with-python",
        "example": {
            "qid": 56572543,
            "link": "https://stackoverflow.com/questions/56572543/combine-pandas-dataframes-eliminating-common-columns-with-python",
            "question": {
                "title": "Combine pandas dataframes eliminating common columns with python",
                "ques_desc": "I have 3 dataframes: I want to combine them together to get the following results: When I try to combine them, I keep getting: The common column (A) is duplicated once for each dataframe used in the concat call. I have tried various combinations on: Some variations have been disastrous while some keep giving the undesired result. Any suggestions would be much appreciated. Thanks. "
            },
            "io": [
                "    A   B   C   D   E   F\n0  A0  B0  C0  D0  E0  F0\n1  A1  B1  C1  D1  E1  F1\n2  A2  B2  C2  D2  E2  F2\n3  A3  B3  C3  D3  E3  F3\n",
                "    A   B   C   D   A   E   A   F\n0  A0  B0  C0  D0  A0  E0  A0  F0\n1  A1  B1  C1  D1  A1  E1  A1  F1\n2  A2  B2  C2  D2  A2  E2  A2  F2\n3  A3  B3  C3  D3  A3  E3  A3  F3\n"
            ],
            "answer": {
                "ans_desc": "Try Output: ",
                "code": [
                    "df4 = (pd.concat((df.set_index('A') for df in (df1,df2,df3)), axis=1)\n         .reset_index()\n      )\n"
                ]
            }
        },
        "expected": [
            "reset_index",
            "set_index"
        ],
        "predicted": [
            [
                "reset_index",
                0.8887807726860046
            ],
            [
                "rename",
                0.8831474781036377
            ],
            [
                "drop_duplicates",
                0.8796613812446594
            ],
            [
                "set_index",
                0.877936840057373
            ],
            [
                "join",
                0.8776938915252686
            ],
            [
                "merge",
                0.8771554827690125
            ],
            [
                "DataFrame",
                0.8767610192298889
            ],
            [
                "sort_index",
                0.874581515789032
            ],
            [
                "append",
                0.8744493722915649
            ],
            [
                "agg",
                0.8741433024406433
            ]
        ]
    },
    {
        "id": 56328404,
        "query": "how to apply a method to a pandas dataframe i have this dataframe i would like to convert it to i know how to create a dataframe (with indexes) for 1 column, but not for multiple columns this code produces this result how can i amend the code above to also add col2 (ideally using vectorisation rather than iteration) (so ideally i wouln't want to have to enter the same code for every column)",
        "link": "https://stackoverflow.com/questions/56328404/how-to-apply-a-method-to-a-pandas-dataframe",
        "example": {
            "qid": 56328404,
            "link": "https://stackoverflow.com/questions/56328404/how-to-apply-a-method-to-a-pandas-dataframe",
            "question": {
                "title": "How to apply a method to a Pandas Dataframe",
                "ques_desc": "I have this dataframe I would like to convert it to I know how to create a dataframe (with indexes) for 1 column, but not for multiple columns This code produces this result how can I amend the code above to also add col2 (ideally using vectorisation rather than iteration) (so ideally I wouln't want to have to enter the same code for every column) "
            },
            "io": [
                "   Col1              Col2\n\n0  A (1000 EUR)  C ( 3000 USD)\n\n1  B (2000 CHF)  D ( 4000 GBP)\n",
                "   Col1  Col2\n\n0  1000  3000\n\n1  2000  4000\n"
            ],
            "answer": {
                "ans_desc": "You can use the apply function to apply your operation to all elements in both rows. Does the trick for me ",
                "code": [
                    "# creates your dataframe\ndf = pd.DataFrame({'Col1':['A (1000 EUR)','B (2000 CHF)'], 'Col2':['C (3000 USD)', 'D (4000 GBP)']})\n\n# use the apply function to  apply your code to all elements of both columns\ndf = df.apply(lambda x: x.str.split('(').str[-1].str.split().str[0].apply(pd.to_numeric,errors='coerce'))\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "apply"
        ],
        "predicted": [
            [
                "transpose",
                0.8918761014938354
            ],
            [
                "shape",
                0.88570237159729
            ],
            [
                "index",
                0.883298397064209
            ],
            [
                "values",
                0.882489800453186
            ],
            [
                "DataFrame",
                0.8821360468864441
            ],
            [
                "loc",
                0.8774892687797546
            ],
            [
                "iloc",
                0.876854419708252
            ],
            [
                "apply",
                0.8758549094200134
            ],
            [
                "append",
                0.8751977682113647
            ],
            [
                "to_string",
                0.8739882707595825
            ]
        ]
    },
    {
        "id": 56036178,
        "query": "how can i sort numbers in a string in pandas? i have a dataframe like this: i want to sort it in desending order like this i tried this: the above function do some kind of sorting but not the way i want it.",
        "link": "https://stackoverflow.com/questions/56036178/how-can-i-sort-numbers-in-a-string-in-pandas",
        "example": {
            "qid": 56036178,
            "link": "https://stackoverflow.com/questions/56036178/how-can-i-sort-numbers-in-a-string-in-pandas",
            "question": {
                "title": "How can I sort numbers in a string in pandas?",
                "ques_desc": "I have a dataframe like this: I want to sort it in desending order like this I tried this: The above function do some kind of sorting but not the way I want it. "
            },
            "io": [
                "id   String\n1    345 -456 -13  879\n2    158 -926 -81  249 35 -4 -53  9\n3    945 -506 -103\n",
                "id   String\n1    879 345 -13 -457\n2    249 158 35 9 -4 -53 -81 -926\n3    945 -103 -506\n"
            ],
            "answer": {
                "ans_desc": "First is necessary convert values to integers, sorting and then convert to strings back: Or: ",
                "code": [
                    "f = lambda x: ' '.join(map(str, sorted(map(int, x.split()), reverse=True)))\ndf['string'] = df['string'].map(f)\n"
                ]
            }
        },
        "expected": [
            "join"
        ],
        "predicted": [
            [
                "rename",
                0.8451144099235535
            ],
            [
                "drop",
                0.8393206596374512
            ],
            [
                "append",
                0.8330103158950806
            ],
            [
                "itertuples",
                0.8327981233596802
            ],
            [
                "transpose",
                0.8285619020462036
            ],
            [
                "add_suffix",
                0.8275729417800903
            ],
            [
                "sort_index",
                0.8254650235176086
            ],
            [
                "join",
                0.8241989612579346
            ],
            [
                "reset_index",
                0.8241227865219116
            ],
            [
                "iterrows",
                0.8232660293579102
            ]
        ]
    },
    {
        "id": 55803354,
        "query": "add values to existing rows -dataframe i'm appending some weather data (from json- dict) - in japanese to dataframe. i would like to have something like this but i have this how could i change the codes to make it like that? here is the code",
        "link": "https://stackoverflow.com/questions/55803354/add-values-to-existing-rows-dataframe",
        "example": {
            "qid": 55803354,
            "link": "https://stackoverflow.com/questions/55803354/add-values-to-existing-rows-dataframe",
            "question": {
                "title": "Add values to existing rows -DataFrame",
                "ques_desc": "I'm appending some weather data (from json- dict) - in Japanese to DataFrame. I would like to have something like this But I have this How Could I change the Codes to make it like that? Here is the code "
            },
            "io": [
                "        \u5929\u6c17             \u98a8\n 0  \u72b6\u614b: Clouds    \u98a8\u901f: 2.1m\n 1     NaN          \u5411\u304d: 230\n",
                "        \u5929\u6c17             \u98a8\n 0  \u72b6\u614b: Clouds        NaN\n 1      NaN          \u98a8\u901f: 2.1m\n 2      NaN           \u5411\u304d: 230\n"
            ],
            "answer": {
                "ans_desc": "One way could be: And print without index ",
                "code": [
                    "df = pd.DataFrame(columns=['\u5929\u6c17','\u98a8'])\ndf = df.append({'\u5929\u6c17': weather_status, '\u98a8': wind_speed}, ignore_index=True)\ndf = df.append({'\u98a8': wind_deg}, ignore_index=True)\nprint(df)\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "append"
        ],
        "predicted": [
            [
                "DataFrame",
                0.8941764235496521
            ],
            [
                "append",
                0.892065703868866
            ],
            [
                "from_dict",
                0.887924313545227
            ],
            [
                "rename",
                0.8868770599365234
            ],
            [
                "itertuples",
                0.8851947784423828
            ],
            [
                "drop",
                0.8844640254974365
            ],
            [
                "to_string",
                0.8835710287094116
            ],
            [
                "join",
                0.8826126456260681
            ],
            [
                "transpose",
                0.8805074691772461
            ],
            [
                "set_index",
                0.8793200254440308
            ]
        ]
    },
    {
        "id": 55661343,
        "query": "how to convert one column in dataframe into a 2d array in python i have an dataframe which contain the observed data as: how can i get an array from the value to form a 2d with shape:(3,3) i try but it gives me which is not what i want",
        "link": "https://stackoverflow.com/questions/55661343/how-to-convert-one-column-in-dataframe-into-a-2d-array-in-python",
        "example": {
            "qid": 55661343,
            "link": "https://stackoverflow.com/questions/55661343/how-to-convert-one-column-in-dataframe-into-a-2d-array-in-python",
            "question": {
                "title": "how to convert one column in dataframe into a 2D array in python",
                "ques_desc": "I have an dataframe which contain the observed data as: how can I get an array from the value to form a 2D with shape:(3,3) I try but it gives me which is not what I want "
            },
            "io": [
                " [[1, 2, 1],\n [5, 4, 6],\n [7, 20, 9]]\n",
                "[list([1, 2, 1]) list([5, 4, 6]) list([7, 20, 9])]\n"
            ],
            "answer": {
                "ans_desc": "You can extract the column lists, then array-ify using a couple of methods below. If the lists are un-evenly sized, this will return a regular 2D array with nans in missing positions. ",
                "code": [
                    "# pd.DataFrame(df['Value'].tolist()).values   #  < v0.24\npd.DataFrame(df['Value'].tolist()).to_numpy() #  v0.24+\n\narray([[ 1.,  2., nan],\n       [ 3., nan, nan],\n       [ 4.,  5.,  6.]])\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "values"
        ],
        "predicted": [
            [
                "shape",
                0.8829782009124756
            ],
            [
                "values",
                0.8777270317077637
            ],
            [
                "DataFrame",
                0.8746359348297119
            ],
            [
                "explode",
                0.869472861289978
            ],
            [
                "to_xarray",
                0.8639143705368042
            ],
            [
                "to_string",
                0.8625544309616089
            ],
            [
                "apply",
                0.8617770671844482
            ],
            [
                "from_dict",
                0.8597698211669922
            ],
            [
                "style",
                0.855514645576477
            ],
            [
                "size",
                0.8554398417472839
            ]
        ]
    },
    {
        "id": 42142756,
        "query": "how can i change a specific row label in a pandas dataframe? i have a dataframe such as: where the final row contains averages. i would like to rename the final row label to so that the dataframe will look like this: i understand columns can be done with . but how can i do this with a specific row label?",
        "link": "https://stackoverflow.com/questions/42142756/how-can-i-change-a-specific-row-label-in-a-pandas-dataframe",
        "example": {
            "qid": 42142756,
            "link": "https://stackoverflow.com/questions/42142756/how-can-i-change-a-specific-row-label-in-a-pandas-dataframe",
            "question": {
                "title": "How can I change a specific row label in a Pandas dataframe?",
                "ques_desc": "I have a dataframe such as: Where the final row contains averages. I would like to rename the final row label to so that the dataframe will look like this: I understand columns can be done with . But how can I do this with a specific row label? "
            },
            "io": [
                "      0     1    2    3    4    5\n0  41.0  22.0  9.0  4.0  2.0  1.0\n1   6.0   1.0  2.0  1.0  1.0  1.0\n2   4.0   2.0  4.0  1.0  0.0  1.0\n3   1.0   2.0  1.0  1.0  1.0  1.0\n4   5.0   1.0  0.0  1.0  0.0  1.0\n5  11.4   5.6  3.2  1.6  0.8  1.0\n",
                "      0     1    2    3    4    5\n0  41.0  22.0  9.0  4.0  2.0  1.0\n1   6.0   1.0  2.0  1.0  1.0  1.0\n2   4.0   2.0  4.0  1.0  0.0  1.0\n3   1.0   2.0  1.0  1.0  1.0  1.0\n4   5.0   1.0  0.0  1.0  0.0  1.0\nA  11.4   5.6  3.2  1.6  0.8  1.0\n"
            ],
            "answer": {
                "ans_desc": "You can get the last index using negative indexing similar to that in Python Then Edit: If you are looking for a one-liner, ",
                "code": [
                    "df.index = df.index[:-1].tolist() + ['a']\n"
                ]
            }
        },
        "expected": [
            "index"
        ],
        "predicted": [
            [
                "rename",
                0.8414939641952515
            ],
            [
                "columns",
                0.8393657207489014
            ],
            [
                "index",
                0.8391708135604858
            ],
            [
                "shape",
                0.8277596831321716
            ],
            [
                "transpose",
                0.8270977735519409
            ],
            [
                "drop",
                0.8269838094711304
            ],
            [
                "reset_index",
                0.8258902430534363
            ],
            [
                "values",
                0.8217633366584778
            ],
            [
                "loc",
                0.8212185502052307
            ],
            [
                "iterrows",
                0.8201063871383667
            ]
        ]
    },
    {
        "id": 54777760,
        "query": "pd.dataframe prints output in a single column i have a file which i'm trying to convert into data frame using pandas. it is inside a loop and returns me the output as shown below. here is the code i'm using: here the tbl file is not tab seperated and to create a tab separation, i've to convert it into list. here is the reult i'm getting: the expected output is like this: any help will be highly appreciated.",
        "link": "https://stackoverflow.com/questions/54777760/pd-dataframe-prints-output-in-a-single-column",
        "example": {
            "qid": 54777760,
            "link": "https://stackoverflow.com/questions/54777760/pd-dataframe-prints-output-in-a-single-column",
            "question": {
                "title": "pd.DataFrame prints output in a single column",
                "ques_desc": "I have a file which I'm trying to convert into data frame using pandas. It is inside a loop and returns me the output as shown below. Here is the code I'm using: Here the tbl file is not tab seperated and to create a tab separation, I've to convert it into list. Here is the reult I'm getting: The expected output is like this: Any help will be highly appreciated. "
            },
            "io": [
                "0    B\n1  244\n2    S\n3    0\n     0\n0    B\n1  245\n2    A\n3    0\n",
                "0    B   244  S  0\n\n0    B   245  A  0\n"
            ],
            "answer": {
                "ans_desc": "Try this: ",
                "code": [
                    "import pandas as pd\nimport csv\ndf=pd.DataFrame()\nwith open('File.tbl', 'r') as  f:\n    P=list(f)\n    del P[0]\nfor o in P:\n    M=o.split()\n    B= M[:4]             #selecting specific columns only\n    df = pd.concat([df,pd.DataFrame(B).T])    #converting into DataFrame\ndf.to_csv('para.csv', sep=',')\n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "to_csv"
        ],
        "predicted": [
            [
                "DataFrame",
                0.8717449307441711
            ],
            [
                "to_string",
                0.8662274479866028
            ],
            [
                "transpose",
                0.8610244393348694
            ],
            [
                "rename",
                0.8544334769248962
            ],
            [
                "itertuples",
                0.8508427143096924
            ],
            [
                "shape",
                0.8505921959877014
            ],
            [
                "to_csv",
                0.8499962091445923
            ],
            [
                "values",
                0.848834753036499
            ],
            [
                "append",
                0.8467527627944946
            ],
            [
                "from_dict",
                0.8461089730262756
            ]
        ]
    },
    {
        "id": 54752268,
        "query": "pandas: complex merge of dataframes with date basis i have 2 pandas 's that i need to merge in a bit of a complex manner so i am in need of some help. dataframe to be inserted: dataframe to receive insertion 1) the needs to establish a common basis for (notice column is different), thus the needs to be organized into , , , where the dates of , , and are used for assembling to correctly reflect the values in , , at a certain date. sample output from step 1: 2) will need to be sorted by date according to and the columns , , will be added as columns in the . i imagine this will follow similar methods as in 1). sample output from step 2: 3) the new columns , , will need to be filled forward with cumsum but i think i got that down: ~ sample output from step 3: so the end goal would result in the values and shares held according to the index. for 's that will not have a column value (since column is \"missing\" some dates) in the resulting , it would be best to make that value but 0 would suffice. happy to clarify anything, i appreciate any/all help as this is a very complex operation (at least for me), thanks in advance! edit: trying to merge in a loop now since number of - pairs may vary. i now have a list of separate for the - pairs: . since number of pairs may vary, it seems best not to based on column labels hence .",
        "link": "https://stackoverflow.com/questions/54752268/pandas-complex-merge-of-dataframes-with-date-basis",
        "example": {
            "qid": 54752268,
            "link": "https://stackoverflow.com/questions/54752268/pandas-complex-merge-of-dataframes-with-date-basis",
            "question": {
                "title": "Pandas: Complex merge of DataFrames with date basis",
                "ques_desc": "I have 2 pandas 's that I need to merge in a bit of a complex manner so I am in need of some help. DataFrame to be inserted: DataFrame to receive insertion 1) The needs to establish a common basis for (notice column is different), thus the needs to be organized into , , , where the dates of , , and are used for assembling to correctly reflect the values in , , at a certain date. Sample output from step 1: 2) will need to be sorted by date according to and the columns , , will be added as columns in the . I imagine this will follow similar methods as in 1). Sample output from step 2: 3) The new columns , , will need to be filled forward with cumsum but I think I got that down: ~ Sample output from step 3: So the end goal would result in the values and shares held according to the index. For 's that will not have a column value (since column is \"missing\" some dates) in the resulting , it would be best to make that value but 0 would suffice. Happy to clarify anything, I appreciate any/all help as this is a very complex operation (at least for me), thanks in advance! EDIT: Trying to merge in a loop now since number of - pairs may vary. I now have a list of separate for the - pairs: . Since number of pairs may vary, it seems best not to based on column labels hence . "
            },
            "io": [
                "               0      1           2        3           4       5\n0     1998-01-02  16.25  2014-03-27   558.46  1998-01-02  131.13\n1     1998-01-05  15.88  2014-03-28   559.99  1998-01-05  130.38\n2     1998-01-06  18.94  2014-03-31   556.97  1998-01-06  131.13\n3     1998-01-07  17.50  2014-04-01   567.16  1998-01-07  129.56\n4     1998-01-08  18.19  2014-04-02   567.00  1998-01-08  130.50\n5     1998-01-09  18.19  2014-04-03   569.74  1998-01-09  127.00\n6     1998-01-12  18.25  2014-04-04   543.14  1998-01-12  129.50\n7     1998-01-13  19.50  2014-04-07   538.15  1998-01-13  132.13\n8     1998-01-14  19.75  2014-04-08   554.90  1998-01-14  131.13\n9     1998-01-15  19.19  2014-04-09   564.14  1998-01-15  132.31\n10    1998-01-16  18.81  2014-04-10   540.95  1998-01-16  135.25\n11    1998-01-20  19.06  2014-04-11   530.60  1998-01-20  137.81\n12    1998-01-21  18.91  2014-04-14   532.52  1998-01-21  137.00\n13    1998-01-22  19.25  2014-04-15   536.44  1998-01-22  138.63\n14    1998-01-23  19.50  2014-04-16   556.54  1998-01-23  138.25\n15    1998-01-26  19.44  2014-04-17   536.10  1998-01-26  141.75\n",
                "               0      1       3       5\n0     1998-01-02  16.25      NA  131.13\n1     1998-01-05  15.88      NA  130.38\n2     1998-01-06  18.94      NA  131.13\n3     1998-01-07  17.50      NA  129.56\n4     1998-01-08  18.19      NA  130.50\n5     1998-01-09  18.19      NA  127.00\n6     1998-01-12  18.25      NA  129.50\n7     1998-01-13  19.50      NA  132.13\n8     1998-01-14  19.75      NA  131.13\n...\n10    2014-04-10  18.81  558.46  135.25\n11    2014-04-11  19.06  559.99  137.81\n12    2014-04-14  18.91  556.97  137.00\n13    2014-04-15  19.25  567.16  138.63\n14    2014-04-16  19.50  567.00  138.25\n15    2014-04-17  19.44  569.74  141.75\n...\n"
            ],
            "answer": {
                "ans_desc": "Based on your clarifications, here's what I understand a solution to be. Given the dataframes given, I added column names to make things more clear, and renamed to just to save on typing: Now, here's where I couldn't test, since your insert df has no matching date indices from your receive df, but that should look like Of course, this assumes that the column is set as the index already as in your sample. If you want a numerical index at the end, you can , or you can leave the date as the index. It looks like you already have a handle on the last part of the task, and I can't test it anyway with the given data. And note also that you can reorder the columns however you want depending how you want your output to look () ",
                "code": [
                    "final_df=pd.merge(left=rec_df, right=insert_df, left_index=True, right_index=True, dropna=False)\n",
                    "df=df[[list of columns in order]]"
                ]
            }
        },
        "expected": [
            "merge"
        ],
        "predicted": [
            [
                "merge",
                0.8948146104812622
            ],
            [
                "rename",
                0.8830735683441162
            ],
            [
                "sort_values",
                0.8793382048606873
            ],
            [
                "reindex",
                0.8781554102897644
            ],
            [
                "sort_index",
                0.8773602843284607
            ],
            [
                "add_suffix",
                0.874055027961731
            ],
            [
                "reset_index",
                0.8732059001922607
            ],
            [
                "index",
                0.8728793859481812
            ],
            [
                "set_index",
                0.871097207069397
            ],
            [
                "filter",
                0.8689107894897461
            ]
        ]
    },
    {
        "id": 54674956,
        "query": "transfer pandas dataframe column names to dictionary i'm trying to convert a pandas dataframe column names into a dictionary. not so worried about the actual data in the dataframe. say i have an example dataframe like this and i'm not too worried about index just now: i'd like to get an output of a dictionary like: not too worried about the order they get printed out, as long as the assigned keys in the dictionary keep the order for each column name's order.",
        "link": "https://stackoverflow.com/questions/54674956/transfer-pandas-dataframe-column-names-to-dictionary",
        "example": {
            "qid": 54674956,
            "link": "https://stackoverflow.com/questions/54674956/transfer-pandas-dataframe-column-names-to-dictionary",
            "question": {
                "title": "Transfer pandas dataframe column names to dictionary",
                "ques_desc": "I'm trying to convert a pandas dataframe column names into a dictionary. Not so worried about the actual data in the dataframe. Say I have an example dataframe like this and I'm not too worried about index just now: I'd like to get an output of a dictionary like: Not too worried about the order they get printed out, as long as the assigned keys in the dictionary keep the order for each column name's order. "
            },
            "io": [
                "Col1 Col2 Col3 Col4\n--------------------\n a    b    c    a\n b    d    e    c\n",
                "{'Col1': 0, 'Col2': 1, 'Col3': 2, 'Col4': 3}\n"
            ],
            "answer": {
                "ans_desc": "That is straight forward with a comprehension as: Code: Test Code: Results: ",
                "code": [
                    "{c: i for i, c in enumerate(df.columns)}\n"
                ]
            }
        },
        "expected": [
            "columns"
        ],
        "predicted": [
            [
                "columns",
                0.8957749009132385
            ],
            [
                "rename",
                0.8952948451042175
            ],
            [
                "itertuples",
                0.8931272625923157
            ],
            [
                "DataFrame",
                0.8916826844215393
            ],
            [
                "append",
                0.8914945125579834
            ],
            [
                "transpose",
                0.8913626670837402
            ],
            [
                "join",
                0.8900229334831238
            ],
            [
                "iteritems",
                0.8895445466041565
            ],
            [
                "items",
                0.8895445466041565
            ],
            [
                "iterrows",
                0.8885655999183655
            ]
        ]
    },
    {
        "id": 54407275,
        "query": "add column to dataframe in a loop let's say i have a very simple pandas dataframe, containing a single indexed column with \"initial values\". i want to read in a loop n other dataframes to fill a single \"comparison\" column, with matching indices. for instance, with my inital dataframe as and the following two dataframes to read in a loop i would like to produce the following result using , or , i only ever seem to be able to create a new column for each iteration of the loop, filling the blanks with . what's the most pandas-pythonic way of achieving this? below an example from the proposed duplicate solution: second edit: @w-b, the following seems to work, but it can't be the case that there isn't a simpler option using proper pandas methods. it also requires turning off warnings, which might be dangerous...",
        "link": "https://stackoverflow.com/questions/54407275/add-column-to-dataframe-in-a-loop",
        "example": {
            "qid": 54407275,
            "link": "https://stackoverflow.com/questions/54407275/add-column-to-dataframe-in-a-loop",
            "question": {
                "title": "Add column to DataFrame in a loop",
                "ques_desc": "Let's say I have a very simple pandas dataframe, containing a single indexed column with \"initial values\". I want to read in a loop N other dataframes to fill a single \"comparison\" column, with matching indices. For instance, with my inital dataframe as and the following two dataframes to read in a loop I would like to produce the following result Using , or , I only ever seem to be able to create a new column for each iteration of the loop, filling the blanks with . What's the most pandas-pythonic way of achieving this? Below an example from the proposed duplicate solution: Second edit: @W-B, the following seems to work, but it can't be the case that there isn't a simpler option using proper pandas methods. It also requires turning off warnings, which might be dangerous... "
            },
            "io": [
                "   Initial\n0        a\n1        b\n2        c\n3        d\n",
                "    Initial Comparison\n0        a           e\n1        b           f\n2        c           g\n3        d           h\n"
            ],
            "answer": {
                "ans_desc": "Ok , since Op need more info Data input Solution ",
                "code": [
                    "import functools\ndf1 = pd.DataFrame(np.array([['a'],['b'],['c'],['d']]), columns=['Initial'])\ndf1['Compare']=np.nan\ndf2 = pd.DataFrame(np.array([['e'],['f']]), columns=['Compare'])\ndf3 = pd.DataFrame(np.array(['g','h','i']), columns=['Compare'],index=[2,3,4])\n"
                ]
            }
        },
        "expected": [
            "DataFrame"
        ],
        "predicted": [
            [
                "append",
                0.8783685564994812
            ],
            [
                "duplicated",
                0.8777154684066772
            ],
            [
                "drop_duplicates",
                0.8734714388847351
            ],
            [
                "rename",
                0.8724902868270874
            ],
            [
                "DataFrame",
                0.8718349933624268
            ],
            [
                "loc",
                0.8706679344177246
            ],
            [
                "shape",
                0.8688395619392395
            ],
            [
                "sort_index",
                0.8674471378326416
            ],
            [
                "transpose",
                0.8670536279678345
            ],
            [
                "iterrows",
                0.8662319779396057
            ]
        ]
    },
    {
        "id": 54333620,
        "query": "select columns in panda&#39;s dataframe that have an integer header i have a dataframe in pandas that looks like this: what i want to do is select specific columns from this data frame. but when i try the following code (the df_matrix being the dataframe displayed at the top) : it does not work and from what i can tell is because it is an integer. i tried to force it with str(100) but gave the same error as before: does anyone know how to get around this? thanks! edit 1: after trying to use it worked as expecte. btw, if someone else is facing this problem and wants to select multiple columns at the same time, you can use: and the output will be:",
        "link": "https://stackoverflow.com/questions/54333620/select-columns-in-pandas-dataframe-that-have-an-integer-header",
        "example": {
            "qid": 54333620,
            "link": "https://stackoverflow.com/questions/54333620/select-columns-in-pandas-dataframe-that-have-an-integer-header",
            "question": {
                "title": "Select columns in panda&#39;s dataframe that have an integer header",
                "ques_desc": "I have a dataframe in pandas that looks like this: What I want to do is select specific columns from this data frame. But when I try the following code (the df_matrix being the dataframe displayed at the top) : It does not work and from what I can tell is because it is an integer. I tried to force it with str(100) but gave the same error as before: Does anyone know how to get around this? Thanks! EDIT 1: After trying to use it worked as expecte. Btw, if someone else is facing this problem and wants to select multiple columns at the same time, you can use: and the output will be: "
            },
            "io": [
                "   100  200  300  400\n0    1    1    0    1\n1    1    1    1    0\n",
                "   100  300\n0    1    0\n1    1    1\n"
            ],
            "answer": {
                "ans_desc": "Simply use below as in this case as your columns are . If you want your columns to be accessed as , Use: and then Output ",
                "code": [
                    "df.columns = [str(x) for x in df.columns]"
                ]
            }
        },
        "expected": [
            "columns"
        ],
        "predicted": [
            [
                "loc",
                0.8685435056686401
            ],
            [
                "columns",
                0.8683382868766785
            ],
            [
                "drop",
                0.867433488368988
            ],
            [
                "query",
                0.8665263652801514
            ],
            [
                "rename",
                0.861316978931427
            ],
            [
                "join",
                0.8607369661331177
            ],
            [
                "transpose",
                0.8604949712753296
            ],
            [
                "set_index",
                0.8599816560745239
            ],
            [
                "iloc",
                0.859272837638855
            ],
            [
                "itertuples",
                0.8575620651245117
            ]
        ]
    },
    {
        "id": 54005974,
        "query": "change values in columns of dataframe depending on values of other columns (values come from lists) i have a data frame in python, for example this one: the code for the creation of the dataframe: let the list1 be ['a','c','e'] and list2 be ['b','d','f']. what i want is following: if in the col1 stays an element from the list1 and in one of the col2-col4 stays an element from the list2, then i want to eliminate the last one (so replace it by ''). i have tried which is not quite what i want but al least goes in the right direction, unfortunately it doesn't work. could someone help please? this is my expected output:",
        "link": "https://stackoverflow.com/questions/54005974/change-values-in-columns-of-dataframe-depending-on-values-of-other-columns-valu",
        "example": {
            "qid": 54005974,
            "link": "https://stackoverflow.com/questions/54005974/change-values-in-columns-of-dataframe-depending-on-values-of-other-columns-valu",
            "question": {
                "title": "Change values in columns of dataframe depending on values of other columns (values come from lists)",
                "ques_desc": "I have a Data Frame in python, for example this one: The code for the creation of the dataframe: Let the list1 be ['A','C','E'] and list2 be ['B','D','F']. What I want is following: if in the col1 stays an element from the list1 and in one of the col2-col4 stays an element from the list2, then i want to eliminate the last one (so replace it by ''). I have tried which is not quite what i want but al least goes in the right direction, unfortunately it doesn't work. Could someone help please? This is my expected output: "
            },
            "io": [
                "  col1 col2 col3 col4\n0    A    C    B    D\n1    C    E    E    A\n2    E    A    E    A\n3    A    D    D    D\n4    B    B    B    B\n5    D    D    D    D\n6    F    F    A    F\n7    E    E    E    E\n8    B    B    B    B\n",
                "  col1 col2 col3 col4\n0    A         B    D\n1    C    E    E    A\n2    E    A    E    A\n3    A         D    D\n4    B    B    B    B\n5    D    D    D    D\n6    F    F    A    F\n7    E    E    E    E\n8    B    B    B    B\n"
            ],
            "answer": {
                "ans_desc": " is a method of , so use it with your dataframe, not with a series. In addition, you can test criteria on multiple series via : Result: ",
                "code": [
                    "m1 = df['col1'].isin(list1)\nm2 = df[['col2', 'col3', 'col4']].isin(list2).any(1)\n\ndf.loc[m1 & m2, 'col2'] = ''\n"
                ]
            }
        },
        "expected": [
            "any",
            "isin",
            "loc"
        ],
        "predicted": [
            [
                "apply",
                0.877593994140625
            ],
            [
                "join",
                0.8724368214607239
            ],
            [
                "isin",
                0.8699599504470825
            ],
            [
                "applymap",
                0.8697509169578552
            ],
            [
                "any",
                0.8688387274742126
            ],
            [
                "all",
                0.8675238490104675
            ],
            [
                "update",
                0.8661127090454102
            ],
            [
                "query",
                0.8650107383728027
            ],
            [
                "values",
                0.8649317026138306
            ],
            [
                "loc",
                0.8638527393341064
            ]
        ]
    },
    {
        "id": 53961914,
        "query": "create a dataframe from arrays python i'm try to construct a dataframe (i'm using pandas library) from some arrays and one matrix. in particular, if i have two array like this: and one matrix like this : can i create a dataset like this? maybe is a stupid question, but i m very new with python and pandas. i seen this : https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.dataframe.html but specify only 'colums'. i should read the matrix row for row and paste in my dataset, but i m think that exist a more easy solution with pandas.",
        "link": "https://stackoverflow.com/questions/53961914/create-a-dataframe-from-arrays-python",
        "example": {
            "qid": 53961914,
            "link": "https://stackoverflow.com/questions/53961914/create-a-dataframe-from-arrays-python",
            "question": {
                "title": "Create a dataframe from arrays python",
                "ques_desc": "I'm try to construct a dataframe (I'm using Pandas library) from some arrays and one matrix. in particular, if I have two array like this: And one matrix like this : Can i create a dataset like this? Maybe is a stupid question, but i m very new with Python and Pandas. I seen this : https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html but specify only 'colums'. I should read the matrix row for row and paste in my dataset, but I m think that exist a more easy solution with Pandas. "
            },
            "io": [
                "1 2 2\n3 3 3\n4 4 4\n",
                "  A B C\nD 1 2 2\nE 3 3 3\nF 4 4 4\n"
            ],
            "answer": {
                "ans_desc": "This should do the trick for you. ",
                "code": [
                    "columns = [\"A\", \"B\", \"C\"]\nrows = [\"D\", \"E\", \"F\"]\ndata = np.array([[1, 2, 2], [3, 3, 3],[4, 4, 4]])\ndf = pd.DataFrame(data=data, index=rows, columns=columns)\n"
                ]
            }
        },
        "expected": [
            "DataFrame"
        ],
        "predicted": [
            [
                "DataFrame",
                0.8603641390800476
            ],
            [
                "from_dict",
                0.8493450880050659
            ],
            [
                "shape",
                0.8450665473937988
            ],
            [
                "values",
                0.8434062004089355
            ],
            [
                "explode",
                0.8392475843429565
            ],
            [
                "apply",
                0.8371670842170715
            ],
            [
                "loc",
                0.8364834189414978
            ],
            [
                "join",
                0.8337666392326355
            ],
            [
                "transpose",
                0.8302786350250244
            ],
            [
                "to_string",
                0.829211950302124
            ]
        ]
    },
    {
        "id": 53939072,
        "query": "column dupe renaming in pandas i have the following csv file of data: pandas currently renames this to: is there a way to customize how this is renamed? for example, i would prefer:",
        "link": "https://stackoverflow.com/questions/53939072/column-dupe-renaming-in-pandas",
        "example": {
            "qid": 53939072,
            "link": "https://stackoverflow.com/questions/53939072/column-dupe-renaming-in-pandas",
            "question": {
                "title": "Column dupe renaming in pandas",
                "ques_desc": "I have the following csv file of data: Pandas currently renames this to: Is there a way to customize how this is renamed? For example, I would prefer: "
            },
            "io": [
                "       id number id.1\n0  132605      1    1\n1  132750      2    1\n",
                "           id number id2\n0  132605      1    1\n1  132750      2    1\n"
            ],
            "answer": {
                "ans_desc": ": use period delimiter Assuming duplicate column labels are the only instances where a column name contains a period (), you can use a custom function with : : robust solution A robust solution is possible with the module from the standard library: ",
                "code": [
                    "from collections import defaultdict\nimport csv\n\n# replace StringIO(file) with open('file.csv', 'r')\nwith StringIO(file) as fin:\n    headers = next(csv.reader(fin))\n\ndef rename_duplicates(original_cols):\n    count = defaultdict(int)\n    for x in original_cols:\n        count[x] += 1\n        yield f'{x}{count[x]}' if count[x] > 1 else x\n\ndf.columns = rename_duplicates(headers)\n"
                ]
            }
        },
        "expected": [
            "columns"
        ],
        "predicted": [
            [
                "rename",
                0.884129524230957
            ],
            [
                "columns",
                0.8707532286643982
            ],
            [
                "drop",
                0.870653510093689
            ],
            [
                "rename_axis",
                0.8626971244812012
            ],
            [
                "transpose",
                0.8626682758331299
            ],
            [
                "itertuples",
                0.8615649938583374
            ],
            [
                "iteritems",
                0.861499011516571
            ],
            [
                "items",
                0.861499011516571
            ],
            [
                "index",
                0.8593096733093262
            ],
            [
                "set_index",
                0.8590943813323975
            ]
        ]
    },
    {
        "id": 53932155,
        "query": "how to append ndarray values into dataframe rows of particular columns? i have a function that returns an like this now, i have a data frame with columns a,b,c,...,z ; but the array we are getting has only 20 values. hence i want to find a way such that for every array i get as output, i am able to store it in like this (a,b,w,x,y,z are to be left blank):",
        "link": "https://stackoverflow.com/questions/53932155/how-to-append-ndarray-values-into-dataframe-rows-of-particular-columns",
        "example": {
            "qid": 53932155,
            "link": "https://stackoverflow.com/questions/53932155/how-to-append-ndarray-values-into-dataframe-rows-of-particular-columns",
            "question": {
                "title": "How to append ndarray values into dataframe rows of particular columns?",
                "ques_desc": "I have a function that returns an like this Now, I have a data frame with columns A,B,C,...,Z ; but the array we are getting has only 20 values. Hence I want to find a way such that for every array I get as output, I am able to store it in like this (A,B,W,X,Y,Z are to be left blank): "
            },
            "io": [
                "[0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0] \n",
                "__| A | B | C | D | E | F | ...\n0 |nan|nan| 0 | 1 | 0 | 0 | ...\n1 |nan|nan| 1 | 1 | 0 | 1 | ...\n.\n. \n.\n"
            ],
            "answer": {
                "ans_desc": "I couldn't get what I wanted through the suggestions posted here. However, I did figure it out myself. I'm sharing it here for the community's reference. ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(columns=[chr(i) for i in range(ord('A'),ord('Z')+1)])\n\nprint(df)\n",
                    "Empty DataFrame\nColumns: [A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z]\nIndex: []\n\n[0 rows x 26 columns]\n",
                    "list1 = [i for i in range(101,121)]\narr1d = np.array(list1)\n\narr1d\n",
                    "# Create alphabet list of uppercase letters\nalphabet = []\nfor letter in range(ord('C'),ord('W')):\n    alphabet.append(chr(letter))\nalphabet\n",
                    "['C',\n 'D',\n 'E',\n 'F',\n 'G',\n 'H',\n 'I',\n 'J',\n 'K',\n 'L',\n 'M',\n 'N',\n 'O',\n 'P',\n 'Q',\n 'R',\n 'S',\n 'T',\n 'U',\n 'V']\n",
                    "df = df.append(pd.Series(arr1d, index=alphabet), ignore_index=True)\n#This line of code can be used for every new value of arr1d \n"
                ]
            }
        },
        "expected": [
            "DataFrame",
            "append"
        ],
        "predicted": [
            [
                "loc",
                0.877273440361023
            ],
            [
                "transpose",
                0.8763254880905151
            ],
            [
                "shape",
                0.8755068182945251
            ],
            [
                "values",
                0.8717382550239563
            ],
            [
                "index",
                0.871361255645752
            ],
            [
                "append",
                0.871095597743988
            ],
            [
                "DataFrame",
                0.8693025708198547
            ],
            [
                "explode",
                0.8672191500663757
            ],
            [
                "iloc",
                0.8658498525619507
            ],
            [
                "join",
                0.8656759858131409
            ]
        ]
    },
    {
        "id": 53769189,
        "query": "rearrange rows of dataframe alternatively i have a dataframe looks like this: and i want to make it look like this: my own way to do it seems to take quite a few lines, aka not pythonic. my code: is there any more pythonic way to accomplish the same result? thank you in advance. edit: i'd like a more general method to deal with dataframe like this",
        "link": "https://stackoverflow.com/questions/53769189/rearrange-rows-of-dataframe-alternatively",
        "example": {
            "qid": 53769189,
            "link": "https://stackoverflow.com/questions/53769189/rearrange-rows-of-dataframe-alternatively",
            "question": {
                "title": "Rearrange rows of Dataframe alternatively",
                "ques_desc": "I have a dataframe looks like this: and I want to make it look like this: My own way to do it seems to take quite a few lines, aka not pythonic. My code: Is there any more pythonic way to accomplish the same result? Thank you in advance. EDIT: I'd like a more general method to deal with dataframe like this "
            },
            "io": [
                "   col1           col2\n0     1  random string\n1    -1  random string\n2     2  random string\n3    -2  random string\n4     3  random string\n5    -3  random string\n6     4  random string\n7    -4  random string\n8     5  random string\n9    -5  random string\n10    6  random string\n11   -6  random string\n12    7  random string\n13   -7  random string\n14    8  random string\n15   -8  random string\n16    9  random string\n17   -9  random string\n18   10  random string\n19  -10  random string\n",
                "   col1           col2\n0     1  random string\n1     2  random string\n2     3  random string\n3     4  random string\n4     5  random string\n5    1x  random string\n6    2x  random string\n7    3x  random string\n8    4x  random string\n9    5x  random string\n10   1y  random string\n11   2y  random string\n12   3y  random string\n13   4y  random string\n14   5y  random string\n"
            ],
            "answer": {
                "ans_desc": "If the size of the subgroups is known, let's call it , and your is chunked with each group following the other, we just need some math: Output: ",
                "code": [
                    "n=5\n\ndf.index = df.index%n + (df.index//n)/(len(df)/n)\ndf = df.sort_index().reset_index(drop=True)\n"
                ]
            }
        },
        "expected": [
            "index",
            "reset_index",
            "sort_index"
        ],
        "predicted": [
            [
                "reset_index",
                0.8652657270431519
            ],
            [
                "loc",
                0.8652452230453491
            ],
            [
                "query",
                0.8607674837112427
            ],
            [
                "append",
                0.8600291609764099
            ],
            [
                "shape",
                0.854216456413269
            ],
            [
                "iloc",
                0.8527852296829224
            ],
            [
                "sort_index",
                0.8527292609214783
            ],
            [
                "duplicated",
                0.8525874018669128
            ],
            [
                "groupby",
                0.8525315523147583
            ],
            [
                "index",
                0.851922869682312
            ]
        ]
    },
    {
        "id": 53184917,
        "query": "interpolation of a dataframe with immediate data appearing before and after it - pandas let's say i've a dataframe like this - i want the dataframe to be transormed to - it takes the average of the immediate data available before and after a nan and updates the missing/nan value accordingly.",
        "link": "https://stackoverflow.com/questions/53184917/interpolation-of-a-dataframe-with-immediate-data-appearing-before-and-after-it",
        "example": {
            "qid": 53184917,
            "link": "https://stackoverflow.com/questions/53184917/interpolation-of-a-dataframe-with-immediate-data-appearing-before-and-after-it",
            "question": {
                "title": "Interpolation of a dataframe with immediate data appearing before and after it - Pandas",
                "ques_desc": "Let's say I've a dataframe like this - I want the dataframe to be transormed to - It takes the average of the immediate data available before and after a NaN and updates the missing/NaN value accordingly. "
            },
            "io": [
                "ID  Weight Height\n1   80.0   180.0\n2   60.0   170.0\n3   NaN    NaN\n4   NaN    NaN\n5   82.0   185.0\n",
                "ID  Weight  Height\n1   80.0    180.0\n2   60.0    170.0\n3   71.0    177.5\n4   76.5    181.25\n5   82.0    185.0\n"
            ],
            "answer": {
                "ans_desc": "You can use interpolation from the library by using the following: Check the arguments on the documentation for the method of interpolation to tune this to your problem case: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.interpolate.html ",
                "code": [
                    "df['Weight'], df['Height'] = df.Weight.interpolate(), df.Height.interpolate()\n"
                ]
            }
        },
        "expected": [
            "interpolate"
        ],
        "predicted": [
            [
                "interpolate",
                0.8293416500091553
            ],
            [
                "resample",
                0.8251960873603821
            ],
            [
                "mean",
                0.8154951930046082
            ],
            [
                "agg",
                0.8117770552635193
            ],
            [
                "sort_values",
                0.8112812638282776
            ],
            [
                "index",
                0.8096685409545898
            ],
            [
                "droplevel",
                0.8086022138595581
            ],
            [
                "reindex",
                0.8078896999359131
            ],
            [
                "quantile",
                0.8074450492858887
            ],
            [
                "transpose",
                0.8073962330818176
            ]
        ]
    },
    {
        "id": 52745166,
        "query": "how to take first one or two digits from float number? i have a pandas dataframe. in a series, i have time in hour and minute represented as , as below. i want only the hours: example of time column values from (1 to 12) : example of time column values from (13 to 24) : i have tried this code but it takes very long time until i close the editor so i didn't see the result",
        "link": "https://stackoverflow.com/questions/52745166/how-to-take-first-one-or-two-digits-from-float-number",
        "example": {
            "qid": 52745166,
            "link": "https://stackoverflow.com/questions/52745166/how-to-take-first-one-or-two-digits-from-float-number",
            "question": {
                "title": "How to take first one or two digits from float number?",
                "ques_desc": "I have a Pandas dataframe. In a series, I have time in hour and minute represented as , as below. I want only the hours: Example of time column values from (1 to 12) : Example of time column values from (13 to 24) : I have tried this code but it takes very long time until I close the editor so I didn't see the result "
            },
            "io": [
                "1000.0 -> 10\n901.0 ->  9\n",
                "1850.0 -> 18\n2301.0 -> 23\n"
            ],
            "answer": {
                "ans_desc": "With Pandas, don't iterate rows when vectorised methods are available. In this case, you can use floor division followed by : ",
                "code": [
                    "df['hour'] = (df['dep_time'] // 100).astype(int)\n"
                ]
            }
        },
        "expected": [
            "astype"
        ],
        "predicted": [
            [
                "values",
                0.8596154451370239
            ],
            [
                "shape",
                0.856353759765625
            ],
            [
                "apply",
                0.8542234897613525
            ],
            [
                "div",
                0.8535906076431274
            ],
            [
                "DataFrame",
                0.853304922580719
            ],
            [
                "to_string",
                0.8516300320625305
            ],
            [
                "astype",
                0.8494935035705566
            ],
            [
                "sort_index",
                0.8479388356208801
            ],
            [
                "loc",
                0.8477479815483093
            ],
            [
                "transpose",
                0.8450623154640198
            ]
        ]
    },
    {
        "id": 52492961,
        "query": "compare each of the column values and return final value based on conditions i currently have a dataframe which looks like this: what i want to do is apply some condition to the column values and return the final result in a new column. the condition is to assign values based on this order of priority where 2 being the first priority: [2,1,3,0,4] i tried to define a function to append the final results but wasnt really getting anywhere...any thoughts? the desired outcome would look something like: where col4 is the new column created. thanks",
        "link": "https://stackoverflow.com/questions/52492961/compare-each-of-the-column-values-and-return-final-value-based-on-conditions",
        "example": {
            "qid": 52492961,
            "link": "https://stackoverflow.com/questions/52492961/compare-each-of-the-column-values-and-return-final-value-based-on-conditions",
            "question": {
                "title": "Compare each of the column values and return final value based on conditions",
                "ques_desc": "I currently have a dataframe which looks like this: What I want to do is apply some condition to the column values and return the final result in a new column. The condition is to assign values based on this order of priority where 2 being the first priority: [2,1,3,0,4] I tried to define a function to append the final results but wasnt really getting anywhere...any thoughts? The desired outcome would look something like: where col4 is the new column created. Thanks "
            },
            "io": [
                "col1  col2  col3\n 1      2     3\n 2      3     NaN\n 3      4     NaN\n 2      NaN   NaN\n 0      2     NaN\n",
                "col1  col2  col3  col4\n 1     2     3     2\n 2     3     NaN   2\n 3     4    NaN    3\n 2     NaN   NaN   2\n 0     2    NaN    2\n"
            ],
            "answer": {
                "ans_desc": "first you may want to get ride of the NaNs: and then apply a function to every row to find your value: Output: ",
                "code": [
                    "def func(x,l=[2,1,3,0,4,5]):\n    for j in l:\n      if(j in x):\n         return j\n\ndf['new'] = df.apply(lambda x: func(list(x)),axis =1)\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "values",
                0.8805509805679321
            ],
            [
                "apply",
                0.8779491186141968
            ],
            [
                "shape",
                0.8769226670265198
            ],
            [
                "any",
                0.8672735691070557
            ],
            [
                "sort_index",
                0.8666457533836365
            ],
            [
                "all",
                0.8666088581085205
            ],
            [
                "quantile",
                0.8657313585281372
            ],
            [
                "max",
                0.8637939095497131
            ],
            [
                "loc",
                0.8635584115982056
            ],
            [
                "sum",
                0.8614850044250488
            ]
        ]
    },
    {
        "id": 52491327,
        "query": "pandas dataframe data are same or new? in python, pandas dataframes are used : dataframe_1 : dataframe_2 : here, dataframe_2 contains ab20, ab10 and ab17 same as dataframe_1 in random order. how to check which elements in dataframe_2 are new and which are same as dataframe_1 ???",
        "link": "https://stackoverflow.com/questions/52491327/pandas-dataframe-data-are-same-or-new",
        "example": {
            "qid": 52491327,
            "link": "https://stackoverflow.com/questions/52491327/pandas-dataframe-data-are-same-or-new",
            "question": {
                "title": "Pandas Dataframe data are same or new?",
                "ques_desc": "In Python, Pandas dataframes are used : dataframe_1 : dataframe_2 : Here, dataframe_2 contains AB20, AB10 and AB17 same as dataframe_1 in random order. How to check which elements in dataframe_2 are new and which are same as dataframe_1 ??? "
            },
            "io": [
                "     id\n0  AB17\n1  AB18\n2  AB19\n3  AB20\n4  AB10\n",
                "     id\n0  AB20\n1  AB10\n2  AB17\n3  AB21\n4  AB09\n"
            ],
            "answer": {
                "ans_desc": "I think need for boolean mask and filter by with , if necessary convert output to : ",
                "code": [
                    "mask = dataframe_2['id'].isin(dataframe_1['id'])\nprint (mask)\n0     True\n1     True\n2     True\n3    False\n4    False\nName: id, dtype: bool\n\nsame = dataframe_2.loc[mask, 'id'].tolist()\ndiff = dataframe_2.loc[~mask, 'id'].tolist()\n\n#if want unique values\n#same = dataframe_2.loc[mask, 'id'].unique().tolist()\n#diff = dataframe_2.loc[~mask, 'id'].unique().tolist()\n\nprint (same)\n['AB20', 'AB10', 'AB17']\n\nprint (diff)\n['AB21', 'AB09']\n"
                ]
            }
        },
        "expected": [
            "isin",
            "loc"
        ],
        "predicted": [
            [
                "isin",
                0.85695481300354
            ],
            [
                "all",
                0.8518319129943848
            ],
            [
                "eq",
                0.8509685397148132
            ],
            [
                "any",
                0.8505220413208008
            ],
            [
                "ne",
                0.8490818738937378
            ],
            [
                "update",
                0.8487027287483215
            ],
            [
                "iloc",
                0.8469308614730835
            ],
            [
                "assign",
                0.8457791209220886
            ],
            [
                "loc",
                0.8450731635093689
            ],
            [
                "apply",
                0.8444994688034058
            ]
        ]
    },
    {
        "id": 51784855,
        "query": "remove decimals in pandas column names i have a dataframe with as the column names. i'd like to change them to be strings and remove the decimal places: i've tried saving the columns out to a list with and changing them there but i've had no luck. any help would be greatly appreciated!",
        "link": "https://stackoverflow.com/questions/51784855/remove-decimals-in-pandas-column-names",
        "example": {
            "qid": 51784855,
            "link": "https://stackoverflow.com/questions/51784855/remove-decimals-in-pandas-column-names",
            "question": {
                "title": "Remove decimals in Pandas column names",
                "ques_desc": "I have a dataframe with as the column names. I'd like to change them to be strings and remove the decimal places: I've tried saving the columns out to a list with and changing them there but I've had no luck. Any help would be greatly appreciated! "
            },
            "io": [
                "df =\n     2006.0   2007.0   2008.0   2009.0\n0       foo      foo      bar      bar\n1       foo      foo      bar      bar\n",
                "df =\n       2006     2007     2008     2009\n0       foo      foo      bar      bar\n1       foo      foo      bar      bar\n"
            ],
            "answer": {
                "ans_desc": "You can convert the type with Or use and convert to string type. ",
                "code": [
                    "In [338]: df.columns.map('{:g}'.format)\nOut[338]: Index(['2006', '2007', '2008', '2009'], dtype='object')\n\nIn [319]: df.columns.map(int)\nOut[319]: Int64Index([2006, 2007, 2008, 2009], dtype='int64')\n"
                ]
            }
        },
        "expected": [
            "columns"
        ],
        "predicted": [
            [
                "columns",
                0.8731473684310913
            ],
            [
                "rename",
                0.8719193935394287
            ],
            [
                "replace",
                0.8677039742469788
            ],
            [
                "astype",
                0.8669373989105225
            ],
            [
                "dtypes",
                0.8667075037956238
            ],
            [
                "values",
                0.8660250902175903
            ],
            [
                "add_suffix",
                0.8657593131065369
            ],
            [
                "add_prefix",
                0.8655310869216919
            ],
            [
                "apply",
                0.8653653860092163
            ],
            [
                "transpose",
                0.8653554916381836
            ]
        ]
    },
    {
        "id": 51408990,
        "query": "how to fill column&#39; value with another column and keep existing? i have this dataframe with two column, and i want to fill with its corresponding if exist, also if already have a value keep it don't change it with . here is an example: input output it's look easy, but i'm beginner in python :( any help please.",
        "link": "https://stackoverflow.com/questions/51408990/how-to-fill-column-value-with-another-column-and-keep-existing",
        "example": {
            "qid": 51408990,
            "link": "https://stackoverflow.com/questions/51408990/how-to-fill-column-value-with-another-column-and-keep-existing",
            "question": {
                "title": "How to fill column&#39; value with another column and keep existing?",
                "ques_desc": "I have this dataframe with two column, and I want to fill with its corresponding if exist, also if already have a value keep it don't change it with . Here is an example: input output It's look easy, but I'm beginner in python :( Any help please. "
            },
            "io": [
                "c1             c2\nHP_0003470  \nHP_8362789    HP_0093723\n              MP_0000231\n              MP_0000231\n",
                "c1             c2\nHP_0003470  \nHP_8362789    HP_0093723\nMP_0000231    MP_0000231\nMP_0000231    MP_0000231\n"
            ],
            "answer": {
                "ans_desc": "You can do so: Output: ",
                "code": [
                    "df['c1'] = df['c1'].replace('',np.NaN).fillna(df['c2'])\ndf['c2'] = df['c2'].replace('',np.NaN).fillna(df['c1'])\n"
                ]
            }
        },
        "expected": [
            "fillna",
            "replace"
        ],
        "predicted": [
            [
                "fillna",
                0.8750563859939575
            ],
            [
                "bfill",
                0.8687710762023926
            ],
            [
                "ffill",
                0.8683045506477356
            ],
            [
                "rename",
                0.8596006631851196
            ],
            [
                "reindex",
                0.8584010004997253
            ],
            [
                "where",
                0.8576194047927856
            ],
            [
                "index",
                0.8544332981109619
            ],
            [
                "replace",
                0.8525903820991516
            ],
            [
                "mask",
                0.8523680567741394
            ],
            [
                "update",
                0.8519697785377502
            ]
        ]
    },
    {
        "id": 51259229,
        "query": "how to select specific column items as list from pandas dataframe? i have a dataframe like this : how to convert it into this form (all zeros appearing are not considered) : and finally into a set of items like :",
        "link": "https://stackoverflow.com/questions/51259229/how-to-select-specific-column-items-as-list-from-pandas-dataframe",
        "example": {
            "qid": 51259229,
            "link": "https://stackoverflow.com/questions/51259229/how-to-select-specific-column-items-as-list-from-pandas-dataframe",
            "question": {
                "title": "How to select specific column items as list from pandas dataframe?",
                "ques_desc": "I have a dataframe like this : How to convert it into this form (All zeros appearing are not considered) : And finally into a set of items like : "
            },
            "io": [
                "  A  B  C   D \n---------------\n0  A  0  C  D\n1  A  0  C  D\n2  0  B  C  0\n3  A  0  0  D\n4  0  B  C  0\n5  A  0  0  0\n",
                "   A  B  C  D    E\n----------------------\n0  A  0  C  D  [A,C,D]\n1  A  0  C  D  [A,C,D]\n2  0  A  C  0  [A,C]\n3  A  0  0  D  [A,D]\n4  0  A  C  0  [A,C]\n5  A  0  0  0  [A]\n"
            ],
            "answer": {
                "ans_desc": "Use nested list comprehension with filtering : And for s: ",
                "code": [
                    "s = [set([y for y in x if y != '0']) for x in df.values.tolist()]\nprint (s)\n[{'A', 'D', 'C'}, {'A', 'D', 'C'}, {'C', 'B'}, {'A', 'D'}, {'C', 'B'}, {'A'}]\n"
                ]
            }
        },
        "expected": [
            "values"
        ],
        "predicted": [
            [
                "join",
                0.8854221105575562
            ],
            [
                "rename",
                0.8771814107894897
            ],
            [
                "set_index",
                0.876764178276062
            ],
            [
                "columns",
                0.8732044696807861
            ],
            [
                "shape",
                0.8730030059814453
            ],
            [
                "values",
                0.8716949224472046
            ],
            [
                "index",
                0.8710607290267944
            ],
            [
                "query",
                0.8710236549377441
            ],
            [
                "apply",
                0.8706234097480774
            ],
            [
                "filter",
                0.8702361583709717
            ]
        ]
    },
    {
        "id": 50717904,
        "query": "pandas delete all rows which contains &quot;required value&quot; in all column i have the following dataframe i want to delete all rows which contains all column a v or n output data frame will be :-",
        "link": "https://stackoverflow.com/questions/50717904/pandas-delete-all-rows-which-contains-required-value-in-all-column",
        "example": {
            "qid": 50717904,
            "link": "https://stackoverflow.com/questions/50717904/pandas-delete-all-rows-which-contains-required-value-in-all-column",
            "question": {
                "title": "Pandas delete all rows which contains &quot;required value&quot; in all column",
                "ques_desc": "I have the following dataframe I want to delete all rows which contains all column a V or N Output data frame will be :- "
            },
            "io": [
                " A     B    C    D\nBUY   150   Q   2018\nSELL  63    Q   2018\nN      N    N    N\n\nV      v    v    v\nSELL  53    Q   2018\n",
                "    A     B     C    D\n   BUY   150    Q   2018\n   SELL  63     Q   2018\n\n   SELL  53     Q   2018\n"
            ],
            "answer": {
                "ans_desc": "Use : Detail: First compare by : Get rows if s per rows: Invert condition by : ",
                "code": [
                    "print (df.isin(['V', 'v', 'N', 'n']))\n       A      B      C      D\n0  False  False  False  False\n1  False  False  False  False\n2   True   True   True   True\n3   True   True   True   True\n4  False  False  False  False\n",
                    "print (df.isin(['V', 'v', 'N', 'n']).all(axis=1))\n0    False\n1    False\n2     True\n3     True\n4    False\ndtype: bool\n",
                    "print (~df.isin(['V', 'v', 'N', 'n']).all(axis=1))\n0     True\n1     True\n2    False\n3    False\n4     True\ndtype: bool\n"
                ]
            }
        },
        "expected": [
            "all",
            "isin"
        ],
        "predicted": [
            [
                "all",
                0.818000078201294
            ],
            [
                "any",
                0.8068945407867432
            ],
            [
                "join",
                0.8056718111038208
            ],
            [
                "rename",
                0.8000435829162598
            ],
            [
                "isin",
                0.798865795135498
            ],
            [
                "apply",
                0.7981457710266113
            ],
            [
                "DataFrame",
                0.7971072196960449
            ],
            [
                "drop",
                0.7967516779899597
            ],
            [
                "columns",
                0.7948451638221741
            ],
            [
                "sort_index",
                0.7946532368659973
            ]
        ]
    },
    {
        "id": 50533522,
        "query": "copying columns within pandas dataframe i want to slice and copy columns in a python dataframe. my data frame looks like the following: i want to make it of the form which basically means that i want to shift the values in columns '1929','1929.1','1930','1930.1' under the column '1928' and '1928.1' for the same, i wrote the code as no copying takes places within the columns. how shall i modify my code??",
        "link": "https://stackoverflow.com/questions/50533522/copying-columns-within-pandas-dataframe",
        "example": {
            "qid": 50533522,
            "link": "https://stackoverflow.com/questions/50533522/copying-columns-within-pandas-dataframe",
            "question": {
                "title": "Copying columns within pandas dataframe",
                "ques_desc": "I want to slice and copy columns in a Python Dataframe. My data frame looks like the following: I want to make it of the form Which basically means that I want to shift the values in Columns '1929','1929.1','1930','1930.1' under the column '1928' and '1928.1' For the same, I wrote the code as No copying takes places within the columns. How shall I modify my code?? "
            },
            "io": [
                "     1928  1928.1  1929  1929.1  1930  1930.1\n 0    0     0       0     0       0     0\n 1    1     3       3     2       2     2\n 2    4     1       3     0       1     2\n",
                "     1928  1928.1  1929 1929.1 1930 1930.1\n 0   0     0            \n 1   1     3          \n 2   4     1                    \n 3   0     0\n 4   3     2\n 5   3     0\n 6   0     0\n 7   2     2\n 8   1     2 \n"
            ],
            "answer": {
                "ans_desc": "Setup with : ",
                "code": [
                    "cols = ['1929', '1929.1', '1930', '1930.1']\nvals = df[cols].values.reshape(-1, 2)\n"
                ]
            }
        },
        "expected": [
            "values"
        ],
        "predicted": [
            [
                "rename",
                0.8754047751426697
            ],
            [
                "drop",
                0.8692153692245483
            ],
            [
                "columns",
                0.8681604862213135
            ],
            [
                "iloc",
                0.8633797764778137
            ],
            [
                "apply",
                0.8625074625015259
            ],
            [
                "DataFrame",
                0.8614976406097412
            ],
            [
                "values",
                0.8606581687927246
            ],
            [
                "loc",
                0.8606188893318176
            ],
            [
                "query",
                0.8601325154304504
            ],
            [
                "shape",
                0.859069287776947
            ]
        ]
    },
    {
        "id": 50269584,
        "query": "append list from pandas column to python list i have values in a list in pandas column, for example: df but when i append col1 to list i got quote around the first element in each sublist. and i got: but i need:",
        "link": "https://stackoverflow.com/questions/50269584/append-list-from-pandas-column-to-python-list",
        "example": {
            "qid": 50269584,
            "link": "https://stackoverflow.com/questions/50269584/append-list-from-pandas-column-to-python-list",
            "question": {
                "title": "Append list from pandas column to python list",
                "ques_desc": "I have values in a list in pandas column, for example: df But when I append col1 to list I got quote around the first element in each sublist. And I got: But I need: "
            },
            "io": [
                "id       col1\n1       [51.97559, 4.12565]\n2       [52.97559, 3.12565]\n3       [49.97559, 5.12565]\n",
                "[[51.97559, 4.12565]\n[52.97559, 3.12565]\n[49.97559, 5.12565]]\n"
            ],
            "answer": {
                "ans_desc": "You can cast all the elements of the list into ",
                "code": [
                    "new_list = []\nfor val in df['col1'].values:\n    new_list.append(list(map(float, val)))\n"
                ]
            }
        },
        "expected": [
            "append",
            "values"
        ],
        "predicted": [
            [
                "explode",
                0.8797361850738525
            ],
            [
                "values",
                0.8743712902069092
            ],
            [
                "apply",
                0.8738303780555725
            ],
            [
                "DataFrame",
                0.8731139898300171
            ],
            [
                "shape",
                0.8672739267349243
            ],
            [
                "append",
                0.8663044571876526
            ],
            [
                "iloc",
                0.8645187616348267
            ],
            [
                "transpose",
                0.8637595176696777
            ],
            [
                "loc",
                0.8628466725349426
            ],
            [
                "iteritems",
                0.862837553024292
            ]
        ]
    },
    {
        "id": 49073547,
        "query": "sum values in third column while putting together corespondinng values in first and second columns i have 3 columns of data. i have data stored in three columns (k, v, t) in csv. for instance, data: i want to get as the following data. basically, sum all the values of t that has the same k and v. this is the code i have so far: and it keeps going until the end. i use \"for loop\" and \"if\" but it is too long. can i use numpy in a short and clean way? or any other better way?",
        "link": "https://stackoverflow.com/questions/49073547/sum-values-in-third-column-while-putting-together-corespondinng-values-in-first",
        "example": {
            "qid": 49073547,
            "link": "https://stackoverflow.com/questions/49073547/sum-values-in-third-column-while-putting-together-corespondinng-values-in-first",
            "question": {
                "title": "Sum values in third column while putting together corespondinng values in first and second columns",
                "ques_desc": "I have 3 columns of data. I have data stored in three columns (k, v, t) in csv. For instance, Data: I want to get as the following data. Basically, sum all the values of t that has the same k and v. this is the code I have so far: and it keeps going until the end. I use \"for loop\" and \"if\" but it is too long. Can I use numpy in a short and clean way? or any other better way? "
            },
            "io": [
                "k v t    \n\na 1 2    \nb 2 3    \nc 3 4    \na 2 3    \nb 3 2    \nb 3 4    \nc 3 5    \nb 2 3\n",
                "a 1 5\nb 2 6\nb 3 6\nc 3 9\n"
            ],
            "answer": {
                "ans_desc": "Here is one solution using . First create a dataframe, then perform a operation. The below code assumes your data is stored in a csv file. Result ",
                "code": [
                    "df = pd.read_csv('file.csv')\n\ng = df.groupby(['k', 'v'], as_index=False)['t'].sum()\n"
                ]
            }
        },
        "expected": [
            "groupby",
            "sum"
        ],
        "predicted": [
            [
                "sum",
                0.8729445934295654
            ],
            [
                "agg",
                0.8688833713531494
            ],
            [
                "values",
                0.8675629496574402
            ],
            [
                "shape",
                0.866158664226532
            ],
            [
                "apply",
                0.8619961738586426
            ],
            [
                "pivot",
                0.8604826927185059
            ],
            [
                "DataFrame",
                0.8561321496963501
            ],
            [
                "groupby",
                0.8546414375305176
            ],
            [
                "div",
                0.853649377822876
            ],
            [
                "transpose",
                0.853611171245575
            ]
        ]
    },
    {
        "id": 49513688,
        "query": "check if string contains sub string from the same column in pandas dataframe hi i have the following dataframe: i want to check for the strings that contain sub string from this column and create a new column that holds the bigger strings if the condition is full filled something like this: thanks in advance",
        "link": "https://stackoverflow.com/questions/49513688/check-if-string-contains-sub-string-from-the-same-column-in-pandas-dataframe",
        "example": {
            "qid": 49513688,
            "link": "https://stackoverflow.com/questions/49513688/check-if-string-contains-sub-string-from-the-same-column-in-pandas-dataframe",
            "question": {
                "title": "check if string contains sub string from the same column in pandas dataframe",
                "ques_desc": "Hi I have the following dataframe: i want to check for the strings that contain sub string from this column and create a new column that holds the bigger strings if the condition is full filled something like this: Thanks in advance "
            },
            "io": [
                "> df1\n  col1  \n0 donald     \n1 mike\n2 donald trump\n3 trump\n4 mike pence\n5 pence\n6 jarred\n",
                "> df1\n  col1           col2\n0 donald        donald trump\n1 mike          mike pence\n2 donald trump  donald trump\n3 trump         donald trump\n4 mike pence    mike pence\n5 pence         mike pence\n6 jarred        jarred\n"
            ],
            "answer": {
                "ans_desc": "This should do it: ",
                "code": [
                    "df['Col2'] = df['Col1'].apply(lambda x: max([i for i in df['Col1'] if x in i], key=len))\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "join",
                0.8819435238838196
            ],
            [
                "apply",
                0.8786808848381042
            ],
            [
                "any",
                0.8720206022262573
            ],
            [
                "values",
                0.8711550235748291
            ],
            [
                "transpose",
                0.8706663250923157
            ],
            [
                "loc",
                0.8700969219207764
            ],
            [
                "fillna",
                0.8680891394615173
            ],
            [
                "sub",
                0.8677984476089478
            ],
            [
                "eval",
                0.8677809834480286
            ],
            [
                "drop",
                0.8677381873130798
            ]
        ]
    },
    {
        "id": 49301344,
        "query": "pass different columns in pandas dataframe in a custom function in df.apply() say i have a dataframe : i wanna have two new columns that are x * y and x * z: so i define a function (just for example) that takes either the string or the string as an argument to indicate which column i want to multiply with the column x: and apply the function to the dataframe : apparently it is wrong here because i didn't specify the , or . question is, just takes function name, how do i tell it to take the two arguments?",
        "link": "https://stackoverflow.com/questions/49301344/pass-different-columns-in-pandas-dataframe-in-a-custom-function-in-df-apply",
        "example": {
            "qid": 49301344,
            "link": "https://stackoverflow.com/questions/49301344/pass-different-columns-in-pandas-dataframe-in-a-custom-function-in-df-apply",
            "question": {
                "title": "Pass Different Columns in Pandas DataFrame in a Custom Function in df.apply()",
                "ques_desc": "Say I have a dataframe : I wanna have two new columns that are x * y and x * z: So I define a function (just for example) that takes either the string or the string as an argument to indicate which column I want to multiply with the column x: And apply the function to the dataframe : Apparently it is wrong here because I didn't specify the , or . Question is, just takes function name, how do I tell it to take the two arguments? "
            },
            "io": [
                "  x y z\n0 1 2 3\n1 4 5 6\n2 7 8 9\n",
                "  x y z xy xz\n0 1 2 3  2  3\n1 4 5 6 20 24\n2 7 8 9 56 63\n"
            ],
            "answer": {
                "ans_desc": "You can use lambda function with specify columns, but also is necessary change : If is not possible change : ",
                "code": [
                    "def func(row, colName):\n    return row * colName\n\ncols = ['y', 'z']\nfor c in cols:\n    df['x' + c] = df.apply(lambda x: func(x['x'], x[c]), axis=1)\n",
                    "def func(row, colName):\n    return row['x'] * row[colName]\n\ncols = ['y', 'z']\nfor c in cols:\n    df['x' + c] = df.apply(lambda x: func(x, c), axis=1)\n"
                ]
            }
        },
        "expected": [
            "apply"
        ],
        "predicted": [
            [
                "apply",
                0.8819193840026855
            ],
            [
                "shape",
                0.8678907155990601
            ],
            [
                "values",
                0.8677344918251038
            ],
            [
                "loc",
                0.8621546626091003
            ],
            [
                "query",
                0.8602561950683594
            ],
            [
                "rename",
                0.8594295978546143
            ],
            [
                "DataFrame",
                0.8588810563087463
            ],
            [
                "iloc",
                0.8586156368255615
            ],
            [
                "columns",
                0.8583366274833679
            ],
            [
                "sort_index",
                0.8574901819229126
            ]
        ]
    }
]