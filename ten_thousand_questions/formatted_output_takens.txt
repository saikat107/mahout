"qid": 68314886
"link": https://stackoverflow.com/questions/68314886/pandas-loop-through-rows-to-update-column-value
"question": {
	"title": Pandas: Loop through rows to update column value
	"desc": Here is sample dataframe look like: From this dataFrame I want to update value. Condition is when or is not immediate next value of will be replaced by previous value afterthat next point value should be reindexed(cycle .1 to .6). eg. in row index(2) when So, the next value should be also 0.3 instead of 0.4, Then in row index(4) point=0.5 will be replaced by 0.4(continue recursively) OUTPUT I want: Code I tried: 
}
"io": {
	"Frame-1": 
		>>> df
		  point    x      y
		0  0.1   NaN    NaN
		1  0.2   NaN    NaN
		2  0.3   5.0    NaN
		3  0.4   NaN    NaN
		4  0.5   NaN    1.0
		5  0.6   NaN    NaN
		6  0.7   1.0    1.0
		7  0.8   NaN    NaN
		8  0.9   NaN    NaN
		9  1.1   NaN    NaN
		10 1.2   NaN    NaN
		11 1.3   NaN    NaN
		12 1.4   NaN    2.0
		13 1.5   NaN    NaN
		14 1.6   NaN    NaN
		15 1.7   NaN    NaN
		16 0.1   NaN    NaN
		17 0.2   NaN    NaN
		18 0.3   NaN    NaN
		19 0.4   NaN    NaN
		20 0.5   NaN    NaN
		21 0.6   2.0    NaN
		22 0.7   NaN    NaN
		23 1.1   NaN    NaN
		
	"Frame-2":
		  point    x      y
		0  0.1   NaN    NaN
		1  0.2   NaN    NaN
		2  0.3   5.0    NaN
		3  0.3   NaN    NaN
		4  0.4   NaN    1.0
		5  0.4   NaN    NaN
		6  0.5   1.0    1.0
		7  0.5   NaN    NaN
		8  0.6   NaN    NaN
		9  1.1   NaN    NaN
		10 1.2   NaN    NaN
		11 1.3   NaN    NaN
		12 1.4   NaN    2.0
		13 1.4   NaN    NaN
		14 1.5   NaN    NaN
		15 1.6   NaN    NaN
		16 0.1   NaN    NaN
		17 0.2   NaN    NaN
		18 0.3   NaN    NaN
		19 0.4   NaN    NaN
		20 0.5   NaN    NaN
		21 0.6   2.0    NaN
		22 0.6   NaN    NaN
		23 1.1   NaN    NaN
		
}
"answer": {
	"desc": %s Can you try that: 
	"code-snippets": [
		mask = df[['x', 'y']].any(axis=1).shift(1, fill_value=False)
		point = df['point'].astype(int)
		group = point.sub(point.shift(1)).ne(0).cumsum()
		
		df['point'] = df['point'].sub(mask.groupby(group).cumsum().div(10))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68264711
"link": https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting
"question": {
	"title": pd.read_html changed number formatting
	"desc": Cannot get from the column of , after format changed to , and my expected result should be keep HTML code Python Code Execution Result Expected Result 
}
"io": {
	"Frame-1": 
		 [        BBBBBB      CCCCCCC  AAAAAAA
		 0       DDDDDD       123456  1234.56
		 1    EEEEEEEEE       123456  1234.56
		 2    EEEEEEEEE       123456  1234.56
		 3    EEEEEEEEE       123456  1234.56
		 4    FFFFFFFFF       123456  1234.56
		 5    GGGGGGGGG       123456  1234.56
		 6    HHHHHHHHH       123456  1234.56
		 7   IIIIIIIIII       123456  1234.56
		 8     JJJJJJJJ       123456  1234.56
		 9     KKKKKKKK  1/2/3/4/5/6  1234.56
		 10    KKKKKKKK  1/2/3/4/5/6  1234.56]
		
	"Frame-2":
		 [        BBBBBB      CCCCCCC  AAAAAAA
		 0       DDDDDD       1,2,3,4,5,6  1234.56
		 1    EEEEEEEEE       1,2,3,4,5,6  1234.56
		 2    EEEEEEEEE       1,2,3,4,5,6  1234.56
		 3    EEEEEEEEE       1,2,3,4,5,6  1234.56
		 4    FFFFFFFFF       1,2,3,4,5,6  1234.56
		 5    GGGGGGGGG       1,2,3,4,5,6  1234.56
		 6    HHHHHHHHH       1,2,3,4,5,6  1234.56
		 7   IIIIIIIIII       1,2,3,4,5,6  1234.56
		 8     JJJJJJJJ       1,2,3,4,5,6  1234.56
		 9     KKKKKKKK       1/2/3/4/5/6  1234.56
		 10    KKKKKKKK       1/2/3/4/5/6  1234.56]
		 
		
}
"answer": {
	"desc": %s You need to add the parameter and set it to by default it's . OUTPUT: 
	"code-snippets": [
		from bs4 import BeautifulSoup
		import pandas as pd
		
		soup = BeautifulSoup(html,'html.parser')
		table = soup.find('div', attrs={'id':'MMMMMMMM'})
		df_list = pd.read_html(str(table), header=1, thousands=None)
		df_list
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68243146
"link": https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas
"question": {
	"title": replace zero with value of an other column using pandas
	"desc": I have a dataframe df1: I want to replace 0 in the id column with value from ref column of the same row So it will become: 
}
"io": {
	"Frame-1": 
		    ref   Name   id  Score
		  8400   John    0     12
		  3840  Peter  414      0
		  7400  David  612     64
		  5200  Karen    0      0
		
	"Frame-2":
		   ref    Name   id   Score
		  8400   John  8400     12
		  3840  Peter  414      0
		  7400  David  612     64
		  5200  Karen 5200      0
		
}
"answer": {
	"desc": %s via : OR via numpy's : 
	"code-snippets": [
		#import numpy as np
		df['id']=np.where(df['id'].eq(0),df['ref'],df['id'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68231389
"link": https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas
"question": {
	"title": Compare two columns that contains timestamps in pandas
	"desc": Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0 
}
"io": {
	"Frame-1": 
		  Col0       Col1                    Col2                   Col3                   Col4
		   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30
		   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30
		
	"Frame-2":
		  Col0       Col1                    Col2               Col3                   Col4
		   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN
		   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN
		
}
"answer": {
	"desc": %s A straightforward way with boolean mask: 
	"code-snippets": [
		dt = df.select_dtypes('datetime')
		dt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))
		
		df.loc[:, dt.columns.tolist()] = dt
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68231104
"link": https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe
"question": {
	"title": Extract part of a 3 D dataframe
	"desc": I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: 
}
"io": {
	"Frame-1": 
		     d1        d2            d3
		   A B C D...   A B C D...   A B C D..
		0  
		1
		2
		
	"Frame-2":
		    d1    d2    d3
		  A  B   A  B   A  B
		0
		1
		2
		
}
"answer": {
	"desc": %s Use on the level 1 values of columns then select with : : Sample Data Used: 
	"code-snippets": [
		filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]
		
		----------------------------------------------------------------------
		import numpy as np
		import pandas as pd
		
		df = pd.DataFrame(
		    np.arange(1, 25).reshape((-1, 8)),
		    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68229806
"link": https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe
"question": {
	"title": Insert values from variable and DataFrame into another DataFrame
	"desc": On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code 
}
"io": {
	"Frame-1": 
		   id  col0  col1  col2
		0   1   3.0    13    23
		1   1   NaN    14    24
		2   1   NaN    15    25
		
	"Frame-2":
		   id  col0  col1  col2
		0   1   3.0    13    23
		1   1   3.0    14    24
		2   1   3.0    15    25
		
}
"answer": {
	"desc": %s Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: 
	"code-snippets": [
		import pandas as pd
		
		id=1
		df1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})
		df2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})
		
		df2.insert(0, "id", id)
		df2.insert(1, "col0", df1[df1['id']==id]['col0'][0])
		
		print(df2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68213612
"link": https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun
"question": {
	"title": How to combine rows in a dataframe in a pairwise fashion while applying some function
	"desc": I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: 
}
"io": {
	"Frame-1": 
		ID    Val1    Val2
		id0     10      20
		id0     11      19
		id1      5       5
		id1      1       1
		id1      2       4
		
	"Frame-2":
		ID      Val1    Val2
		id0_1   10.5    19.5
		id1_1   3       3
		id1_2   1.5     2.5
		
}
"answer": {
	"desc": %s You can use after grouping by : 
	"code-snippets": [
		out = df.groupby('ID').rolling(2).mean() \
		        .dropna(how='all').reset_index(level=1, drop=True)
		
		out.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68211888
"link": https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base
"question": {
	"title": Loop through multiple small Pandas dataframes and create summary dataframes based on a single column
	"desc": I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. 
}
"io": {
	"Frame-1": 
		NAME     VAL1  VAL2  VAL3
		player1  3     5     7
		player2  2     6     8
		player3  3     6     7
		
		NAME     VAL1  VAL2  VAL3
		player2  5     7     7
		player3  2     6     8
		player5  3     6     7
		
	"Frame-2":
		NAME     VAL1  VAL2  VAL3
		player1  3     5     7
		
		NAME     VAL1  VAL2  VAL3
		player2  2     6     8
		player2  5     7     7
		
		NAME     VAL1  VAL2  VAL3
		player3  3     6     7
		player3  2     6     8
		
		NAME     VAL1  VAL2  VAL3
		player5  3     6     7
		
}
"answer": {
	"desc": %s Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : 
	"code-snippets": [
		dfs = {group_name: df_
		       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}
		
		----------------------------------------------------------------------
		dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68193558
"link": https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values
"question": {
	"title": pandas group many columns to one column where every cell is a list of values
	"desc": I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? 
}
"io": {
	"Frame-1": 
		df = 
		c1 c2 c3 c4 c5
		1.  2. 3. 1. 5
		8.  2. 1. 3. 8
		4.  9. 1  2. 3
		
	"Frame-2":
		df = 
		    l
		[1,2,3,1,5]
		[8,2,1,3,8]
		[4,9,1,2,3]
		
}
"answer": {
	"desc": %s Try: 
	"code-snippets": [
		#best way:
		df['l']=df.values.tolist()
		#OR
		df['l']=df.to_numpy().tolist()
		
		
		#another way:
		df['l']=df.agg(list,1)
		#OR
		df['l']=df.apply(list,1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68193521
"link": https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame
"question": {
	"title": Concatenate values and column names in a data frame to create a new data frame
	"desc": I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used 
}
"io": {
	"Frame-1": 
		  Value col1 col2 col3
		0     a   aa   ab   ac
		1     b   ba   bb   bc
		2     c   ca   cb   cc
		3     d   da   db   dc
		4     e   ea   eb   ec
		
	"Frame-2":
		      Value Col 1
		0   a_Col 1    aa
		1   a_Col 2    ab
		2   a_Col 3    ac
		3   b_Col 1    ba
		4   b_Col 2    bb
		5   b_Col 3    bc
		6   c_Col 1    ca
		7   c_Col 2    cb
		8   c_Col 3    cc
		9   d_Col 1    da
		10  d_Col 2    db
		11  d_Col 3    dc
		12  e_Col 1    ea
		13  e_Col 2    eb
		14  e_Col 3    ec
		
}
"answer": {
	"desc": %s Try: Prints: Optionally, you can sort values afterwards: 
	"code-snippets": [
		x = df.melt("Value", value_name="Col 1")
		x.Value += "_" + x.variable
		x = x.drop(columns="variable")
		print(x)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68174614
"link": https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json
"question": {
	"title": Why does it add .0 to the value while converting Dataframe columns to JSON
	"desc": I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? 
}
"io": {
	"Frame-1": 
		A   B   C   D
		2   6   5   8.0
		6   11  2   3.6 
		1   5   7   5.2
		
	"Frame-2":
		{"A":2.0, "B":6.0, "C":5.0, "D":8.0}
		{"A":6.0, "B":11.0, "C":2.0, "D":3.6}
		{"A":1.0, "B":5.0, "C":7.0, "D":5.2}
		
}
"answer": {
	"desc": %s The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: 
	"code-snippets": [
		df.iloc[0]
		A    2.0
		B    6.0
		C    5.0
		D    8.0
		Name: 0, dtype: float64
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68174113
"link": https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction
"question": {
	"title": Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries
	"desc": I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? 
}
"io": {
	"Frame-1": 
		bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}
		
	"Frame-2":
		ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}
		
}
"answer": {
	"desc": %s A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: 
	"code-snippets": [
		def reclass(group, name):
		    bins = bins_dic[name]
		    ids = ids_dic[name]
		    return pd.cut(group, bins, labels=ids)
		    
		df['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68150020
"link": https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector
"question": {
	"title": Getting first/second/third... value in row of numpy array after nan using vectorization
	"desc": I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. 
}
"io": {
	"Frame-1": 
		f(offset=0)
		
		
		| 0  | 1  |
		| -- | -- |
		| 1  | 25 |
		| 2  | 29 |
		| 3  | 33 |
		| 4  | 31 |
		| 5  | 30 |
		| 6  | 35 |
		| 7  | 31 |
		| 8  | 33 |
		| 9  | 26 |
		| 10 | 27 |
		| 11 | 35 |
		| 12 | 33 |
		| 13 | 28 |
		| 14 | 25 |
		| 15 | 25 |
		| 16 | 26 |
		| 17 | 34 |
		| 18 | 28 |
		| 19 | 34 |
		| 20 | 28 |
		
	"Frame-2":
		f(offset=1)
		
		| 0  | 1   |
		| -- | --- |
		| 1  | nan |
		| 2  | nan |
		| 3  | nan |
		| 4  | 35  |
		| 5  | 34  |
		| 6  | 34  |
		| 7  | 26  |
		| 8  | 25  |
		| 9  | 31  |
		| 10 | 26  |
		| 11 | 25  |
		| 12 | 35  |
		| 13 | 25  |
		| 14 | 25  |
		| 15 | 26  |
		| 16 | 31  |
		| 17 | 29  |
		| 18 | 29  |
		| 19 | 26  |
		| 20 | 30  |
		
}
"answer": {
	"desc": %s Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster 
	"code-snippets": [
		def first_valid(arr, offset=0):
		    m = ~np.isnan(arr)
		    i =  m.argmax(axis=1) + offset
		    iy = np.clip(i, 0, arr.shape[1] - 1)
		
		    vals = arr[np.r_[:arr.shape[0]], iy]
		    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan
		    return vals
		
		----------------------------------------------------------------------
		def first_valid(df, offset=0):
		    return df.stack().groupby(level=0)\
		                     .nth(offset).reindex(df.index)
		
		----------------------------------------------------------------------
		# Sample dataframe for testing purpose
		df_test = pd.concat([df] * 10000, ignore_index=True)
		
		%%timeit # Numpy approach
		_ = first_valid(df_test.to_numpy(), 1)
		# 6.9 ms ± 212 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		
		
		%%timeit # Pandas approach
		_ = first_valid(df_test, 1)
		# 90 ms ± 867 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
		
		
		%%timeit # OP's approach
		_ = f(df_test, 1)
		# 2.03 s ± 183 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57033657
"link": https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe
"question": {
	"title": How to Extract Month Name and Year from Date column of DataFrame
	"desc": I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. 
}
"io": {
	"Frame-1": 
		45    2018-01-01
		73    2018-02-08
		74    2018-02-08
		75    2018-02-08
		76    2018-02-08
		
	"Frame-2":
		45    Jan-2018
		73    Feb-2018
		74    Feb-2018
		75    Feb-2018
		76    Feb-2018
		
}
"answer": {
	"desc": %s Cast you date from object to actual datetime and use dt to access what you need. 
	"code-snippets": [
		import pandas as pd
		
		df = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})
		
		df['Date'] = pd.to_datetime(df['Date'])
		
		# You can format your date as you wish
		df['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')
		
		# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.
		
		print(df['Mon_Year'])
		
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68107298
"link": https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data
"question": {
	"title": How to Create a Correlation Dataframe from already related data
	"desc": I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? 
}
"io": {
	"Frame-1": 
		    0       1       2
		0   English Spanish 0.50
		1   English Russian 0.15
		
	"Frame-2":
		        English Spanish Russian
		English 1       0.5     0.15
		Spanish 0.5     1       -
		Russian 0.15    -       1
		
}
"answer": {
	"desc": %s I assume that the pairs are unique, i.e. if there's , there won't be a . 
	"code-snippets": [
		# Rename the columns. Because working with columns
		# named like numbers is a pain in the butt
		df.columns = ['lang1', 'lang2', 'corr']
		
		# Create the opposite pairs
		tmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)
		
		# Create the identity pairs
		all_lang = np.unique(df[['lang1', 'lang2']])
		tmp2 = pd.DataFrame({
		    'lang1': all_lang,
		    'lang2': all_lang,
		    'corr': 1.0
		})
		
		# Concatenate all 3 together and beat the data frame into shape
		result = (
		    pd.concat([df, tmp1, tmp2], axis=0)
		        .set_index(['lang1', 'lang2'])
		        .unstack()
		        .droplevel(0, axis=1)
		        .rename_axis(index=[None], columns=[None])
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68098150
"link": https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas
"question": {
	"title": set some rule to groupby in pandas
	"desc": I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have "dup by" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. 
}
"io": {
	"Frame-1": 
		1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
	"Frame-2":
		1|F08210020403|GO|2014-05-17 00:00:00|yes
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
}
"answer": {
	"desc": %s IIUC: try: 
	"code-snippets": [
		c=df['keep'].str.contains('dup by')
		#created a condition which check if 'keep' column contains 'dup by' or not
		df['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')
		most_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()
		#excluded those rows in groupby where 'keep' contains 'dup by'
		df['most_recent_date']=df['VIP_ID'].map(most_recent_date)
		df['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])
		df['keep'] = np.where(
		    df['keep'] != 'same tier',df['keep'],
		    (np.where(
		         df['most_recent_date'] == df['datetime'],
		         'yes',
		         'dup by ' + df['VIP_ID'].astype(str)))
		)
		df.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'
		df = df.drop(columns = ['both','most_recent_date'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68264711
"link": https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting
"question": {
	"title": pd.read_html changed number formatting
	"desc": Cannot get from the column of , after format changed to , and my expected result should be keep HTML code Python Code Execution Result Expected Result 
}
"io": {
	"Frame-1": 
		 [        BBBBBB      CCCCCCC  AAAAAAA
		 0       DDDDDD       123456  1234.56
		 1    EEEEEEEEE       123456  1234.56
		 2    EEEEEEEEE       123456  1234.56
		 3    EEEEEEEEE       123456  1234.56
		 4    FFFFFFFFF       123456  1234.56
		 5    GGGGGGGGG       123456  1234.56
		 6    HHHHHHHHH       123456  1234.56
		 7   IIIIIIIIII       123456  1234.56
		 8     JJJJJJJJ       123456  1234.56
		 9     KKKKKKKK  1/2/3/4/5/6  1234.56
		 10    KKKKKKKK  1/2/3/4/5/6  1234.56]
		
	"Frame-2":
		 [        BBBBBB      CCCCCCC  AAAAAAA
		 0       DDDDDD       1,2,3,4,5,6  1234.56
		 1    EEEEEEEEE       1,2,3,4,5,6  1234.56
		 2    EEEEEEEEE       1,2,3,4,5,6  1234.56
		 3    EEEEEEEEE       1,2,3,4,5,6  1234.56
		 4    FFFFFFFFF       1,2,3,4,5,6  1234.56
		 5    GGGGGGGGG       1,2,3,4,5,6  1234.56
		 6    HHHHHHHHH       1,2,3,4,5,6  1234.56
		 7   IIIIIIIIII       1,2,3,4,5,6  1234.56
		 8     JJJJJJJJ       1,2,3,4,5,6  1234.56
		 9     KKKKKKKK       1/2/3/4/5/6  1234.56
		 10    KKKKKKKK       1/2/3/4/5/6  1234.56]
		 
		
}
"answer": {
	"desc": %s You need to add the parameter and set it to by default it's . OUTPUT: 
	"code-snippets": [
		from bs4 import BeautifulSoup
		import pandas as pd
		
		soup = BeautifulSoup(html,'html.parser')
		table = soup.find('div', attrs={'id':'MMMMMMMM'})
		df_list = pd.read_html(str(table), header=1, thousands=None)
		df_list
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68243146
"link": https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas
"question": {
	"title": replace zero with value of an other column using pandas
	"desc": I have a dataframe df1: I want to replace 0 in the id column with value from ref column of the same row So it will become: 
}
"io": {
	"Frame-1": 
		    ref   Name   id  Score
		  8400   John    0     12
		  3840  Peter  414      0
		  7400  David  612     64
		  5200  Karen    0      0
		
	"Frame-2":
		   ref    Name   id   Score
		  8400   John  8400     12
		  3840  Peter  414      0
		  7400  David  612     64
		  5200  Karen 5200      0
		
}
"answer": {
	"desc": %s via : OR via numpy's : 
	"code-snippets": [
		#import numpy as np
		df['id']=np.where(df['id'].eq(0),df['ref'],df['id'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68231389
"link": https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas
"question": {
	"title": Compare two columns that contains timestamps in pandas
	"desc": Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0 
}
"io": {
	"Frame-1": 
		  Col0       Col1                    Col2                   Col3                   Col4
		   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30
		   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30
		
	"Frame-2":
		  Col0       Col1                    Col2               Col3                   Col4
		   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN
		   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN
		
}
"answer": {
	"desc": %s A straightforward way with boolean mask: 
	"code-snippets": [
		dt = df.select_dtypes('datetime')
		dt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))
		
		df.loc[:, dt.columns.tolist()] = dt
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68231104
"link": https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe
"question": {
	"title": Extract part of a 3 D dataframe
	"desc": I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: 
}
"io": {
	"Frame-1": 
		     d1        d2            d3
		   A B C D...   A B C D...   A B C D..
		0  
		1
		2
		
	"Frame-2":
		    d1    d2    d3
		  A  B   A  B   A  B
		0
		1
		2
		
}
"answer": {
	"desc": %s Use on the level 1 values of columns then select with : : Sample Data Used: 
	"code-snippets": [
		filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]
		
		----------------------------------------------------------------------
		import numpy as np
		import pandas as pd
		
		df = pd.DataFrame(
		    np.arange(1, 25).reshape((-1, 8)),
		    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68229806
"link": https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe
"question": {
	"title": Insert values from variable and DataFrame into another DataFrame
	"desc": On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code 
}
"io": {
	"Frame-1": 
		   id  col0  col1  col2
		0   1   3.0    13    23
		1   1   NaN    14    24
		2   1   NaN    15    25
		
	"Frame-2":
		   id  col0  col1  col2
		0   1   3.0    13    23
		1   1   3.0    14    24
		2   1   3.0    15    25
		
}
"answer": {
	"desc": %s Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: 
	"code-snippets": [
		import pandas as pd
		
		id=1
		df1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})
		df2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})
		
		df2.insert(0, "id", id)
		df2.insert(1, "col0", df1[df1['id']==id]['col0'][0])
		
		print(df2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68213612
"link": https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun
"question": {
	"title": How to combine rows in a dataframe in a pairwise fashion while applying some function
	"desc": I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: 
}
"io": {
	"Frame-1": 
		ID    Val1    Val2
		id0     10      20
		id0     11      19
		id1      5       5
		id1      1       1
		id1      2       4
		
	"Frame-2":
		ID      Val1    Val2
		id0_1   10.5    19.5
		id1_1   3       3
		id1_2   1.5     2.5
		
}
"answer": {
	"desc": %s You can use after grouping by : 
	"code-snippets": [
		out = df.groupby('ID').rolling(2).mean() \
		        .dropna(how='all').reset_index(level=1, drop=True)
		
		out.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68211888
"link": https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base
"question": {
	"title": Loop through multiple small Pandas dataframes and create summary dataframes based on a single column
	"desc": I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. 
}
"io": {
	"Frame-1": 
		NAME     VAL1  VAL2  VAL3
		player1  3     5     7
		player2  2     6     8
		player3  3     6     7
		
		NAME     VAL1  VAL2  VAL3
		player2  5     7     7
		player3  2     6     8
		player5  3     6     7
		
	"Frame-2":
		NAME     VAL1  VAL2  VAL3
		player1  3     5     7
		
		NAME     VAL1  VAL2  VAL3
		player2  2     6     8
		player2  5     7     7
		
		NAME     VAL1  VAL2  VAL3
		player3  3     6     7
		player3  2     6     8
		
		NAME     VAL1  VAL2  VAL3
		player5  3     6     7
		
}
"answer": {
	"desc": %s Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : 
	"code-snippets": [
		dfs = {group_name: df_
		       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}
		
		----------------------------------------------------------------------
		dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68193558
"link": https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values
"question": {
	"title": pandas group many columns to one column where every cell is a list of values
	"desc": I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? 
}
"io": {
	"Frame-1": 
		df = 
		c1 c2 c3 c4 c5
		1.  2. 3. 1. 5
		8.  2. 1. 3. 8
		4.  9. 1  2. 3
		
	"Frame-2":
		df = 
		    l
		[1,2,3,1,5]
		[8,2,1,3,8]
		[4,9,1,2,3]
		
}
"answer": {
	"desc": %s Try: 
	"code-snippets": [
		#best way:
		df['l']=df.values.tolist()
		#OR
		df['l']=df.to_numpy().tolist()
		
		
		#another way:
		df['l']=df.agg(list,1)
		#OR
		df['l']=df.apply(list,1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68193521
"link": https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame
"question": {
	"title": Concatenate values and column names in a data frame to create a new data frame
	"desc": I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used 
}
"io": {
	"Frame-1": 
		  Value col1 col2 col3
		0     a   aa   ab   ac
		1     b   ba   bb   bc
		2     c   ca   cb   cc
		3     d   da   db   dc
		4     e   ea   eb   ec
		
	"Frame-2":
		      Value Col 1
		0   a_Col 1    aa
		1   a_Col 2    ab
		2   a_Col 3    ac
		3   b_Col 1    ba
		4   b_Col 2    bb
		5   b_Col 3    bc
		6   c_Col 1    ca
		7   c_Col 2    cb
		8   c_Col 3    cc
		9   d_Col 1    da
		10  d_Col 2    db
		11  d_Col 3    dc
		12  e_Col 1    ea
		13  e_Col 2    eb
		14  e_Col 3    ec
		
}
"answer": {
	"desc": %s Try: Prints: Optionally, you can sort values afterwards: 
	"code-snippets": [
		x = df.melt("Value", value_name="Col 1")
		x.Value += "_" + x.variable
		x = x.drop(columns="variable")
		print(x)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68174614
"link": https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json
"question": {
	"title": Why does it add .0 to the value while converting Dataframe columns to JSON
	"desc": I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? 
}
"io": {
	"Frame-1": 
		A   B   C   D
		2   6   5   8.0
		6   11  2   3.6 
		1   5   7   5.2
		
	"Frame-2":
		{"A":2.0, "B":6.0, "C":5.0, "D":8.0}
		{"A":6.0, "B":11.0, "C":2.0, "D":3.6}
		{"A":1.0, "B":5.0, "C":7.0, "D":5.2}
		
}
"answer": {
	"desc": %s The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: 
	"code-snippets": [
		df.iloc[0]
		A    2.0
		B    6.0
		C    5.0
		D    8.0
		Name: 0, dtype: float64
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68174113
"link": https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction
"question": {
	"title": Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries
	"desc": I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? 
}
"io": {
	"Frame-1": 
		bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}
		
	"Frame-2":
		ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}
		
}
"answer": {
	"desc": %s A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: 
	"code-snippets": [
		def reclass(group, name):
		    bins = bins_dic[name]
		    ids = ids_dic[name]
		    return pd.cut(group, bins, labels=ids)
		    
		df['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68150020
"link": https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector
"question": {
	"title": Getting first/second/third... value in row of numpy array after nan using vectorization
	"desc": I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. 
}
"io": {
	"Frame-1": 
		f(offset=0)
		
		
		| 0  | 1  |
		| -- | -- |
		| 1  | 25 |
		| 2  | 29 |
		| 3  | 33 |
		| 4  | 31 |
		| 5  | 30 |
		| 6  | 35 |
		| 7  | 31 |
		| 8  | 33 |
		| 9  | 26 |
		| 10 | 27 |
		| 11 | 35 |
		| 12 | 33 |
		| 13 | 28 |
		| 14 | 25 |
		| 15 | 25 |
		| 16 | 26 |
		| 17 | 34 |
		| 18 | 28 |
		| 19 | 34 |
		| 20 | 28 |
		
	"Frame-2":
		f(offset=1)
		
		| 0  | 1   |
		| -- | --- |
		| 1  | nan |
		| 2  | nan |
		| 3  | nan |
		| 4  | 35  |
		| 5  | 34  |
		| 6  | 34  |
		| 7  | 26  |
		| 8  | 25  |
		| 9  | 31  |
		| 10 | 26  |
		| 11 | 25  |
		| 12 | 35  |
		| 13 | 25  |
		| 14 | 25  |
		| 15 | 26  |
		| 16 | 31  |
		| 17 | 29  |
		| 18 | 29  |
		| 19 | 26  |
		| 20 | 30  |
		
}
"answer": {
	"desc": %s Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster 
	"code-snippets": [
		def first_valid(arr, offset=0):
		    m = ~np.isnan(arr)
		    i =  m.argmax(axis=1) + offset
		    iy = np.clip(i, 0, arr.shape[1] - 1)
		
		    vals = arr[np.r_[:arr.shape[0]], iy]
		    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan
		    return vals
		
		----------------------------------------------------------------------
		def first_valid(df, offset=0):
		    return df.stack().groupby(level=0)\
		                     .nth(offset).reindex(df.index)
		
		----------------------------------------------------------------------
		# Sample dataframe for testing purpose
		df_test = pd.concat([df] * 10000, ignore_index=True)
		
		%%timeit # Numpy approach
		_ = first_valid(df_test.to_numpy(), 1)
		# 6.9 ms ± 212 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		
		
		%%timeit # Pandas approach
		_ = first_valid(df_test, 1)
		# 90 ms ± 867 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
		
		
		%%timeit # OP's approach
		_ = f(df_test, 1)
		# 2.03 s ± 183 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57033657
"link": https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe
"question": {
	"title": How to Extract Month Name and Year from Date column of DataFrame
	"desc": I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. 
}
"io": {
	"Frame-1": 
		45    2018-01-01
		73    2018-02-08
		74    2018-02-08
		75    2018-02-08
		76    2018-02-08
		
	"Frame-2":
		45    Jan-2018
		73    Feb-2018
		74    Feb-2018
		75    Feb-2018
		76    Feb-2018
		
}
"answer": {
	"desc": %s Cast you date from object to actual datetime and use dt to access what you need. 
	"code-snippets": [
		import pandas as pd
		
		df = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})
		
		df['Date'] = pd.to_datetime(df['Date'])
		
		# You can format your date as you wish
		df['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')
		
		# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.
		
		print(df['Mon_Year'])
		
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68107298
"link": https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data
"question": {
	"title": How to Create a Correlation Dataframe from already related data
	"desc": I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? 
}
"io": {
	"Frame-1": 
		    0       1       2
		0   English Spanish 0.50
		1   English Russian 0.15
		
	"Frame-2":
		        English Spanish Russian
		English 1       0.5     0.15
		Spanish 0.5     1       -
		Russian 0.15    -       1
		
}
"answer": {
	"desc": %s I assume that the pairs are unique, i.e. if there's , there won't be a . 
	"code-snippets": [
		# Rename the columns. Because working with columns
		# named like numbers is a pain in the butt
		df.columns = ['lang1', 'lang2', 'corr']
		
		# Create the opposite pairs
		tmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)
		
		# Create the identity pairs
		all_lang = np.unique(df[['lang1', 'lang2']])
		tmp2 = pd.DataFrame({
		    'lang1': all_lang,
		    'lang2': all_lang,
		    'corr': 1.0
		})
		
		# Concatenate all 3 together and beat the data frame into shape
		result = (
		    pd.concat([df, tmp1, tmp2], axis=0)
		        .set_index(['lang1', 'lang2'])
		        .unstack()
		        .droplevel(0, axis=1)
		        .rename_axis(index=[None], columns=[None])
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68098150
"link": https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas
"question": {
	"title": set some rule to groupby in pandas
	"desc": I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have "dup by" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. 
}
"io": {
	"Frame-1": 
		1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
	"Frame-2":
		1|F08210020403|GO|2014-05-17 00:00:00|yes
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
}
"answer": {
	"desc": %s IIUC: try: 
	"code-snippets": [
		c=df['keep'].str.contains('dup by')
		#created a condition which check if 'keep' column contains 'dup by' or not
		df['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')
		most_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()
		#excluded those rows in groupby where 'keep' contains 'dup by'
		df['most_recent_date']=df['VIP_ID'].map(most_recent_date)
		df['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])
		df['keep'] = np.where(
		    df['keep'] != 'same tier',df['keep'],
		    (np.where(
		         df['most_recent_date'] == df['datetime'],
		         'yes',
		         'dup by ' + df['VIP_ID'].astype(str)))
		)
		df.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'
		df = df.drop(columns = ['both','most_recent_date'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68090463
"link": https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas
"question": {
	"title": Merging more than two columns of the same dataframe in pandas
	"desc": Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? 
}
"io": {
	"Frame-1": 
		VAR 1   VAR 2   VAR 3   GROUP 
		                3   [0-10]
		1               3   [0-10]
		1               3   [0-10]
		1       2           [0-10]
		        2           [0-10]
		3              3    [10-20]
		3       1           [10-20]
		        1           [10-20]
		        2           [10-20]
		               2    [10-20]
		               2    [10-20]
		
	"Frame-2":
		VAR_MERGED  GROUP 
		1           [0-10]
		1           [0-10]
		1           [0-10]
		2           [0-10]
		2           [0-10]
		3           [0-10]
		3           [0-10]
		3           [0-10]
		1           [10-20]
		1           [10-20]
		2           [10-20]
		2           [10-20]
		2           [10-20]
		3           [10-20]
		3           [10-20]
		3           [10-20]
		
}
"answer": {
	"desc": %s Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: 
	"code-snippets": [
		id_vars = df.columns[~df.columns.str.startswith('VAR')]
		
		out = df.melt(id_vars, value_name='VAR_MERGED') \
		        .dropna() \
		        .sort_values(['GROUP', 'VAR_MERGED']) \
		        .reset_index(drop=True) \
		        [['VAR_MERGED'] + id_vars]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68231104
"link": https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe
"question": {
	"title": Extract part of a 3 D dataframe
	"desc": I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: 
}
"io": {
	"Frame-1": 
		     d1        d2            d3
		   A B C D...   A B C D...   A B C D..
		0  
		1
		2
		
	"Frame-2":
		    d1    d2    d3
		  A  B   A  B   A  B
		0
		1
		2
		
}
"answer": {
	"desc": %s Use on the level 1 values of columns then select with : : Sample Data Used: 
	"code-snippets": [
		filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]
		
		----------------------------------------------------------------------
		import numpy as np
		import pandas as pd
		
		df = pd.DataFrame(
		    np.arange(1, 25).reshape((-1, 8)),
		    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68229806
"link": https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe
"question": {
	"title": Insert values from variable and DataFrame into another DataFrame
	"desc": On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code 
}
"io": {
	"Frame-1": 
		   id  col0  col1  col2
		0   1   3.0    13    23
		1   1   NaN    14    24
		2   1   NaN    15    25
		
	"Frame-2":
		   id  col0  col1  col2
		0   1   3.0    13    23
		1   1   3.0    14    24
		2   1   3.0    15    25
		
}
"answer": {
	"desc": %s Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: 
	"code-snippets": [
		import pandas as pd
		
		id=1
		df1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})
		df2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})
		
		df2.insert(0, "id", id)
		df2.insert(1, "col0", df1[df1['id']==id]['col0'][0])
		
		print(df2)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68213612
"link": https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun
"question": {
	"title": How to combine rows in a dataframe in a pairwise fashion while applying some function
	"desc": I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: 
}
"io": {
	"Frame-1": 
		ID    Val1    Val2
		id0     10      20
		id0     11      19
		id1      5       5
		id1      1       1
		id1      2       4
		
	"Frame-2":
		ID      Val1    Val2
		id0_1   10.5    19.5
		id1_1   3       3
		id1_2   1.5     2.5
		
}
"answer": {
	"desc": %s You can use after grouping by : 
	"code-snippets": [
		out = df.groupby('ID').rolling(2).mean() \
		        .dropna(how='all').reset_index(level=1, drop=True)
		
		out.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68211888
"link": https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base
"question": {
	"title": Loop through multiple small Pandas dataframes and create summary dataframes based on a single column
	"desc": I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. 
}
"io": {
	"Frame-1": 
		NAME     VAL1  VAL2  VAL3
		player1  3     5     7
		player2  2     6     8
		player3  3     6     7
		
		NAME     VAL1  VAL2  VAL3
		player2  5     7     7
		player3  2     6     8
		player5  3     6     7
		
	"Frame-2":
		NAME     VAL1  VAL2  VAL3
		player1  3     5     7
		
		NAME     VAL1  VAL2  VAL3
		player2  2     6     8
		player2  5     7     7
		
		NAME     VAL1  VAL2  VAL3
		player3  3     6     7
		player3  2     6     8
		
		NAME     VAL1  VAL2  VAL3
		player5  3     6     7
		
}
"answer": {
	"desc": %s Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : 
	"code-snippets": [
		dfs = {group_name: df_
		       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}
		
		----------------------------------------------------------------------
		dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68193558
"link": https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values
"question": {
	"title": pandas group many columns to one column where every cell is a list of values
	"desc": I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? 
}
"io": {
	"Frame-1": 
		df = 
		c1 c2 c3 c4 c5
		1.  2. 3. 1. 5
		8.  2. 1. 3. 8
		4.  9. 1  2. 3
		
	"Frame-2":
		df = 
		    l
		[1,2,3,1,5]
		[8,2,1,3,8]
		[4,9,1,2,3]
		
}
"answer": {
	"desc": %s Try: 
	"code-snippets": [
		#best way:
		df['l']=df.values.tolist()
		#OR
		df['l']=df.to_numpy().tolist()
		
		
		#another way:
		df['l']=df.agg(list,1)
		#OR
		df['l']=df.apply(list,1)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68193521
"link": https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame
"question": {
	"title": Concatenate values and column names in a data frame to create a new data frame
	"desc": I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used 
}
"io": {
	"Frame-1": 
		  Value col1 col2 col3
		0     a   aa   ab   ac
		1     b   ba   bb   bc
		2     c   ca   cb   cc
		3     d   da   db   dc
		4     e   ea   eb   ec
		
	"Frame-2":
		      Value Col 1
		0   a_Col 1    aa
		1   a_Col 2    ab
		2   a_Col 3    ac
		3   b_Col 1    ba
		4   b_Col 2    bb
		5   b_Col 3    bc
		6   c_Col 1    ca
		7   c_Col 2    cb
		8   c_Col 3    cc
		9   d_Col 1    da
		10  d_Col 2    db
		11  d_Col 3    dc
		12  e_Col 1    ea
		13  e_Col 2    eb
		14  e_Col 3    ec
		
}
"answer": {
	"desc": %s Try: Prints: Optionally, you can sort values afterwards: 
	"code-snippets": [
		x = df.melt("Value", value_name="Col 1")
		x.Value += "_" + x.variable
		x = x.drop(columns="variable")
		print(x)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68174614
"link": https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json
"question": {
	"title": Why does it add .0 to the value while converting Dataframe columns to JSON
	"desc": I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? 
}
"io": {
	"Frame-1": 
		A   B   C   D
		2   6   5   8.0
		6   11  2   3.6 
		1   5   7   5.2
		
	"Frame-2":
		{"A":2.0, "B":6.0, "C":5.0, "D":8.0}
		{"A":6.0, "B":11.0, "C":2.0, "D":3.6}
		{"A":1.0, "B":5.0, "C":7.0, "D":5.2}
		
}
"answer": {
	"desc": %s The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: 
	"code-snippets": [
		df.iloc[0]
		A    2.0
		B    6.0
		C    5.0
		D    8.0
		Name: 0, dtype: float64
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68174113
"link": https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction
"question": {
	"title": Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries
	"desc": I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? 
}
"io": {
	"Frame-1": 
		bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}
		
	"Frame-2":
		ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}
		
}
"answer": {
	"desc": %s A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: 
	"code-snippets": [
		def reclass(group, name):
		    bins = bins_dic[name]
		    ids = ids_dic[name]
		    return pd.cut(group, bins, labels=ids)
		    
		df['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68150020
"link": https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector
"question": {
	"title": Getting first/second/third... value in row of numpy array after nan using vectorization
	"desc": I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. 
}
"io": {
	"Frame-1": 
		f(offset=0)
		
		
		| 0  | 1  |
		| -- | -- |
		| 1  | 25 |
		| 2  | 29 |
		| 3  | 33 |
		| 4  | 31 |
		| 5  | 30 |
		| 6  | 35 |
		| 7  | 31 |
		| 8  | 33 |
		| 9  | 26 |
		| 10 | 27 |
		| 11 | 35 |
		| 12 | 33 |
		| 13 | 28 |
		| 14 | 25 |
		| 15 | 25 |
		| 16 | 26 |
		| 17 | 34 |
		| 18 | 28 |
		| 19 | 34 |
		| 20 | 28 |
		
	"Frame-2":
		f(offset=1)
		
		| 0  | 1   |
		| -- | --- |
		| 1  | nan |
		| 2  | nan |
		| 3  | nan |
		| 4  | 35  |
		| 5  | 34  |
		| 6  | 34  |
		| 7  | 26  |
		| 8  | 25  |
		| 9  | 31  |
		| 10 | 26  |
		| 11 | 25  |
		| 12 | 35  |
		| 13 | 25  |
		| 14 | 25  |
		| 15 | 26  |
		| 16 | 31  |
		| 17 | 29  |
		| 18 | 29  |
		| 19 | 26  |
		| 20 | 30  |
		
}
"answer": {
	"desc": %s Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster 
	"code-snippets": [
		def first_valid(arr, offset=0):
		    m = ~np.isnan(arr)
		    i =  m.argmax(axis=1) + offset
		    iy = np.clip(i, 0, arr.shape[1] - 1)
		
		    vals = arr[np.r_[:arr.shape[0]], iy]
		    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan
		    return vals
		
		----------------------------------------------------------------------
		def first_valid(df, offset=0):
		    return df.stack().groupby(level=0)\
		                     .nth(offset).reindex(df.index)
		
		----------------------------------------------------------------------
		# Sample dataframe for testing purpose
		df_test = pd.concat([df] * 10000, ignore_index=True)
		
		%%timeit # Numpy approach
		_ = first_valid(df_test.to_numpy(), 1)
		# 6.9 ms ± 212 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		
		
		%%timeit # Pandas approach
		_ = first_valid(df_test, 1)
		# 90 ms ± 867 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
		
		
		%%timeit # OP's approach
		_ = f(df_test, 1)
		# 2.03 s ± 183 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57033657
"link": https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe
"question": {
	"title": How to Extract Month Name and Year from Date column of DataFrame
	"desc": I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. 
}
"io": {
	"Frame-1": 
		45    2018-01-01
		73    2018-02-08
		74    2018-02-08
		75    2018-02-08
		76    2018-02-08
		
	"Frame-2":
		45    Jan-2018
		73    Feb-2018
		74    Feb-2018
		75    Feb-2018
		76    Feb-2018
		
}
"answer": {
	"desc": %s Cast you date from object to actual datetime and use dt to access what you need. 
	"code-snippets": [
		import pandas as pd
		
		df = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})
		
		df['Date'] = pd.to_datetime(df['Date'])
		
		# You can format your date as you wish
		df['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')
		
		# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.
		
		print(df['Mon_Year'])
		
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68107298
"link": https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data
"question": {
	"title": How to Create a Correlation Dataframe from already related data
	"desc": I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? 
}
"io": {
	"Frame-1": 
		    0       1       2
		0   English Spanish 0.50
		1   English Russian 0.15
		
	"Frame-2":
		        English Spanish Russian
		English 1       0.5     0.15
		Spanish 0.5     1       -
		Russian 0.15    -       1
		
}
"answer": {
	"desc": %s I assume that the pairs are unique, i.e. if there's , there won't be a . 
	"code-snippets": [
		# Rename the columns. Because working with columns
		# named like numbers is a pain in the butt
		df.columns = ['lang1', 'lang2', 'corr']
		
		# Create the opposite pairs
		tmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)
		
		# Create the identity pairs
		all_lang = np.unique(df[['lang1', 'lang2']])
		tmp2 = pd.DataFrame({
		    'lang1': all_lang,
		    'lang2': all_lang,
		    'corr': 1.0
		})
		
		# Concatenate all 3 together and beat the data frame into shape
		result = (
		    pd.concat([df, tmp1, tmp2], axis=0)
		        .set_index(['lang1', 'lang2'])
		        .unstack()
		        .droplevel(0, axis=1)
		        .rename_axis(index=[None], columns=[None])
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68098150
"link": https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas
"question": {
	"title": set some rule to groupby in pandas
	"desc": I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have "dup by" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. 
}
"io": {
	"Frame-1": 
		1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
	"Frame-2":
		1|F08210020403|GO|2014-05-17 00:00:00|yes
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
}
"answer": {
	"desc": %s IIUC: try: 
	"code-snippets": [
		c=df['keep'].str.contains('dup by')
		#created a condition which check if 'keep' column contains 'dup by' or not
		df['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')
		most_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()
		#excluded those rows in groupby where 'keep' contains 'dup by'
		df['most_recent_date']=df['VIP_ID'].map(most_recent_date)
		df['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])
		df['keep'] = np.where(
		    df['keep'] != 'same tier',df['keep'],
		    (np.where(
		         df['most_recent_date'] == df['datetime'],
		         'yes',
		         'dup by ' + df['VIP_ID'].astype(str)))
		)
		df.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'
		df = df.drop(columns = ['both','most_recent_date'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68090463
"link": https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas
"question": {
	"title": Merging more than two columns of the same dataframe in pandas
	"desc": Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? 
}
"io": {
	"Frame-1": 
		VAR 1   VAR 2   VAR 3   GROUP 
		                3   [0-10]
		1               3   [0-10]
		1               3   [0-10]
		1       2           [0-10]
		        2           [0-10]
		3              3    [10-20]
		3       1           [10-20]
		        1           [10-20]
		        2           [10-20]
		               2    [10-20]
		               2    [10-20]
		
	"Frame-2":
		VAR_MERGED  GROUP 
		1           [0-10]
		1           [0-10]
		1           [0-10]
		2           [0-10]
		2           [0-10]
		3           [0-10]
		3           [0-10]
		3           [0-10]
		1           [10-20]
		1           [10-20]
		2           [10-20]
		2           [10-20]
		2           [10-20]
		3           [10-20]
		3           [10-20]
		3           [10-20]
		
}
"answer": {
	"desc": %s Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: 
	"code-snippets": [
		id_vars = df.columns[~df.columns.str.startswith('VAR')]
		
		out = df.melt(id_vars, value_name='VAR_MERGED') \
		        .dropna() \
		        .sort_values(['GROUP', 'VAR_MERGED']) \
		        .reset_index(drop=True) \
		        [['VAR_MERGED'] + id_vars]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 33130586
"link": https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val
"question": {
	"title": python pandas - creating a column which keeps a running count of consecutive values
	"desc": I am trying to create a column (“consec”) which will keep a running count of consecutive values in another (“binary”) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. 
}
"io": {
	"Frame-1": 
		.    binary consec
		1       0      0
		2       1      1
		3       1      2
		4       1      3
		5       1      4
		5       0      0
		6       1      1
		7       1      2
		8       0      0
		
	"Frame-2":
		.  binary   consec
		0     1       NaN
		1     1       1
		2     1       1
		3     0       0
		4     1       1
		5     0       0
		6     1       1
		7     1       1
		8     1       1
		9     0       0
		
}
"answer": {
	"desc": %s You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. 
	"code-snippets": [
		>>> (df["binary"] == 0)
		0     True
		1    False
		2    False
		3    False
		4     True
		5     True
		6    False
		7    False
		8     True
		Name: binary, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62099066
"link": https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da
"question": {
	"title": is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?
	"desc": I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: 
}
"io": {
	"Frame-1": 
		(a) (b) (c) (d)
		a1  b1  c1  d1
		a2  b2  c2  d2
		
	"Frame-2":
		(a) (b) (e) (f)
		a3  b3  e1  f1
		a4  b4  e1  f1
		a5  b5  e2  f2
		a6  b6  e4  f4
		a7  a8  e4  f5
		
}
"answer": {
	"desc": %s Here you go: Check for the output. 
	"code-snippets": [
		import pandas as pd
		from pandas import ExcelWriter
		
		def detect_duplicate(group):
		    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)
		    return group
		
		with ExcelWriter('output.xlsx') as output:
		    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():
		        df = df.drop(['a', 'b'], axis=1)
		        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)
		        df = df.groupby(['concat']).apply(detect_duplicate)
		        df = df.drop_duplicates(keep='last', subset=['concat'])
		        df.to_excel(output, sheet_name=sheet_name, index=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68174614
"link": https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json
"question": {
	"title": Why does it add .0 to the value while converting Dataframe columns to JSON
	"desc": I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? 
}
"io": {
	"Frame-1": 
		A   B   C   D
		2   6   5   8.0
		6   11  2   3.6 
		1   5   7   5.2
		
	"Frame-2":
		{"A":2.0, "B":6.0, "C":5.0, "D":8.0}
		{"A":6.0, "B":11.0, "C":2.0, "D":3.6}
		{"A":1.0, "B":5.0, "C":7.0, "D":5.2}
		
}
"answer": {
	"desc": %s The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: 
	"code-snippets": [
		df.iloc[0]
		A    2.0
		B    6.0
		C    5.0
		D    8.0
		Name: 0, dtype: float64
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68174113
"link": https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction
"question": {
	"title": Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries
	"desc": I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? 
}
"io": {
	"Frame-1": 
		bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}
		
	"Frame-2":
		ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}
		
}
"answer": {
	"desc": %s A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: 
	"code-snippets": [
		def reclass(group, name):
		    bins = bins_dic[name]
		    ids = ids_dic[name]
		    return pd.cut(group, bins, labels=ids)
		    
		df['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68150020
"link": https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector
"question": {
	"title": Getting first/second/third... value in row of numpy array after nan using vectorization
	"desc": I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. 
}
"io": {
	"Frame-1": 
		f(offset=0)
		
		
		| 0  | 1  |
		| -- | -- |
		| 1  | 25 |
		| 2  | 29 |
		| 3  | 33 |
		| 4  | 31 |
		| 5  | 30 |
		| 6  | 35 |
		| 7  | 31 |
		| 8  | 33 |
		| 9  | 26 |
		| 10 | 27 |
		| 11 | 35 |
		| 12 | 33 |
		| 13 | 28 |
		| 14 | 25 |
		| 15 | 25 |
		| 16 | 26 |
		| 17 | 34 |
		| 18 | 28 |
		| 19 | 34 |
		| 20 | 28 |
		
	"Frame-2":
		f(offset=1)
		
		| 0  | 1   |
		| -- | --- |
		| 1  | nan |
		| 2  | nan |
		| 3  | nan |
		| 4  | 35  |
		| 5  | 34  |
		| 6  | 34  |
		| 7  | 26  |
		| 8  | 25  |
		| 9  | 31  |
		| 10 | 26  |
		| 11 | 25  |
		| 12 | 35  |
		| 13 | 25  |
		| 14 | 25  |
		| 15 | 26  |
		| 16 | 31  |
		| 17 | 29  |
		| 18 | 29  |
		| 19 | 26  |
		| 20 | 30  |
		
}
"answer": {
	"desc": %s Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster 
	"code-snippets": [
		def first_valid(arr, offset=0):
		    m = ~np.isnan(arr)
		    i =  m.argmax(axis=1) + offset
		    iy = np.clip(i, 0, arr.shape[1] - 1)
		
		    vals = arr[np.r_[:arr.shape[0]], iy]
		    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan
		    return vals
		
		----------------------------------------------------------------------
		def first_valid(df, offset=0):
		    return df.stack().groupby(level=0)\
		                     .nth(offset).reindex(df.index)
		
		----------------------------------------------------------------------
		# Sample dataframe for testing purpose
		df_test = pd.concat([df] * 10000, ignore_index=True)
		
		%%timeit # Numpy approach
		_ = first_valid(df_test.to_numpy(), 1)
		# 6.9 ms ± 212 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
		
		
		%%timeit # Pandas approach
		_ = first_valid(df_test, 1)
		# 90 ms ± 867 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
		
		
		%%timeit # OP's approach
		_ = f(df_test, 1)
		# 2.03 s ± 183 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57033657
"link": https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe
"question": {
	"title": How to Extract Month Name and Year from Date column of DataFrame
	"desc": I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. 
}
"io": {
	"Frame-1": 
		45    2018-01-01
		73    2018-02-08
		74    2018-02-08
		75    2018-02-08
		76    2018-02-08
		
	"Frame-2":
		45    Jan-2018
		73    Feb-2018
		74    Feb-2018
		75    Feb-2018
		76    Feb-2018
		
}
"answer": {
	"desc": %s Cast you date from object to actual datetime and use dt to access what you need. 
	"code-snippets": [
		import pandas as pd
		
		df = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})
		
		df['Date'] = pd.to_datetime(df['Date'])
		
		# You can format your date as you wish
		df['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')
		
		# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.
		
		print(df['Mon_Year'])
		
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68107298
"link": https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data
"question": {
	"title": How to Create a Correlation Dataframe from already related data
	"desc": I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? 
}
"io": {
	"Frame-1": 
		    0       1       2
		0   English Spanish 0.50
		1   English Russian 0.15
		
	"Frame-2":
		        English Spanish Russian
		English 1       0.5     0.15
		Spanish 0.5     1       -
		Russian 0.15    -       1
		
}
"answer": {
	"desc": %s I assume that the pairs are unique, i.e. if there's , there won't be a . 
	"code-snippets": [
		# Rename the columns. Because working with columns
		# named like numbers is a pain in the butt
		df.columns = ['lang1', 'lang2', 'corr']
		
		# Create the opposite pairs
		tmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)
		
		# Create the identity pairs
		all_lang = np.unique(df[['lang1', 'lang2']])
		tmp2 = pd.DataFrame({
		    'lang1': all_lang,
		    'lang2': all_lang,
		    'corr': 1.0
		})
		
		# Concatenate all 3 together and beat the data frame into shape
		result = (
		    pd.concat([df, tmp1, tmp2], axis=0)
		        .set_index(['lang1', 'lang2'])
		        .unstack()
		        .droplevel(0, axis=1)
		        .rename_axis(index=[None], columns=[None])
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68098150
"link": https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas
"question": {
	"title": set some rule to groupby in pandas
	"desc": I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have "dup by" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. 
}
"io": {
	"Frame-1": 
		1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
	"Frame-2":
		1|F08210020403|GO|2014-05-17 00:00:00|yes
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
}
"answer": {
	"desc": %s IIUC: try: 
	"code-snippets": [
		c=df['keep'].str.contains('dup by')
		#created a condition which check if 'keep' column contains 'dup by' or not
		df['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')
		most_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()
		#excluded those rows in groupby where 'keep' contains 'dup by'
		df['most_recent_date']=df['VIP_ID'].map(most_recent_date)
		df['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])
		df['keep'] = np.where(
		    df['keep'] != 'same tier',df['keep'],
		    (np.where(
		         df['most_recent_date'] == df['datetime'],
		         'yes',
		         'dup by ' + df['VIP_ID'].astype(str)))
		)
		df.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'
		df = df.drop(columns = ['both','most_recent_date'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68090463
"link": https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas
"question": {
	"title": Merging more than two columns of the same dataframe in pandas
	"desc": Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? 
}
"io": {
	"Frame-1": 
		VAR 1   VAR 2   VAR 3   GROUP 
		                3   [0-10]
		1               3   [0-10]
		1               3   [0-10]
		1       2           [0-10]
		        2           [0-10]
		3              3    [10-20]
		3       1           [10-20]
		        1           [10-20]
		        2           [10-20]
		               2    [10-20]
		               2    [10-20]
		
	"Frame-2":
		VAR_MERGED  GROUP 
		1           [0-10]
		1           [0-10]
		1           [0-10]
		2           [0-10]
		2           [0-10]
		3           [0-10]
		3           [0-10]
		3           [0-10]
		1           [10-20]
		1           [10-20]
		2           [10-20]
		2           [10-20]
		2           [10-20]
		3           [10-20]
		3           [10-20]
		3           [10-20]
		
}
"answer": {
	"desc": %s Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: 
	"code-snippets": [
		id_vars = df.columns[~df.columns.str.startswith('VAR')]
		
		out = df.melt(id_vars, value_name='VAR_MERGED') \
		        .dropna() \
		        .sort_values(['GROUP', 'VAR_MERGED']) \
		        .reset_index(drop=True) \
		        [['VAR_MERGED'] + id_vars]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 33130586
"link": https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val
"question": {
	"title": python pandas - creating a column which keeps a running count of consecutive values
	"desc": I am trying to create a column (“consec”) which will keep a running count of consecutive values in another (“binary”) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. 
}
"io": {
	"Frame-1": 
		.    binary consec
		1       0      0
		2       1      1
		3       1      2
		4       1      3
		5       1      4
		5       0      0
		6       1      1
		7       1      2
		8       0      0
		
	"Frame-2":
		.  binary   consec
		0     1       NaN
		1     1       1
		2     1       1
		3     0       0
		4     1       1
		5     0       0
		6     1       1
		7     1       1
		8     1       1
		9     0       0
		
}
"answer": {
	"desc": %s You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. 
	"code-snippets": [
		>>> (df["binary"] == 0)
		0     True
		1    False
		2    False
		3    False
		4     True
		5     True
		6    False
		7    False
		8     True
		Name: binary, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62099066
"link": https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da
"question": {
	"title": is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?
	"desc": I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: 
}
"io": {
	"Frame-1": 
		(a) (b) (c) (d)
		a1  b1  c1  d1
		a2  b2  c2  d2
		
	"Frame-2":
		(a) (b) (e) (f)
		a3  b3  e1  f1
		a4  b4  e1  f1
		a5  b5  e2  f2
		a6  b6  e4  f4
		a7  a8  e4  f5
		
}
"answer": {
	"desc": %s Here you go: Check for the output. 
	"code-snippets": [
		import pandas as pd
		from pandas import ExcelWriter
		
		def detect_duplicate(group):
		    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)
		    return group
		
		with ExcelWriter('output.xlsx') as output:
		    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():
		        df = df.drop(['a', 'b'], axis=1)
		        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)
		        df = df.groupby(['concat']).apply(detect_duplicate)
		        df = df.drop_duplicates(keep='last', subset=['concat'])
		        df.to_excel(output, sheet_name=sheet_name, index=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67989744
"link": https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column
"question": {
	"title": Pandas replacing values in a column by values in another column
	"desc": Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: 
}
"io": {
	"Frame-1": 
		    ppid  col2 ...
		1   'id1'  '1'
		2   'id2'  '2'
		3   'id3'  '3'
		...
		
	"Frame-2":
		    ppid  val
		1   'id1' '5'
		2   'id2' '6'
		
}
"answer": {
	"desc": %s Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 
	"code-snippets": [
		df1 = df1.set_index('ppid')
		df2 = df2.set_index('ppid')
		df1.update(df2)
		df1.reset_index(inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67986537
"link": https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe
"question": {
	"title": How do I check for conflict between columns in a pandas dataframe?
	"desc": I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. 
}
"io": {
	"Frame-1": 
		    Item  Local A  Local B  Local C
		0  Item1      NaN      6.0        5
		1  Item2      6.0      7.0        5
		2  Item3      NaN      NaN        5
		3  Item4      5.0      5.0        5
		4  Item5      5.0      NaN        5
		
	"Frame-2":
		    Item  Local A  Local B  Local C Conflict
		0  Item1      NaN      6.0        5      yes
		1  Item2      6.0      7.0        5      yes
		2  Item3      NaN      NaN        5      NaN
		3  Item4      5.0      5.0        5      NaN
		4  Item5      5.0      NaN        5      NaN
		
}
"answer": {
	"desc": %s IIUC, try: Output: 
	"code-snippets": [
		df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 46124699
"link": https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files
"question": {
	"title": Splitting a dataframe into separate CSV files
	"desc": I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. 
}
"io": {
	"Frame-1": 
		+---------+---------+
		| Column1 | Column2 |
		+---------+---------+
		|       1 |   93644 |
		|       2 |   63246 |
		|       3 |   47790 |
		|       3 |   39644 |
		|       3 |   32585 |
		|       1 |   19593 |
		|       1 |   12707 |
		|       2 |   53480 |
		+---------+---------+
		
	"Frame-2":
		+---+-------+----------------+
		| 1 | 19593 | NewColumnValue |
		| 1 | 93644 | NewColumnValue |
		| 1 | 12707 | NewColumnValue |
		+---+-------+----------------+
		
		+---+-------+-----------------+
		| 2 | 63246 | NewColumnValue |
		| 2 | 53480 | NewColumnValue |
		+---+-------+-----------------+
		
		+---+-------+-----------------+
		| 3 | 47790 | NewColumnValue |
		| 3 | 39644 | NewColumnValue |
		| 3 | 32585 | NewColumnValue |
		+---+-------+-----------------+
		
}
"answer": {
	"desc": %s Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. 
	"code-snippets": [
		for i, g in df.groupby('Column1'):
		    g.to_csv('{}.csv'.format(i), header=False, index_label=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 57033657
"link": https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe
"question": {
	"title": How to Extract Month Name and Year from Date column of DataFrame
	"desc": I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. 
}
"io": {
	"Frame-1": 
		45    2018-01-01
		73    2018-02-08
		74    2018-02-08
		75    2018-02-08
		76    2018-02-08
		
	"Frame-2":
		45    Jan-2018
		73    Feb-2018
		74    Feb-2018
		75    Feb-2018
		76    Feb-2018
		
}
"answer": {
	"desc": %s Cast you date from object to actual datetime and use dt to access what you need. 
	"code-snippets": [
		import pandas as pd
		
		df = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})
		
		df['Date'] = pd.to_datetime(df['Date'])
		
		# You can format your date as you wish
		df['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')
		
		# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.
		
		print(df['Mon_Year'])
		
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68107298
"link": https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data
"question": {
	"title": How to Create a Correlation Dataframe from already related data
	"desc": I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? 
}
"io": {
	"Frame-1": 
		    0       1       2
		0   English Spanish 0.50
		1   English Russian 0.15
		
	"Frame-2":
		        English Spanish Russian
		English 1       0.5     0.15
		Spanish 0.5     1       -
		Russian 0.15    -       1
		
}
"answer": {
	"desc": %s I assume that the pairs are unique, i.e. if there's , there won't be a . 
	"code-snippets": [
		# Rename the columns. Because working with columns
		# named like numbers is a pain in the butt
		df.columns = ['lang1', 'lang2', 'corr']
		
		# Create the opposite pairs
		tmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)
		
		# Create the identity pairs
		all_lang = np.unique(df[['lang1', 'lang2']])
		tmp2 = pd.DataFrame({
		    'lang1': all_lang,
		    'lang2': all_lang,
		    'corr': 1.0
		})
		
		# Concatenate all 3 together and beat the data frame into shape
		result = (
		    pd.concat([df, tmp1, tmp2], axis=0)
		        .set_index(['lang1', 'lang2'])
		        .unstack()
		        .droplevel(0, axis=1)
		        .rename_axis(index=[None], columns=[None])
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68098150
"link": https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas
"question": {
	"title": set some rule to groupby in pandas
	"desc": I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have "dup by" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. 
}
"io": {
	"Frame-1": 
		1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
	"Frame-2":
		1|F08210020403|GO|2014-05-17 00:00:00|yes
		2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403
		3|F08210020403|FO||dup by F08210020403
		4|F08210020403|FO||dup by F08210020403
		5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403
		6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403
		7|F08210020403|FO||dup by F08210020403
		8|F08210020403|FO||dup by F08210020403
		
}
"answer": {
	"desc": %s IIUC: try: 
	"code-snippets": [
		c=df['keep'].str.contains('dup by')
		#created a condition which check if 'keep' column contains 'dup by' or not
		df['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')
		most_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()
		#excluded those rows in groupby where 'keep' contains 'dup by'
		df['most_recent_date']=df['VIP_ID'].map(most_recent_date)
		df['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])
		df['keep'] = np.where(
		    df['keep'] != 'same tier',df['keep'],
		    (np.where(
		         df['most_recent_date'] == df['datetime'],
		         'yes',
		         'dup by ' + df['VIP_ID'].astype(str)))
		)
		df.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'
		df = df.drop(columns = ['both','most_recent_date'])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68090463
"link": https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas
"question": {
	"title": Merging more than two columns of the same dataframe in pandas
	"desc": Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? 
}
"io": {
	"Frame-1": 
		VAR 1   VAR 2   VAR 3   GROUP 
		                3   [0-10]
		1               3   [0-10]
		1               3   [0-10]
		1       2           [0-10]
		        2           [0-10]
		3              3    [10-20]
		3       1           [10-20]
		        1           [10-20]
		        2           [10-20]
		               2    [10-20]
		               2    [10-20]
		
	"Frame-2":
		VAR_MERGED  GROUP 
		1           [0-10]
		1           [0-10]
		1           [0-10]
		2           [0-10]
		2           [0-10]
		3           [0-10]
		3           [0-10]
		3           [0-10]
		1           [10-20]
		1           [10-20]
		2           [10-20]
		2           [10-20]
		2           [10-20]
		3           [10-20]
		3           [10-20]
		3           [10-20]
		
}
"answer": {
	"desc": %s Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: 
	"code-snippets": [
		id_vars = df.columns[~df.columns.str.startswith('VAR')]
		
		out = df.melt(id_vars, value_name='VAR_MERGED') \
		        .dropna() \
		        .sort_values(['GROUP', 'VAR_MERGED']) \
		        .reset_index(drop=True) \
		        [['VAR_MERGED'] + id_vars]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 33130586
"link": https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val
"question": {
	"title": python pandas - creating a column which keeps a running count of consecutive values
	"desc": I am trying to create a column (“consec”) which will keep a running count of consecutive values in another (“binary”) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. 
}
"io": {
	"Frame-1": 
		.    binary consec
		1       0      0
		2       1      1
		3       1      2
		4       1      3
		5       1      4
		5       0      0
		6       1      1
		7       1      2
		8       0      0
		
	"Frame-2":
		.  binary   consec
		0     1       NaN
		1     1       1
		2     1       1
		3     0       0
		4     1       1
		5     0       0
		6     1       1
		7     1       1
		8     1       1
		9     0       0
		
}
"answer": {
	"desc": %s You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. 
	"code-snippets": [
		>>> (df["binary"] == 0)
		0     True
		1    False
		2    False
		3    False
		4     True
		5     True
		6    False
		7    False
		8     True
		Name: binary, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62099066
"link": https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da
"question": {
	"title": is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?
	"desc": I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: 
}
"io": {
	"Frame-1": 
		(a) (b) (c) (d)
		a1  b1  c1  d1
		a2  b2  c2  d2
		
	"Frame-2":
		(a) (b) (e) (f)
		a3  b3  e1  f1
		a4  b4  e1  f1
		a5  b5  e2  f2
		a6  b6  e4  f4
		a7  a8  e4  f5
		
}
"answer": {
	"desc": %s Here you go: Check for the output. 
	"code-snippets": [
		import pandas as pd
		from pandas import ExcelWriter
		
		def detect_duplicate(group):
		    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)
		    return group
		
		with ExcelWriter('output.xlsx') as output:
		    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():
		        df = df.drop(['a', 'b'], axis=1)
		        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)
		        df = df.groupby(['concat']).apply(detect_duplicate)
		        df = df.drop_duplicates(keep='last', subset=['concat'])
		        df.to_excel(output, sheet_name=sheet_name, index=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67989744
"link": https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column
"question": {
	"title": Pandas replacing values in a column by values in another column
	"desc": Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: 
}
"io": {
	"Frame-1": 
		    ppid  col2 ...
		1   'id1'  '1'
		2   'id2'  '2'
		3   'id3'  '3'
		...
		
	"Frame-2":
		    ppid  val
		1   'id1' '5'
		2   'id2' '6'
		
}
"answer": {
	"desc": %s Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 
	"code-snippets": [
		df1 = df1.set_index('ppid')
		df2 = df2.set_index('ppid')
		df1.update(df2)
		df1.reset_index(inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67986537
"link": https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe
"question": {
	"title": How do I check for conflict between columns in a pandas dataframe?
	"desc": I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. 
}
"io": {
	"Frame-1": 
		    Item  Local A  Local B  Local C
		0  Item1      NaN      6.0        5
		1  Item2      6.0      7.0        5
		2  Item3      NaN      NaN        5
		3  Item4      5.0      5.0        5
		4  Item5      5.0      NaN        5
		
	"Frame-2":
		    Item  Local A  Local B  Local C Conflict
		0  Item1      NaN      6.0        5      yes
		1  Item2      6.0      7.0        5      yes
		2  Item3      NaN      NaN        5      NaN
		3  Item4      5.0      5.0        5      NaN
		4  Item5      5.0      NaN        5      NaN
		
}
"answer": {
	"desc": %s IIUC, try: Output: 
	"code-snippets": [
		df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 46124699
"link": https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files
"question": {
	"title": Splitting a dataframe into separate CSV files
	"desc": I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. 
}
"io": {
	"Frame-1": 
		+---------+---------+
		| Column1 | Column2 |
		+---------+---------+
		|       1 |   93644 |
		|       2 |   63246 |
		|       3 |   47790 |
		|       3 |   39644 |
		|       3 |   32585 |
		|       1 |   19593 |
		|       1 |   12707 |
		|       2 |   53480 |
		+---------+---------+
		
	"Frame-2":
		+---+-------+----------------+
		| 1 | 19593 | NewColumnValue |
		| 1 | 93644 | NewColumnValue |
		| 1 | 12707 | NewColumnValue |
		+---+-------+----------------+
		
		+---+-------+-----------------+
		| 2 | 63246 | NewColumnValue |
		| 2 | 53480 | NewColumnValue |
		+---+-------+-----------------+
		
		+---+-------+-----------------+
		| 3 | 47790 | NewColumnValue |
		| 3 | 39644 | NewColumnValue |
		| 3 | 32585 | NewColumnValue |
		+---+-------+-----------------+
		
}
"answer": {
	"desc": %s Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. 
	"code-snippets": [
		for i, g in df.groupby('Column1'):
		    g.to_csv('{}.csv'.format(i), header=False, index_label=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67919385
"link": https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes
"question": {
	"title": Find unique column values out of two different Dataframes
	"desc": How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read 
}
"io": {
	"Frame-1": 
		67      Hij
		14      Xyz 
		87      Pqr
		
	"Frame-2":
		43      Def
		67      Lmn
		14      Xyz
		
}
"answer": {
	"desc": %s TRY: NOTE : Replace in with the first column name. 
	"code-snippets": [
		unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67917573
"link": https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas
"question": {
	"title": replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas
	"desc": I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! 
}
"io": {
	"Frame-1": 
		 L1      D1     L2      D2         L3
		 1.0    ABC     1.1     4.1        NaN
		 NaN    NaN     1.7     NaN        NaN
		 NaN    4.1     NaN     NaN        NaN
		 NaN    1.8     3.2     PQR        NaN
		 NaN    NaN     1.6     NaN        NaN
		
	"Frame-2":
		 L1      D1      L2      D2         L3
		 1.0    ABC     1.1     4.1        -
		 NaN    NaN     1.7     -          -
		 NaN    4.1      -      -          -
		 NaN    1.8     3.2     PQR        -
		 NaN    NaN     1.6     -          -
		
		
}
"answer": {
	"desc": %s Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get 
	"code-snippets": [
		minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)
		out = df.mask(minus_mask, "-")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 68090463
"link": https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas
"question": {
	"title": Merging more than two columns of the same dataframe in pandas
	"desc": Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? 
}
"io": {
	"Frame-1": 
		VAR 1   VAR 2   VAR 3   GROUP 
		                3   [0-10]
		1               3   [0-10]
		1               3   [0-10]
		1       2           [0-10]
		        2           [0-10]
		3              3    [10-20]
		3       1           [10-20]
		        1           [10-20]
		        2           [10-20]
		               2    [10-20]
		               2    [10-20]
		
	"Frame-2":
		VAR_MERGED  GROUP 
		1           [0-10]
		1           [0-10]
		1           [0-10]
		2           [0-10]
		2           [0-10]
		3           [0-10]
		3           [0-10]
		3           [0-10]
		1           [10-20]
		1           [10-20]
		2           [10-20]
		2           [10-20]
		2           [10-20]
		3           [10-20]
		3           [10-20]
		3           [10-20]
		
}
"answer": {
	"desc": %s Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: 
	"code-snippets": [
		id_vars = df.columns[~df.columns.str.startswith('VAR')]
		
		out = df.melt(id_vars, value_name='VAR_MERGED') \
		        .dropna() \
		        .sort_values(['GROUP', 'VAR_MERGED']) \
		        .reset_index(drop=True) \
		        [['VAR_MERGED'] + id_vars]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 33130586
"link": https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val
"question": {
	"title": python pandas - creating a column which keeps a running count of consecutive values
	"desc": I am trying to create a column (“consec”) which will keep a running count of consecutive values in another (“binary”) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. 
}
"io": {
	"Frame-1": 
		.    binary consec
		1       0      0
		2       1      1
		3       1      2
		4       1      3
		5       1      4
		5       0      0
		6       1      1
		7       1      2
		8       0      0
		
	"Frame-2":
		.  binary   consec
		0     1       NaN
		1     1       1
		2     1       1
		3     0       0
		4     1       1
		5     0       0
		6     1       1
		7     1       1
		8     1       1
		9     0       0
		
}
"answer": {
	"desc": %s You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. 
	"code-snippets": [
		>>> (df["binary"] == 0)
		0     True
		1    False
		2    False
		3    False
		4     True
		5     True
		6    False
		7    False
		8     True
		Name: binary, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62099066
"link": https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da
"question": {
	"title": is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?
	"desc": I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: 
}
"io": {
	"Frame-1": 
		(a) (b) (c) (d)
		a1  b1  c1  d1
		a2  b2  c2  d2
		
	"Frame-2":
		(a) (b) (e) (f)
		a3  b3  e1  f1
		a4  b4  e1  f1
		a5  b5  e2  f2
		a6  b6  e4  f4
		a7  a8  e4  f5
		
}
"answer": {
	"desc": %s Here you go: Check for the output. 
	"code-snippets": [
		import pandas as pd
		from pandas import ExcelWriter
		
		def detect_duplicate(group):
		    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)
		    return group
		
		with ExcelWriter('output.xlsx') as output:
		    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():
		        df = df.drop(['a', 'b'], axis=1)
		        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)
		        df = df.groupby(['concat']).apply(detect_duplicate)
		        df = df.drop_duplicates(keep='last', subset=['concat'])
		        df.to_excel(output, sheet_name=sheet_name, index=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67989744
"link": https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column
"question": {
	"title": Pandas replacing values in a column by values in another column
	"desc": Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: 
}
"io": {
	"Frame-1": 
		    ppid  col2 ...
		1   'id1'  '1'
		2   'id2'  '2'
		3   'id3'  '3'
		...
		
	"Frame-2":
		    ppid  val
		1   'id1' '5'
		2   'id2' '6'
		
}
"answer": {
	"desc": %s Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 
	"code-snippets": [
		df1 = df1.set_index('ppid')
		df2 = df2.set_index('ppid')
		df1.update(df2)
		df1.reset_index(inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67986537
"link": https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe
"question": {
	"title": How do I check for conflict between columns in a pandas dataframe?
	"desc": I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. 
}
"io": {
	"Frame-1": 
		    Item  Local A  Local B  Local C
		0  Item1      NaN      6.0        5
		1  Item2      6.0      7.0        5
		2  Item3      NaN      NaN        5
		3  Item4      5.0      5.0        5
		4  Item5      5.0      NaN        5
		
	"Frame-2":
		    Item  Local A  Local B  Local C Conflict
		0  Item1      NaN      6.0        5      yes
		1  Item2      6.0      7.0        5      yes
		2  Item3      NaN      NaN        5      NaN
		3  Item4      5.0      5.0        5      NaN
		4  Item5      5.0      NaN        5      NaN
		
}
"answer": {
	"desc": %s IIUC, try: Output: 
	"code-snippets": [
		df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 46124699
"link": https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files
"question": {
	"title": Splitting a dataframe into separate CSV files
	"desc": I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. 
}
"io": {
	"Frame-1": 
		+---------+---------+
		| Column1 | Column2 |
		+---------+---------+
		|       1 |   93644 |
		|       2 |   63246 |
		|       3 |   47790 |
		|       3 |   39644 |
		|       3 |   32585 |
		|       1 |   19593 |
		|       1 |   12707 |
		|       2 |   53480 |
		+---------+---------+
		
	"Frame-2":
		+---+-------+----------------+
		| 1 | 19593 | NewColumnValue |
		| 1 | 93644 | NewColumnValue |
		| 1 | 12707 | NewColumnValue |
		+---+-------+----------------+
		
		+---+-------+-----------------+
		| 2 | 63246 | NewColumnValue |
		| 2 | 53480 | NewColumnValue |
		+---+-------+-----------------+
		
		+---+-------+-----------------+
		| 3 | 47790 | NewColumnValue |
		| 3 | 39644 | NewColumnValue |
		| 3 | 32585 | NewColumnValue |
		+---+-------+-----------------+
		
}
"answer": {
	"desc": %s Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. 
	"code-snippets": [
		for i, g in df.groupby('Column1'):
		    g.to_csv('{}.csv'.format(i), header=False, index_label=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67919385
"link": https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes
"question": {
	"title": Find unique column values out of two different Dataframes
	"desc": How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read 
}
"io": {
	"Frame-1": 
		67      Hij
		14      Xyz 
		87      Pqr
		
	"Frame-2":
		43      Def
		67      Lmn
		14      Xyz
		
}
"answer": {
	"desc": %s TRY: NOTE : Replace in with the first column name. 
	"code-snippets": [
		unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67917573
"link": https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas
"question": {
	"title": replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas
	"desc": I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! 
}
"io": {
	"Frame-1": 
		 L1      D1     L2      D2         L3
		 1.0    ABC     1.1     4.1        NaN
		 NaN    NaN     1.7     NaN        NaN
		 NaN    4.1     NaN     NaN        NaN
		 NaN    1.8     3.2     PQR        NaN
		 NaN    NaN     1.6     NaN        NaN
		
	"Frame-2":
		 L1      D1      L2      D2         L3
		 1.0    ABC     1.1     4.1        -
		 NaN    NaN     1.7     -          -
		 NaN    4.1      -      -          -
		 NaN    1.8     3.2     PQR        -
		 NaN    NaN     1.6     -          -
		
		
}
"answer": {
	"desc": %s Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get 
	"code-snippets": [
		minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)
		out = df.mask(minus_mask, "-")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67910688
"link": https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe
"question": {
	"title": move column above and delete rows in pandas python dataframe
	"desc": I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be 
}
"io": {
	"Frame-1": 
		A        B        C        D        E        F        G        H
		a.1      b.1     
		                  
		                  c.1      d.1 
		                  c.2      d.2           e.1      f.1 
		                                                      
		
		                                                     g.1       h.1
		  
		
		
		
	"Frame-2":
		A        B        C        D        E        F        G        H
		a.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1
		                  c.2      d.2                                                   
		
}
"answer": {
	"desc": %s You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: 
	"code-snippets": [
		out = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		         .dropna(how="all")
		         .fillna(""))
		
		----------------------------------------------------------------------
		index = df.index
		df = df.reset_index(drop=True)
		df = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		        .dropna(how="all")
		        .fillna(""))
		df.index = index[:len(df)]
		
		----------------------------------------------------------------------
		def pull_up(s):
		    # this will be a column number; `s.name` is the column name
		    col_index = df.columns.get_indexer([s.name])
		
		   # for example: if `col_index` is either 7 or 8, pull by 4
		   if col_index in (7, 8):
		       return s.shift(-4)
		   else:
		       # otherwise, pull as much
		       return s.shift(-s.first_valid_index())
		
		# applying
		df.apply(pull_up)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67905723
"link": https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner
"question": {
	"title": Replicating the DataFrame row in a special manner
	"desc": I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: 
}
"io": {
	"Frame-1": 
		col1        mob_no             col3
		 a    9382949201/3245622535    45
		 b    8383459345/4325562678    67
		 c    8976247543/1827472398    89
		 d    7844329432               09
		
	"Frame-2":
		col1    mob_no      col3
		 a    9382949201     45
		 a    3245622535     45
		 b    8383459345     67
		 b    4325562678     67
		 c    8976247543     89
		 c    1827472398     89
		 d    7844329432     09
		
}
"answer": {
	"desc": %s Try with + : 
	"code-snippets": [
		df['mob_no'] = df['mob_no'].str.split('/')
		df = df.explode('mob_no')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67882067
"link": https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe
"question": {
	"title": Dictionary making for a transportation model from a Dataframe
	"desc": I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. 
}
"io": {
	"Frame-1": 
		 d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand
		 M = {p1:500, p2:500, p3:500}               # factory capacity
		 I = [c1,c2,c3,c4,c5]                         # Customers
		 J = [p1,p2,p3]                             # Factories
		 cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,
		 (p1,c4):8,    (p1,c5):10, ......
		  } 
		
	"Frame-2":
		  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		df1 = df.set_index(["Unnamed: 0", "Unnamed: 1"])
		plants = df1.loc[np.NaN]  # remove demand from dataframe
		
		d = dict(df1.loc["demand"].T.squeeze().dropna().iteritems())
		M = dict(plants["capacity"].iteritems())
		I = list(plants.drop(columns="capacity").columns)
		J = list(plants.index)
		cost = dict(plants.drop(columns="capacity").stack().iteritems())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 33130586
"link": https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val
"question": {
	"title": python pandas - creating a column which keeps a running count of consecutive values
	"desc": I am trying to create a column (“consec”) which will keep a running count of consecutive values in another (“binary”) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. 
}
"io": {
	"Frame-1": 
		.    binary consec
		1       0      0
		2       1      1
		3       1      2
		4       1      3
		5       1      4
		5       0      0
		6       1      1
		7       1      2
		8       0      0
		
	"Frame-2":
		.  binary   consec
		0     1       NaN
		1     1       1
		2     1       1
		3     0       0
		4     1       1
		5     0       0
		6     1       1
		7     1       1
		8     1       1
		9     0       0
		
}
"answer": {
	"desc": %s You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. 
	"code-snippets": [
		>>> (df["binary"] == 0)
		0     True
		1    False
		2    False
		3    False
		4     True
		5     True
		6    False
		7    False
		8     True
		Name: binary, dtype: bool
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 62099066
"link": https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da
"question": {
	"title": is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?
	"desc": I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: 
}
"io": {
	"Frame-1": 
		(a) (b) (c) (d)
		a1  b1  c1  d1
		a2  b2  c2  d2
		
	"Frame-2":
		(a) (b) (e) (f)
		a3  b3  e1  f1
		a4  b4  e1  f1
		a5  b5  e2  f2
		a6  b6  e4  f4
		a7  a8  e4  f5
		
}
"answer": {
	"desc": %s Here you go: Check for the output. 
	"code-snippets": [
		import pandas as pd
		from pandas import ExcelWriter
		
		def detect_duplicate(group):
		    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)
		    return group
		
		with ExcelWriter('output.xlsx') as output:
		    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():
		        df = df.drop(['a', 'b'], axis=1)
		        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)
		        df = df.groupby(['concat']).apply(detect_duplicate)
		        df = df.drop_duplicates(keep='last', subset=['concat'])
		        df.to_excel(output, sheet_name=sheet_name, index=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67989744
"link": https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column
"question": {
	"title": Pandas replacing values in a column by values in another column
	"desc": Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: 
}
"io": {
	"Frame-1": 
		    ppid  col2 ...
		1   'id1'  '1'
		2   'id2'  '2'
		3   'id3'  '3'
		...
		
	"Frame-2":
		    ppid  val
		1   'id1' '5'
		2   'id2' '6'
		
}
"answer": {
	"desc": %s Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 
	"code-snippets": [
		df1 = df1.set_index('ppid')
		df2 = df2.set_index('ppid')
		df1.update(df2)
		df1.reset_index(inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67986537
"link": https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe
"question": {
	"title": How do I check for conflict between columns in a pandas dataframe?
	"desc": I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. 
}
"io": {
	"Frame-1": 
		    Item  Local A  Local B  Local C
		0  Item1      NaN      6.0        5
		1  Item2      6.0      7.0        5
		2  Item3      NaN      NaN        5
		3  Item4      5.0      5.0        5
		4  Item5      5.0      NaN        5
		
	"Frame-2":
		    Item  Local A  Local B  Local C Conflict
		0  Item1      NaN      6.0        5      yes
		1  Item2      6.0      7.0        5      yes
		2  Item3      NaN      NaN        5      NaN
		3  Item4      5.0      5.0        5      NaN
		4  Item5      5.0      NaN        5      NaN
		
}
"answer": {
	"desc": %s IIUC, try: Output: 
	"code-snippets": [
		df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 46124699
"link": https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files
"question": {
	"title": Splitting a dataframe into separate CSV files
	"desc": I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. 
}
"io": {
	"Frame-1": 
		+---------+---------+
		| Column1 | Column2 |
		+---------+---------+
		|       1 |   93644 |
		|       2 |   63246 |
		|       3 |   47790 |
		|       3 |   39644 |
		|       3 |   32585 |
		|       1 |   19593 |
		|       1 |   12707 |
		|       2 |   53480 |
		+---------+---------+
		
	"Frame-2":
		+---+-------+----------------+
		| 1 | 19593 | NewColumnValue |
		| 1 | 93644 | NewColumnValue |
		| 1 | 12707 | NewColumnValue |
		+---+-------+----------------+
		
		+---+-------+-----------------+
		| 2 | 63246 | NewColumnValue |
		| 2 | 53480 | NewColumnValue |
		+---+-------+-----------------+
		
		+---+-------+-----------------+
		| 3 | 47790 | NewColumnValue |
		| 3 | 39644 | NewColumnValue |
		| 3 | 32585 | NewColumnValue |
		+---+-------+-----------------+
		
}
"answer": {
	"desc": %s Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. 
	"code-snippets": [
		for i, g in df.groupby('Column1'):
		    g.to_csv('{}.csv'.format(i), header=False, index_label=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67919385
"link": https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes
"question": {
	"title": Find unique column values out of two different Dataframes
	"desc": How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read 
}
"io": {
	"Frame-1": 
		67      Hij
		14      Xyz 
		87      Pqr
		
	"Frame-2":
		43      Def
		67      Lmn
		14      Xyz
		
}
"answer": {
	"desc": %s TRY: NOTE : Replace in with the first column name. 
	"code-snippets": [
		unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67917573
"link": https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas
"question": {
	"title": replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas
	"desc": I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! 
}
"io": {
	"Frame-1": 
		 L1      D1     L2      D2         L3
		 1.0    ABC     1.1     4.1        NaN
		 NaN    NaN     1.7     NaN        NaN
		 NaN    4.1     NaN     NaN        NaN
		 NaN    1.8     3.2     PQR        NaN
		 NaN    NaN     1.6     NaN        NaN
		
	"Frame-2":
		 L1      D1      L2      D2         L3
		 1.0    ABC     1.1     4.1        -
		 NaN    NaN     1.7     -          -
		 NaN    4.1      -      -          -
		 NaN    1.8     3.2     PQR        -
		 NaN    NaN     1.6     -          -
		
		
}
"answer": {
	"desc": %s Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get 
	"code-snippets": [
		minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)
		out = df.mask(minus_mask, "-")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67910688
"link": https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe
"question": {
	"title": move column above and delete rows in pandas python dataframe
	"desc": I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be 
}
"io": {
	"Frame-1": 
		A        B        C        D        E        F        G        H
		a.1      b.1     
		                  
		                  c.1      d.1 
		                  c.2      d.2           e.1      f.1 
		                                                      
		
		                                                     g.1       h.1
		  
		
		
		
	"Frame-2":
		A        B        C        D        E        F        G        H
		a.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1
		                  c.2      d.2                                                   
		
}
"answer": {
	"desc": %s You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: 
	"code-snippets": [
		out = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		         .dropna(how="all")
		         .fillna(""))
		
		----------------------------------------------------------------------
		index = df.index
		df = df.reset_index(drop=True)
		df = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		        .dropna(how="all")
		        .fillna(""))
		df.index = index[:len(df)]
		
		----------------------------------------------------------------------
		def pull_up(s):
		    # this will be a column number; `s.name` is the column name
		    col_index = df.columns.get_indexer([s.name])
		
		   # for example: if `col_index` is either 7 or 8, pull by 4
		   if col_index in (7, 8):
		       return s.shift(-4)
		   else:
		       # otherwise, pull as much
		       return s.shift(-s.first_valid_index())
		
		# applying
		df.apply(pull_up)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67905723
"link": https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner
"question": {
	"title": Replicating the DataFrame row in a special manner
	"desc": I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: 
}
"io": {
	"Frame-1": 
		col1        mob_no             col3
		 a    9382949201/3245622535    45
		 b    8383459345/4325562678    67
		 c    8976247543/1827472398    89
		 d    7844329432               09
		
	"Frame-2":
		col1    mob_no      col3
		 a    9382949201     45
		 a    3245622535     45
		 b    8383459345     67
		 b    4325562678     67
		 c    8976247543     89
		 c    1827472398     89
		 d    7844329432     09
		
}
"answer": {
	"desc": %s Try with + : 
	"code-snippets": [
		df['mob_no'] = df['mob_no'].str.split('/')
		df = df.explode('mob_no')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67882067
"link": https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe
"question": {
	"title": Dictionary making for a transportation model from a Dataframe
	"desc": I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. 
}
"io": {
	"Frame-1": 
		 d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand
		 M = {p1:500, p2:500, p3:500}               # factory capacity
		 I = [c1,c2,c3,c4,c5]                         # Customers
		 J = [p1,p2,p3]                             # Factories
		 cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,
		 (p1,c4):8,    (p1,c5):10, ......
		  } 
		
	"Frame-2":
		  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		df1 = df.set_index(["Unnamed: 0", "Unnamed: 1"])
		plants = df1.loc[np.NaN]  # remove demand from dataframe
		
		d = dict(df1.loc["demand"].T.squeeze().dropna().iteritems())
		M = dict(plants["capacity"].iteritems())
		I = list(plants.drop(columns="capacity").columns)
		J = list(plants.index)
		cost = dict(plants.drop(columns="capacity").stack().iteritems())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870323
"link": https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe
"question": {
	"title": How to get list of previous n values of a column conditionally in DataFrame?
	"desc": My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? 
}
"io": {
	"Frame-1": 
		Subject     Score
		    1       15
		    2       0
		    3       18
		    2       30
		    3       17
		    1       5
		    4       9
		    2       7
		    1       20
		    1       8
		    2       9
		    1       12
		
	"Frame-2":
		Subject   Score Previous
		1       15      []
		2       0       []
		3       18      []
		2       30      [0]
		3       17      [18]
		1       5       [15]
		4       9       []
		2       7       [30,0]
		1       20      [5,15]
		1       8       [20,5,15]
		2       9       [7,30,0]
		1       12      [8,20,5]
		
}
"answer": {
	"desc": %s Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: 
	"code-snippets": [
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [
		    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)
		]
		
		----------------------------------------------------------------------
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [x.agg(list)[-2::-1]
		                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]
		df = df.sort_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870585
"link": https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column
"question": {
	"title": Python dataframe create index column based on other id column
	"desc": I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: 
}
"io": {
	"Frame-1": 
		ID                  Price
		000afb96ded6677c    1514.5
		000afb96ded6677c    13.0
		000afb96ded6677c    611.0
		000afb96ded6677c    723.0
		000afb96ded6677c    2065.0
		ffea14e87a4e1269    2286.0
		ffea14e87a4e1269    1150.0
		ffea14e87a4e1269    80.0
		fff455057ad492da    650.0
		fff5fc66c1fd66c2    450.0
		
	"Frame-2":
		ID                  Price    ID 2
		000afb96ded6677c    1514.5   1
		000afb96ded6677c    13.0     1
		000afb96ded6677c    611.0    1
		000afb96ded6677c    723.0    1
		000afb96ded6677c    2065.0   1
		ffea14e87a4e1269    2286.0   2
		ffea14e87a4e1269    1150.0   2
		ffea14e87a4e1269    80.0     2
		fff455057ad492da    650.0    3
		fff5fc66c1fd66c2    450.0    4
		
}
"answer": {
	"desc": %s Try + 1 : Or with : : 
	"code-snippets": [
		df['ID_2'] = df.groupby('ID').ngroup() + 1
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67856992
"link": https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list
"question": {
	"title": Element wise numeric comparison in Pandas dataframe column value with list
	"desc": I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: 
}
"io": {
	"Frame-1": 
		  |          A          |           B           |          C         |
		  |         Val         |          Val          |         Val        |
		  |---------------------|-----------------------|--------------------|
		0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		
	"Frame-2":
		  |  A    |   B   |  C   |
		  |  Max  |  Max  |  Max |
		  |-------|-------|------|
		0 | 28.68 | 18.42 | 1.37 |
		1 | 29.50 | 17.31 | 1.47 |
		2 | 29.87 | 20.45 | 1.39 |
		
}
"answer": {
	"desc": %s Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 µs, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 µs, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms 
	"code-snippets": [
		def bool_check(row):
		    col = row.name[0]
		    min_val = df1[pd.IndexSlice[col]].to_numpy()
		    max_val = df3[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		----------------------------------------------------------------------
		def bool_check(row):
		    col = row.name[0]
		    min_val = min_df[pd.IndexSlice[col]].to_numpy()
		    max_val = max_df[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		res = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67845362
"link": https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column
"question": {
	"title": Sort pandas df subset of rows (within a group) by specific column
	"desc": I have the following dataframe let’s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? 
}
"io": {
	"Frame-1": 
		
		A B C D E
		z k s 7 d
		z k s 6 l
		x t r 2 e
		x t r 1 x
		u c r 8 f
		u c r 9 h
		y t s 5 l
		y t s 2 o
		
	"Frame-2":
		
		A B C D E
		z k s 6 l
		z k s 7 d
		x t r 1 x
		x t r 2 e
		u c r 8 f
		u c r 9 h
		y t s 2 o
		y t s 5 l
		
}
"answer": {
	"desc": %s I think it should be as simple as this: 
	"code-snippets": [
		df = df.sort_values(["A", "B", "C", "D"])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61624957
"link": https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json
"question": {
	"title": Creating a table in pandas from json
	"desc": I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! 
}
"io": {
	"Frame-1": 
		0     website
		0  1  NaN
		1  2  NaN 
		
	"Frame-2":
		0     website
		0  1  https://bitcoin.org/
		1  2  https://litecoin.org/
		
}
"answer": {
	"desc": %s You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' 
	"code-snippets": [
		(
		    pd.DataFrame(data['data'])
		    .loc['urls']
		    .apply(lambda x: (x['technical_doc']+[''])[0])
		    .to_frame('website')
		)
		
		----------------------------------------------------------------------
		(
		    pd.DataFrame(data['data'])
		    .loc['logo']
		    .to_frame('logo')
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67989744
"link": https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column
"question": {
	"title": Pandas replacing values in a column by values in another column
	"desc": Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: 
}
"io": {
	"Frame-1": 
		    ppid  col2 ...
		1   'id1'  '1'
		2   'id2'  '2'
		3   'id3'  '3'
		...
		
	"Frame-2":
		    ppid  val
		1   'id1' '5'
		2   'id2' '6'
		
}
"answer": {
	"desc": %s Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 
	"code-snippets": [
		df1 = df1.set_index('ppid')
		df2 = df2.set_index('ppid')
		df1.update(df2)
		df1.reset_index(inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67986537
"link": https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe
"question": {
	"title": How do I check for conflict between columns in a pandas dataframe?
	"desc": I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. 
}
"io": {
	"Frame-1": 
		    Item  Local A  Local B  Local C
		0  Item1      NaN      6.0        5
		1  Item2      6.0      7.0        5
		2  Item3      NaN      NaN        5
		3  Item4      5.0      5.0        5
		4  Item5      5.0      NaN        5
		
	"Frame-2":
		    Item  Local A  Local B  Local C Conflict
		0  Item1      NaN      6.0        5      yes
		1  Item2      6.0      7.0        5      yes
		2  Item3      NaN      NaN        5      NaN
		3  Item4      5.0      5.0        5      NaN
		4  Item5      5.0      NaN        5      NaN
		
}
"answer": {
	"desc": %s IIUC, try: Output: 
	"code-snippets": [
		df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 46124699
"link": https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files
"question": {
	"title": Splitting a dataframe into separate CSV files
	"desc": I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. 
}
"io": {
	"Frame-1": 
		+---------+---------+
		| Column1 | Column2 |
		+---------+---------+
		|       1 |   93644 |
		|       2 |   63246 |
		|       3 |   47790 |
		|       3 |   39644 |
		|       3 |   32585 |
		|       1 |   19593 |
		|       1 |   12707 |
		|       2 |   53480 |
		+---------+---------+
		
	"Frame-2":
		+---+-------+----------------+
		| 1 | 19593 | NewColumnValue |
		| 1 | 93644 | NewColumnValue |
		| 1 | 12707 | NewColumnValue |
		+---+-------+----------------+
		
		+---+-------+-----------------+
		| 2 | 63246 | NewColumnValue |
		| 2 | 53480 | NewColumnValue |
		+---+-------+-----------------+
		
		+---+-------+-----------------+
		| 3 | 47790 | NewColumnValue |
		| 3 | 39644 | NewColumnValue |
		| 3 | 32585 | NewColumnValue |
		+---+-------+-----------------+
		
}
"answer": {
	"desc": %s Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. 
	"code-snippets": [
		for i, g in df.groupby('Column1'):
		    g.to_csv('{}.csv'.format(i), header=False, index_label=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67919385
"link": https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes
"question": {
	"title": Find unique column values out of two different Dataframes
	"desc": How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read 
}
"io": {
	"Frame-1": 
		67      Hij
		14      Xyz 
		87      Pqr
		
	"Frame-2":
		43      Def
		67      Lmn
		14      Xyz
		
}
"answer": {
	"desc": %s TRY: NOTE : Replace in with the first column name. 
	"code-snippets": [
		unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67917573
"link": https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas
"question": {
	"title": replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas
	"desc": I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! 
}
"io": {
	"Frame-1": 
		 L1      D1     L2      D2         L3
		 1.0    ABC     1.1     4.1        NaN
		 NaN    NaN     1.7     NaN        NaN
		 NaN    4.1     NaN     NaN        NaN
		 NaN    1.8     3.2     PQR        NaN
		 NaN    NaN     1.6     NaN        NaN
		
	"Frame-2":
		 L1      D1      L2      D2         L3
		 1.0    ABC     1.1     4.1        -
		 NaN    NaN     1.7     -          -
		 NaN    4.1      -      -          -
		 NaN    1.8     3.2     PQR        -
		 NaN    NaN     1.6     -          -
		
		
}
"answer": {
	"desc": %s Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get 
	"code-snippets": [
		minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)
		out = df.mask(minus_mask, "-")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67910688
"link": https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe
"question": {
	"title": move column above and delete rows in pandas python dataframe
	"desc": I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be 
}
"io": {
	"Frame-1": 
		A        B        C        D        E        F        G        H
		a.1      b.1     
		                  
		                  c.1      d.1 
		                  c.2      d.2           e.1      f.1 
		                                                      
		
		                                                     g.1       h.1
		  
		
		
		
	"Frame-2":
		A        B        C        D        E        F        G        H
		a.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1
		                  c.2      d.2                                                   
		
}
"answer": {
	"desc": %s You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: 
	"code-snippets": [
		out = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		         .dropna(how="all")
		         .fillna(""))
		
		----------------------------------------------------------------------
		index = df.index
		df = df.reset_index(drop=True)
		df = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		        .dropna(how="all")
		        .fillna(""))
		df.index = index[:len(df)]
		
		----------------------------------------------------------------------
		def pull_up(s):
		    # this will be a column number; `s.name` is the column name
		    col_index = df.columns.get_indexer([s.name])
		
		   # for example: if `col_index` is either 7 or 8, pull by 4
		   if col_index in (7, 8):
		       return s.shift(-4)
		   else:
		       # otherwise, pull as much
		       return s.shift(-s.first_valid_index())
		
		# applying
		df.apply(pull_up)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67905723
"link": https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner
"question": {
	"title": Replicating the DataFrame row in a special manner
	"desc": I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: 
}
"io": {
	"Frame-1": 
		col1        mob_no             col3
		 a    9382949201/3245622535    45
		 b    8383459345/4325562678    67
		 c    8976247543/1827472398    89
		 d    7844329432               09
		
	"Frame-2":
		col1    mob_no      col3
		 a    9382949201     45
		 a    3245622535     45
		 b    8383459345     67
		 b    4325562678     67
		 c    8976247543     89
		 c    1827472398     89
		 d    7844329432     09
		
}
"answer": {
	"desc": %s Try with + : 
	"code-snippets": [
		df['mob_no'] = df['mob_no'].str.split('/')
		df = df.explode('mob_no')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67882067
"link": https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe
"question": {
	"title": Dictionary making for a transportation model from a Dataframe
	"desc": I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. 
}
"io": {
	"Frame-1": 
		 d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand
		 M = {p1:500, p2:500, p3:500}               # factory capacity
		 I = [c1,c2,c3,c4,c5]                         # Customers
		 J = [p1,p2,p3]                             # Factories
		 cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,
		 (p1,c4):8,    (p1,c5):10, ......
		  } 
		
	"Frame-2":
		  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		df1 = df.set_index(["Unnamed: 0", "Unnamed: 1"])
		plants = df1.loc[np.NaN]  # remove demand from dataframe
		
		d = dict(df1.loc["demand"].T.squeeze().dropna().iteritems())
		M = dict(plants["capacity"].iteritems())
		I = list(plants.drop(columns="capacity").columns)
		J = list(plants.index)
		cost = dict(plants.drop(columns="capacity").stack().iteritems())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870323
"link": https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe
"question": {
	"title": How to get list of previous n values of a column conditionally in DataFrame?
	"desc": My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? 
}
"io": {
	"Frame-1": 
		Subject     Score
		    1       15
		    2       0
		    3       18
		    2       30
		    3       17
		    1       5
		    4       9
		    2       7
		    1       20
		    1       8
		    2       9
		    1       12
		
	"Frame-2":
		Subject   Score Previous
		1       15      []
		2       0       []
		3       18      []
		2       30      [0]
		3       17      [18]
		1       5       [15]
		4       9       []
		2       7       [30,0]
		1       20      [5,15]
		1       8       [20,5,15]
		2       9       [7,30,0]
		1       12      [8,20,5]
		
}
"answer": {
	"desc": %s Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: 
	"code-snippets": [
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [
		    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)
		]
		
		----------------------------------------------------------------------
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [x.agg(list)[-2::-1]
		                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]
		df = df.sort_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870585
"link": https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column
"question": {
	"title": Python dataframe create index column based on other id column
	"desc": I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: 
}
"io": {
	"Frame-1": 
		ID                  Price
		000afb96ded6677c    1514.5
		000afb96ded6677c    13.0
		000afb96ded6677c    611.0
		000afb96ded6677c    723.0
		000afb96ded6677c    2065.0
		ffea14e87a4e1269    2286.0
		ffea14e87a4e1269    1150.0
		ffea14e87a4e1269    80.0
		fff455057ad492da    650.0
		fff5fc66c1fd66c2    450.0
		
	"Frame-2":
		ID                  Price    ID 2
		000afb96ded6677c    1514.5   1
		000afb96ded6677c    13.0     1
		000afb96ded6677c    611.0    1
		000afb96ded6677c    723.0    1
		000afb96ded6677c    2065.0   1
		ffea14e87a4e1269    2286.0   2
		ffea14e87a4e1269    1150.0   2
		ffea14e87a4e1269    80.0     2
		fff455057ad492da    650.0    3
		fff5fc66c1fd66c2    450.0    4
		
}
"answer": {
	"desc": %s Try + 1 : Or with : : 
	"code-snippets": [
		df['ID_2'] = df.groupby('ID').ngroup() + 1
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67856992
"link": https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list
"question": {
	"title": Element wise numeric comparison in Pandas dataframe column value with list
	"desc": I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: 
}
"io": {
	"Frame-1": 
		  |          A          |           B           |          C         |
		  |         Val         |          Val          |         Val        |
		  |---------------------|-----------------------|--------------------|
		0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		
	"Frame-2":
		  |  A    |   B   |  C   |
		  |  Max  |  Max  |  Max |
		  |-------|-------|------|
		0 | 28.68 | 18.42 | 1.37 |
		1 | 29.50 | 17.31 | 1.47 |
		2 | 29.87 | 20.45 | 1.39 |
		
}
"answer": {
	"desc": %s Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 µs, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 µs, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms 
	"code-snippets": [
		def bool_check(row):
		    col = row.name[0]
		    min_val = df1[pd.IndexSlice[col]].to_numpy()
		    max_val = df3[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		----------------------------------------------------------------------
		def bool_check(row):
		    col = row.name[0]
		    min_val = min_df[pd.IndexSlice[col]].to_numpy()
		    max_val = max_df[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		res = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67845362
"link": https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column
"question": {
	"title": Sort pandas df subset of rows (within a group) by specific column
	"desc": I have the following dataframe let’s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? 
}
"io": {
	"Frame-1": 
		
		A B C D E
		z k s 7 d
		z k s 6 l
		x t r 2 e
		x t r 1 x
		u c r 8 f
		u c r 9 h
		y t s 5 l
		y t s 2 o
		
	"Frame-2":
		
		A B C D E
		z k s 6 l
		z k s 7 d
		x t r 1 x
		x t r 2 e
		u c r 8 f
		u c r 9 h
		y t s 2 o
		y t s 5 l
		
}
"answer": {
	"desc": %s I think it should be as simple as this: 
	"code-snippets": [
		df = df.sort_values(["A", "B", "C", "D"])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61624957
"link": https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json
"question": {
	"title": Creating a table in pandas from json
	"desc": I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! 
}
"io": {
	"Frame-1": 
		0     website
		0  1  NaN
		1  2  NaN 
		
	"Frame-2":
		0     website
		0  1  https://bitcoin.org/
		1  2  https://litecoin.org/
		
}
"answer": {
	"desc": %s You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' 
	"code-snippets": [
		(
		    pd.DataFrame(data['data'])
		    .loc['urls']
		    .apply(lambda x: (x['technical_doc']+[''])[0])
		    .to_frame('website')
		)
		
		----------------------------------------------------------------------
		(
		    pd.DataFrame(data['data'])
		    .loc['logo']
		    .to_frame('logo')
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67822403
"link": https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1
"question": {
	"title": Replicate dataframe n number of times and increment another column by 1
	"desc": I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. 
}
"io": {
	"Frame-1": 
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		
	"Frame-2":
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		2      123     abc     2.20
		2      234     bcd     1.19
		2      345     cde     1.22
		3      123     abc     2.20
		3      234     bcd     1.19
		3      345     cde     1.22
		
}
"answer": {
	"desc": %s Something along these lines should work: 
	"code-snippets": [
		df1=df.copy()
		for i in range(1,20):
		    df['S/No']=df['S/No']+1
		    df1=pd.concat([df1,df])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67919385
"link": https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes
"question": {
	"title": Find unique column values out of two different Dataframes
	"desc": How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read 
}
"io": {
	"Frame-1": 
		67      Hij
		14      Xyz 
		87      Pqr
		
	"Frame-2":
		43      Def
		67      Lmn
		14      Xyz
		
}
"answer": {
	"desc": %s TRY: NOTE : Replace in with the first column name. 
	"code-snippets": [
		unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67917573
"link": https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas
"question": {
	"title": replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas
	"desc": I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! 
}
"io": {
	"Frame-1": 
		 L1      D1     L2      D2         L3
		 1.0    ABC     1.1     4.1        NaN
		 NaN    NaN     1.7     NaN        NaN
		 NaN    4.1     NaN     NaN        NaN
		 NaN    1.8     3.2     PQR        NaN
		 NaN    NaN     1.6     NaN        NaN
		
	"Frame-2":
		 L1      D1      L2      D2         L3
		 1.0    ABC     1.1     4.1        -
		 NaN    NaN     1.7     -          -
		 NaN    4.1      -      -          -
		 NaN    1.8     3.2     PQR        -
		 NaN    NaN     1.6     -          -
		
		
}
"answer": {
	"desc": %s Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get 
	"code-snippets": [
		minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)
		out = df.mask(minus_mask, "-")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67910688
"link": https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe
"question": {
	"title": move column above and delete rows in pandas python dataframe
	"desc": I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be 
}
"io": {
	"Frame-1": 
		A        B        C        D        E        F        G        H
		a.1      b.1     
		                  
		                  c.1      d.1 
		                  c.2      d.2           e.1      f.1 
		                                                      
		
		                                                     g.1       h.1
		  
		
		
		
	"Frame-2":
		A        B        C        D        E        F        G        H
		a.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1
		                  c.2      d.2                                                   
		
}
"answer": {
	"desc": %s You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: 
	"code-snippets": [
		out = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		         .dropna(how="all")
		         .fillna(""))
		
		----------------------------------------------------------------------
		index = df.index
		df = df.reset_index(drop=True)
		df = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		        .dropna(how="all")
		        .fillna(""))
		df.index = index[:len(df)]
		
		----------------------------------------------------------------------
		def pull_up(s):
		    # this will be a column number; `s.name` is the column name
		    col_index = df.columns.get_indexer([s.name])
		
		   # for example: if `col_index` is either 7 or 8, pull by 4
		   if col_index in (7, 8):
		       return s.shift(-4)
		   else:
		       # otherwise, pull as much
		       return s.shift(-s.first_valid_index())
		
		# applying
		df.apply(pull_up)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67905723
"link": https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner
"question": {
	"title": Replicating the DataFrame row in a special manner
	"desc": I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: 
}
"io": {
	"Frame-1": 
		col1        mob_no             col3
		 a    9382949201/3245622535    45
		 b    8383459345/4325562678    67
		 c    8976247543/1827472398    89
		 d    7844329432               09
		
	"Frame-2":
		col1    mob_no      col3
		 a    9382949201     45
		 a    3245622535     45
		 b    8383459345     67
		 b    4325562678     67
		 c    8976247543     89
		 c    1827472398     89
		 d    7844329432     09
		
}
"answer": {
	"desc": %s Try with + : 
	"code-snippets": [
		df['mob_no'] = df['mob_no'].str.split('/')
		df = df.explode('mob_no')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67882067
"link": https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe
"question": {
	"title": Dictionary making for a transportation model from a Dataframe
	"desc": I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. 
}
"io": {
	"Frame-1": 
		 d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand
		 M = {p1:500, p2:500, p3:500}               # factory capacity
		 I = [c1,c2,c3,c4,c5]                         # Customers
		 J = [p1,p2,p3]                             # Factories
		 cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,
		 (p1,c4):8,    (p1,c5):10, ......
		  } 
		
	"Frame-2":
		  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		df1 = df.set_index(["Unnamed: 0", "Unnamed: 1"])
		plants = df1.loc[np.NaN]  # remove demand from dataframe
		
		d = dict(df1.loc["demand"].T.squeeze().dropna().iteritems())
		M = dict(plants["capacity"].iteritems())
		I = list(plants.drop(columns="capacity").columns)
		J = list(plants.index)
		cost = dict(plants.drop(columns="capacity").stack().iteritems())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870323
"link": https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe
"question": {
	"title": How to get list of previous n values of a column conditionally in DataFrame?
	"desc": My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? 
}
"io": {
	"Frame-1": 
		Subject     Score
		    1       15
		    2       0
		    3       18
		    2       30
		    3       17
		    1       5
		    4       9
		    2       7
		    1       20
		    1       8
		    2       9
		    1       12
		
	"Frame-2":
		Subject   Score Previous
		1       15      []
		2       0       []
		3       18      []
		2       30      [0]
		3       17      [18]
		1       5       [15]
		4       9       []
		2       7       [30,0]
		1       20      [5,15]
		1       8       [20,5,15]
		2       9       [7,30,0]
		1       12      [8,20,5]
		
}
"answer": {
	"desc": %s Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: 
	"code-snippets": [
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [
		    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)
		]
		
		----------------------------------------------------------------------
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [x.agg(list)[-2::-1]
		                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]
		df = df.sort_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870585
"link": https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column
"question": {
	"title": Python dataframe create index column based on other id column
	"desc": I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: 
}
"io": {
	"Frame-1": 
		ID                  Price
		000afb96ded6677c    1514.5
		000afb96ded6677c    13.0
		000afb96ded6677c    611.0
		000afb96ded6677c    723.0
		000afb96ded6677c    2065.0
		ffea14e87a4e1269    2286.0
		ffea14e87a4e1269    1150.0
		ffea14e87a4e1269    80.0
		fff455057ad492da    650.0
		fff5fc66c1fd66c2    450.0
		
	"Frame-2":
		ID                  Price    ID 2
		000afb96ded6677c    1514.5   1
		000afb96ded6677c    13.0     1
		000afb96ded6677c    611.0    1
		000afb96ded6677c    723.0    1
		000afb96ded6677c    2065.0   1
		ffea14e87a4e1269    2286.0   2
		ffea14e87a4e1269    1150.0   2
		ffea14e87a4e1269    80.0     2
		fff455057ad492da    650.0    3
		fff5fc66c1fd66c2    450.0    4
		
}
"answer": {
	"desc": %s Try + 1 : Or with : : 
	"code-snippets": [
		df['ID_2'] = df.groupby('ID').ngroup() + 1
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67856992
"link": https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list
"question": {
	"title": Element wise numeric comparison in Pandas dataframe column value with list
	"desc": I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: 
}
"io": {
	"Frame-1": 
		  |          A          |           B           |          C         |
		  |         Val         |          Val          |         Val        |
		  |---------------------|-----------------------|--------------------|
		0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		
	"Frame-2":
		  |  A    |   B   |  C   |
		  |  Max  |  Max  |  Max |
		  |-------|-------|------|
		0 | 28.68 | 18.42 | 1.37 |
		1 | 29.50 | 17.31 | 1.47 |
		2 | 29.87 | 20.45 | 1.39 |
		
}
"answer": {
	"desc": %s Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 µs, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 µs, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms 
	"code-snippets": [
		def bool_check(row):
		    col = row.name[0]
		    min_val = df1[pd.IndexSlice[col]].to_numpy()
		    max_val = df3[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		----------------------------------------------------------------------
		def bool_check(row):
		    col = row.name[0]
		    min_val = min_df[pd.IndexSlice[col]].to_numpy()
		    max_val = max_df[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		res = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67845362
"link": https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column
"question": {
	"title": Sort pandas df subset of rows (within a group) by specific column
	"desc": I have the following dataframe let’s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? 
}
"io": {
	"Frame-1": 
		
		A B C D E
		z k s 7 d
		z k s 6 l
		x t r 2 e
		x t r 1 x
		u c r 8 f
		u c r 9 h
		y t s 5 l
		y t s 2 o
		
	"Frame-2":
		
		A B C D E
		z k s 6 l
		z k s 7 d
		x t r 1 x
		x t r 2 e
		u c r 8 f
		u c r 9 h
		y t s 2 o
		y t s 5 l
		
}
"answer": {
	"desc": %s I think it should be as simple as this: 
	"code-snippets": [
		df = df.sort_values(["A", "B", "C", "D"])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61624957
"link": https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json
"question": {
	"title": Creating a table in pandas from json
	"desc": I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! 
}
"io": {
	"Frame-1": 
		0     website
		0  1  NaN
		1  2  NaN 
		
	"Frame-2":
		0     website
		0  1  https://bitcoin.org/
		1  2  https://litecoin.org/
		
}
"answer": {
	"desc": %s You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' 
	"code-snippets": [
		(
		    pd.DataFrame(data['data'])
		    .loc['urls']
		    .apply(lambda x: (x['technical_doc']+[''])[0])
		    .to_frame('website')
		)
		
		----------------------------------------------------------------------
		(
		    pd.DataFrame(data['data'])
		    .loc['logo']
		    .to_frame('logo')
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67822403
"link": https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1
"question": {
	"title": Replicate dataframe n number of times and increment another column by 1
	"desc": I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. 
}
"io": {
	"Frame-1": 
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		
	"Frame-2":
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		2      123     abc     2.20
		2      234     bcd     1.19
		2      345     cde     1.22
		3      123     abc     2.20
		3      234     bcd     1.19
		3      345     cde     1.22
		
}
"answer": {
	"desc": %s Something along these lines should work: 
	"code-snippets": [
		df1=df.copy()
		for i in range(1,20):
		    df['S/No']=df['S/No']+1
		    df1=pd.concat([df1,df])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67782727
"link": https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas
"question": {
	"title": Python - Delete lines from dataframe (pandas)
	"desc": I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. 
}
"io": {
	"Frame-1": 
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		10     3927905909    ///    ///
		11     3872948210    ///   0.15
		12  9783293003781    ///   0.15
		13  9783423246842    ///    ///
		14  9783423247146    ///    ///
		15  9783423246934    ///    ///
		16     387294116x    ///    ///
		17  9783935597456   0,16   0.15
		18  9783423204545    ///    ///
		
	"Frame-2":
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		
}
"answer": {
	"desc": %s Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. 
	"code-snippets": [
		data[['1', '2']] = data[['1', '2']].replace({"///": np.nan, ",": "."}, regex=True)
		                                   .astype(float)
		data = data.loc[data[["1", "2"]].ge(1.).all(axis="columns")]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67770056
"link": https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe
"question": {
	"title": How to create columns from a string in a dataframe?
	"desc": WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. 
}
"io": {
	"Frame-1": 
		    long string
		0   ha: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ho...
		1   hi: (tra: 1 la: 2) \n ha: (tra: 1 la: 2) \n ho...
		2   ho: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ha...
		
	"Frame-2":
		    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la
		0   1       2       1       2       1       2
		1   1       2       1       2       1       2
		2   1       2       1       2       1       2
		
}
"answer": {
	"desc": %s  Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get 
	"code-snippets": [
		ndf = (df["long string"]
		         .str.extractall(r"(ha|hi|ho):\s\((?:tra|la):\s(\d+)\s(?:tra|la):\s(\d+)\)")
		         .droplevel("match")
		         .set_index(0, append=True)
		         .set_axis(["tra", "la"], axis=1)
		         .unstack()
		         .swaplevel(axis=1))
		ndf.columns = ndf.columns.map("-".join)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67910688
"link": https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe
"question": {
	"title": move column above and delete rows in pandas python dataframe
	"desc": I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be 
}
"io": {
	"Frame-1": 
		A        B        C        D        E        F        G        H
		a.1      b.1     
		                  
		                  c.1      d.1 
		                  c.2      d.2           e.1      f.1 
		                                                      
		
		                                                     g.1       h.1
		  
		
		
		
	"Frame-2":
		A        B        C        D        E        F        G        H
		a.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1
		                  c.2      d.2                                                   
		
}
"answer": {
	"desc": %s You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: 
	"code-snippets": [
		out = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		         .dropna(how="all")
		         .fillna(""))
		
		----------------------------------------------------------------------
		index = df.index
		df = df.reset_index(drop=True)
		df = (df.apply(lambda s: s.shift(-s.first_valid_index()))
		        .dropna(how="all")
		        .fillna(""))
		df.index = index[:len(df)]
		
		----------------------------------------------------------------------
		def pull_up(s):
		    # this will be a column number; `s.name` is the column name
		    col_index = df.columns.get_indexer([s.name])
		
		   # for example: if `col_index` is either 7 or 8, pull by 4
		   if col_index in (7, 8):
		       return s.shift(-4)
		   else:
		       # otherwise, pull as much
		       return s.shift(-s.first_valid_index())
		
		# applying
		df.apply(pull_up)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67905723
"link": https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner
"question": {
	"title": Replicating the DataFrame row in a special manner
	"desc": I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: 
}
"io": {
	"Frame-1": 
		col1        mob_no             col3
		 a    9382949201/3245622535    45
		 b    8383459345/4325562678    67
		 c    8976247543/1827472398    89
		 d    7844329432               09
		
	"Frame-2":
		col1    mob_no      col3
		 a    9382949201     45
		 a    3245622535     45
		 b    8383459345     67
		 b    4325562678     67
		 c    8976247543     89
		 c    1827472398     89
		 d    7844329432     09
		
}
"answer": {
	"desc": %s Try with + : 
	"code-snippets": [
		df['mob_no'] = df['mob_no'].str.split('/')
		df = df.explode('mob_no')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67882067
"link": https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe
"question": {
	"title": Dictionary making for a transportation model from a Dataframe
	"desc": I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. 
}
"io": {
	"Frame-1": 
		 d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand
		 M = {p1:500, p2:500, p3:500}               # factory capacity
		 I = [c1,c2,c3,c4,c5]                         # Customers
		 J = [p1,p2,p3]                             # Factories
		 cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,
		 (p1,c4):8,    (p1,c5):10, ......
		  } 
		
	"Frame-2":
		  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  
		
}
"answer": {
	"desc": %s  
	"code-snippets": [
		df1 = df.set_index(["Unnamed: 0", "Unnamed: 1"])
		plants = df1.loc[np.NaN]  # remove demand from dataframe
		
		d = dict(df1.loc["demand"].T.squeeze().dropna().iteritems())
		M = dict(plants["capacity"].iteritems())
		I = list(plants.drop(columns="capacity").columns)
		J = list(plants.index)
		cost = dict(plants.drop(columns="capacity").stack().iteritems())
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870323
"link": https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe
"question": {
	"title": How to get list of previous n values of a column conditionally in DataFrame?
	"desc": My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? 
}
"io": {
	"Frame-1": 
		Subject     Score
		    1       15
		    2       0
		    3       18
		    2       30
		    3       17
		    1       5
		    4       9
		    2       7
		    1       20
		    1       8
		    2       9
		    1       12
		
	"Frame-2":
		Subject   Score Previous
		1       15      []
		2       0       []
		3       18      []
		2       30      [0]
		3       17      [18]
		1       5       [15]
		4       9       []
		2       7       [30,0]
		1       20      [5,15]
		1       8       [20,5,15]
		2       9       [7,30,0]
		1       12      [8,20,5]
		
}
"answer": {
	"desc": %s Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: 
	"code-snippets": [
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [
		    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)
		]
		
		----------------------------------------------------------------------
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [x.agg(list)[-2::-1]
		                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]
		df = df.sort_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870585
"link": https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column
"question": {
	"title": Python dataframe create index column based on other id column
	"desc": I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: 
}
"io": {
	"Frame-1": 
		ID                  Price
		000afb96ded6677c    1514.5
		000afb96ded6677c    13.0
		000afb96ded6677c    611.0
		000afb96ded6677c    723.0
		000afb96ded6677c    2065.0
		ffea14e87a4e1269    2286.0
		ffea14e87a4e1269    1150.0
		ffea14e87a4e1269    80.0
		fff455057ad492da    650.0
		fff5fc66c1fd66c2    450.0
		
	"Frame-2":
		ID                  Price    ID 2
		000afb96ded6677c    1514.5   1
		000afb96ded6677c    13.0     1
		000afb96ded6677c    611.0    1
		000afb96ded6677c    723.0    1
		000afb96ded6677c    2065.0   1
		ffea14e87a4e1269    2286.0   2
		ffea14e87a4e1269    1150.0   2
		ffea14e87a4e1269    80.0     2
		fff455057ad492da    650.0    3
		fff5fc66c1fd66c2    450.0    4
		
}
"answer": {
	"desc": %s Try + 1 : Or with : : 
	"code-snippets": [
		df['ID_2'] = df.groupby('ID').ngroup() + 1
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67856992
"link": https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list
"question": {
	"title": Element wise numeric comparison in Pandas dataframe column value with list
	"desc": I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: 
}
"io": {
	"Frame-1": 
		  |          A          |           B           |          C         |
		  |         Val         |          Val          |         Val        |
		  |---------------------|-----------------------|--------------------|
		0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		
	"Frame-2":
		  |  A    |   B   |  C   |
		  |  Max  |  Max  |  Max |
		  |-------|-------|------|
		0 | 28.68 | 18.42 | 1.37 |
		1 | 29.50 | 17.31 | 1.47 |
		2 | 29.87 | 20.45 | 1.39 |
		
}
"answer": {
	"desc": %s Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 µs, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 µs, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms 
	"code-snippets": [
		def bool_check(row):
		    col = row.name[0]
		    min_val = df1[pd.IndexSlice[col]].to_numpy()
		    max_val = df3[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		----------------------------------------------------------------------
		def bool_check(row):
		    col = row.name[0]
		    min_val = min_df[pd.IndexSlice[col]].to_numpy()
		    max_val = max_df[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		res = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67845362
"link": https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column
"question": {
	"title": Sort pandas df subset of rows (within a group) by specific column
	"desc": I have the following dataframe let’s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? 
}
"io": {
	"Frame-1": 
		
		A B C D E
		z k s 7 d
		z k s 6 l
		x t r 2 e
		x t r 1 x
		u c r 8 f
		u c r 9 h
		y t s 5 l
		y t s 2 o
		
	"Frame-2":
		
		A B C D E
		z k s 6 l
		z k s 7 d
		x t r 1 x
		x t r 2 e
		u c r 8 f
		u c r 9 h
		y t s 2 o
		y t s 5 l
		
}
"answer": {
	"desc": %s I think it should be as simple as this: 
	"code-snippets": [
		df = df.sort_values(["A", "B", "C", "D"])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61624957
"link": https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json
"question": {
	"title": Creating a table in pandas from json
	"desc": I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! 
}
"io": {
	"Frame-1": 
		0     website
		0  1  NaN
		1  2  NaN 
		
	"Frame-2":
		0     website
		0  1  https://bitcoin.org/
		1  2  https://litecoin.org/
		
}
"answer": {
	"desc": %s You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' 
	"code-snippets": [
		(
		    pd.DataFrame(data['data'])
		    .loc['urls']
		    .apply(lambda x: (x['technical_doc']+[''])[0])
		    .to_frame('website')
		)
		
		----------------------------------------------------------------------
		(
		    pd.DataFrame(data['data'])
		    .loc['logo']
		    .to_frame('logo')
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67822403
"link": https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1
"question": {
	"title": Replicate dataframe n number of times and increment another column by 1
	"desc": I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. 
}
"io": {
	"Frame-1": 
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		
	"Frame-2":
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		2      123     abc     2.20
		2      234     bcd     1.19
		2      345     cde     1.22
		3      123     abc     2.20
		3      234     bcd     1.19
		3      345     cde     1.22
		
}
"answer": {
	"desc": %s Something along these lines should work: 
	"code-snippets": [
		df1=df.copy()
		for i in range(1,20):
		    df['S/No']=df['S/No']+1
		    df1=pd.concat([df1,df])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67782727
"link": https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas
"question": {
	"title": Python - Delete lines from dataframe (pandas)
	"desc": I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. 
}
"io": {
	"Frame-1": 
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		10     3927905909    ///    ///
		11     3872948210    ///   0.15
		12  9783293003781    ///   0.15
		13  9783423246842    ///    ///
		14  9783423247146    ///    ///
		15  9783423246934    ///    ///
		16     387294116x    ///    ///
		17  9783935597456   0,16   0.15
		18  9783423204545    ///    ///
		
	"Frame-2":
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		
}
"answer": {
	"desc": %s Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. 
	"code-snippets": [
		data[['1', '2']] = data[['1', '2']].replace({"///": np.nan, ",": "."}, regex=True)
		                                   .astype(float)
		data = data.loc[data[["1", "2"]].ge(1.).all(axis="columns")]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67770056
"link": https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe
"question": {
	"title": How to create columns from a string in a dataframe?
	"desc": WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. 
}
"io": {
	"Frame-1": 
		    long string
		0   ha: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ho...
		1   hi: (tra: 1 la: 2) \n ha: (tra: 1 la: 2) \n ho...
		2   ho: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ha...
		
	"Frame-2":
		    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la
		0   1       2       1       2       1       2
		1   1       2       1       2       1       2
		2   1       2       1       2       1       2
		
}
"answer": {
	"desc": %s  Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get 
	"code-snippets": [
		ndf = (df["long string"]
		         .str.extractall(r"(ha|hi|ho):\s\((?:tra|la):\s(\d+)\s(?:tra|la):\s(\d+)\)")
		         .droplevel("match")
		         .set_index(0, append=True)
		         .set_axis(["tra", "la"], axis=1)
		         .unstack()
		         .swaplevel(axis=1))
		ndf.columns = ndf.columns.map("-".join)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67696814
"link": https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe
"question": {
	"title": Convert dictionary with sub-list of dictionaries into pandas dataframe
	"desc": I have this code with a dictionary "dict": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! 
}
"io": {
	"Frame-1": 
		                                                 0
		2000  {'team': 'Manchester United', 'points': '91'}
		2001  {'team': 'Manchester United', 'points': '80'}
		2002            {'team': 'Arsenal', 'points': '87'}
		
		
	"Frame-2":
		        team                  points
		2000    Manchester United     91
		2001    Manchester United     80
		2002    Arsenal               87
		
		
}
"answer": {
	"desc": %s Tr this. This would depend on how large your data is. 
	"code-snippets": [
		
		diction =  {
		    '2000': [{'team': 'Manchester United', 'points': '91'}],
		    '2001': [{'team': 'Manchester United', 'points': '80'}],
		    '2002': [{'team': 'Arsenal', 'points': '87'}]
		}
		transformed_dict= {x:d for x,y in diction.items() for d in y }
		df = pd.DataFrame(transformed_dict)
		df.T
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870323
"link": https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe
"question": {
	"title": How to get list of previous n values of a column conditionally in DataFrame?
	"desc": My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? 
}
"io": {
	"Frame-1": 
		Subject     Score
		    1       15
		    2       0
		    3       18
		    2       30
		    3       17
		    1       5
		    4       9
		    2       7
		    1       20
		    1       8
		    2       9
		    1       12
		
	"Frame-2":
		Subject   Score Previous
		1       15      []
		2       0       []
		3       18      []
		2       30      [0]
		3       17      [18]
		1       5       [15]
		4       9       []
		2       7       [30,0]
		1       20      [5,15]
		1       8       [20,5,15]
		2       9       [7,30,0]
		1       12      [8,20,5]
		
}
"answer": {
	"desc": %s Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: 
	"code-snippets": [
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [
		    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)
		]
		
		----------------------------------------------------------------------
		window = 3
		df = df.sort_values('Subject')
		df['Previous'] = [x.agg(list)[-2::-1]
		                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]
		df = df.sort_index()
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67870585
"link": https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column
"question": {
	"title": Python dataframe create index column based on other id column
	"desc": I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: 
}
"io": {
	"Frame-1": 
		ID                  Price
		000afb96ded6677c    1514.5
		000afb96ded6677c    13.0
		000afb96ded6677c    611.0
		000afb96ded6677c    723.0
		000afb96ded6677c    2065.0
		ffea14e87a4e1269    2286.0
		ffea14e87a4e1269    1150.0
		ffea14e87a4e1269    80.0
		fff455057ad492da    650.0
		fff5fc66c1fd66c2    450.0
		
	"Frame-2":
		ID                  Price    ID 2
		000afb96ded6677c    1514.5   1
		000afb96ded6677c    13.0     1
		000afb96ded6677c    611.0    1
		000afb96ded6677c    723.0    1
		000afb96ded6677c    2065.0   1
		ffea14e87a4e1269    2286.0   2
		ffea14e87a4e1269    1150.0   2
		ffea14e87a4e1269    80.0     2
		fff455057ad492da    650.0    3
		fff5fc66c1fd66c2    450.0    4
		
}
"answer": {
	"desc": %s Try + 1 : Or with : : 
	"code-snippets": [
		df['ID_2'] = df.groupby('ID').ngroup() + 1
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67856992
"link": https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list
"question": {
	"title": Element wise numeric comparison in Pandas dataframe column value with list
	"desc": I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: 
}
"io": {
	"Frame-1": 
		  |          A          |           B           |          C         |
		  |         Val         |          Val          |         Val        |
		  |---------------------|-----------------------|--------------------|
		0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |
		
	"Frame-2":
		  |  A    |   B   |  C   |
		  |  Max  |  Max  |  Max |
		  |-------|-------|------|
		0 | 28.68 | 18.42 | 1.37 |
		1 | 29.50 | 17.31 | 1.47 |
		2 | 29.87 | 20.45 | 1.39 |
		
}
"answer": {
	"desc": %s Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 µs, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 µs, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms 
	"code-snippets": [
		def bool_check(row):
		    col = row.name[0]
		    min_val = df1[pd.IndexSlice[col]].to_numpy()
		    max_val = df3[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		----------------------------------------------------------------------
		def bool_check(row):
		    col = row.name[0]
		    min_val = min_df[pd.IndexSlice[col]].to_numpy()
		    max_val = max_df[pd.IndexSlice[col]].to_numpy()
		    x = np.array(row.tolist())
		    return list((x >= min_val) & (x <= max_val))
		
		res = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67845362
"link": https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column
"question": {
	"title": Sort pandas df subset of rows (within a group) by specific column
	"desc": I have the following dataframe let’s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? 
}
"io": {
	"Frame-1": 
		
		A B C D E
		z k s 7 d
		z k s 6 l
		x t r 2 e
		x t r 1 x
		u c r 8 f
		u c r 9 h
		y t s 5 l
		y t s 2 o
		
	"Frame-2":
		
		A B C D E
		z k s 6 l
		z k s 7 d
		x t r 1 x
		x t r 2 e
		u c r 8 f
		u c r 9 h
		y t s 2 o
		y t s 5 l
		
}
"answer": {
	"desc": %s I think it should be as simple as this: 
	"code-snippets": [
		df = df.sort_values(["A", "B", "C", "D"])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 61624957
"link": https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json
"question": {
	"title": Creating a table in pandas from json
	"desc": I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! 
}
"io": {
	"Frame-1": 
		0     website
		0  1  NaN
		1  2  NaN 
		
	"Frame-2":
		0     website
		0  1  https://bitcoin.org/
		1  2  https://litecoin.org/
		
}
"answer": {
	"desc": %s You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' 
	"code-snippets": [
		(
		    pd.DataFrame(data['data'])
		    .loc['urls']
		    .apply(lambda x: (x['technical_doc']+[''])[0])
		    .to_frame('website')
		)
		
		----------------------------------------------------------------------
		(
		    pd.DataFrame(data['data'])
		    .loc['logo']
		    .to_frame('logo')
		)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67822403
"link": https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1
"question": {
	"title": Replicate dataframe n number of times and increment another column by 1
	"desc": I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. 
}
"io": {
	"Frame-1": 
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		
	"Frame-2":
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		2      123     abc     2.20
		2      234     bcd     1.19
		2      345     cde     1.22
		3      123     abc     2.20
		3      234     bcd     1.19
		3      345     cde     1.22
		
}
"answer": {
	"desc": %s Something along these lines should work: 
	"code-snippets": [
		df1=df.copy()
		for i in range(1,20):
		    df['S/No']=df['S/No']+1
		    df1=pd.concat([df1,df])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67782727
"link": https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas
"question": {
	"title": Python - Delete lines from dataframe (pandas)
	"desc": I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. 
}
"io": {
	"Frame-1": 
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		10     3927905909    ///    ///
		11     3872948210    ///   0.15
		12  9783293003781    ///   0.15
		13  9783423246842    ///    ///
		14  9783423247146    ///    ///
		15  9783423246934    ///    ///
		16     387294116x    ///    ///
		17  9783935597456   0,16   0.15
		18  9783423204545    ///    ///
		
	"Frame-2":
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		
}
"answer": {
	"desc": %s Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. 
	"code-snippets": [
		data[['1', '2']] = data[['1', '2']].replace({"///": np.nan, ",": "."}, regex=True)
		                                   .astype(float)
		data = data.loc[data[["1", "2"]].ge(1.).all(axis="columns")]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67770056
"link": https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe
"question": {
	"title": How to create columns from a string in a dataframe?
	"desc": WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. 
}
"io": {
	"Frame-1": 
		    long string
		0   ha: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ho...
		1   hi: (tra: 1 la: 2) \n ha: (tra: 1 la: 2) \n ho...
		2   ho: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ha...
		
	"Frame-2":
		    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la
		0   1       2       1       2       1       2
		1   1       2       1       2       1       2
		2   1       2       1       2       1       2
		
}
"answer": {
	"desc": %s  Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get 
	"code-snippets": [
		ndf = (df["long string"]
		         .str.extractall(r"(ha|hi|ho):\s\((?:tra|la):\s(\d+)\s(?:tra|la):\s(\d+)\)")
		         .droplevel("match")
		         .set_index(0, append=True)
		         .set_axis(["tra", "la"], axis=1)
		         .unstack()
		         .swaplevel(axis=1))
		ndf.columns = ndf.columns.map("-".join)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67696814
"link": https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe
"question": {
	"title": Convert dictionary with sub-list of dictionaries into pandas dataframe
	"desc": I have this code with a dictionary "dict": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! 
}
"io": {
	"Frame-1": 
		                                                 0
		2000  {'team': 'Manchester United', 'points': '91'}
		2001  {'team': 'Manchester United', 'points': '80'}
		2002            {'team': 'Arsenal', 'points': '87'}
		
		
	"Frame-2":
		        team                  points
		2000    Manchester United     91
		2001    Manchester United     80
		2002    Arsenal               87
		
		
}
"answer": {
	"desc": %s Tr this. This would depend on how large your data is. 
	"code-snippets": [
		
		diction =  {
		    '2000': [{'team': 'Manchester United', 'points': '91'}],
		    '2001': [{'team': 'Manchester United', 'points': '80'}],
		    '2002': [{'team': 'Arsenal', 'points': '87'}]
		}
		transformed_dict= {x:d for x,y in diction.items() for d in y }
		df = pd.DataFrame(transformed_dict)
		df.T
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67672199
"link": https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa
"question": {
	"title": How to concat the row output of iterrows to another pandas DataFrame with the same columns?
	"desc": Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? 
}
"io": {
	"Frame-1": 
		A    B    C
		4    c    12
		5    d    19
		2    b    43
		
	"Frame-2":
		    0   A   B   C
		A   2   NaN     NaN     NaN
		B   b   NaN     NaN     NaN
		C   43  NaN     NaN     NaN
		0   NaN     4.0     c   12.0
		1   NaN     5.0     d   19.0
		
}
"answer": {
	"desc": %s I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: 
	"code-snippets": [
		pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67662415
"link": https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe
"question": {
	"title": Working with list inside a Pandas dataframe
	"desc": I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? 
}
"io": {
	"Frame-1": 
		ID | Column1 |
		0  |  []     | 
		1  |  [1,2]  | 
		2  |  []     |
		
	"Frame-2":
		ID | Column1 | Column2 |
		0  |  []     |   0     |
		1  |  [1,2]  |   2     |
		2  |  []     |   0     |
		
}
"answer": {
	"desc": %s I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: 
	"code-snippets": [
		df['Column2'] = [len(x) for x in df['Column1']]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67822403
"link": https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1
"question": {
	"title": Replicate dataframe n number of times and increment another column by 1
	"desc": I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. 
}
"io": {
	"Frame-1": 
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		
	"Frame-2":
		S/No Column1 Column2 Column3
		1      123     abc     2.20
		1      234     bcd     1.19
		1      345     cde     1.22
		2      123     abc     2.20
		2      234     bcd     1.19
		2      345     cde     1.22
		3      123     abc     2.20
		3      234     bcd     1.19
		3      345     cde     1.22
		
}
"answer": {
	"desc": %s Something along these lines should work: 
	"code-snippets": [
		df1=df.copy()
		for i in range(1,20):
		    df['S/No']=df['S/No']+1
		    df1=pd.concat([df1,df])
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67782727
"link": https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas
"question": {
	"title": Python - Delete lines from dataframe (pandas)
	"desc": I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. 
}
"io": {
	"Frame-1": 
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		10     3927905909    ///    ///
		11     3872948210    ///   0.15
		12  9783293003781    ///   0.15
		13  9783423246842    ///    ///
		14  9783423247146    ///    ///
		15  9783423246934    ///    ///
		16     387294116x    ///    ///
		17  9783935597456   0,16   0.15
		18  9783423204545    ///    ///
		
	"Frame-2":
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		
}
"answer": {
	"desc": %s Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. 
	"code-snippets": [
		data[['1', '2']] = data[['1', '2']].replace({"///": np.nan, ",": "."}, regex=True)
		                                   .astype(float)
		data = data.loc[data[["1", "2"]].ge(1.).all(axis="columns")]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67770056
"link": https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe
"question": {
	"title": How to create columns from a string in a dataframe?
	"desc": WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. 
}
"io": {
	"Frame-1": 
		    long string
		0   ha: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ho...
		1   hi: (tra: 1 la: 2) \n ha: (tra: 1 la: 2) \n ho...
		2   ho: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ha...
		
	"Frame-2":
		    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la
		0   1       2       1       2       1       2
		1   1       2       1       2       1       2
		2   1       2       1       2       1       2
		
}
"answer": {
	"desc": %s  Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get 
	"code-snippets": [
		ndf = (df["long string"]
		         .str.extractall(r"(ha|hi|ho):\s\((?:tra|la):\s(\d+)\s(?:tra|la):\s(\d+)\)")
		         .droplevel("match")
		         .set_index(0, append=True)
		         .set_axis(["tra", "la"], axis=1)
		         .unstack()
		         .swaplevel(axis=1))
		ndf.columns = ndf.columns.map("-".join)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67696814
"link": https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe
"question": {
	"title": Convert dictionary with sub-list of dictionaries into pandas dataframe
	"desc": I have this code with a dictionary "dict": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! 
}
"io": {
	"Frame-1": 
		                                                 0
		2000  {'team': 'Manchester United', 'points': '91'}
		2001  {'team': 'Manchester United', 'points': '80'}
		2002            {'team': 'Arsenal', 'points': '87'}
		
		
	"Frame-2":
		        team                  points
		2000    Manchester United     91
		2001    Manchester United     80
		2002    Arsenal               87
		
		
}
"answer": {
	"desc": %s Tr this. This would depend on how large your data is. 
	"code-snippets": [
		
		diction =  {
		    '2000': [{'team': 'Manchester United', 'points': '91'}],
		    '2001': [{'team': 'Manchester United', 'points': '80'}],
		    '2002': [{'team': 'Arsenal', 'points': '87'}]
		}
		transformed_dict= {x:d for x,y in diction.items() for d in y }
		df = pd.DataFrame(transformed_dict)
		df.T
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67672199
"link": https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa
"question": {
	"title": How to concat the row output of iterrows to another pandas DataFrame with the same columns?
	"desc": Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? 
}
"io": {
	"Frame-1": 
		A    B    C
		4    c    12
		5    d    19
		2    b    43
		
	"Frame-2":
		    0   A   B   C
		A   2   NaN     NaN     NaN
		B   b   NaN     NaN     NaN
		C   43  NaN     NaN     NaN
		0   NaN     4.0     c   12.0
		1   NaN     5.0     d   19.0
		
}
"answer": {
	"desc": %s I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: 
	"code-snippets": [
		pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67662415
"link": https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe
"question": {
	"title": Working with list inside a Pandas dataframe
	"desc": I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? 
}
"io": {
	"Frame-1": 
		ID | Column1 |
		0  |  []     | 
		1  |  [1,2]  | 
		2  |  []     |
		
	"Frame-2":
		ID | Column1 | Column2 |
		0  |  []     |   0     |
		1  |  [1,2]  |   2     |
		2  |  []     |   0     |
		
}
"answer": {
	"desc": %s I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: 
	"code-snippets": [
		df['Column2'] = [len(x) for x in df['Column1']]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67595888
"link": https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list
"question": {
	"title": how to extract each numbers from pandas string column to list?
	"desc": How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: 
}
"io": {
	"Frame-1": 
		Column_A
		11.2 some text 17 some text 21
		some text 25.2 4.1 some text 53 17 78
		121.1 bla bla bla 14 some text
		12 some text
		
	"Frame-2":
		listA[0] = 11.2 listA[1] = 17 listA[2] = 21
		listB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78
		listC[0] = 121.1 listC[1] = 14
		listD[0] = 12
		
}
"answer": {
	"desc": %s You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: 
	"code-snippets": [
		df['Column_A'].apply(lambda x: re.findall(r"[-+]?\d*\.\d+|\d+", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()
		
		----------------------------------------------------------------------
		listA, listB, listC, listD = df.Column_A.str.findall(r"[-+]?\d*\.\d+|\d+")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67782727
"link": https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas
"question": {
	"title": Python - Delete lines from dataframe (pandas)
	"desc": I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. 
}
"io": {
	"Frame-1": 
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		10     3927905909    ///    ///
		11     3872948210    ///   0.15
		12  9783293003781    ///   0.15
		13  9783423246842    ///    ///
		14  9783423247146    ///    ///
		15  9783423246934    ///    ///
		16     387294116x    ///    ///
		17  9783935597456   0,16   0.15
		18  9783423204545    ///    ///
		
	"Frame-2":
		                0      1      2
		0   9783630876672  12,35   2.62
		1   9783423282789  11,67   6.07
		2   9783833879500  17,25  12.40
		3   9783898798822   6,91   1.16
		4   9783453281417  12,93   2.84
		5   9783630876672  12,35   4.08
		6   9783423282789  11,67   6.07
		7   9783833879500  17,25   9.94
		8   9783898798822   6,91   2.96
		9   9783453281417  12,93   2.68
		
}
"answer": {
	"desc": %s Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. 
	"code-snippets": [
		data[['1', '2']] = data[['1', '2']].replace({"///": np.nan, ",": "."}, regex=True)
		                                   .astype(float)
		data = data.loc[data[["1", "2"]].ge(1.).all(axis="columns")]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67770056
"link": https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe
"question": {
	"title": How to create columns from a string in a dataframe?
	"desc": WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. 
}
"io": {
	"Frame-1": 
		    long string
		0   ha: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ho...
		1   hi: (tra: 1 la: 2) \n ha: (tra: 1 la: 2) \n ho...
		2   ho: (tra: 1 la: 2) \n hi: (tra: 1 la: 2) \n ha...
		
	"Frame-2":
		    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la
		0   1       2       1       2       1       2
		1   1       2       1       2       1       2
		2   1       2       1       2       1       2
		
}
"answer": {
	"desc": %s  Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get 
	"code-snippets": [
		ndf = (df["long string"]
		         .str.extractall(r"(ha|hi|ho):\s\((?:tra|la):\s(\d+)\s(?:tra|la):\s(\d+)\)")
		         .droplevel("match")
		         .set_index(0, append=True)
		         .set_axis(["tra", "la"], axis=1)
		         .unstack()
		         .swaplevel(axis=1))
		ndf.columns = ndf.columns.map("-".join)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67696814
"link": https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe
"question": {
	"title": Convert dictionary with sub-list of dictionaries into pandas dataframe
	"desc": I have this code with a dictionary "dict": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! 
}
"io": {
	"Frame-1": 
		                                                 0
		2000  {'team': 'Manchester United', 'points': '91'}
		2001  {'team': 'Manchester United', 'points': '80'}
		2002            {'team': 'Arsenal', 'points': '87'}
		
		
	"Frame-2":
		        team                  points
		2000    Manchester United     91
		2001    Manchester United     80
		2002    Arsenal               87
		
		
}
"answer": {
	"desc": %s Tr this. This would depend on how large your data is. 
	"code-snippets": [
		
		diction =  {
		    '2000': [{'team': 'Manchester United', 'points': '91'}],
		    '2001': [{'team': 'Manchester United', 'points': '80'}],
		    '2002': [{'team': 'Arsenal', 'points': '87'}]
		}
		transformed_dict= {x:d for x,y in diction.items() for d in y }
		df = pd.DataFrame(transformed_dict)
		df.T
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67672199
"link": https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa
"question": {
	"title": How to concat the row output of iterrows to another pandas DataFrame with the same columns?
	"desc": Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? 
}
"io": {
	"Frame-1": 
		A    B    C
		4    c    12
		5    d    19
		2    b    43
		
	"Frame-2":
		    0   A   B   C
		A   2   NaN     NaN     NaN
		B   b   NaN     NaN     NaN
		C   43  NaN     NaN     NaN
		0   NaN     4.0     c   12.0
		1   NaN     5.0     d   19.0
		
}
"answer": {
	"desc": %s I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: 
	"code-snippets": [
		pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67662415
"link": https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe
"question": {
	"title": Working with list inside a Pandas dataframe
	"desc": I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? 
}
"io": {
	"Frame-1": 
		ID | Column1 |
		0  |  []     | 
		1  |  [1,2]  | 
		2  |  []     |
		
	"Frame-2":
		ID | Column1 | Column2 |
		0  |  []     |   0     |
		1  |  [1,2]  |   2     |
		2  |  []     |   0     |
		
}
"answer": {
	"desc": %s I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: 
	"code-snippets": [
		df['Column2'] = [len(x) for x in df['Column1']]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67595888
"link": https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list
"question": {
	"title": how to extract each numbers from pandas string column to list?
	"desc": How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: 
}
"io": {
	"Frame-1": 
		Column_A
		11.2 some text 17 some text 21
		some text 25.2 4.1 some text 53 17 78
		121.1 bla bla bla 14 some text
		12 some text
		
	"Frame-2":
		listA[0] = 11.2 listA[1] = 17 listA[2] = 21
		listB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78
		listC[0] = 121.1 listC[1] = 14
		listD[0] = 12
		
}
"answer": {
	"desc": %s You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: 
	"code-snippets": [
		df['Column_A'].apply(lambda x: re.findall(r"[-+]?\d*\.\d+|\d+", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()
		
		----------------------------------------------------------------------
		listA, listB, listC, listD = df.Column_A.str.findall(r"[-+]?\d*\.\d+|\d+")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67580031
"link": https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas
"question": {
	"title": Groupby, counts in ranges and spread in Pandas
	"desc": I want to group by "" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then "transpose" the dataframe and making the ranges new columns Expected output: 
}
"io": {
	"Frame-1": 
		
		           a    b
		   a        
		(0, 10]     2   BBB
		(10, 20]    3   BBB
		(20, 30]    1   AAA
		
	"Frame-2":
		    (0, 10]   (10, 20]   (20, 30]
		 
		AAA    0          0         1      
		BBB    2          3         0
		
		
}
"answer": {
	"desc": %s You can use : : 
	"code-snippets": [
		df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67561501
"link": https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns
"question": {
	"title": splitting list in dataframe columns to separate columns
	"desc": my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success 
}
"io": {
	"Frame-1": 
		    col1     col2     col3
		0  [1, a]  [1, a1]  [1, a2]
		1  [2, b]  [2, b1]  [2, b2]
		2  [3, c]  [3, c1]  [3, c2]
		
	"Frame-2":
		   col1     col2     col3  col4
		0  a         a1      a2    1
		1  b         b1      b2    2
		2  c         c1      c2    3
		
}
"answer": {
	"desc": %s Here is a way using and : 
	"code-snippets": [
		df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67696814
"link": https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe
"question": {
	"title": Convert dictionary with sub-list of dictionaries into pandas dataframe
	"desc": I have this code with a dictionary "dict": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! 
}
"io": {
	"Frame-1": 
		                                                 0
		2000  {'team': 'Manchester United', 'points': '91'}
		2001  {'team': 'Manchester United', 'points': '80'}
		2002            {'team': 'Arsenal', 'points': '87'}
		
		
	"Frame-2":
		        team                  points
		2000    Manchester United     91
		2001    Manchester United     80
		2002    Arsenal               87
		
		
}
"answer": {
	"desc": %s Tr this. This would depend on how large your data is. 
	"code-snippets": [
		
		diction =  {
		    '2000': [{'team': 'Manchester United', 'points': '91'}],
		    '2001': [{'team': 'Manchester United', 'points': '80'}],
		    '2002': [{'team': 'Arsenal', 'points': '87'}]
		}
		transformed_dict= {x:d for x,y in diction.items() for d in y }
		df = pd.DataFrame(transformed_dict)
		df.T
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67672199
"link": https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa
"question": {
	"title": How to concat the row output of iterrows to another pandas DataFrame with the same columns?
	"desc": Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? 
}
"io": {
	"Frame-1": 
		A    B    C
		4    c    12
		5    d    19
		2    b    43
		
	"Frame-2":
		    0   A   B   C
		A   2   NaN     NaN     NaN
		B   b   NaN     NaN     NaN
		C   43  NaN     NaN     NaN
		0   NaN     4.0     c   12.0
		1   NaN     5.0     d   19.0
		
}
"answer": {
	"desc": %s I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: 
	"code-snippets": [
		pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67662415
"link": https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe
"question": {
	"title": Working with list inside a Pandas dataframe
	"desc": I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? 
}
"io": {
	"Frame-1": 
		ID | Column1 |
		0  |  []     | 
		1  |  [1,2]  | 
		2  |  []     |
		
	"Frame-2":
		ID | Column1 | Column2 |
		0  |  []     |   0     |
		1  |  [1,2]  |   2     |
		2  |  []     |   0     |
		
}
"answer": {
	"desc": %s I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: 
	"code-snippets": [
		df['Column2'] = [len(x) for x in df['Column1']]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67595888
"link": https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list
"question": {
	"title": how to extract each numbers from pandas string column to list?
	"desc": How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: 
}
"io": {
	"Frame-1": 
		Column_A
		11.2 some text 17 some text 21
		some text 25.2 4.1 some text 53 17 78
		121.1 bla bla bla 14 some text
		12 some text
		
	"Frame-2":
		listA[0] = 11.2 listA[1] = 17 listA[2] = 21
		listB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78
		listC[0] = 121.1 listC[1] = 14
		listD[0] = 12
		
}
"answer": {
	"desc": %s You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: 
	"code-snippets": [
		df['Column_A'].apply(lambda x: re.findall(r"[-+]?\d*\.\d+|\d+", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()
		
		----------------------------------------------------------------------
		listA, listB, listC, listD = df.Column_A.str.findall(r"[-+]?\d*\.\d+|\d+")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67580031
"link": https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas
"question": {
	"title": Groupby, counts in ranges and spread in Pandas
	"desc": I want to group by "" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then "transpose" the dataframe and making the ranges new columns Expected output: 
}
"io": {
	"Frame-1": 
		
		           a    b
		   a        
		(0, 10]     2   BBB
		(10, 20]    3   BBB
		(20, 30]    1   AAA
		
	"Frame-2":
		    (0, 10]   (10, 20]   (20, 30]
		 
		AAA    0          0         1      
		BBB    2          3         0
		
		
}
"answer": {
	"desc": %s You can use : : 
	"code-snippets": [
		df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67561501
"link": https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns
"question": {
	"title": splitting list in dataframe columns to separate columns
	"desc": my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success 
}
"io": {
	"Frame-1": 
		    col1     col2     col3
		0  [1, a]  [1, a1]  [1, a2]
		1  [2, b]  [2, b1]  [2, b2]
		2  [3, c]  [3, c1]  [3, c2]
		
	"Frame-2":
		   col1     col2     col3  col4
		0  a         a1      a2    1
		1  b         b1      b2    2
		2  c         c1      c2    3
		
}
"answer": {
	"desc": %s Here is a way using and : 
	"code-snippets": [
		df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67506798
"link": https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met
"question": {
	"title": Reformat Dataframe / Add rows when condition is met
	"desc": I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only "1's". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! 
}
"io": {
	"Frame-1": 
		   ColA        ColB
		2021-03-09       1
		2021-03-09       3
		2021-03-10       2
		2021-03-10       1
		2021-03-10       2
		2021-03-11       2
		
	"Frame-2":
		   ColA         ColB
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-11       1
		2021-03-11       1
		
}
"answer": {
	"desc": %s Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		df = (pd.DataFrame(np.repeat(df.ColA, df.ColB))
		        .assign(ColB=1))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67672199
"link": https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa
"question": {
	"title": How to concat the row output of iterrows to another pandas DataFrame with the same columns?
	"desc": Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? 
}
"io": {
	"Frame-1": 
		A    B    C
		4    c    12
		5    d    19
		2    b    43
		
	"Frame-2":
		    0   A   B   C
		A   2   NaN     NaN     NaN
		B   b   NaN     NaN     NaN
		C   43  NaN     NaN     NaN
		0   NaN     4.0     c   12.0
		1   NaN     5.0     d   19.0
		
}
"answer": {
	"desc": %s I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: 
	"code-snippets": [
		pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67662415
"link": https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe
"question": {
	"title": Working with list inside a Pandas dataframe
	"desc": I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? 
}
"io": {
	"Frame-1": 
		ID | Column1 |
		0  |  []     | 
		1  |  [1,2]  | 
		2  |  []     |
		
	"Frame-2":
		ID | Column1 | Column2 |
		0  |  []     |   0     |
		1  |  [1,2]  |   2     |
		2  |  []     |   0     |
		
}
"answer": {
	"desc": %s I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: 
	"code-snippets": [
		df['Column2'] = [len(x) for x in df['Column1']]
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67595888
"link": https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list
"question": {
	"title": how to extract each numbers from pandas string column to list?
	"desc": How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: 
}
"io": {
	"Frame-1": 
		Column_A
		11.2 some text 17 some text 21
		some text 25.2 4.1 some text 53 17 78
		121.1 bla bla bla 14 some text
		12 some text
		
	"Frame-2":
		listA[0] = 11.2 listA[1] = 17 listA[2] = 21
		listB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78
		listC[0] = 121.1 listC[1] = 14
		listD[0] = 12
		
}
"answer": {
	"desc": %s You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: 
	"code-snippets": [
		df['Column_A'].apply(lambda x: re.findall(r"[-+]?\d*\.\d+|\d+", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()
		
		----------------------------------------------------------------------
		listA, listB, listC, listD = df.Column_A.str.findall(r"[-+]?\d*\.\d+|\d+")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67580031
"link": https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas
"question": {
	"title": Groupby, counts in ranges and spread in Pandas
	"desc": I want to group by "" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then "transpose" the dataframe and making the ranges new columns Expected output: 
}
"io": {
	"Frame-1": 
		
		           a    b
		   a        
		(0, 10]     2   BBB
		(10, 20]    3   BBB
		(20, 30]    1   AAA
		
	"Frame-2":
		    (0, 10]   (10, 20]   (20, 30]
		 
		AAA    0          0         1      
		BBB    2          3         0
		
		
}
"answer": {
	"desc": %s You can use : : 
	"code-snippets": [
		df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67561501
"link": https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns
"question": {
	"title": splitting list in dataframe columns to separate columns
	"desc": my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success 
}
"io": {
	"Frame-1": 
		    col1     col2     col3
		0  [1, a]  [1, a1]  [1, a2]
		1  [2, b]  [2, b1]  [2, b2]
		2  [3, c]  [3, c1]  [3, c2]
		
	"Frame-2":
		   col1     col2     col3  col4
		0  a         a1      a2    1
		1  b         b1      b2    2
		2  c         c1      c2    3
		
}
"answer": {
	"desc": %s Here is a way using and : 
	"code-snippets": [
		df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67506798
"link": https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met
"question": {
	"title": Reformat Dataframe / Add rows when condition is met
	"desc": I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only "1's". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! 
}
"io": {
	"Frame-1": 
		   ColA        ColB
		2021-03-09       1
		2021-03-09       3
		2021-03-10       2
		2021-03-10       1
		2021-03-10       2
		2021-03-11       2
		
	"Frame-2":
		   ColA         ColB
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-11       1
		2021-03-11       1
		
}
"answer": {
	"desc": %s Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		df = (pd.DataFrame(np.repeat(df.ColA, df.ColB))
		        .assign(ColB=1))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67595888
"link": https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list
"question": {
	"title": how to extract each numbers from pandas string column to list?
	"desc": How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: 
}
"io": {
	"Frame-1": 
		Column_A
		11.2 some text 17 some text 21
		some text 25.2 4.1 some text 53 17 78
		121.1 bla bla bla 14 some text
		12 some text
		
	"Frame-2":
		listA[0] = 11.2 listA[1] = 17 listA[2] = 21
		listB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78
		listC[0] = 121.1 listC[1] = 14
		listD[0] = 12
		
}
"answer": {
	"desc": %s You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: 
	"code-snippets": [
		df['Column_A'].apply(lambda x: re.findall(r"[-+]?\d*\.\d+|\d+", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()
		
		----------------------------------------------------------------------
		listA, listB, listC, listD = df.Column_A.str.findall(r"[-+]?\d*\.\d+|\d+")
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67580031
"link": https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas
"question": {
	"title": Groupby, counts in ranges and spread in Pandas
	"desc": I want to group by "" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then "transpose" the dataframe and making the ranges new columns Expected output: 
}
"io": {
	"Frame-1": 
		
		           a    b
		   a        
		(0, 10]     2   BBB
		(10, 20]    3   BBB
		(20, 30]    1   AAA
		
	"Frame-2":
		    (0, 10]   (10, 20]   (20, 30]
		 
		AAA    0          0         1      
		BBB    2          3         0
		
		
}
"answer": {
	"desc": %s You can use : : 
	"code-snippets": [
		df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67561501
"link": https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns
"question": {
	"title": splitting list in dataframe columns to separate columns
	"desc": my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success 
}
"io": {
	"Frame-1": 
		    col1     col2     col3
		0  [1, a]  [1, a1]  [1, a2]
		1  [2, b]  [2, b1]  [2, b2]
		2  [3, c]  [3, c1]  [3, c2]
		
	"Frame-2":
		   col1     col2     col3  col4
		0  a         a1      a2    1
		1  b         b1      b2    2
		2  c         c1      c2    3
		
}
"answer": {
	"desc": %s Here is a way using and : 
	"code-snippets": [
		df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67506798
"link": https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met
"question": {
	"title": Reformat Dataframe / Add rows when condition is met
	"desc": I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only "1's". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! 
}
"io": {
	"Frame-1": 
		   ColA        ColB
		2021-03-09       1
		2021-03-09       3
		2021-03-10       2
		2021-03-10       1
		2021-03-10       2
		2021-03-11       2
		
	"Frame-2":
		   ColA         ColB
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-11       1
		2021-03-11       1
		
}
"answer": {
	"desc": %s Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		df = (pd.DataFrame(np.repeat(df.ColA, df.ColB))
		        .assign(ColB=1))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67580031
"link": https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas
"question": {
	"title": Groupby, counts in ranges and spread in Pandas
	"desc": I want to group by "" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then "transpose" the dataframe and making the ranges new columns Expected output: 
}
"io": {
	"Frame-1": 
		
		           a    b
		   a        
		(0, 10]     2   BBB
		(10, 20]    3   BBB
		(20, 30]    1   AAA
		
	"Frame-2":
		    (0, 10]   (10, 20]   (20, 30]
		 
		AAA    0          0         1      
		BBB    2          3         0
		
		
}
"answer": {
	"desc": %s You can use : : 
	"code-snippets": [
		df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67561501
"link": https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns
"question": {
	"title": splitting list in dataframe columns to separate columns
	"desc": my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success 
}
"io": {
	"Frame-1": 
		    col1     col2     col3
		0  [1, a]  [1, a1]  [1, a2]
		1  [2, b]  [2, b1]  [2, b2]
		2  [3, c]  [3, c1]  [3, c2]
		
	"Frame-2":
		   col1     col2     col3  col4
		0  a         a1      a2    1
		1  b         b1      b2    2
		2  c         c1      c2    3
		
}
"answer": {
	"desc": %s Here is a way using and : 
	"code-snippets": [
		df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67506798
"link": https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met
"question": {
	"title": Reformat Dataframe / Add rows when condition is met
	"desc": I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only "1's". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! 
}
"io": {
	"Frame-1": 
		   ColA        ColB
		2021-03-09       1
		2021-03-09       3
		2021-03-10       2
		2021-03-10       1
		2021-03-10       2
		2021-03-11       2
		
	"Frame-2":
		   ColA         ColB
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-11       1
		2021-03-11       1
		
}
"answer": {
	"desc": %s Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		df = (pd.DataFrame(np.repeat(df.ColA, df.ColB))
		        .assign(ColB=1))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67361824
"link": https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns
"question": {
	"title": Convert dataframe objects to float by iterating over columns
	"desc": I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance 
}
"io": {
	"Frame-1": 
		   c1   c2
		0  -    75.0%
		1 -5.5% 65.8%
		.
		n  -    6.9%
		
	"Frame-2":
		   c1    c2
		0  0.0   75.0
		1 -5.5   65.8
		.
		n  0.0    6.9
		
}
"answer": {
	"desc": %s EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. 
	"code-snippets": [
		# Thanks to @tdy
		df.replace({'\%':'', r'^\s*-\s*$':0}, regex=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67357814
"link": https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan
"question": {
	"title": Python pandas dataframe apply result of function to multiple columns where NaN
	"desc": I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result: 
}
"io": {
	"Frame-1": 
		    x   y   z
		0   a   1.0 2.0
		1   b   1.0 2.0
		2   c   1.0 2.0
		3   d   NaN NaN
		4   e   NaN NaN
		5   f   NaN NaN
		
	"Frame-2":
		   x    y    z
		0  a  1.0   2.0
		1  b  1.0   2.0
		2  c  1.0   2.0
		3  d   a1   a2
		4  e   b2   b1
		5  f   c3   c4
		
}
"answer": {
	"desc": %s you can fillna after calculating for the full dataframe and 
	"code-snippets": [
		out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')
		                       .set_axis(['y','z'],inplace=False,axis=1)))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67506798
"link": https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met
"question": {
	"title": Reformat Dataframe / Add rows when condition is met
	"desc": I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only "1's". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! 
}
"io": {
	"Frame-1": 
		   ColA        ColB
		2021-03-09       1
		2021-03-09       3
		2021-03-10       2
		2021-03-10       1
		2021-03-10       2
		2021-03-11       2
		
	"Frame-2":
		   ColA         ColB
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-09       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-10       1
		2021-03-11       1
		2021-03-11       1
		
}
"answer": {
	"desc": %s Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: 
	"code-snippets": [
		import pandas as pd
		import numpy as np
		
		df = (pd.DataFrame(np.repeat(df.ColA, df.ColB))
		        .assign(ColB=1))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67361824
"link": https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns
"question": {
	"title": Convert dataframe objects to float by iterating over columns
	"desc": I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance 
}
"io": {
	"Frame-1": 
		   c1   c2
		0  -    75.0%
		1 -5.5% 65.8%
		.
		n  -    6.9%
		
	"Frame-2":
		   c1    c2
		0  0.0   75.0
		1 -5.5   65.8
		.
		n  0.0    6.9
		
}
"answer": {
	"desc": %s EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. 
	"code-snippets": [
		# Thanks to @tdy
		df.replace({'\%':'', r'^\s*-\s*$':0}, regex=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67357814
"link": https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan
"question": {
	"title": Python pandas dataframe apply result of function to multiple columns where NaN
	"desc": I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result: 
}
"io": {
	"Frame-1": 
		    x   y   z
		0   a   1.0 2.0
		1   b   1.0 2.0
		2   c   1.0 2.0
		3   d   NaN NaN
		4   e   NaN NaN
		5   f   NaN NaN
		
	"Frame-2":
		   x    y    z
		0  a  1.0   2.0
		1  b  1.0   2.0
		2  c  1.0   2.0
		3  d   a1   a2
		4  e   b2   b1
		5  f   c3   c4
		
}
"answer": {
	"desc": %s you can fillna after calculating for the full dataframe and 
	"code-snippets": [
		out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')
		                       .set_axis(['y','z'],inplace=False,axis=1)))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67335759
"link": https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python
"question": {
	"title": Checking if column headers match PYTHON
	"desc": I have two dataframes: df1: df2 I want to write a function that checks if the column headers are matching/the same as columns in df1. IF not we get a message telling us what column is missing. Example of the message given these dataframes: I want a generalized code that can work for any given dataframe. Is this possible on python? 
}
"io": {
	"Frame-1": 
		      ID  Open High Low  
		       1  64   66   52   
		
	"Frame-2":
		      ID Open High  Volume
		      1   33   45   30043
		
}
"answer": {
	"desc": %s You can have access to the column names via and then use set operations to check what you want: And it gives the expected output: 
	"code-snippets": [
		import pandas as pd
		
		df1 = pd.DataFrame(
		    {
		        "ID": [1],
		        "Open": [64],
		        "High": [66],
		        "Low": [52]
		    }
		)
		
		df2 = pd.DataFrame(
		    {
		        "ID": [1],
		        "Open": [33],
		        "High": [45],
		        "Volume": [30043]
		    }
		)
		
		df1_columns = set(df1.columns)
		df2_columns = set(df2.columns)
		
		common_columns = df1_columns & df2_columns
		
		df1_columns_only = df1_columns - common_columns
		df2_columns_only = df2_columns - common_columns
		
		print("Columns only available in df1", df1_columns_only)
		print("Columns only available in df2", df2_columns_only)
		
		----------------------------------------------------------------------
		Columns only available in df1 {'Low'}
		Columns only available in df2 {'Volume'}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 26837998
"link": https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string
"question": {
	"title": Pandas Replace NaN with blank/empty string
	"desc": I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so: 
}
"io": {
	"Frame-1": 
		    1    2       3
		 0  a  NaN    read
		 1  b    l  unread
		 2  c  NaN    read
		
	"Frame-2":
		    1    2       3
		 0  a   ""    read
		 1  b    l  unread
		 2  c   ""    read
		
}
"answer": {
	"desc": %s  This might help. It will replace all NaNs with an empty string. 
	"code-snippets": [
		import numpy as np
		df1 = df.replace(np.nan, '', regex=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67361824
"link": https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns
"question": {
	"title": Convert dataframe objects to float by iterating over columns
	"desc": I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance 
}
"io": {
	"Frame-1": 
		   c1   c2
		0  -    75.0%
		1 -5.5% 65.8%
		.
		n  -    6.9%
		
	"Frame-2":
		   c1    c2
		0  0.0   75.0
		1 -5.5   65.8
		.
		n  0.0    6.9
		
}
"answer": {
	"desc": %s EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. 
	"code-snippets": [
		# Thanks to @tdy
		df.replace({'\%':'', r'^\s*-\s*$':0}, regex=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67357814
"link": https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan
"question": {
	"title": Python pandas dataframe apply result of function to multiple columns where NaN
	"desc": I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result: 
}
"io": {
	"Frame-1": 
		    x   y   z
		0   a   1.0 2.0
		1   b   1.0 2.0
		2   c   1.0 2.0
		3   d   NaN NaN
		4   e   NaN NaN
		5   f   NaN NaN
		
	"Frame-2":
		   x    y    z
		0  a  1.0   2.0
		1  b  1.0   2.0
		2  c  1.0   2.0
		3  d   a1   a2
		4  e   b2   b1
		5  f   c3   c4
		
}
"answer": {
	"desc": %s you can fillna after calculating for the full dataframe and 
	"code-snippets": [
		out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')
		                       .set_axis(['y','z'],inplace=False,axis=1)))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67335759
"link": https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python
"question": {
	"title": Checking if column headers match PYTHON
	"desc": I have two dataframes: df1: df2 I want to write a function that checks if the column headers are matching/the same as columns in df1. IF not we get a message telling us what column is missing. Example of the message given these dataframes: I want a generalized code that can work for any given dataframe. Is this possible on python? 
}
"io": {
	"Frame-1": 
		      ID  Open High Low  
		       1  64   66   52   
		
	"Frame-2":
		      ID Open High  Volume
		      1   33   45   30043
		
}
"answer": {
	"desc": %s You can have access to the column names via and then use set operations to check what you want: And it gives the expected output: 
	"code-snippets": [
		import pandas as pd
		
		df1 = pd.DataFrame(
		    {
		        "ID": [1],
		        "Open": [64],
		        "High": [66],
		        "Low": [52]
		    }
		)
		
		df2 = pd.DataFrame(
		    {
		        "ID": [1],
		        "Open": [33],
		        "High": [45],
		        "Volume": [30043]
		    }
		)
		
		df1_columns = set(df1.columns)
		df2_columns = set(df2.columns)
		
		common_columns = df1_columns & df2_columns
		
		df1_columns_only = df1_columns - common_columns
		df2_columns_only = df2_columns - common_columns
		
		print("Columns only available in df1", df1_columns_only)
		print("Columns only available in df2", df2_columns_only)
		
		----------------------------------------------------------------------
		Columns only available in df1 {'Low'}
		Columns only available in df2 {'Volume'}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 26837998
"link": https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string
"question": {
	"title": Pandas Replace NaN with blank/empty string
	"desc": I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so: 
}
"io": {
	"Frame-1": 
		    1    2       3
		 0  a  NaN    read
		 1  b    l  unread
		 2  c  NaN    read
		
	"Frame-2":
		    1    2       3
		 0  a   ""    read
		 1  b    l  unread
		 2  c   ""    read
		
}
"answer": {
	"desc": %s  This might help. It will replace all NaNs with an empty string. 
	"code-snippets": [
		import numpy as np
		df1 = df.replace(np.nan, '', regex=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67288220
"link": https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition
"question": {
	"title": How can I add a new line in pandas dataframe based in a condition?
	"desc": I have this Dataframe that is populated from a file. The first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition. Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this.. Taking for example the lines number 4 and 5: So I need to add a new line with the last number + 100, and the last column needs to be zero: Any ideas how can I achieve that? Thanks in advance. Edit: I just need to add the line once in the DataFrame. 
}
"io": {
	"Frame-1": 
		[1]   [2] [3]
		  1     30  2
		  1     30  1
		  1     30  3
		  1     90  3
		  1    370  3
		  1    430  3
		  1    705  3
		  1    805  3
		  1    880  2
		  1    905  3
		  1   1005  3
		  1   1170  3
		  1   1230  3
		  1   1970  3
		  1   2030  3
		  1   2970  3
		  1   3030  3
		  1   3970  3
		  1   4030  3
		  1   4423  3
		  1   4539  3
		  1   4575  3
		  1   4630  2
		  1   4635  3
		  1   4671  3
		  1   4787  3
		  1   4957  3
		  1   5057  3
		  1   5270  3
		  1   5330  3
		  1   5970  3
		  1   6030  3
		  1   6970  3
		  1   7030  3
		  1   7970  3
		  1   8030  3
		  1   8158  3
		  1   8257  3
		  1   8332  2
		  1   8357  3
		  1   8457  3
		  1   8970  3
		  1   9030  3
		  1   9970  3
		  1  10030  3
		  1  10970  3
		  1  11030  3
		  1  11470  3
		  1  11530  3
		  1  11853  3
		  1  11953  3
		
	"Frame-2":
		  1     90  3
		  1    190  0
		  1    370  3
		
}
"answer": {
	"desc": %s Try: Prints: EDIT: To change only one value: 
	"code-snippets": [
		m = df["[2]"].diff() > 100
		
		df.loc[m, "[2]"] = pd.Series(
		    [
		        [str(df.iloc[v - 1]["[2]"] + 100), df.iloc[v]["[2]"]]
		        for v in df.index[m]
		    ],
		    index=df.index[m],
		)
		
		df = df.explode("[2]")
		df["[3]"] = np.where(
		    df["[2]"].apply(lambda x: isinstance(x, str)), 0, df["[3]"]
		)
		df["[2]"] = df["[2]"].astype(int)
		print(df)
		
		----------------------------------------------------------------------
		mask = df["[2]"].diff() > 100
		if True in mask:
		    m = [False] * len(df)
		    m[mask.idxmax()] = True
		
		    df.loc[m, "[2]"] = pd.Series(
		        [
		            [str(df.iloc[v - 1]["[2]"] + 100), df.iloc[v]["[2]"]]
		            for v in df.index[m]
		        ],
		        index=df.index[m],
		    )
		
		    df = df.explode("[2]")
		    df["[3]"] = np.where(
		        df["[2]"].apply(lambda x: isinstance(x, str)), 0, df["[3]"]
		    )
		    df["[2]"] = df["[2]"].astype(int)
		    print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67257898
"link": https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column
"question": {
	"title": How to add a value to a new column by referencing the values in a column
	"desc": I have a dataframe like this: The xy column must be filled with the value of the column names in the reason column. Let's look at the first row. The reason column shows our value x1. So our value in column xy, will be the value of x1 column in the first row. Like this: Is there a way to do this? 
}
"io": {
	"Frame-1": 
		id  reason  x1    x2   x3   x4   x5 
		 1  x1      100   15   10   20   25
		 2  x1      15    16   14   10   10
		 3  x4      10    50   40   30   25
		 4  x3      12    15   60   5    1
		 5  x1      80    15   10   20   25
		 6  x1      15    19   84   10   10
		 7  x4      90    40   90   30   25
		 8  x4      12    85   60   50   10
		
	"Frame-2":
		id  reason  x1    x2   x3   x4   x5   xy
		 1  x1      100   15   10   20   25   100
		 2  x1      15    16   14   10   10   15
		 3  x4      10    50   40   30   25   30
		 4  x3      12    15   60   5    1    60
		 5  x1      80    15   10   20   25   80
		 6  x1      15    19   84   10   10   15
		 7  x4      90    40   90   30   25   30
		 8  x4      12    85   60   50   10   50
		
}
"answer": {
	"desc": %s  Prints: 
	"code-snippets": [
		df["xy"] = df.apply(lambda x: x[x["reason"]], axis=1)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67255732
"link": https://stackoverflow.com/questions/67255732/summation-of-operation-in-dataframe
"question": {
	"title": Summation of operation in dataframe
	"desc": I want to implement a function that does the operation that you can see in the image: But i not sure how to implement the Summation for the moment i doing something like that: And the problem is in the summation. If someone can help me. For example, i have two dataframes like that: Where for the abs operation we will obtain this: And for the sum: Finally we do the summation: where and 
}
"io": {
	"Frame-1": 
		   A  B
		0  5  14
		1  4  2
		
	"Frame-2":
		   A   B
		0  11  38
		1  8   36
		
}
"answer": {
	"desc": %s Use to sum the dataframe across columns/rows: Prints: 
	"code-snippets": [
		result = (
		    dataframe1.sub(dataframe2).abs().sum().sum()
		    / dataframe1.add(dataframe2).sum().sum()
		) * 100
		print(result)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67243081
"link": https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas
"question": {
	"title": Best way to change column data for all rows over multiple dataframes in pandas?
	"desc": Consider dataframes , , and . and have an column, and has a and column. I need to iterate over all rows of , and replace and with new unique randomly generated UUIDs, and then update those in and where (before the change to UUID). I originally wanted to iterate over all rows of and simply check both and if they contain the original or inside the column before replacing both, but I found that iterating over pandas rows is a bad idea and slow. I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes. My current method that I believe to be slow and inefficient: Here and are above mentioned and , and is Example Example : Example : 
}
"io": {
	"Frame-1": 
		+---+----+
		|   | id |
		+---+----+
		| 1 | a1 |
		+---+----+
		| 2 | c1 |
		+---+----+
		
	"Frame-2":
		+---+----+
		|   | id |
		+---+----+
		| 1 | b1 |
		+---+----+
		
}
"answer": {
	"desc": %s A very simple approach: 
	"code-snippets": [
		import itertools
		import uuid
		
		def rand_uuid():
		    return uuid.uuid4()
		
		rep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}
		
		df3.replace(rep_dict, inplace=True)
		df3.id = df3.id.map(lambda x: rand_uuid())
		
		df1.replace(rep_dict, inplace=True)
		df2.replace(rep_dict, inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67361824
"link": https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns
"question": {
	"title": Convert dataframe objects to float by iterating over columns
	"desc": I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance 
}
"io": {
	"Frame-1": 
		   c1   c2
		0  -    75.0%
		1 -5.5% 65.8%
		.
		n  -    6.9%
		
	"Frame-2":
		   c1    c2
		0  0.0   75.0
		1 -5.5   65.8
		.
		n  0.0    6.9
		
}
"answer": {
	"desc": %s EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. 
	"code-snippets": [
		# Thanks to @tdy
		df.replace({'\%':'', r'^\s*-\s*$':0}, regex=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67357814
"link": https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan
"question": {
	"title": Python pandas dataframe apply result of function to multiple columns where NaN
	"desc": I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result: 
}
"io": {
	"Frame-1": 
		    x   y   z
		0   a   1.0 2.0
		1   b   1.0 2.0
		2   c   1.0 2.0
		3   d   NaN NaN
		4   e   NaN NaN
		5   f   NaN NaN
		
	"Frame-2":
		   x    y    z
		0  a  1.0   2.0
		1  b  1.0   2.0
		2  c  1.0   2.0
		3  d   a1   a2
		4  e   b2   b1
		5  f   c3   c4
		
}
"answer": {
	"desc": %s you can fillna after calculating for the full dataframe and 
	"code-snippets": [
		out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')
		                       .set_axis(['y','z'],inplace=False,axis=1)))
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67335759
"link": https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python
"question": {
	"title": Checking if column headers match PYTHON
	"desc": I have two dataframes: df1: df2 I want to write a function that checks if the column headers are matching/the same as columns in df1. IF not we get a message telling us what column is missing. Example of the message given these dataframes: I want a generalized code that can work for any given dataframe. Is this possible on python? 
}
"io": {
	"Frame-1": 
		      ID  Open High Low  
		       1  64   66   52   
		
	"Frame-2":
		      ID Open High  Volume
		      1   33   45   30043
		
}
"answer": {
	"desc": %s You can have access to the column names via and then use set operations to check what you want: And it gives the expected output: 
	"code-snippets": [
		import pandas as pd
		
		df1 = pd.DataFrame(
		    {
		        "ID": [1],
		        "Open": [64],
		        "High": [66],
		        "Low": [52]
		    }
		)
		
		df2 = pd.DataFrame(
		    {
		        "ID": [1],
		        "Open": [33],
		        "High": [45],
		        "Volume": [30043]
		    }
		)
		
		df1_columns = set(df1.columns)
		df2_columns = set(df2.columns)
		
		common_columns = df1_columns & df2_columns
		
		df1_columns_only = df1_columns - common_columns
		df2_columns_only = df2_columns - common_columns
		
		print("Columns only available in df1", df1_columns_only)
		print("Columns only available in df2", df2_columns_only)
		
		----------------------------------------------------------------------
		Columns only available in df1 {'Low'}
		Columns only available in df2 {'Volume'}
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 26837998
"link": https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string
"question": {
	"title": Pandas Replace NaN with blank/empty string
	"desc": I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so: 
}
"io": {
	"Frame-1": 
		    1    2       3
		 0  a  NaN    read
		 1  b    l  unread
		 2  c  NaN    read
		
	"Frame-2":
		    1    2       3
		 0  a   ""    read
		 1  b    l  unread
		 2  c   ""    read
		
}
"answer": {
	"desc": %s  This might help. It will replace all NaNs with an empty string. 
	"code-snippets": [
		import numpy as np
		df1 = df.replace(np.nan, '', regex=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67288220
"link": https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition
"question": {
	"title": How can I add a new line in pandas dataframe based in a condition?
	"desc": I have this Dataframe that is populated from a file. The first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition. Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this.. Taking for example the lines number 4 and 5: So I need to add a new line with the last number + 100, and the last column needs to be zero: Any ideas how can I achieve that? Thanks in advance. Edit: I just need to add the line once in the DataFrame. 
}
"io": {
	"Frame-1": 
		[1]   [2] [3]
		  1     30  2
		  1     30  1
		  1     30  3
		  1     90  3
		  1    370  3
		  1    430  3
		  1    705  3
		  1    805  3
		  1    880  2
		  1    905  3
		  1   1005  3
		  1   1170  3
		  1   1230  3
		  1   1970  3
		  1   2030  3
		  1   2970  3
		  1   3030  3
		  1   3970  3
		  1   4030  3
		  1   4423  3
		  1   4539  3
		  1   4575  3
		  1   4630  2
		  1   4635  3
		  1   4671  3
		  1   4787  3
		  1   4957  3
		  1   5057  3
		  1   5270  3
		  1   5330  3
		  1   5970  3
		  1   6030  3
		  1   6970  3
		  1   7030  3
		  1   7970  3
		  1   8030  3
		  1   8158  3
		  1   8257  3
		  1   8332  2
		  1   8357  3
		  1   8457  3
		  1   8970  3
		  1   9030  3
		  1   9970  3
		  1  10030  3
		  1  10970  3
		  1  11030  3
		  1  11470  3
		  1  11530  3
		  1  11853  3
		  1  11953  3
		
	"Frame-2":
		  1     90  3
		  1    190  0
		  1    370  3
		
}
"answer": {
	"desc": %s Try: Prints: EDIT: To change only one value: 
	"code-snippets": [
		m = df["[2]"].diff() > 100
		
		df.loc[m, "[2]"] = pd.Series(
		    [
		        [str(df.iloc[v - 1]["[2]"] + 100), df.iloc[v]["[2]"]]
		        for v in df.index[m]
		    ],
		    index=df.index[m],
		)
		
		df = df.explode("[2]")
		df["[3]"] = np.where(
		    df["[2]"].apply(lambda x: isinstance(x, str)), 0, df["[3]"]
		)
		df["[2]"] = df["[2]"].astype(int)
		print(df)
		
		----------------------------------------------------------------------
		mask = df["[2]"].diff() > 100
		if True in mask:
		    m = [False] * len(df)
		    m[mask.idxmax()] = True
		
		    df.loc[m, "[2]"] = pd.Series(
		        [
		            [str(df.iloc[v - 1]["[2]"] + 100), df.iloc[v]["[2]"]]
		            for v in df.index[m]
		        ],
		        index=df.index[m],
		    )
		
		    df = df.explode("[2]")
		    df["[3]"] = np.where(
		        df["[2]"].apply(lambda x: isinstance(x, str)), 0, df["[3]"]
		    )
		    df["[2]"] = df["[2]"].astype(int)
		    print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67257898
"link": https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column
"question": {
	"title": How to add a value to a new column by referencing the values in a column
	"desc": I have a dataframe like this: The xy column must be filled with the value of the column names in the reason column. Let's look at the first row. The reason column shows our value x1. So our value in column xy, will be the value of x1 column in the first row. Like this: Is there a way to do this? 
}
"io": {
	"Frame-1": 
		id  reason  x1    x2   x3   x4   x5 
		 1  x1      100   15   10   20   25
		 2  x1      15    16   14   10   10
		 3  x4      10    50   40   30   25
		 4  x3      12    15   60   5    1
		 5  x1      80    15   10   20   25
		 6  x1      15    19   84   10   10
		 7  x4      90    40   90   30   25
		 8  x4      12    85   60   50   10
		
	"Frame-2":
		id  reason  x1    x2   x3   x4   x5   xy
		 1  x1      100   15   10   20   25   100
		 2  x1      15    16   14   10   10   15
		 3  x4      10    50   40   30   25   30
		 4  x3      12    15   60   5    1    60
		 5  x1      80    15   10   20   25   80
		 6  x1      15    19   84   10   10   15
		 7  x4      90    40   90   30   25   30
		 8  x4      12    85   60   50   10   50
		
}
"answer": {
	"desc": %s  Prints: 
	"code-snippets": [
		df["xy"] = df.apply(lambda x: x[x["reason"]], axis=1)
		print(df)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67255732
"link": https://stackoverflow.com/questions/67255732/summation-of-operation-in-dataframe
"question": {
	"title": Summation of operation in dataframe
	"desc": I want to implement a function that does the operation that you can see in the image: But i not sure how to implement the Summation for the moment i doing something like that: And the problem is in the summation. If someone can help me. For example, i have two dataframes like that: Where for the abs operation we will obtain this: And for the sum: Finally we do the summation: where and 
}
"io": {
	"Frame-1": 
		   A  B
		0  5  14
		1  4  2
		
	"Frame-2":
		   A   B
		0  11  38
		1  8   36
		
}
"answer": {
	"desc": %s Use to sum the dataframe across columns/rows: Prints: 
	"code-snippets": [
		result = (
		    dataframe1.sub(dataframe2).abs().sum().sum()
		    / dataframe1.add(dataframe2).sum().sum()
		) * 100
		print(result)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67243081
"link": https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas
"question": {
	"title": Best way to change column data for all rows over multiple dataframes in pandas?
	"desc": Consider dataframes , , and . and have an column, and has a and column. I need to iterate over all rows of , and replace and with new unique randomly generated UUIDs, and then update those in and where (before the change to UUID). I originally wanted to iterate over all rows of and simply check both and if they contain the original or inside the column before replacing both, but I found that iterating over pandas rows is a bad idea and slow. I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes. My current method that I believe to be slow and inefficient: Here and are above mentioned and , and is Example Example : Example : 
}
"io": {
	"Frame-1": 
		+---+----+
		|   | id |
		+---+----+
		| 1 | a1 |
		+---+----+
		| 2 | c1 |
		+---+----+
		
	"Frame-2":
		+---+----+
		|   | id |
		+---+----+
		| 1 | b1 |
		+---+----+
		
}
"answer": {
	"desc": %s A very simple approach: 
	"code-snippets": [
		import itertools
		import uuid
		
		def rand_uuid():
		    return uuid.uuid4()
		
		rep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}
		
		df3.replace(rep_dict, inplace=True)
		df3.id = df3.id.map(lambda x: rand_uuid())
		
		df1.replace(rep_dict, inplace=True)
		df2.replace(rep_dict, inplace=True)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67246859
"link": https://stackoverflow.com/questions/67246859/how-to-convert-rows-into-columns-and-filter-using-the-id
"question": {
	"title": How to convert rows into columns and filter using the ID
	"desc": I have a CSV file that looks like this: and I would like to use simple python or pandas to: Make each unique customer id in a separate row convert key_id to the columns titles and the values are the quantity The output table should look like this: I have been struggling to find a good data structure to do this but I couldn't. and using pandas I also couldn't filter using 2 ids. Any tips? 
}
"io": {
	"Frame-1": 
		customer_id |  key_id.  |  quantity |
		1           |    777    |    3      |
		1           |    888    |    2      |
		1           |    999    |    3      |
		2           |    777    |    6      |
		2           |    888    |    1      |
		
	"Frame-2":
		            |  777    |  888  |   999  | 
		1           |   3     |   2   |    3   |
		2           |   6     |   1   |    0   |
		
}
"answer": {
	"desc": %s You can pivot into columns using : To handle duplicates, averages them by default. To override this aggregation method, you can set the param (, , , , , etc.): 
	"code-snippets": [
		df.pivot_table(
		    index='customer_id',
		    columns='key_id',
		    values='quantity',
		    aggfunc='max',
		).fillna(0)
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


"qid": 67213950
"link": https://stackoverflow.com/questions/67213950/how-can-i-compare-each-row-from-a-dataframe-against-every-row-from-another-dataf
"question": {
	"title": How can I compare each row from a dataframe against every row from another dataframe and see the difference between values?
	"desc": I have two dataframes: df1 df2 df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be: ABD123 = 1 (because it has 1 different character from ABC123) DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one) GHI789 = 3 (because it has an equivalent at df1) I know how to check for the differences of each code individually and save the "length" of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way? 
}
"io": {
	"Frame-1": 
		     Code     Number
		0   ABC123      1
		1   DEF456      2
		2   GHI789      3
		3   DEA456      4
		
	"Frame-2":
		     Code 
		0   ABD123
		1   DEA458
		2   GHI789
		
}
"answer": {
	"desc": %s  don't know how to compare each row from df2 against all rows from df1. Nested loops will work. If you had a function named it would look like this... Nested loops are usually not ideal when working with Pandas or Numpy but they do work. There may be better solutions. DataFrame.iterrows() 
	"code-snippets": [
		for index2, row2 in df2.iterrows():
		    for index1, row1 in df1.iterrows():
		        difference = compare(row2,row1)
		        #do something with the difference.
		
		----------------------------------------------------------------------
	]
}
====================================================================================================


