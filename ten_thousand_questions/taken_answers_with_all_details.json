[
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "csv"
        ],
        "owner": {
            "reputation": 49,
            "user_id": 16373605,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/Am4kI.jpg?s=128&g=1",
            "display_name": "python noob",
            "link": "https://stackoverflow.com/users/16373605/python-noob"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 68315969,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1625861541,
        "creation_date": 1625824728,
        "last_edit_date": 1625825340,
        "question_id": 68314886,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68314886/pandas-loop-through-rows-to-update-column-value",
        "title": "Pandas: Loop through rows to update column value",
        "body": "<p>Here is sample dataframe look like:</p>\n<pre><code>&gt;&gt;&gt; df\n  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.4   NaN    NaN\n4  0.5   NaN    1.0\n5  0.6   NaN    NaN\n6  0.7   1.0    1.0\n7  0.8   NaN    NaN\n8  0.9   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.5   NaN    NaN\n14 1.6   NaN    NaN\n15 1.7   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.7   NaN    NaN\n23 1.1   NaN    NaN\n</code></pre>\n<p>From this dataFrame I want to update <code>point</code> value. Condition is when <code>x</code> or <code>y</code> is not <code>NaN</code> immediate next value of <code>point</code> will be replaced by previous <code>point</code> value afterthat next point value should be reindexed(<strong>cycle .1 to .6</strong>). eg. in row index(2) when <code>point=0.3, x=5.0</code> So, the next <code>point</code> value should be also <strong>0.3 instead of 0.4</strong>, Then in row index(4) point=0.5 will be replaced by 0.4(continue recursively)</p>\n<p>OUTPUT I want:</p>\n<pre><code>  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.3   NaN    NaN\n4  0.4   NaN    1.0\n5  0.4   NaN    NaN\n6  0.5   1.0    1.0\n7  0.5   NaN    NaN\n8  0.6   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.4   NaN    NaN\n14 1.5   NaN    NaN\n15 1.6   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.6   NaN    NaN\n23 1.1   NaN    NaN\n</code></pre>\n<p>Code I tried:</p>\n<pre><code>import pandas as pd\ndf = pd.read_csv(&quot;data.csv&quot;)\ndf['point'] = df.groupby() #Don't know how should I approach\n</code></pre>\n",
        "answer_body": "<p>Can you try that:</p>\n<pre><code>mask = df[['x', 'y']].any(axis=1).shift(1, fill_value=False)\npoint = df['point'].astype(int)\ngroup = point.sub(point.shift(1)).ne(0).cumsum()\n\ndf['point'] = df['point'].sub(mask.groupby(group).cumsum().div(10))\n</code></pre>\n<pre><code>&gt;&gt;&gt; df\n    point    x    y\n0     0.1  NaN  NaN\n1     0.2  NaN  NaN\n2     0.3  5.0  NaN\n3     0.3  NaN  NaN\n4     0.4  NaN  1.0\n5     0.4  NaN  NaN\n6     0.5  1.0  1.0\n7     0.5  NaN  NaN\n8     0.6  NaN  NaN\n9     1.1  NaN  NaN\n10    1.2  NaN  NaN\n11    1.3  NaN  NaN\n12    1.4  NaN  2.0\n13    1.4  NaN  NaN\n14    1.5  NaN  NaN\n15    1.6  NaN  NaN\n16    0.1  NaN  NaN\n17    0.2  NaN  NaN\n18    0.3  NaN  NaN\n19    0.4  NaN  NaN\n20    0.5  NaN  NaN\n21    0.6  2.0  NaN\n22    0.6  NaN  NaN\n23    1.1  NaN  NaN\n</code></pre>\n",
        "question_body": "<p>Here is sample dataframe look like:</p>\n<pre><code>&gt;&gt;&gt; df\n  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.4   NaN    NaN\n4  0.5   NaN    1.0\n5  0.6   NaN    NaN\n6  0.7   1.0    1.0\n7  0.8   NaN    NaN\n8  0.9   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.5   NaN    NaN\n14 1.6   NaN    NaN\n15 1.7   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.7   NaN    NaN\n23 1.1   NaN    NaN\n</code></pre>\n<p>From this dataFrame I want to update <code>point</code> value. Condition is when <code>x</code> or <code>y</code> is not <code>NaN</code> immediate next value of <code>point</code> will be replaced by previous <code>point</code> value afterthat next point value should be reindexed(<strong>cycle .1 to .6</strong>). eg. in row index(2) when <code>point=0.3, x=5.0</code> So, the next <code>point</code> value should be also <strong>0.3 instead of 0.4</strong>, Then in row index(4) point=0.5 will be replaced by 0.4(continue recursively)</p>\n<p>OUTPUT I want:</p>\n<pre><code>  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.3   NaN    NaN\n4  0.4   NaN    1.0\n5  0.4   NaN    NaN\n6  0.5   1.0    1.0\n7  0.5   NaN    NaN\n8  0.6   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.4   NaN    NaN\n14 1.5   NaN    NaN\n15 1.6   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.6   NaN    NaN\n23 1.1   NaN    NaN\n</code></pre>\n<p>Code I tried:</p>\n<pre><code>import pandas as pd\ndf = pd.read_csv(&quot;data.csv&quot;)\ndf['point'] = df.groupby() #Don't know how should I approach\n</code></pre>\n",
        "formatted_input": {
            "qid": 68314886,
            "link": "https://stackoverflow.com/questions/68314886/pandas-loop-through-rows-to-update-column-value",
            "question": {
                "title": "Pandas: Loop through rows to update column value",
                "ques_desc": "Here is sample dataframe look like: From this dataFrame I want to update value. Condition is when or is not immediate next value of will be replaced by previous value afterthat next point value should be reindexed(cycle .1 to .6). eg. in row index(2) when So, the next value should be also 0.3 instead of 0.4, Then in row index(4) point=0.5 will be replaced by 0.4(continue recursively) OUTPUT I want: Code I tried: "
            },
            "io": [
                ">>> df\n  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.4   NaN    NaN\n4  0.5   NaN    1.0\n5  0.6   NaN    NaN\n6  0.7   1.0    1.0\n7  0.8   NaN    NaN\n8  0.9   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.5   NaN    NaN\n14 1.6   NaN    NaN\n15 1.7   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.7   NaN    NaN\n23 1.1   NaN    NaN\n",
                "  point    x      y\n0  0.1   NaN    NaN\n1  0.2   NaN    NaN\n2  0.3   5.0    NaN\n3  0.3   NaN    NaN\n4  0.4   NaN    1.0\n5  0.4   NaN    NaN\n6  0.5   1.0    1.0\n7  0.5   NaN    NaN\n8  0.6   NaN    NaN\n9  1.1   NaN    NaN\n10 1.2   NaN    NaN\n11 1.3   NaN    NaN\n12 1.4   NaN    2.0\n13 1.4   NaN    NaN\n14 1.5   NaN    NaN\n15 1.6   NaN    NaN\n16 0.1   NaN    NaN\n17 0.2   NaN    NaN\n18 0.3   NaN    NaN\n19 0.4   NaN    NaN\n20 0.5   NaN    NaN\n21 0.6   2.0    NaN\n22 0.6   NaN    NaN\n23 1.1   NaN    NaN\n"
            ],
            "answer": {
                "ans_desc": "Can you try that: ",
                "code": [
                    "mask = df[['x', 'y']].any(axis=1).shift(1, fill_value=False)\npoint = df['point'].astype(int)\ngroup = point.sub(point.shift(1)).ne(0).cumsum()\n\ndf['point'] = df['point'].sub(mask.groupby(group).cumsum().div(10))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe",
            "beautifulsoup"
        ],
        "owner": {
            "reputation": 623,
            "user_id": 9846358,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/n1efb.jpg?s=128&g=1",
            "display_name": "Mary",
            "link": "https://stackoverflow.com/users/9846358/mary"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 68264773,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625547795,
        "creation_date": 1625547085,
        "last_edit_date": 1625547795,
        "question_id": 68264711,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting",
        "title": "pd.read_html changed number formatting",
        "body": "<p>Cannot get <code>1,2,3,4,5,6</code> from the column of <code>CCCCCCC</code>, after <code>pd.read_html</code> format changed to <code>123456</code>, and my <strong>expected result</strong> should be keep <code>1,2,3,4,5,6</code></p>\n<p><strong>HTML code</strong></p>\n<pre><code>html = &quot;&quot;&quot;&lt;html&gt;\n&lt;body&gt;\n&lt;div id=&quot;MMMMMMMM&quot; class=&quot;MMMMMMMMMMM&quot; style=&quot;&quot;&gt;\n        &lt;table class=&quot;OOOOOOOO&quot; style=&quot;&quot;&gt;\n            &lt;thead&gt;\n                &lt;tr class=&quot;PPPPPPPPPP&quot;&gt;\n                    &lt;td colspan=&quot;3&quot; style=&quot;font-size:14px;font-weight:bold;&quot; class=&quot;QQQQQQQQQQ&quot;&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n                &lt;tr class=&quot;RRRRRRRRRR&quot;&gt;\n                    &lt;td&gt;BBBBBB&lt;/td&gt;\n                    &lt;td&gt;CCCCCCC&lt;/td&gt;\n                    &lt;td&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;\n                    &lt;tr class=&quot;SSSSSSSS&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;DDDDDD&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;3&quot;&gt;EEEEEEEEE&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;FFFFFFFFF&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;GGGGGGGGG&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;HHHHHHHHH&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;IIIIIIIIII&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;JJJJJJJJ&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;2&quot;&gt;KKKKKKKK&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;TTTTTT&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n            &lt;/tbody&gt;\n        &lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;&quot;&quot;&quot;\n</code></pre>\n<p><strong>Python Code</strong></p>\n<pre><code>from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1)\ndf_list\n</code></pre>\n<p><strong>Execution Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       123456  1234.56\n 1    EEEEEEEEE       123456  1234.56\n 2    EEEEEEEEE       123456  1234.56\n 3    EEEEEEEEE       123456  1234.56\n 4    FFFFFFFFF       123456  1234.56\n 5    GGGGGGGGG       123456  1234.56\n 6    HHHHHHHHH       123456  1234.56\n 7   IIIIIIIIII       123456  1234.56\n 8     JJJJJJJJ       123456  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n</code></pre>\n<p><strong>Expected Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF       1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG       1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH       1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII       1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ       1,2,3,4,5,6  1234.56\n 9     KKKKKKKK       1/2/3/4/5/6  1234.56\n 10    KKKKKKKK       1/2/3/4/5/6  1234.56]\n \n</code></pre>\n",
        "answer_body": "<p>You need to add the <code>thousands</code> parameter and set it to <code>None</code> by default it's <code>','</code>.</p>\n<pre><code>from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1, thousands=None)\ndf_list\n</code></pre>\n<h5>OUTPUT:</h5>\n<pre><code>[        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD  1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE  1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE  1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE  1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF  1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG  1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH  1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII  1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ  1,2,3,4,5,6  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n</code></pre>\n",
        "question_body": "<p>Cannot get <code>1,2,3,4,5,6</code> from the column of <code>CCCCCCC</code>, after <code>pd.read_html</code> format changed to <code>123456</code>, and my <strong>expected result</strong> should be keep <code>1,2,3,4,5,6</code></p>\n<p><strong>HTML code</strong></p>\n<pre><code>html = &quot;&quot;&quot;&lt;html&gt;\n&lt;body&gt;\n&lt;div id=&quot;MMMMMMMM&quot; class=&quot;MMMMMMMMMMM&quot; style=&quot;&quot;&gt;\n        &lt;table class=&quot;OOOOOOOO&quot; style=&quot;&quot;&gt;\n            &lt;thead&gt;\n                &lt;tr class=&quot;PPPPPPPPPP&quot;&gt;\n                    &lt;td colspan=&quot;3&quot; style=&quot;font-size:14px;font-weight:bold;&quot; class=&quot;QQQQQQQQQQ&quot;&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n                &lt;tr class=&quot;RRRRRRRRRR&quot;&gt;\n                    &lt;td&gt;BBBBBB&lt;/td&gt;\n                    &lt;td&gt;CCCCCCC&lt;/td&gt;\n                    &lt;td&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;\n                    &lt;tr class=&quot;SSSSSSSS&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;DDDDDD&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;3&quot;&gt;EEEEEEEEE&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;FFFFFFFFF&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;GGGGGGGGG&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;HHHHHHHHH&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;IIIIIIIIII&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;JJJJJJJJ&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;2&quot;&gt;KKKKKKKK&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;TTTTTT&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n            &lt;/tbody&gt;\n        &lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;&quot;&quot;&quot;\n</code></pre>\n<p><strong>Python Code</strong></p>\n<pre><code>from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1)\ndf_list\n</code></pre>\n<p><strong>Execution Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       123456  1234.56\n 1    EEEEEEEEE       123456  1234.56\n 2    EEEEEEEEE       123456  1234.56\n 3    EEEEEEEEE       123456  1234.56\n 4    FFFFFFFFF       123456  1234.56\n 5    GGGGGGGGG       123456  1234.56\n 6    HHHHHHHHH       123456  1234.56\n 7   IIIIIIIIII       123456  1234.56\n 8     JJJJJJJJ       123456  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n</code></pre>\n<p><strong>Expected Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF       1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG       1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH       1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII       1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ       1,2,3,4,5,6  1234.56\n 9     KKKKKKKK       1/2/3/4/5/6  1234.56\n 10    KKKKKKKK       1/2/3/4/5/6  1234.56]\n \n</code></pre>\n",
        "formatted_input": {
            "qid": 68264711,
            "link": "https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting",
            "question": {
                "title": "pd.read_html changed number formatting",
                "ques_desc": "Cannot get from the column of , after format changed to , and my expected result should be keep HTML code Python Code Execution Result Expected Result "
            },
            "io": [
                " [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       123456  1234.56\n 1    EEEEEEEEE       123456  1234.56\n 2    EEEEEEEEE       123456  1234.56\n 3    EEEEEEEEE       123456  1234.56\n 4    FFFFFFFFF       123456  1234.56\n 5    GGGGGGGGG       123456  1234.56\n 6    HHHHHHHHH       123456  1234.56\n 7   IIIIIIIIII       123456  1234.56\n 8     JJJJJJJJ       123456  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n",
                " [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF       1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG       1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH       1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII       1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ       1,2,3,4,5,6  1234.56\n 9     KKKKKKKK       1/2/3/4/5/6  1234.56\n 10    KKKKKKKK       1/2/3/4/5/6  1234.56]\n \n"
            ],
            "answer": {
                "ans_desc": "You need to add the parameter and set it to by default it's . OUTPUT: ",
                "code": [
                    "from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1, thousands=None)\ndf_list\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "user_type": "does_not_exist",
            "display_name": "user16253345"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68243166,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625389829,
        "creation_date": 1625389469,
        "question_id": 68243146,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas",
        "title": "replace zero with value of an other column using pandas",
        "body": "<p>I have a dataframe df1:</p>\n<pre><code>    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n</code></pre>\n<p>I want to replace 0 in the <strong>id column</strong> with value from <strong>ref column</strong> of the same row So it will become:</p>\n<pre><code>   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n</code></pre>\n",
        "answer_body": "<p>via <code>mask()</code>:</p>\n<pre><code>df['id']=df['id'].mask(df['id'].eq(0),df['ref'])\n</code></pre>\n<p>OR</p>\n<p>via numpy's <code>where()</code>:</p>\n<pre><code>#import numpy as np\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe df1:</p>\n<pre><code>    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n</code></pre>\n<p>I want to replace 0 in the <strong>id column</strong> with value from <strong>ref column</strong> of the same row So it will become:</p>\n<pre><code>   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n</code></pre>\n",
        "formatted_input": {
            "qid": 68243146,
            "link": "https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas",
            "question": {
                "title": "replace zero with value of an other column using pandas",
                "ques_desc": "I have a dataframe df1: I want to replace 0 in the id column with value from ref column of the same row So it will become: "
            },
            "io": [
                "    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n",
                "   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n"
            ],
            "answer": {
                "ans_desc": "via : OR via numpy's : ",
                "code": [
                    "#import numpy as np\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "timestamp"
        ],
        "owner": {
            "reputation": 255,
            "user_id": 14073111,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-v9-fmI_5DnM/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckI-maisNcyjtLpr8roYSBRbOvrhw/photo.jpg?sz=128",
            "display_name": "user14073111",
            "link": "https://stackoverflow.com/users/14073111/user14073111"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 68232113,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625301132,
        "creation_date": 1625261258,
        "last_edit_date": 1625262116,
        "question_id": 68231389,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas",
        "title": "Compare two columns that contains timestamps in pandas",
        "body": "<p>Lets say I have a dataframe like this one:</p>\n<pre><code>  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n</code></pre>\n<p>I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4).</p>\n<p>I tried this one:</p>\n<pre><code>df['Col1'] = pd.to_datetime(df['Col1'])\ndf['Col2'] = pd.to_datetime(df['Col2'])\ndf['Col3'] = pd.to_datetime(df['Col3'])\nk= (df['Col1'] &gt; df['Col2']).astype(int)\np=(df['Col2'] &gt; df['Col3']).astype(int)\n\nif k&gt;0:\n    df['Col2']=np.nan\n    df['Col3']=np.nan\n    df['Col4']=np.nan\nelif p&gt;0:\n    df['Col3']=np.nan\n    df['Col4']=np.nan \n</code></pre>\n<p>But it is showing me this error:</p>\n<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n<p>My desirable output would look like this:</p>\n<pre><code>  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n</code></pre>\n<p>EDITED:\nAdded Col0</p>\n",
        "answer_body": "<p>A straightforward way with boolean mask:</p>\n<pre><code>dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n</code></pre>\n<pre><code>&gt;&gt;&gt; df\n    Col0                Col1                Col2 Col3 Col4\n0  1.txt 2021-06-23 15:04:30                 NaT  NaT  NaT\n1  2.txt 2021-06-23 14:25:30 2021-06-23 15:30:30  NaT  NaT\n</code></pre>\n",
        "question_body": "<p>Lets say I have a dataframe like this one:</p>\n<pre><code>  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n</code></pre>\n<p>I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4).</p>\n<p>I tried this one:</p>\n<pre><code>df['Col1'] = pd.to_datetime(df['Col1'])\ndf['Col2'] = pd.to_datetime(df['Col2'])\ndf['Col3'] = pd.to_datetime(df['Col3'])\nk= (df['Col1'] &gt; df['Col2']).astype(int)\np=(df['Col2'] &gt; df['Col3']).astype(int)\n\nif k&gt;0:\n    df['Col2']=np.nan\n    df['Col3']=np.nan\n    df['Col4']=np.nan\nelif p&gt;0:\n    df['Col3']=np.nan\n    df['Col4']=np.nan \n</code></pre>\n<p>But it is showing me this error:</p>\n<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n<p>My desirable output would look like this:</p>\n<pre><code>  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n</code></pre>\n<p>EDITED:\nAdded Col0</p>\n",
        "formatted_input": {
            "qid": 68231389,
            "link": "https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas",
            "question": {
                "title": "Compare two columns that contains timestamps in pandas",
                "ques_desc": "Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0 "
            },
            "io": [
                "  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n",
                "  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n"
            ],
            "answer": {
                "ans_desc": "A straightforward way with boolean mask: ",
                "code": [
                    "dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 49,
            "user_id": 16317158,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyBUnbLv2PI4_Agtm14oXQyk_ozZyrNZkI9w4DE=k-s128",
            "display_name": "Jewel_R",
            "link": "https://stackoverflow.com/users/16317158/jewel-r"
        },
        "is_answered": true,
        "view_count": 20,
        "accepted_answer_id": 68231157,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625259165,
        "creation_date": 1625258774,
        "question_id": 68231104,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe",
        "title": "Extract part of a 3 D dataframe",
        "body": "<p>I have a 3d dataframe. looks like this:</p>\n<pre><code>     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n</code></pre>\n<p>How could I extract only column  A &amp; B from every d1,d2.....? I desire to take the dataframe like this:</p>\n<pre><code>    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.isin.html#pandas-index-isin\" rel=\"nofollow noreferrer\"><code>Index.isin</code></a> on the level 1 values of columns then select with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>loc</code></a>:</p>\n<pre><code>filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n</code></pre>\n<p><code>filtered_df</code>:</p>\n<pre><code>   d1      d2    \n    A   B   A   B\n0   1   2   5   6\n1   9  10  13  14\n2  17  18  21  22\n</code></pre>\n<hr />\n<p>Sample Data Used:</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n</code></pre>\n<pre><code>   d1              d2            \n    A   B   C   D   A   B   C   D\n0   1   2   3   4   5   6   7   8\n1   9  10  11  12  13  14  15  16\n2  17  18  19  20  21  22  23  24\n</code></pre>\n",
        "question_body": "<p>I have a 3d dataframe. looks like this:</p>\n<pre><code>     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n</code></pre>\n<p>How could I extract only column  A &amp; B from every d1,d2.....? I desire to take the dataframe like this:</p>\n<pre><code>    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n</code></pre>\n",
        "formatted_input": {
            "qid": 68231104,
            "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe",
            "question": {
                "title": "Extract part of a 3 D dataframe",
                "ques_desc": "I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: "
            },
            "io": [
                "     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n",
                "    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n"
            ],
            "answer": {
                "ans_desc": "Use on the level 1 values of columns then select with : : Sample Data Used: ",
                "code": [
                    "filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n",
                    "import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 11566142,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d2e69536157f140a0c82dc8f714c295d?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ivan7",
            "link": "https://stackoverflow.com/users/11566142/ivan7"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68229969,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1625254302,
        "creation_date": 1625250274,
        "last_edit_date": 1625251248,
        "question_id": 68229806,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe",
        "title": "Insert values from variable and DataFrame into another DataFrame",
        "body": "<p>On start I have two DataFrames and one variable:</p>\n<pre><code>id=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n</code></pre>\n<p>I have to map <strong>id</strong> variable and the corresponding <strong>col0</strong> cell from <strong>df1</strong> DataFrame to all rows in <strong>df2</strong> DataFrame. I tryed and as the result I made the code below:</p>\n<pre><code>df2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'])\n</code></pre>\n<p>It seems to me that the code should work correctly, but unfortunatelly I have a <strong>NaN</strong> value in the <strong>col0</strong> column.</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n</code></pre>\n<p>The expected result was:</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n</code></pre>\n<p>I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please:</p>\n<ol>\n<li>explain briefly why I am getting the error</li>\n<li>fix my mistake in the code</li>\n</ol>\n",
        "answer_body": "<p>Your mistake is on this string <code>df1[df1['id']==id]['col0']</code> when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value.</p>\n<p>To solve this issue is very very very simple, you just have to call the first item at the Series object like this: <code>df1[df1['id']==id]['col0'][0]</code></p>\n<p>Your code with the ajustment must look like this</p>\n<pre><code>import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n</code></pre>\n<p>Then your new df2 is like this:</p>\n<pre><code>   id  col0  col1  col2\n0   1     3    13    23\n1   1     3    14    24\n2   1     3    15    25\n</code></pre>\n",
        "question_body": "<p>On start I have two DataFrames and one variable:</p>\n<pre><code>id=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n</code></pre>\n<p>I have to map <strong>id</strong> variable and the corresponding <strong>col0</strong> cell from <strong>df1</strong> DataFrame to all rows in <strong>df2</strong> DataFrame. I tryed and as the result I made the code below:</p>\n<pre><code>df2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'])\n</code></pre>\n<p>It seems to me that the code should work correctly, but unfortunatelly I have a <strong>NaN</strong> value in the <strong>col0</strong> column.</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n</code></pre>\n<p>The expected result was:</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n</code></pre>\n<p>I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please:</p>\n<ol>\n<li>explain briefly why I am getting the error</li>\n<li>fix my mistake in the code</li>\n</ol>\n",
        "formatted_input": {
            "qid": 68229806,
            "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe",
            "question": {
                "title": "Insert values from variable and DataFrame into another DataFrame",
                "ques_desc": "On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code "
            },
            "io": [
                "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n",
                "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n"
            ],
            "answer": {
                "ans_desc": "Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: ",
                "code": [
                    "import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, \"id\", id)\ndf2.insert(1, \"col0\", df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "algorithm",
            "dataframe",
            "pairwise"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 16238148,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a87a2e4b04f8d0cd6d19f6e68eab5a4e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Eugene Zinder",
            "link": "https://stackoverflow.com/users/16238148/eugene-zinder"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 68213724,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625157900,
        "creation_date": 1625155883,
        "question_id": 68213612,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun",
        "title": "How to combine rows in a dataframe in a pairwise fashion while applying some function",
        "body": "<p>I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2:</p>\n<pre><code>ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n</code></pre>\n<p>I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is.</p>\n<p>Here is the resulting dataframe:</p>\n<pre><code>ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n</code></pre>\n<p>In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row.</p>\n<p>(id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows.</p>\n<p><strong>I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way?</strong></p>\n<p>Code:</p>\n<pre><code>df = pd.DataFrame(columns=['ID', 'Val1', 'Val2'], data=[['id0', 10, 20], ['id0', 11, 19], ['id1', 5, 5], ['id1', 1, 1], ['id1', 2, 4]])\n</code></pre>\n",
        "answer_body": "<p>You can use <code>df.rolling</code> after grouping by <code>ID</code>:</p>\n<pre><code>out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n</code></pre>\n<pre><code>&gt;&gt;&gt; out\n       Val1  Val2\nid0_1  10.5  19.5\nid1_1   3.0   3.0\nid1_2   1.5   2.5\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2:</p>\n<pre><code>ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n</code></pre>\n<p>I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is.</p>\n<p>Here is the resulting dataframe:</p>\n<pre><code>ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n</code></pre>\n<p>In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row.</p>\n<p>(id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows.</p>\n<p><strong>I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way?</strong></p>\n<p>Code:</p>\n<pre><code>df = pd.DataFrame(columns=['ID', 'Val1', 'Val2'], data=[['id0', 10, 20], ['id0', 11, 19], ['id1', 5, 5], ['id1', 1, 1], ['id1', 2, 4]])\n</code></pre>\n",
        "formatted_input": {
            "qid": 68213612,
            "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun",
            "question": {
                "title": "How to combine rows in a dataframe in a pairwise fashion while applying some function",
                "ques_desc": "I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: "
            },
            "io": [
                "ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n",
                "ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n"
            ],
            "answer": {
                "ans_desc": "You can use after grouping by : ",
                "code": [
                    "out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 173,
            "user_id": 9254726,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9070bdb0341821aed239bfa229c8f43a?s=128&d=identicon&r=PG&f=1",
            "display_name": "5E4ME",
            "link": "https://stackoverflow.com/users/9254726/5e4me"
        },
        "is_answered": true,
        "view_count": 15,
        "accepted_answer_id": 68211988,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625149939,
        "creation_date": 1625149090,
        "question_id": 68211888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base",
        "title": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column",
        "body": "<p>I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time.</p>\n<p>The starting dataframes look like this:</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n</code></pre>\n<p>And I would like to get to a series of frames looking like this</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n</code></pre>\n<p>My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible.</p>\n<p>I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.</p>\n",
        "answer_body": "<p>Let's try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> then build out a <code>dict</code>:</p>\n<pre><code>dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n</code></pre>\n<p><code>dfs</code>:</p>\n<pre><code>{'player1':       NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7,\n 'player2':       NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7,\n 'player3':       NAME  VAL1  VAL2  VAL3\n2  player3     3     6     7\n1  player3     2     6     8,\n 'player5':       NAME  VAL1  VAL2  VAL3\n2  player5     3     6     7}\n</code></pre>\n<p>Each player's DataFrame can then be accessed like:</p>\n<p><code>dfs['player1']</code>:</p>\n<pre><code>      NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7\n</code></pre>\n<hr />\n<p>Or as a <code>list</code>:</p>\n<pre><code>dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n</code></pre>\n<p><code>dfs</code>:</p>\n<pre><code>[      NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7,\n       NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7,\n       NAME  VAL1  VAL2  VAL3\n2  player3     3     6     7\n1  player3     2     6     8,\n       NAME  VAL1  VAL2  VAL3\n2  player5     3     6     7]\n</code></pre>\n<p>Each player's DataFrame can then be accessed like:</p>\n<p><code>dfs[1]</code>:</p>\n<pre><code>      NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7\n</code></pre>\n",
        "question_body": "<p>I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time.</p>\n<p>The starting dataframes look like this:</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n</code></pre>\n<p>And I would like to get to a series of frames looking like this</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n</code></pre>\n<p>My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible.</p>\n<p>I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.</p>\n",
        "formatted_input": {
            "qid": 68211888,
            "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base",
            "question": {
                "title": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column",
                "ques_desc": "I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. "
            },
            "io": [
                "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n",
                "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n"
            ],
            "answer": {
                "ans_desc": "Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : ",
                "code": [
                    "dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n",
                    "dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-munging"
        ],
        "owner": {
            "reputation": 724,
            "user_id": 6057371,
            "user_type": "registered",
            "accept_rate": 31,
            "profile_image": "https://www.gravatar.com/avatar/b744e3dbff9c7fdb801570de75f6eff7?s=128&d=identicon&r=PG&f=1",
            "display_name": "okuoub",
            "link": "https://stackoverflow.com/users/6057371/okuoub"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68193597,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625051576,
        "creation_date": 1625051004,
        "question_id": 68193558,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values",
        "title": "pandas group many columns to one column where every cell is a list of values",
        "body": "<p>I have the dataframe</p>\n<pre><code>df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n</code></pre>\n<p>And I want to group all columns to a single list that will be the only columns, so I will get:</p>\n<pre><code>df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n</code></pre>\n<p>(Shape of df was change from (3,5) to (3,1))</p>\n<p>What is the best way to do this?</p>\n",
        "answer_body": "<p>Try:</p>\n<pre><code>#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n</code></pre>\n",
        "question_body": "<p>I have the dataframe</p>\n<pre><code>df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n</code></pre>\n<p>And I want to group all columns to a single list that will be the only columns, so I will get:</p>\n<pre><code>df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n</code></pre>\n<p>(Shape of df was change from (3,5) to (3,1))</p>\n<p>What is the best way to do this?</p>\n",
        "formatted_input": {
            "qid": 68193558,
            "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values",
            "question": {
                "title": "pandas group many columns to one column where every cell is a list of values",
                "ques_desc": "I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? "
            },
            "io": [
                "df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n",
                "df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n"
            ],
            "answer": {
                "ans_desc": "Try: ",
                "code": [
                    "#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 15759786,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/002b02baee3603027b66ed2361da99e2?s=128&d=identicon&r=PG&f=1",
            "display_name": "big sad",
            "link": "https://stackoverflow.com/users/15759786/big-sad"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68193581,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625051129,
        "creation_date": 1625050872,
        "question_id": 68193521,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame",
        "title": "Concatenate values and column names in a data frame to create a new data frame",
        "body": "<p>I have the following data frame(<code>df1</code>):</p>\n<pre><code>  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n</code></pre>\n<p>I need to derive the data frame(<code>df2</code>) from <code>df1</code> such that column 1 of <code>d2</code> will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of <code>d2</code> will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate.   :</p>\n<pre><code>      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n</code></pre>\n<p>I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process?</p>\n<p>Below is the code I have used</p>\n<pre><code>d = {'Value': ['a','b','c','d','e'],'col1': ['aa','ba','ca','da','ea'], 'col2' : ['ab','bb','cb','db','eb'],'col3': ['ac','bc','cc','dc','ec']}\ndf1 = pd.DataFrame(data = d)\n\n# Repeat every value is Value column 3 times.\nX = df1['Value'].repeat(4).reset_index(drop=True)\n\n# Create separate series with Col 1, Col 2, Col 3 names.\nY = pd.Series(df1.columns[1:])\n\n# Repeated series Y to the length of data df1\nYY = pd.Series(np.tile(Y.values, len(df1)))\n\n# Create the first column by concatenating X and YY\nfirst_column_1 = X + &quot;_&quot; + YY\n\nZ = df1.set_index('Value')\nZZ = np.ravel(Z.values)\n\n#Create 2nd column from ZZ\nsecond_column = pd.Series(ZZ)\n\n#Create df2\ndf2 = pd.DataFrame([first_column, second_column]).T\n</code></pre>\n",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = df.melt(&quot;Value&quot;, value_name=&quot;Col 1&quot;)\nx.Value += &quot;_&quot; + x.variable\nx = x.drop(columns=&quot;variable&quot;)\nprint(x)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>     Value Col 1\n0   a_col1    aa\n1   b_col1    ba\n2   c_col1    ca\n3   d_col1    da\n4   e_col1    ea\n5   a_col2    ab\n6   b_col2    bb\n7   c_col2    cb\n8   d_col2    db\n9   e_col2    eb\n10  a_col3    ac\n11  b_col3    bc\n12  c_col3    cc\n13  d_col3    dc\n14  e_col3    ec\n</code></pre>\n<hr />\n<p>Optionally, you can sort values afterwards:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = x.sort_values(by=&quot;Value&quot;).reset_index(drop=True)\nprint(x)\n\n     Value Col 1\n0   a_col1    aa\n1   a_col2    ab\n2   a_col3    ac\n3   b_col1    ba\n4   b_col2    bb\n5   b_col3    bc\n6   c_col1    ca\n7   c_col2    cb\n8   c_col3    cc\n9   d_col1    da\n10  d_col2    db\n11  d_col3    dc\n12  e_col1    ea\n13  e_col2    eb\n14  e_col3    ec\n</code></pre>\n",
        "question_body": "<p>I have the following data frame(<code>df1</code>):</p>\n<pre><code>  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n</code></pre>\n<p>I need to derive the data frame(<code>df2</code>) from <code>df1</code> such that column 1 of <code>d2</code> will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of <code>d2</code> will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate.   :</p>\n<pre><code>      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n</code></pre>\n<p>I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process?</p>\n<p>Below is the code I have used</p>\n<pre><code>d = {'Value': ['a','b','c','d','e'],'col1': ['aa','ba','ca','da','ea'], 'col2' : ['ab','bb','cb','db','eb'],'col3': ['ac','bc','cc','dc','ec']}\ndf1 = pd.DataFrame(data = d)\n\n# Repeat every value is Value column 3 times.\nX = df1['Value'].repeat(4).reset_index(drop=True)\n\n# Create separate series with Col 1, Col 2, Col 3 names.\nY = pd.Series(df1.columns[1:])\n\n# Repeated series Y to the length of data df1\nYY = pd.Series(np.tile(Y.values, len(df1)))\n\n# Create the first column by concatenating X and YY\nfirst_column_1 = X + &quot;_&quot; + YY\n\nZ = df1.set_index('Value')\nZZ = np.ravel(Z.values)\n\n#Create 2nd column from ZZ\nsecond_column = pd.Series(ZZ)\n\n#Create df2\ndf2 = pd.DataFrame([first_column, second_column]).T\n</code></pre>\n",
        "formatted_input": {
            "qid": 68193521,
            "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame",
            "question": {
                "title": "Concatenate values and column names in a data frame to create a new data frame",
                "ques_desc": "I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used "
            },
            "io": [
                "  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n",
                "      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n"
            ],
            "answer": {
                "ans_desc": "Try: Prints: Optionally, you can sort values afterwards: ",
                "code": [
                    "x = df.melt(\"Value\", value_name=\"Col 1\")\nx.Value += \"_\" + x.variable\nx = x.drop(columns=\"variable\")\nprint(x)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe",
            "data-structures"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 9079043,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/974eaeafce0b16f35dce0911a26ade93?s=128&d=identicon&r=PG&f=1",
            "display_name": "Isha",
            "link": "https://stackoverflow.com/users/9079043/isha"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 68174753,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624955956,
        "creation_date": 1624953161,
        "last_edit_date": 1624953363,
        "question_id": 68174614,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
        "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
        "body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "answer_body": "<p>The problem here is, when you call apply on <code>axis=1</code>, pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; s = pd.Series([1, 2, 3.2])\ns\n0    1.0\n1    2.0\n2    3.2\ndtype: float64\n</code></pre>\n<p>As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to :</p>\n<pre><code>df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n</code></pre>\n<p>There's already an issue <a href=\"https://github.com/pandas-dev/pandas/issues/23230\" rel=\"nofollow noreferrer\">DataFrame.apply unintuitively changes int to float because of another column</a> on github for this upcasting behavior of pandas <code>apply</code>.\nSo, one possible option for you is as I have mentioned in the comment, to call <code>to_json</code> on the entire dataframe as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt;df.to_json(orient='index')\n'{&quot;0&quot;:{&quot;A&quot;:2,&quot;B&quot;:6,&quot;C&quot;:5,&quot;D&quot;:8.0},&quot;1&quot;:{&quot;A&quot;:6,&quot;B&quot;:11,&quot;C&quot;:2,&quot;D&quot;:3.6},&quot;2&quot;:{&quot;A&quot;:1,&quot;B&quot;:5,&quot;C&quot;:7,&quot;D&quot;:5.2}}'\n</code></pre>\n<p>A working solution for you may be using python's <code>json</code> module alongwith <code>DataFrame.to_json()</code>, but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['total'] = list(map(json.dumps, [*json.loads(df.to_json(orient='index')).values()]))\n</code></pre>\n<p><strong>OUTPUT:</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>   A   B  C    D                                total\n0  2   6  5  8.0   {&quot;A&quot;: 2, &quot;B&quot;: 6, &quot;C&quot;: 5, &quot;D&quot;: 8.0}\n1  6  11  2  3.6  {&quot;A&quot;: 6, &quot;B&quot;: 11, &quot;C&quot;: 2, &quot;D&quot;: 3.6}\n2  1   5  7  5.2   {&quot;A&quot;: 1, &quot;B&quot;: 5, &quot;C&quot;: 7, &quot;D&quot;: 5.2}\n</code></pre>\n",
        "question_body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "formatted_input": {
            "qid": 68174614,
            "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
            "question": {
                "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
                "ques_desc": "I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? "
            },
            "io": [
                "A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n",
                "{\"A\":2.0, \"B\":6.0, \"C\":5.0, \"D\":8.0}\n{\"A\":6.0, \"B\":11.0, \"C\":2.0, \"D\":3.6}\n{\"A\":1.0, \"B\":5.0, \"C\":7.0, \"D\":5.2}\n"
            ],
            "answer": {
                "ans_desc": "The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: ",
                "code": [
                    "df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 4149213,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/vBcjv.jpg?s=128&g=1",
            "display_name": "openwater",
            "link": "https://stackoverflow.com/users/4149213/openwater"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 68174309,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624951870,
        "creation_date": 1624950821,
        "last_edit_date": 1624951870,
        "question_id": 68174113,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
        "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
        "body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "answer_body": "<p>A simpler solution would be to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.GroupBy.apply.html\" rel=\"nofollow noreferrer\"><code>apply</code></a> a custom function on each group. In this case, we can define a function <code>reclass</code> that obtains the correct bins and ids and then uses <code>pd.cut</code>:</p>\n<pre><code>def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n</code></pre>\n<p>Result:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "formatted_input": {
            "qid": 68174113,
            "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
            "question": {
                "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
                "ques_desc": "I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? "
            },
            "io": [
                "bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n",
                "ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n"
            ],
            "answer": {
                "ans_desc": "A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: ",
                "code": [
                    "def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 121,
            "user_id": 2643948,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/3423e43b576ab333117f731daef5aad9?s=128&d=identicon&r=PG&f=1",
            "display_name": "RebeccaKennedy",
            "link": "https://stackoverflow.com/users/2643948/rebeccakennedy"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 68150849,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624808663,
        "creation_date": 1624788441,
        "question_id": 68150020,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
        "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
        "body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "answer_body": "<h3>Numpy approach</h3>\n<p>We can define a function <code>first_value</code> which takes a <code>2D</code> array and <code>offset</code> (n) as input arguments and returns <code>1D</code> array. Basically, for each row it returns the <code>nth</code> value after the first <code>non-nan</code> value</p>\n<pre><code>def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i &gt;= arr.shape[1])] = np.nan\n    return vals\n</code></pre>\n<h3>Pandas approach</h3>\n<p>We can <code>stack</code> the dataframe to reshape then group the dataframe on <code>level=0</code> and aggregate using <code>nth</code>, then <code>reindex</code> to conform the index of aggregated frame according to original frame</p>\n<pre><code>def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n</code></pre>\n<h3>Sample run</h3>\n<pre><code>&gt;&gt;&gt; first_valid(df, 0)\nDate\n1     25.0\n2     29.0\n3     33.0\n4     31.0\n5     30.0\n6     35.0\n7     31.0\n8     33.0\n9     26.0\n10    27.0\n11    35.0\n12    33.0\n13    28.0\n14    25.0\n15    25.0\n16    26.0\n17    34.0\n18    28.0\n19    34.0\n20    28.0\ndtype: float64\n\n\n&gt;&gt;&gt; first_valid(df, 1)\nDate\n1      NaN\n2      NaN\n3      NaN\n4     35.0\n5     34.0\n6     34.0\n7     26.0\n8     25.0\n9     31.0\n10    26.0\n11    25.0\n12    35.0\n13    25.0\n14    25.0\n15    26.0\n16    31.0\n17    29.0\n18    29.0\n19    26.0\n20    30.0\ndtype: float64\n\n&gt;&gt;&gt; first_valid(df, 2)\nDate\n1      NaN\n2      NaN\n3      NaN\n4      NaN\n5      NaN\n6      NaN\n7      NaN\n8     31.0\n9      NaN\n10    28.0\n11    29.0\n12    28.0\n13    35.0\n14    28.0\n15    31.0\n16    27.0\n17    25.0\n18    31.0\n19     NaN\n20     NaN\ndtype: float64\n</code></pre>\n<h3>Performance</h3>\n<pre><code># Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p><strong>Numpy based approach is approximately <code>300x</code> faster than the <code>OP's</code> given approach</strong> while pandas based approach is approximately <code>22x</code> faster</p>\n",
        "question_body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "formatted_input": {
            "qid": 68150020,
            "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
            "question": {
                "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
                "ques_desc": "I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. "
            },
            "io": [
                "f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n",
                "f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n"
            ],
            "answer": {
                "ans_desc": "Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster ",
                "code": [
                    "def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan\n    return vals\n",
                    "def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n",
                    "# Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 51,
            "user_id": 10082987,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/141965c0e67ba4408bdb2267fd7c04f2?s=128&d=identicon&r=PG&f=1",
            "display_name": "syedmfk",
            "link": "https://stackoverflow.com/users/10082987/syedmfk"
        },
        "is_answered": true,
        "view_count": 9367,
        "accepted_answer_id": 57033774,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1624643356,
        "creation_date": 1563168421,
        "last_edit_date": 1563169986,
        "question_id": 57033657,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
        "title": "How to Extract Month Name and Year from Date column of DataFrame",
        "body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "answer_body": "<p>Cast you date from object to actual datetime and use dt to access what you need.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n</code></pre>\n",
        "question_body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "formatted_input": {
            "qid": 57033657,
            "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
            "question": {
                "title": "How to Extract Month Name and Year from Date column of DataFrame",
                "ques_desc": "I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. "
            },
            "io": [
                "45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n",
                "45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n"
            ],
            "answer": {
                "ans_desc": "Cast you date from object to actual datetime and use dt to access what you need. ",
                "code": [
                    "import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 183,
            "user_id": 12939325,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-ylhVvK83HnQ/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdDak9Ir2cS6IzUxpxSksMsp-B7tA/photo.jpg?sz=128",
            "display_name": "Steak",
            "link": "https://stackoverflow.com/users/12939325/steak"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68107510,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1624488621,
        "creation_date": 1624484332,
        "last_edit_date": 1624488621,
        "question_id": 68107298,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
        "title": "How to Create a Correlation Dataframe from already related data",
        "body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "answer_body": "<p>I assume that the pairs are unique, i.e. if there's <code>English - Spanish</code>, there won't be a <code>Spanish - English</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code># Rename the columns. Because working with columns\n# named like numbers is a pain in the butt\ndf.columns = ['lang1', 'lang2', 'corr']\n\n# Create the opposite pairs\ntmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)\n\n# Create the identity pairs\nall_lang = np.unique(df[['lang1', 'lang2']])\ntmp2 = pd.DataFrame({\n    'lang1': all_lang,\n    'lang2': all_lang,\n    'corr': 1.0\n})\n\n# Concatenate all 3 together and beat the data frame into shape\nresult = (\n    pd.concat([df, tmp1, tmp2], axis=0)\n        .set_index(['lang1', 'lang2'])\n        .unstack()\n        .droplevel(0, axis=1)\n        .rename_axis(index=[None], columns=[None])\n)\n</code></pre>\n",
        "question_body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "formatted_input": {
            "qid": 68107298,
            "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
            "question": {
                "title": "How to Create a Correlation Dataframe from already related data",
                "ques_desc": "I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? "
            },
            "io": [
                "    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n",
                "        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n"
            ],
            "answer": {
                "ans_desc": "I assume that the pairs are unique, i.e. if there's , there won't be a . ",
                "code": [
                    "# Rename the columns. Because working with columns\n# named like numbers is a pain in the butt\ndf.columns = ['lang1', 'lang2', 'corr']\n\n# Create the opposite pairs\ntmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)\n\n# Create the identity pairs\nall_lang = np.unique(df[['lang1', 'lang2']])\ntmp2 = pd.DataFrame({\n    'lang1': all_lang,\n    'lang2': all_lang,\n    'corr': 1.0\n})\n\n# Concatenate all 3 together and beat the data frame into shape\nresult = (\n    pd.concat([df, tmp1, tmp2], axis=0)\n        .set_index(['lang1', 'lang2'])\n        .unstack()\n        .droplevel(0, axis=1)\n        .rename_axis(index=[None], columns=[None])\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16124075,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxaHbBnxKKjuXo_wOrkhGQHbT5P4fY54mYRG-h6=k-s128",
            "display_name": "hang",
            "link": "https://stackoverflow.com/users/16124075/hang"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 68098622,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624446318,
        "creation_date": 1624444563,
        "question_id": 68098150,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
        "title": "set some rule to groupby in pandas",
        "body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "answer_body": "<p>IIUC:</p>\n<p>try:</p>\n<pre><code>c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') &amp; c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n</code></pre>\n",
        "question_body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "formatted_input": {
            "qid": 68098150,
            "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
            "question": {
                "title": "set some rule to groupby in pandas",
                "ques_desc": "I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have \"dup by\" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. "
            },
            "io": [
                "1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n",
                "1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n"
            ],
            "answer": {
                "ans_desc": "IIUC: try: ",
                "code": [
                    "c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe",
            "beautifulsoup"
        ],
        "owner": {
            "reputation": 623,
            "user_id": 9846358,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/n1efb.jpg?s=128&g=1",
            "display_name": "Mary",
            "link": "https://stackoverflow.com/users/9846358/mary"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 68264773,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1625547795,
        "creation_date": 1625547085,
        "last_edit_date": 1625547795,
        "question_id": 68264711,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting",
        "title": "pd.read_html changed number formatting",
        "body": "<p>Cannot get <code>1,2,3,4,5,6</code> from the column of <code>CCCCCCC</code>, after <code>pd.read_html</code> format changed to <code>123456</code>, and my <strong>expected result</strong> should be keep <code>1,2,3,4,5,6</code></p>\n<p><strong>HTML code</strong></p>\n<pre><code>html = &quot;&quot;&quot;&lt;html&gt;\n&lt;body&gt;\n&lt;div id=&quot;MMMMMMMM&quot; class=&quot;MMMMMMMMMMM&quot; style=&quot;&quot;&gt;\n        &lt;table class=&quot;OOOOOOOO&quot; style=&quot;&quot;&gt;\n            &lt;thead&gt;\n                &lt;tr class=&quot;PPPPPPPPPP&quot;&gt;\n                    &lt;td colspan=&quot;3&quot; style=&quot;font-size:14px;font-weight:bold;&quot; class=&quot;QQQQQQQQQQ&quot;&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n                &lt;tr class=&quot;RRRRRRRRRR&quot;&gt;\n                    &lt;td&gt;BBBBBB&lt;/td&gt;\n                    &lt;td&gt;CCCCCCC&lt;/td&gt;\n                    &lt;td&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;\n                    &lt;tr class=&quot;SSSSSSSS&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;DDDDDD&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;3&quot;&gt;EEEEEEEEE&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;FFFFFFFFF&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;GGGGGGGGG&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;HHHHHHHHH&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;IIIIIIIIII&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;JJJJJJJJ&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;2&quot;&gt;KKKKKKKK&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;TTTTTT&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n            &lt;/tbody&gt;\n        &lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;&quot;&quot;&quot;\n</code></pre>\n<p><strong>Python Code</strong></p>\n<pre><code>from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1)\ndf_list\n</code></pre>\n<p><strong>Execution Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       123456  1234.56\n 1    EEEEEEEEE       123456  1234.56\n 2    EEEEEEEEE       123456  1234.56\n 3    EEEEEEEEE       123456  1234.56\n 4    FFFFFFFFF       123456  1234.56\n 5    GGGGGGGGG       123456  1234.56\n 6    HHHHHHHHH       123456  1234.56\n 7   IIIIIIIIII       123456  1234.56\n 8     JJJJJJJJ       123456  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n</code></pre>\n<p><strong>Expected Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF       1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG       1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH       1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII       1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ       1,2,3,4,5,6  1234.56\n 9     KKKKKKKK       1/2/3/4/5/6  1234.56\n 10    KKKKKKKK       1/2/3/4/5/6  1234.56]\n \n</code></pre>\n",
        "answer_body": "<p>You need to add the <code>thousands</code> parameter and set it to <code>None</code> by default it's <code>','</code>.</p>\n<pre><code>from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1, thousands=None)\ndf_list\n</code></pre>\n<h5>OUTPUT:</h5>\n<pre><code>[        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD  1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE  1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE  1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE  1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF  1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG  1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH  1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII  1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ  1,2,3,4,5,6  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n</code></pre>\n",
        "question_body": "<p>Cannot get <code>1,2,3,4,5,6</code> from the column of <code>CCCCCCC</code>, after <code>pd.read_html</code> format changed to <code>123456</code>, and my <strong>expected result</strong> should be keep <code>1,2,3,4,5,6</code></p>\n<p><strong>HTML code</strong></p>\n<pre><code>html = &quot;&quot;&quot;&lt;html&gt;\n&lt;body&gt;\n&lt;div id=&quot;MMMMMMMM&quot; class=&quot;MMMMMMMMMMM&quot; style=&quot;&quot;&gt;\n        &lt;table class=&quot;OOOOOOOO&quot; style=&quot;&quot;&gt;\n            &lt;thead&gt;\n                &lt;tr class=&quot;PPPPPPPPPP&quot;&gt;\n                    &lt;td colspan=&quot;3&quot; style=&quot;font-size:14px;font-weight:bold;&quot; class=&quot;QQQQQQQQQQ&quot;&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n                &lt;tr class=&quot;RRRRRRRRRR&quot;&gt;\n                    &lt;td&gt;BBBBBB&lt;/td&gt;\n                    &lt;td&gt;CCCCCCC&lt;/td&gt;\n                    &lt;td&gt;AAAAAAA&lt;/td&gt;\n                &lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;\n                    &lt;tr class=&quot;SSSSSSSS&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;DDDDDD&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;3&quot;&gt;EEEEEEEEE&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                        &lt;tr class=&quot;&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;FFFFFFFFF&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;GGGGGGGGG&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;HHHHHHHHH&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;IIIIIIIIII&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;&quot;&gt;\n                        &lt;td rowspan=&quot;1&quot;&gt;JJJJJJJJ&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1,2,3,4,5,6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                    &lt;tr class=&quot;TTTTT&quot;&gt;\n                        &lt;td rowspan=&quot;2&quot;&gt;KKKKKKKK&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                        &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                    &lt;/tr&gt;\n                        &lt;tr class=&quot;TTTTTT&quot;&gt;\n                            &lt;td class=&quot;L_LLLL67&quot;&gt;1/2/3/4/5/6&lt;/td&gt;\n                            &lt;td class=&quot;L_LLLL67 f_tar&quot;&gt;1234.56&lt;/td&gt;\n                        &lt;/tr&gt;\n            &lt;/tbody&gt;\n        &lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;&quot;&quot;&quot;\n</code></pre>\n<p><strong>Python Code</strong></p>\n<pre><code>from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1)\ndf_list\n</code></pre>\n<p><strong>Execution Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       123456  1234.56\n 1    EEEEEEEEE       123456  1234.56\n 2    EEEEEEEEE       123456  1234.56\n 3    EEEEEEEEE       123456  1234.56\n 4    FFFFFFFFF       123456  1234.56\n 5    GGGGGGGGG       123456  1234.56\n 6    HHHHHHHHH       123456  1234.56\n 7   IIIIIIIIII       123456  1234.56\n 8     JJJJJJJJ       123456  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n</code></pre>\n<p><strong>Expected Result</strong></p>\n<pre><code> [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF       1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG       1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH       1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII       1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ       1,2,3,4,5,6  1234.56\n 9     KKKKKKKK       1/2/3/4/5/6  1234.56\n 10    KKKKKKKK       1/2/3/4/5/6  1234.56]\n \n</code></pre>\n",
        "formatted_input": {
            "qid": 68264711,
            "link": "https://stackoverflow.com/questions/68264711/pd-read-html-changed-number-formatting",
            "question": {
                "title": "pd.read_html changed number formatting",
                "ques_desc": "Cannot get from the column of , after format changed to , and my expected result should be keep HTML code Python Code Execution Result Expected Result "
            },
            "io": [
                " [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       123456  1234.56\n 1    EEEEEEEEE       123456  1234.56\n 2    EEEEEEEEE       123456  1234.56\n 3    EEEEEEEEE       123456  1234.56\n 4    FFFFFFFFF       123456  1234.56\n 5    GGGGGGGGG       123456  1234.56\n 6    HHHHHHHHH       123456  1234.56\n 7   IIIIIIIIII       123456  1234.56\n 8     JJJJJJJJ       123456  1234.56\n 9     KKKKKKKK  1/2/3/4/5/6  1234.56\n 10    KKKKKKKK  1/2/3/4/5/6  1234.56]\n",
                " [        BBBBBB      CCCCCCC  AAAAAAA\n 0       DDDDDD       1,2,3,4,5,6  1234.56\n 1    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 2    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 3    EEEEEEEEE       1,2,3,4,5,6  1234.56\n 4    FFFFFFFFF       1,2,3,4,5,6  1234.56\n 5    GGGGGGGGG       1,2,3,4,5,6  1234.56\n 6    HHHHHHHHH       1,2,3,4,5,6  1234.56\n 7   IIIIIIIIII       1,2,3,4,5,6  1234.56\n 8     JJJJJJJJ       1,2,3,4,5,6  1234.56\n 9     KKKKKKKK       1/2/3/4/5/6  1234.56\n 10    KKKKKKKK       1/2/3/4/5/6  1234.56]\n \n"
            ],
            "answer": {
                "ans_desc": "You need to add the parameter and set it to by default it's . OUTPUT: ",
                "code": [
                    "from bs4 import BeautifulSoup\nimport pandas as pd\n\nsoup = BeautifulSoup(html,'html.parser')\ntable = soup.find('div', attrs={'id':'MMMMMMMM'})\ndf_list = pd.read_html(str(table), header=1, thousands=None)\ndf_list\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "user_type": "does_not_exist",
            "display_name": "user16253345"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68243166,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625389829,
        "creation_date": 1625389469,
        "question_id": 68243146,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas",
        "title": "replace zero with value of an other column using pandas",
        "body": "<p>I have a dataframe df1:</p>\n<pre><code>    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n</code></pre>\n<p>I want to replace 0 in the <strong>id column</strong> with value from <strong>ref column</strong> of the same row So it will become:</p>\n<pre><code>   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n</code></pre>\n",
        "answer_body": "<p>via <code>mask()</code>:</p>\n<pre><code>df['id']=df['id'].mask(df['id'].eq(0),df['ref'])\n</code></pre>\n<p>OR</p>\n<p>via numpy's <code>where()</code>:</p>\n<pre><code>#import numpy as np\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe df1:</p>\n<pre><code>    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n</code></pre>\n<p>I want to replace 0 in the <strong>id column</strong> with value from <strong>ref column</strong> of the same row So it will become:</p>\n<pre><code>   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n</code></pre>\n",
        "formatted_input": {
            "qid": 68243146,
            "link": "https://stackoverflow.com/questions/68243146/replace-zero-with-value-of-an-other-column-using-pandas",
            "question": {
                "title": "replace zero with value of an other column using pandas",
                "ques_desc": "I have a dataframe df1: I want to replace 0 in the id column with value from ref column of the same row So it will become: "
            },
            "io": [
                "    ref   Name   id  Score\n  8400   John    0     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen    0      0\n",
                "   ref    Name   id   Score\n  8400   John  8400     12\n  3840  Peter  414      0\n  7400  David  612     64\n  5200  Karen 5200      0\n"
            ],
            "answer": {
                "ans_desc": "via : OR via numpy's : ",
                "code": [
                    "#import numpy as np\ndf['id']=np.where(df['id'].eq(0),df['ref'],df['id'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "timestamp"
        ],
        "owner": {
            "reputation": 255,
            "user_id": 14073111,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-v9-fmI_5DnM/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckI-maisNcyjtLpr8roYSBRbOvrhw/photo.jpg?sz=128",
            "display_name": "user14073111",
            "link": "https://stackoverflow.com/users/14073111/user14073111"
        },
        "is_answered": true,
        "view_count": 35,
        "accepted_answer_id": 68232113,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1625301132,
        "creation_date": 1625261258,
        "last_edit_date": 1625262116,
        "question_id": 68231389,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas",
        "title": "Compare two columns that contains timestamps in pandas",
        "body": "<p>Lets say I have a dataframe like this one:</p>\n<pre><code>  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n</code></pre>\n<p>I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4).</p>\n<p>I tried this one:</p>\n<pre><code>df['Col1'] = pd.to_datetime(df['Col1'])\ndf['Col2'] = pd.to_datetime(df['Col2'])\ndf['Col3'] = pd.to_datetime(df['Col3'])\nk= (df['Col1'] &gt; df['Col2']).astype(int)\np=(df['Col2'] &gt; df['Col3']).astype(int)\n\nif k&gt;0:\n    df['Col2']=np.nan\n    df['Col3']=np.nan\n    df['Col4']=np.nan\nelif p&gt;0:\n    df['Col3']=np.nan\n    df['Col4']=np.nan \n</code></pre>\n<p>But it is showing me this error:</p>\n<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n<p>My desirable output would look like this:</p>\n<pre><code>  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n</code></pre>\n<p>EDITED:\nAdded Col0</p>\n",
        "answer_body": "<p>A straightforward way with boolean mask:</p>\n<pre><code>dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n</code></pre>\n<pre><code>&gt;&gt;&gt; df\n    Col0                Col1                Col2 Col3 Col4\n0  1.txt 2021-06-23 15:04:30                 NaT  NaT  NaT\n1  2.txt 2021-06-23 14:25:30 2021-06-23 15:30:30  NaT  NaT\n</code></pre>\n",
        "question_body": "<p>Lets say I have a dataframe like this one:</p>\n<pre><code>  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n</code></pre>\n<p>I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4).</p>\n<p>I tried this one:</p>\n<pre><code>df['Col1'] = pd.to_datetime(df['Col1'])\ndf['Col2'] = pd.to_datetime(df['Col2'])\ndf['Col3'] = pd.to_datetime(df['Col3'])\nk= (df['Col1'] &gt; df['Col2']).astype(int)\np=(df['Col2'] &gt; df['Col3']).astype(int)\n\nif k&gt;0:\n    df['Col2']=np.nan\n    df['Col3']=np.nan\n    df['Col4']=np.nan\nelif p&gt;0:\n    df['Col3']=np.nan\n    df['Col4']=np.nan \n</code></pre>\n<p>But it is showing me this error:</p>\n<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n</code></pre>\n<p>My desirable output would look like this:</p>\n<pre><code>  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n</code></pre>\n<p>EDITED:\nAdded Col0</p>\n",
        "formatted_input": {
            "qid": 68231389,
            "link": "https://stackoverflow.com/questions/68231389/compare-two-columns-that-contains-timestamps-in-pandas",
            "question": {
                "title": "Compare two columns that contains timestamps in pandas",
                "ques_desc": "Lets say I have a dataframe like this one: I want to compare if the timestamp in Col1 is greater than in Col2 and if that is true I want to remove the timestamps from the other columns (Col2, Col3, Col4). I also want to check if timestamp in Col2 is greater than in Col3 and if that is true I want to remove timestamp from other columns Col3, Col4). I tried this one: But it is showing me this error: My desirable output would look like this: EDITED: Added Col0 "
            },
            "io": [
                "  Col0       Col1                    Col2                   Col3                   Col4\n   1.txt  2021-06-23 15:04:30   2021-06-23 14:10:30   2021-06-23 14:15:30   2021-06-23 14:20:30\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30   2021-06-23 14:35:30   2021-06-23 14:40:30\n",
                "  Col0       Col1                    Col2               Col3                   Col4\n   1.txt  2021-06-23 15:04:30        NaN                 NaN                    NaN\n   2.txt  2021-06-23 14:25:30   2021-06-23 15:30:30      NaN                    NaN\n"
            ],
            "answer": {
                "ans_desc": "A straightforward way with boolean mask: ",
                "code": [
                    "dt = df.select_dtypes('datetime')\ndt = dt.mask(dt.lt(dt.shift(axis=1)).cumsum(axis=1).astype(bool))\n\ndf.loc[:, dt.columns.tolist()] = dt\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 49,
            "user_id": 16317158,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyBUnbLv2PI4_Agtm14oXQyk_ozZyrNZkI9w4DE=k-s128",
            "display_name": "Jewel_R",
            "link": "https://stackoverflow.com/users/16317158/jewel-r"
        },
        "is_answered": true,
        "view_count": 20,
        "accepted_answer_id": 68231157,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625259165,
        "creation_date": 1625258774,
        "question_id": 68231104,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe",
        "title": "Extract part of a 3 D dataframe",
        "body": "<p>I have a 3d dataframe. looks like this:</p>\n<pre><code>     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n</code></pre>\n<p>How could I extract only column  A &amp; B from every d1,d2.....? I desire to take the dataframe like this:</p>\n<pre><code>    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.isin.html#pandas-index-isin\" rel=\"nofollow noreferrer\"><code>Index.isin</code></a> on the level 1 values of columns then select with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>loc</code></a>:</p>\n<pre><code>filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n</code></pre>\n<p><code>filtered_df</code>:</p>\n<pre><code>   d1      d2    \n    A   B   A   B\n0   1   2   5   6\n1   9  10  13  14\n2  17  18  21  22\n</code></pre>\n<hr />\n<p>Sample Data Used:</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n</code></pre>\n<pre><code>   d1              d2            \n    A   B   C   D   A   B   C   D\n0   1   2   3   4   5   6   7   8\n1   9  10  11  12  13  14  15  16\n2  17  18  19  20  21  22  23  24\n</code></pre>\n",
        "question_body": "<p>I have a 3d dataframe. looks like this:</p>\n<pre><code>     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n</code></pre>\n<p>How could I extract only column  A &amp; B from every d1,d2.....? I desire to take the dataframe like this:</p>\n<pre><code>    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n</code></pre>\n",
        "formatted_input": {
            "qid": 68231104,
            "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe",
            "question": {
                "title": "Extract part of a 3 D dataframe",
                "ques_desc": "I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: "
            },
            "io": [
                "     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n",
                "    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n"
            ],
            "answer": {
                "ans_desc": "Use on the level 1 values of columns then select with : : Sample Data Used: ",
                "code": [
                    "filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n",
                    "import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 11566142,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d2e69536157f140a0c82dc8f714c295d?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ivan7",
            "link": "https://stackoverflow.com/users/11566142/ivan7"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68229969,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1625254302,
        "creation_date": 1625250274,
        "last_edit_date": 1625251248,
        "question_id": 68229806,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe",
        "title": "Insert values from variable and DataFrame into another DataFrame",
        "body": "<p>On start I have two DataFrames and one variable:</p>\n<pre><code>id=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n</code></pre>\n<p>I have to map <strong>id</strong> variable and the corresponding <strong>col0</strong> cell from <strong>df1</strong> DataFrame to all rows in <strong>df2</strong> DataFrame. I tryed and as the result I made the code below:</p>\n<pre><code>df2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'])\n</code></pre>\n<p>It seems to me that the code should work correctly, but unfortunatelly I have a <strong>NaN</strong> value in the <strong>col0</strong> column.</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n</code></pre>\n<p>The expected result was:</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n</code></pre>\n<p>I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please:</p>\n<ol>\n<li>explain briefly why I am getting the error</li>\n<li>fix my mistake in the code</li>\n</ol>\n",
        "answer_body": "<p>Your mistake is on this string <code>df1[df1['id']==id]['col0']</code> when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value.</p>\n<p>To solve this issue is very very very simple, you just have to call the first item at the Series object like this: <code>df1[df1['id']==id]['col0'][0]</code></p>\n<p>Your code with the ajustment must look like this</p>\n<pre><code>import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n</code></pre>\n<p>Then your new df2 is like this:</p>\n<pre><code>   id  col0  col1  col2\n0   1     3    13    23\n1   1     3    14    24\n2   1     3    15    25\n</code></pre>\n",
        "question_body": "<p>On start I have two DataFrames and one variable:</p>\n<pre><code>id=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n</code></pre>\n<p>I have to map <strong>id</strong> variable and the corresponding <strong>col0</strong> cell from <strong>df1</strong> DataFrame to all rows in <strong>df2</strong> DataFrame. I tryed and as the result I made the code below:</p>\n<pre><code>df2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'])\n</code></pre>\n<p>It seems to me that the code should work correctly, but unfortunatelly I have a <strong>NaN</strong> value in the <strong>col0</strong> column.</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n</code></pre>\n<p>The expected result was:</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n</code></pre>\n<p>I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please:</p>\n<ol>\n<li>explain briefly why I am getting the error</li>\n<li>fix my mistake in the code</li>\n</ol>\n",
        "formatted_input": {
            "qid": 68229806,
            "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe",
            "question": {
                "title": "Insert values from variable and DataFrame into another DataFrame",
                "ques_desc": "On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code "
            },
            "io": [
                "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n",
                "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n"
            ],
            "answer": {
                "ans_desc": "Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: ",
                "code": [
                    "import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, \"id\", id)\ndf2.insert(1, \"col0\", df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "algorithm",
            "dataframe",
            "pairwise"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 16238148,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a87a2e4b04f8d0cd6d19f6e68eab5a4e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Eugene Zinder",
            "link": "https://stackoverflow.com/users/16238148/eugene-zinder"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 68213724,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625157900,
        "creation_date": 1625155883,
        "question_id": 68213612,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun",
        "title": "How to combine rows in a dataframe in a pairwise fashion while applying some function",
        "body": "<p>I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2:</p>\n<pre><code>ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n</code></pre>\n<p>I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is.</p>\n<p>Here is the resulting dataframe:</p>\n<pre><code>ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n</code></pre>\n<p>In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row.</p>\n<p>(id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows.</p>\n<p><strong>I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way?</strong></p>\n<p>Code:</p>\n<pre><code>df = pd.DataFrame(columns=['ID', 'Val1', 'Val2'], data=[['id0', 10, 20], ['id0', 11, 19], ['id1', 5, 5], ['id1', 1, 1], ['id1', 2, 4]])\n</code></pre>\n",
        "answer_body": "<p>You can use <code>df.rolling</code> after grouping by <code>ID</code>:</p>\n<pre><code>out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n</code></pre>\n<pre><code>&gt;&gt;&gt; out\n       Val1  Val2\nid0_1  10.5  19.5\nid1_1   3.0   3.0\nid1_2   1.5   2.5\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2:</p>\n<pre><code>ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n</code></pre>\n<p>I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is.</p>\n<p>Here is the resulting dataframe:</p>\n<pre><code>ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n</code></pre>\n<p>In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row.</p>\n<p>(id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows.</p>\n<p><strong>I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way?</strong></p>\n<p>Code:</p>\n<pre><code>df = pd.DataFrame(columns=['ID', 'Val1', 'Val2'], data=[['id0', 10, 20], ['id0', 11, 19], ['id1', 5, 5], ['id1', 1, 1], ['id1', 2, 4]])\n</code></pre>\n",
        "formatted_input": {
            "qid": 68213612,
            "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun",
            "question": {
                "title": "How to combine rows in a dataframe in a pairwise fashion while applying some function",
                "ques_desc": "I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: "
            },
            "io": [
                "ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n",
                "ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n"
            ],
            "answer": {
                "ans_desc": "You can use after grouping by : ",
                "code": [
                    "out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 173,
            "user_id": 9254726,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9070bdb0341821aed239bfa229c8f43a?s=128&d=identicon&r=PG&f=1",
            "display_name": "5E4ME",
            "link": "https://stackoverflow.com/users/9254726/5e4me"
        },
        "is_answered": true,
        "view_count": 15,
        "accepted_answer_id": 68211988,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625149939,
        "creation_date": 1625149090,
        "question_id": 68211888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base",
        "title": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column",
        "body": "<p>I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time.</p>\n<p>The starting dataframes look like this:</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n</code></pre>\n<p>And I would like to get to a series of frames looking like this</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n</code></pre>\n<p>My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible.</p>\n<p>I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.</p>\n",
        "answer_body": "<p>Let's try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> then build out a <code>dict</code>:</p>\n<pre><code>dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n</code></pre>\n<p><code>dfs</code>:</p>\n<pre><code>{'player1':       NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7,\n 'player2':       NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7,\n 'player3':       NAME  VAL1  VAL2  VAL3\n2  player3     3     6     7\n1  player3     2     6     8,\n 'player5':       NAME  VAL1  VAL2  VAL3\n2  player5     3     6     7}\n</code></pre>\n<p>Each player's DataFrame can then be accessed like:</p>\n<p><code>dfs['player1']</code>:</p>\n<pre><code>      NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7\n</code></pre>\n<hr />\n<p>Or as a <code>list</code>:</p>\n<pre><code>dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n</code></pre>\n<p><code>dfs</code>:</p>\n<pre><code>[      NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7,\n       NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7,\n       NAME  VAL1  VAL2  VAL3\n2  player3     3     6     7\n1  player3     2     6     8,\n       NAME  VAL1  VAL2  VAL3\n2  player5     3     6     7]\n</code></pre>\n<p>Each player's DataFrame can then be accessed like:</p>\n<p><code>dfs[1]</code>:</p>\n<pre><code>      NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7\n</code></pre>\n",
        "question_body": "<p>I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time.</p>\n<p>The starting dataframes look like this:</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n</code></pre>\n<p>And I would like to get to a series of frames looking like this</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n</code></pre>\n<p>My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible.</p>\n<p>I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.</p>\n",
        "formatted_input": {
            "qid": 68211888,
            "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base",
            "question": {
                "title": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column",
                "ques_desc": "I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. "
            },
            "io": [
                "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n",
                "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n"
            ],
            "answer": {
                "ans_desc": "Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : ",
                "code": [
                    "dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n",
                    "dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-munging"
        ],
        "owner": {
            "reputation": 724,
            "user_id": 6057371,
            "user_type": "registered",
            "accept_rate": 31,
            "profile_image": "https://www.gravatar.com/avatar/b744e3dbff9c7fdb801570de75f6eff7?s=128&d=identicon&r=PG&f=1",
            "display_name": "okuoub",
            "link": "https://stackoverflow.com/users/6057371/okuoub"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68193597,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625051576,
        "creation_date": 1625051004,
        "question_id": 68193558,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values",
        "title": "pandas group many columns to one column where every cell is a list of values",
        "body": "<p>I have the dataframe</p>\n<pre><code>df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n</code></pre>\n<p>And I want to group all columns to a single list that will be the only columns, so I will get:</p>\n<pre><code>df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n</code></pre>\n<p>(Shape of df was change from (3,5) to (3,1))</p>\n<p>What is the best way to do this?</p>\n",
        "answer_body": "<p>Try:</p>\n<pre><code>#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n</code></pre>\n",
        "question_body": "<p>I have the dataframe</p>\n<pre><code>df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n</code></pre>\n<p>And I want to group all columns to a single list that will be the only columns, so I will get:</p>\n<pre><code>df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n</code></pre>\n<p>(Shape of df was change from (3,5) to (3,1))</p>\n<p>What is the best way to do this?</p>\n",
        "formatted_input": {
            "qid": 68193558,
            "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values",
            "question": {
                "title": "pandas group many columns to one column where every cell is a list of values",
                "ques_desc": "I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? "
            },
            "io": [
                "df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n",
                "df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n"
            ],
            "answer": {
                "ans_desc": "Try: ",
                "code": [
                    "#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 15759786,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/002b02baee3603027b66ed2361da99e2?s=128&d=identicon&r=PG&f=1",
            "display_name": "big sad",
            "link": "https://stackoverflow.com/users/15759786/big-sad"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68193581,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625051129,
        "creation_date": 1625050872,
        "question_id": 68193521,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame",
        "title": "Concatenate values and column names in a data frame to create a new data frame",
        "body": "<p>I have the following data frame(<code>df1</code>):</p>\n<pre><code>  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n</code></pre>\n<p>I need to derive the data frame(<code>df2</code>) from <code>df1</code> such that column 1 of <code>d2</code> will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of <code>d2</code> will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate.   :</p>\n<pre><code>      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n</code></pre>\n<p>I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process?</p>\n<p>Below is the code I have used</p>\n<pre><code>d = {'Value': ['a','b','c','d','e'],'col1': ['aa','ba','ca','da','ea'], 'col2' : ['ab','bb','cb','db','eb'],'col3': ['ac','bc','cc','dc','ec']}\ndf1 = pd.DataFrame(data = d)\n\n# Repeat every value is Value column 3 times.\nX = df1['Value'].repeat(4).reset_index(drop=True)\n\n# Create separate series with Col 1, Col 2, Col 3 names.\nY = pd.Series(df1.columns[1:])\n\n# Repeated series Y to the length of data df1\nYY = pd.Series(np.tile(Y.values, len(df1)))\n\n# Create the first column by concatenating X and YY\nfirst_column_1 = X + &quot;_&quot; + YY\n\nZ = df1.set_index('Value')\nZZ = np.ravel(Z.values)\n\n#Create 2nd column from ZZ\nsecond_column = pd.Series(ZZ)\n\n#Create df2\ndf2 = pd.DataFrame([first_column, second_column]).T\n</code></pre>\n",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = df.melt(&quot;Value&quot;, value_name=&quot;Col 1&quot;)\nx.Value += &quot;_&quot; + x.variable\nx = x.drop(columns=&quot;variable&quot;)\nprint(x)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>     Value Col 1\n0   a_col1    aa\n1   b_col1    ba\n2   c_col1    ca\n3   d_col1    da\n4   e_col1    ea\n5   a_col2    ab\n6   b_col2    bb\n7   c_col2    cb\n8   d_col2    db\n9   e_col2    eb\n10  a_col3    ac\n11  b_col3    bc\n12  c_col3    cc\n13  d_col3    dc\n14  e_col3    ec\n</code></pre>\n<hr />\n<p>Optionally, you can sort values afterwards:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = x.sort_values(by=&quot;Value&quot;).reset_index(drop=True)\nprint(x)\n\n     Value Col 1\n0   a_col1    aa\n1   a_col2    ab\n2   a_col3    ac\n3   b_col1    ba\n4   b_col2    bb\n5   b_col3    bc\n6   c_col1    ca\n7   c_col2    cb\n8   c_col3    cc\n9   d_col1    da\n10  d_col2    db\n11  d_col3    dc\n12  e_col1    ea\n13  e_col2    eb\n14  e_col3    ec\n</code></pre>\n",
        "question_body": "<p>I have the following data frame(<code>df1</code>):</p>\n<pre><code>  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n</code></pre>\n<p>I need to derive the data frame(<code>df2</code>) from <code>df1</code> such that column 1 of <code>d2</code> will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of <code>d2</code> will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate.   :</p>\n<pre><code>      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n</code></pre>\n<p>I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process?</p>\n<p>Below is the code I have used</p>\n<pre><code>d = {'Value': ['a','b','c','d','e'],'col1': ['aa','ba','ca','da','ea'], 'col2' : ['ab','bb','cb','db','eb'],'col3': ['ac','bc','cc','dc','ec']}\ndf1 = pd.DataFrame(data = d)\n\n# Repeat every value is Value column 3 times.\nX = df1['Value'].repeat(4).reset_index(drop=True)\n\n# Create separate series with Col 1, Col 2, Col 3 names.\nY = pd.Series(df1.columns[1:])\n\n# Repeated series Y to the length of data df1\nYY = pd.Series(np.tile(Y.values, len(df1)))\n\n# Create the first column by concatenating X and YY\nfirst_column_1 = X + &quot;_&quot; + YY\n\nZ = df1.set_index('Value')\nZZ = np.ravel(Z.values)\n\n#Create 2nd column from ZZ\nsecond_column = pd.Series(ZZ)\n\n#Create df2\ndf2 = pd.DataFrame([first_column, second_column]).T\n</code></pre>\n",
        "formatted_input": {
            "qid": 68193521,
            "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame",
            "question": {
                "title": "Concatenate values and column names in a data frame to create a new data frame",
                "ques_desc": "I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used "
            },
            "io": [
                "  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n",
                "      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n"
            ],
            "answer": {
                "ans_desc": "Try: Prints: Optionally, you can sort values afterwards: ",
                "code": [
                    "x = df.melt(\"Value\", value_name=\"Col 1\")\nx.Value += \"_\" + x.variable\nx = x.drop(columns=\"variable\")\nprint(x)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe",
            "data-structures"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 9079043,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/974eaeafce0b16f35dce0911a26ade93?s=128&d=identicon&r=PG&f=1",
            "display_name": "Isha",
            "link": "https://stackoverflow.com/users/9079043/isha"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 68174753,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624955956,
        "creation_date": 1624953161,
        "last_edit_date": 1624953363,
        "question_id": 68174614,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
        "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
        "body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "answer_body": "<p>The problem here is, when you call apply on <code>axis=1</code>, pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; s = pd.Series([1, 2, 3.2])\ns\n0    1.0\n1    2.0\n2    3.2\ndtype: float64\n</code></pre>\n<p>As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to :</p>\n<pre><code>df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n</code></pre>\n<p>There's already an issue <a href=\"https://github.com/pandas-dev/pandas/issues/23230\" rel=\"nofollow noreferrer\">DataFrame.apply unintuitively changes int to float because of another column</a> on github for this upcasting behavior of pandas <code>apply</code>.\nSo, one possible option for you is as I have mentioned in the comment, to call <code>to_json</code> on the entire dataframe as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt;df.to_json(orient='index')\n'{&quot;0&quot;:{&quot;A&quot;:2,&quot;B&quot;:6,&quot;C&quot;:5,&quot;D&quot;:8.0},&quot;1&quot;:{&quot;A&quot;:6,&quot;B&quot;:11,&quot;C&quot;:2,&quot;D&quot;:3.6},&quot;2&quot;:{&quot;A&quot;:1,&quot;B&quot;:5,&quot;C&quot;:7,&quot;D&quot;:5.2}}'\n</code></pre>\n<p>A working solution for you may be using python's <code>json</code> module alongwith <code>DataFrame.to_json()</code>, but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['total'] = list(map(json.dumps, [*json.loads(df.to_json(orient='index')).values()]))\n</code></pre>\n<p><strong>OUTPUT:</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>   A   B  C    D                                total\n0  2   6  5  8.0   {&quot;A&quot;: 2, &quot;B&quot;: 6, &quot;C&quot;: 5, &quot;D&quot;: 8.0}\n1  6  11  2  3.6  {&quot;A&quot;: 6, &quot;B&quot;: 11, &quot;C&quot;: 2, &quot;D&quot;: 3.6}\n2  1   5  7  5.2   {&quot;A&quot;: 1, &quot;B&quot;: 5, &quot;C&quot;: 7, &quot;D&quot;: 5.2}\n</code></pre>\n",
        "question_body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "formatted_input": {
            "qid": 68174614,
            "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
            "question": {
                "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
                "ques_desc": "I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? "
            },
            "io": [
                "A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n",
                "{\"A\":2.0, \"B\":6.0, \"C\":5.0, \"D\":8.0}\n{\"A\":6.0, \"B\":11.0, \"C\":2.0, \"D\":3.6}\n{\"A\":1.0, \"B\":5.0, \"C\":7.0, \"D\":5.2}\n"
            ],
            "answer": {
                "ans_desc": "The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: ",
                "code": [
                    "df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 4149213,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/vBcjv.jpg?s=128&g=1",
            "display_name": "openwater",
            "link": "https://stackoverflow.com/users/4149213/openwater"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 68174309,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624951870,
        "creation_date": 1624950821,
        "last_edit_date": 1624951870,
        "question_id": 68174113,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
        "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
        "body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "answer_body": "<p>A simpler solution would be to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.GroupBy.apply.html\" rel=\"nofollow noreferrer\"><code>apply</code></a> a custom function on each group. In this case, we can define a function <code>reclass</code> that obtains the correct bins and ids and then uses <code>pd.cut</code>:</p>\n<pre><code>def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n</code></pre>\n<p>Result:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "formatted_input": {
            "qid": 68174113,
            "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
            "question": {
                "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
                "ques_desc": "I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? "
            },
            "io": [
                "bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n",
                "ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n"
            ],
            "answer": {
                "ans_desc": "A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: ",
                "code": [
                    "def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 121,
            "user_id": 2643948,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/3423e43b576ab333117f731daef5aad9?s=128&d=identicon&r=PG&f=1",
            "display_name": "RebeccaKennedy",
            "link": "https://stackoverflow.com/users/2643948/rebeccakennedy"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 68150849,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624808663,
        "creation_date": 1624788441,
        "question_id": 68150020,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
        "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
        "body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "answer_body": "<h3>Numpy approach</h3>\n<p>We can define a function <code>first_value</code> which takes a <code>2D</code> array and <code>offset</code> (n) as input arguments and returns <code>1D</code> array. Basically, for each row it returns the <code>nth</code> value after the first <code>non-nan</code> value</p>\n<pre><code>def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i &gt;= arr.shape[1])] = np.nan\n    return vals\n</code></pre>\n<h3>Pandas approach</h3>\n<p>We can <code>stack</code> the dataframe to reshape then group the dataframe on <code>level=0</code> and aggregate using <code>nth</code>, then <code>reindex</code> to conform the index of aggregated frame according to original frame</p>\n<pre><code>def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n</code></pre>\n<h3>Sample run</h3>\n<pre><code>&gt;&gt;&gt; first_valid(df, 0)\nDate\n1     25.0\n2     29.0\n3     33.0\n4     31.0\n5     30.0\n6     35.0\n7     31.0\n8     33.0\n9     26.0\n10    27.0\n11    35.0\n12    33.0\n13    28.0\n14    25.0\n15    25.0\n16    26.0\n17    34.0\n18    28.0\n19    34.0\n20    28.0\ndtype: float64\n\n\n&gt;&gt;&gt; first_valid(df, 1)\nDate\n1      NaN\n2      NaN\n3      NaN\n4     35.0\n5     34.0\n6     34.0\n7     26.0\n8     25.0\n9     31.0\n10    26.0\n11    25.0\n12    35.0\n13    25.0\n14    25.0\n15    26.0\n16    31.0\n17    29.0\n18    29.0\n19    26.0\n20    30.0\ndtype: float64\n\n&gt;&gt;&gt; first_valid(df, 2)\nDate\n1      NaN\n2      NaN\n3      NaN\n4      NaN\n5      NaN\n6      NaN\n7      NaN\n8     31.0\n9      NaN\n10    28.0\n11    29.0\n12    28.0\n13    35.0\n14    28.0\n15    31.0\n16    27.0\n17    25.0\n18    31.0\n19     NaN\n20     NaN\ndtype: float64\n</code></pre>\n<h3>Performance</h3>\n<pre><code># Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p><strong>Numpy based approach is approximately <code>300x</code> faster than the <code>OP's</code> given approach</strong> while pandas based approach is approximately <code>22x</code> faster</p>\n",
        "question_body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "formatted_input": {
            "qid": 68150020,
            "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
            "question": {
                "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
                "ques_desc": "I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. "
            },
            "io": [
                "f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n",
                "f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n"
            ],
            "answer": {
                "ans_desc": "Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster ",
                "code": [
                    "def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan\n    return vals\n",
                    "def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n",
                    "# Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 51,
            "user_id": 10082987,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/141965c0e67ba4408bdb2267fd7c04f2?s=128&d=identicon&r=PG&f=1",
            "display_name": "syedmfk",
            "link": "https://stackoverflow.com/users/10082987/syedmfk"
        },
        "is_answered": true,
        "view_count": 9367,
        "accepted_answer_id": 57033774,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1624643356,
        "creation_date": 1563168421,
        "last_edit_date": 1563169986,
        "question_id": 57033657,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
        "title": "How to Extract Month Name and Year from Date column of DataFrame",
        "body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "answer_body": "<p>Cast you date from object to actual datetime and use dt to access what you need.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n</code></pre>\n",
        "question_body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "formatted_input": {
            "qid": 57033657,
            "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
            "question": {
                "title": "How to Extract Month Name and Year from Date column of DataFrame",
                "ques_desc": "I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. "
            },
            "io": [
                "45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n",
                "45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n"
            ],
            "answer": {
                "ans_desc": "Cast you date from object to actual datetime and use dt to access what you need. ",
                "code": [
                    "import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 183,
            "user_id": 12939325,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-ylhVvK83HnQ/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdDak9Ir2cS6IzUxpxSksMsp-B7tA/photo.jpg?sz=128",
            "display_name": "Steak",
            "link": "https://stackoverflow.com/users/12939325/steak"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68107510,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1624488621,
        "creation_date": 1624484332,
        "last_edit_date": 1624488621,
        "question_id": 68107298,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
        "title": "How to Create a Correlation Dataframe from already related data",
        "body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "answer_body": "<p>I assume that the pairs are unique, i.e. if there's <code>English - Spanish</code>, there won't be a <code>Spanish - English</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code># Rename the columns. Because working with columns\n# named like numbers is a pain in the butt\ndf.columns = ['lang1', 'lang2', 'corr']\n\n# Create the opposite pairs\ntmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)\n\n# Create the identity pairs\nall_lang = np.unique(df[['lang1', 'lang2']])\ntmp2 = pd.DataFrame({\n    'lang1': all_lang,\n    'lang2': all_lang,\n    'corr': 1.0\n})\n\n# Concatenate all 3 together and beat the data frame into shape\nresult = (\n    pd.concat([df, tmp1, tmp2], axis=0)\n        .set_index(['lang1', 'lang2'])\n        .unstack()\n        .droplevel(0, axis=1)\n        .rename_axis(index=[None], columns=[None])\n)\n</code></pre>\n",
        "question_body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "formatted_input": {
            "qid": 68107298,
            "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
            "question": {
                "title": "How to Create a Correlation Dataframe from already related data",
                "ques_desc": "I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? "
            },
            "io": [
                "    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n",
                "        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n"
            ],
            "answer": {
                "ans_desc": "I assume that the pairs are unique, i.e. if there's , there won't be a . ",
                "code": [
                    "# Rename the columns. Because working with columns\n# named like numbers is a pain in the butt\ndf.columns = ['lang1', 'lang2', 'corr']\n\n# Create the opposite pairs\ntmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)\n\n# Create the identity pairs\nall_lang = np.unique(df[['lang1', 'lang2']])\ntmp2 = pd.DataFrame({\n    'lang1': all_lang,\n    'lang2': all_lang,\n    'corr': 1.0\n})\n\n# Concatenate all 3 together and beat the data frame into shape\nresult = (\n    pd.concat([df, tmp1, tmp2], axis=0)\n        .set_index(['lang1', 'lang2'])\n        .unstack()\n        .droplevel(0, axis=1)\n        .rename_axis(index=[None], columns=[None])\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16124075,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxaHbBnxKKjuXo_wOrkhGQHbT5P4fY54mYRG-h6=k-s128",
            "display_name": "hang",
            "link": "https://stackoverflow.com/users/16124075/hang"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 68098622,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624446318,
        "creation_date": 1624444563,
        "question_id": 68098150,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
        "title": "set some rule to groupby in pandas",
        "body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "answer_body": "<p>IIUC:</p>\n<p>try:</p>\n<pre><code>c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') &amp; c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n</code></pre>\n",
        "question_body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "formatted_input": {
            "qid": 68098150,
            "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
            "question": {
                "title": "set some rule to groupby in pandas",
                "ques_desc": "I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have \"dup by\" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. "
            },
            "io": [
                "1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n",
                "1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n"
            ],
            "answer": {
                "ans_desc": "IIUC: try: ",
                "code": [
                    "c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 13920381,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgbR0vHurgfWnrlCJifphJzuFAkbDbJV3Ls033L6g=k-s128",
            "display_name": "dinn_",
            "link": "https://stackoverflow.com/users/13920381/dinn"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68090682,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624397667,
        "creation_date": 1624394378,
        "question_id": 68090463,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
        "title": "Merging more than two columns of the same dataframe in pandas",
        "body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "answer_body": "<p>Input data:</p>\n<pre><code>&gt;&gt;&gt; df\n    VAR 1  VAR 2  VAR 3    GROUP  ANOTHER\n0     NaN    NaN    3.0   [0-10]  another\n1     1.0    NaN    3.0   [0-10]  another\n2     1.0    NaN    3.0   [0-10]  another\n3     1.0    2.0    NaN   [0-10]  another\n4     NaN    2.0    NaN   [0-10]  another\n5     3.0    NaN    3.0  [10-20]  another\n6     3.0    1.0    NaN  [10-20]  another\n7     NaN    1.0    NaN  [10-20]  another\n8     NaN    2.0    NaN  [10-20]  another\n9     NaN    NaN    2.0  [10-20]  another\n10    NaN    NaN    2.0  [10-20]  another\n</code></pre>\n<p>You can use <code>melt</code>. To fully understand, you can execute the code line by line (<code>df.melt(...)</code>, <code>df.melt(...).dropna()</code>, <code>df.melt(...).dropna.sort_values(...)</code> and so on):</p>\n<pre><code>id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n</code></pre>\n<p>Result output:</p>\n<pre><code>&gt;&gt;&gt; out\n    VAR_MERGED    GROUP  ANOTHER\n0          1.0   [0-10]  another\n1          1.0   [0-10]  another\n2          1.0   [0-10]  another\n3          2.0   [0-10]  another\n4          2.0   [0-10]  another\n5          3.0   [0-10]  another\n6          3.0   [0-10]  another\n7          3.0   [0-10]  another\n8          1.0  [10-20]  another\n9          1.0  [10-20]  another\n10         2.0  [10-20]  another\n11         2.0  [10-20]  another\n12         2.0  [10-20]  another\n13         3.0  [10-20]  another\n14         3.0  [10-20]  another\n15         3.0  [10-20]  another\n</code></pre>\n",
        "question_body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "formatted_input": {
            "qid": 68090463,
            "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
            "question": {
                "title": "Merging more than two columns of the same dataframe in pandas",
                "ques_desc": "Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? "
            },
            "io": [
                "VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n",
                "VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n"
            ],
            "answer": {
                "ans_desc": "Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: ",
                "code": [
                    "id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 49,
            "user_id": 16317158,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyBUnbLv2PI4_Agtm14oXQyk_ozZyrNZkI9w4DE=k-s128",
            "display_name": "Jewel_R",
            "link": "https://stackoverflow.com/users/16317158/jewel-r"
        },
        "is_answered": true,
        "view_count": 20,
        "accepted_answer_id": 68231157,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625259165,
        "creation_date": 1625258774,
        "question_id": 68231104,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe",
        "title": "Extract part of a 3 D dataframe",
        "body": "<p>I have a 3d dataframe. looks like this:</p>\n<pre><code>     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n</code></pre>\n<p>How could I extract only column  A &amp; B from every d1,d2.....? I desire to take the dataframe like this:</p>\n<pre><code>    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n</code></pre>\n",
        "answer_body": "<p>Use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.isin.html#pandas-index-isin\" rel=\"nofollow noreferrer\"><code>Index.isin</code></a> on the level 1 values of columns then select with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\" rel=\"nofollow noreferrer\"><code>loc</code></a>:</p>\n<pre><code>filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n</code></pre>\n<p><code>filtered_df</code>:</p>\n<pre><code>   d1      d2    \n    A   B   A   B\n0   1   2   5   6\n1   9  10  13  14\n2  17  18  21  22\n</code></pre>\n<hr />\n<p>Sample Data Used:</p>\n<pre><code>import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n</code></pre>\n<pre><code>   d1              d2            \n    A   B   C   D   A   B   C   D\n0   1   2   3   4   5   6   7   8\n1   9  10  11  12  13  14  15  16\n2  17  18  19  20  21  22  23  24\n</code></pre>\n",
        "question_body": "<p>I have a 3d dataframe. looks like this:</p>\n<pre><code>     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n</code></pre>\n<p>How could I extract only column  A &amp; B from every d1,d2.....? I desire to take the dataframe like this:</p>\n<pre><code>    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n</code></pre>\n",
        "formatted_input": {
            "qid": 68231104,
            "link": "https://stackoverflow.com/questions/68231104/extract-part-of-a-3-d-dataframe",
            "question": {
                "title": "Extract part of a 3 D dataframe",
                "ques_desc": "I have a 3d dataframe. looks like this: How could I extract only column A & B from every d1,d2.....? I desire to take the dataframe like this: "
            },
            "io": [
                "     d1        d2            d3\n   A B C D...   A B C D...   A B C D..\n0  \n1\n2\n",
                "    d1    d2    d3\n  A  B   A  B   A  B\n0\n1\n2\n"
            ],
            "answer": {
                "ans_desc": "Use on the level 1 values of columns then select with : : Sample Data Used: ",
                "code": [
                    "filtered_df = df.loc[:, df.columns.isin(['A', 'B'], level=1)]\n",
                    "import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    np.arange(1, 25).reshape((-1, 8)),\n    columns=pd.MultiIndex.from_product((['d1', 'd2'], list('ABCD')))\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 11566142,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/d2e69536157f140a0c82dc8f714c295d?s=128&d=identicon&r=PG&f=1",
            "display_name": "Ivan7",
            "link": "https://stackoverflow.com/users/11566142/ivan7"
        },
        "is_answered": true,
        "view_count": 43,
        "accepted_answer_id": 68229969,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1625254302,
        "creation_date": 1625250274,
        "last_edit_date": 1625251248,
        "question_id": 68229806,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe",
        "title": "Insert values from variable and DataFrame into another DataFrame",
        "body": "<p>On start I have two DataFrames and one variable:</p>\n<pre><code>id=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n</code></pre>\n<p>I have to map <strong>id</strong> variable and the corresponding <strong>col0</strong> cell from <strong>df1</strong> DataFrame to all rows in <strong>df2</strong> DataFrame. I tryed and as the result I made the code below:</p>\n<pre><code>df2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'])\n</code></pre>\n<p>It seems to me that the code should work correctly, but unfortunatelly I have a <strong>NaN</strong> value in the <strong>col0</strong> column.</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n</code></pre>\n<p>The expected result was:</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n</code></pre>\n<p>I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please:</p>\n<ol>\n<li>explain briefly why I am getting the error</li>\n<li>fix my mistake in the code</li>\n</ol>\n",
        "answer_body": "<p>Your mistake is on this string <code>df1[df1['id']==id]['col0']</code> when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value.</p>\n<p>To solve this issue is very very very simple, you just have to call the first item at the Series object like this: <code>df1[df1['id']==id]['col0'][0]</code></p>\n<p>Your code with the ajustment must look like this</p>\n<pre><code>import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n</code></pre>\n<p>Then your new df2 is like this:</p>\n<pre><code>   id  col0  col1  col2\n0   1     3    13    23\n1   1     3    14    24\n2   1     3    15    25\n</code></pre>\n",
        "question_body": "<p>On start I have two DataFrames and one variable:</p>\n<pre><code>id=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n</code></pre>\n<p>I have to map <strong>id</strong> variable and the corresponding <strong>col0</strong> cell from <strong>df1</strong> DataFrame to all rows in <strong>df2</strong> DataFrame. I tryed and as the result I made the code below:</p>\n<pre><code>df2.insert(0, &quot;id&quot;, id)\ndf2.insert(1, &quot;col0&quot;, df1[df1['id']==id]['col0'])\n</code></pre>\n<p>It seems to me that the code should work correctly, but unfortunatelly I have a <strong>NaN</strong> value in the <strong>col0</strong> column.</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n</code></pre>\n<p>The expected result was:</p>\n<pre><code>   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n</code></pre>\n<p>I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please:</p>\n<ol>\n<li>explain briefly why I am getting the error</li>\n<li>fix my mistake in the code</li>\n</ol>\n",
        "formatted_input": {
            "qid": 68229806,
            "link": "https://stackoverflow.com/questions/68229806/insert-values-from-variable-and-dataframe-into-another-dataframe",
            "question": {
                "title": "Insert values from variable and DataFrame into another DataFrame",
                "ques_desc": "On start I have two DataFrames and one variable: I have to map id variable and the corresponding col0 cell from df1 DataFrame to all rows in df2 DataFrame. I tryed and as the result I made the code below: It seems to me that the code should work correctly, but unfortunatelly I have a NaN value in the col0 column. The expected result was: I've spent over an hour and can't figure out why I'm getting this kind of result. If possible, could you, please: explain briefly why I am getting the error fix my mistake in the code "
            },
            "io": [
                "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   NaN    14    24\n2   1   NaN    15    25\n",
                "   id  col0  col1  col2\n0   1   3.0    13    23\n1   1   3.0    14    24\n2   1   3.0    15    25\n"
            ],
            "answer": {
                "ans_desc": "Your mistake is on this string when you use this, it returns a Series type. Yes it just have a value, but is still a Series with just one value. To solve this issue is very very very simple, you just have to call the first item at the Series object like this: Your code with the ajustment must look like this Then your new df2 is like this: ",
                "code": [
                    "import pandas as pd\n\nid=1\ndf1 = pd.DataFrame({'id': [1, 2], 'col0': [3, 4]})\ndf2 = pd.DataFrame({'col1': [13, 14, 15],'col2': [23, 24, 25]})\n\ndf2.insert(0, \"id\", id)\ndf2.insert(1, \"col0\", df1[df1['id']==id]['col0'][0])\n\nprint(df2)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "algorithm",
            "dataframe",
            "pairwise"
        ],
        "owner": {
            "reputation": 45,
            "user_id": 16238148,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a87a2e4b04f8d0cd6d19f6e68eab5a4e?s=128&d=identicon&r=PG&f=1",
            "display_name": "Eugene Zinder",
            "link": "https://stackoverflow.com/users/16238148/eugene-zinder"
        },
        "is_answered": true,
        "view_count": 40,
        "accepted_answer_id": 68213724,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625157900,
        "creation_date": 1625155883,
        "question_id": 68213612,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun",
        "title": "How to combine rows in a dataframe in a pairwise fashion while applying some function",
        "body": "<p>I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2:</p>\n<pre><code>ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n</code></pre>\n<p>I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is.</p>\n<p>Here is the resulting dataframe:</p>\n<pre><code>ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n</code></pre>\n<p>In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row.</p>\n<p>(id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows.</p>\n<p><strong>I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way?</strong></p>\n<p>Code:</p>\n<pre><code>df = pd.DataFrame(columns=['ID', 'Val1', 'Val2'], data=[['id0', 10, 20], ['id0', 11, 19], ['id1', 5, 5], ['id1', 1, 1], ['id1', 2, 4]])\n</code></pre>\n",
        "answer_body": "<p>You can use <code>df.rolling</code> after grouping by <code>ID</code>:</p>\n<pre><code>out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n</code></pre>\n<pre><code>&gt;&gt;&gt; out\n       Val1  Val2\nid0_1  10.5  19.5\nid1_1   3.0   3.0\nid1_2   1.5   2.5\n</code></pre>\n",
        "question_body": "<p>I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2:</p>\n<pre><code>ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n</code></pre>\n<p>I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is.</p>\n<p>Here is the resulting dataframe:</p>\n<pre><code>ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n</code></pre>\n<p>In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row.</p>\n<p>(id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows.</p>\n<p><strong>I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way?</strong></p>\n<p>Code:</p>\n<pre><code>df = pd.DataFrame(columns=['ID', 'Val1', 'Val2'], data=[['id0', 10, 20], ['id0', 11, 19], ['id1', 5, 5], ['id1', 1, 1], ['id1', 2, 4]])\n</code></pre>\n",
        "formatted_input": {
            "qid": 68213612,
            "link": "https://stackoverflow.com/questions/68213612/how-to-combine-rows-in-a-dataframe-in-a-pairwise-fashion-while-applying-some-fun",
            "question": {
                "title": "How to combine rows in a dataframe in a pairwise fashion while applying some function",
                "ques_desc": "I have a dataframe that stores keys as ID, and some numerical values in Val1/Val2: I would like to go over this dataframe and combine the rows pairwise while getting the averages of Val1/Val2 for rows with the same ID. A suffix should be appended to the new row's ID based on which number pair it is. Here is the resulting dataframe: In this example, there are only 3 rows left. (id0, 10, 20) gets averaged with (id0,11,19) and combined into one row. (id1,5,5) gets averaged with (id1,1,1,) and (id1,1,1) gets averaged with (id1,2,4) to form 2 remaining rows. I can think of an iterative approach to this, but that would be very slow. How could I do this in a proper pythonic/pandas way? Code: "
            },
            "io": [
                "ID    Val1    Val2\nid0     10      20\nid0     11      19\nid1      5       5\nid1      1       1\nid1      2       4\n",
                "ID      Val1    Val2\nid0_1   10.5    19.5\nid1_1   3       3\nid1_2   1.5     2.5\n"
            ],
            "answer": {
                "ans_desc": "You can use after grouping by : ",
                "code": [
                    "out = df.groupby('ID').rolling(2).mean() \\\n        .dropna(how='all').reset_index(level=1, drop=True)\n\nout.index += '_' + out.groupby(level=0).cumcount().add(1).astype(str)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 173,
            "user_id": 9254726,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/9070bdb0341821aed239bfa229c8f43a?s=128&d=identicon&r=PG&f=1",
            "display_name": "5E4ME",
            "link": "https://stackoverflow.com/users/9254726/5e4me"
        },
        "is_answered": true,
        "view_count": 15,
        "accepted_answer_id": 68211988,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1625149939,
        "creation_date": 1625149090,
        "question_id": 68211888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base",
        "title": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column",
        "body": "<p>I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time.</p>\n<p>The starting dataframes look like this:</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n</code></pre>\n<p>And I would like to get to a series of frames looking like this</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n</code></pre>\n<p>My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible.</p>\n<p>I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.</p>\n",
        "answer_body": "<p>Let's try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> then build out a <code>dict</code>:</p>\n<pre><code>dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n</code></pre>\n<p><code>dfs</code>:</p>\n<pre><code>{'player1':       NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7,\n 'player2':       NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7,\n 'player3':       NAME  VAL1  VAL2  VAL3\n2  player3     3     6     7\n1  player3     2     6     8,\n 'player5':       NAME  VAL1  VAL2  VAL3\n2  player5     3     6     7}\n</code></pre>\n<p>Each player's DataFrame can then be accessed like:</p>\n<p><code>dfs['player1']</code>:</p>\n<pre><code>      NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7\n</code></pre>\n<hr />\n<p>Or as a <code>list</code>:</p>\n<pre><code>dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n</code></pre>\n<p><code>dfs</code>:</p>\n<pre><code>[      NAME  VAL1  VAL2  VAL3\n0  player1     3     5     7,\n       NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7,\n       NAME  VAL1  VAL2  VAL3\n2  player3     3     6     7\n1  player3     2     6     8,\n       NAME  VAL1  VAL2  VAL3\n2  player5     3     6     7]\n</code></pre>\n<p>Each player's DataFrame can then be accessed like:</p>\n<p><code>dfs[1]</code>:</p>\n<pre><code>      NAME  VAL1  VAL2  VAL3\n1  player2     2     6     8\n0  player2     5     7     7\n</code></pre>\n",
        "question_body": "<p>I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time.</p>\n<p>The starting dataframes look like this:</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n</code></pre>\n<p>And I would like to get to a series of frames looking like this</p>\n<pre><code>NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n</code></pre>\n<p>My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible.</p>\n<p>I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this.</p>\n",
        "formatted_input": {
            "qid": 68211888,
            "link": "https://stackoverflow.com/questions/68211888/loop-through-multiple-small-pandas-dataframes-and-create-summary-dataframes-base",
            "question": {
                "title": "Loop through multiple small Pandas dataframes and create summary dataframes based on a single column",
                "ques_desc": "I have a bunch of small dataframes each representing a single match in a game. I would like to take these dataframes and consolidate them into a single dataframe for each player without knowing the player's names ahead of time. The starting dataframes look like this: And I would like to get to a series of frames looking like this My problem is that the solutions that I've found so far all require me to know the player names ahead of time and manually set up a dataframe for each player. Since I'll be working with 40-50 players and I won't know all their names until I have the raw data I'd like to avoid that if at all possible. I have a loose plan to create a dictionary of players with each player key containing a dict of their rows from the dataframes. Once all the match dataframes are processed I would convert the dict of dicts into individual player dataframes. I'm not sure if this is the best approach though and am hoping that there's a more efficient way to do this. "
            },
            "io": [
                "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\nplayer2  2     6     8\nplayer3  3     6     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  5     7     7\nplayer3  2     6     8\nplayer5  3     6     7\n",
                "NAME     VAL1  VAL2  VAL3\nplayer1  3     5     7\n\nNAME     VAL1  VAL2  VAL3\nplayer2  2     6     8\nplayer2  5     7     7\n\nNAME     VAL1  VAL2  VAL3\nplayer3  3     6     7\nplayer3  2     6     8\n\nNAME     VAL1  VAL2  VAL3\nplayer5  3     6     7\n"
            ],
            "answer": {
                "ans_desc": "Let's try + then build out a : : Each player's DataFrame can then be accessed like: : Or as a : : Each player's DataFrame can then be accessed like: : ",
                "code": [
                    "dfs = {group_name: df_\n       for group_name, df_ in pd.concat([df1, df2]).groupby('NAME')}\n",
                    "dfs = [df_ for _, df_ in pd.concat([df1, df2]).groupby('NAME')]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "data-munging"
        ],
        "owner": {
            "reputation": 724,
            "user_id": 6057371,
            "user_type": "registered",
            "accept_rate": 31,
            "profile_image": "https://www.gravatar.com/avatar/b744e3dbff9c7fdb801570de75f6eff7?s=128&d=identicon&r=PG&f=1",
            "display_name": "okuoub",
            "link": "https://stackoverflow.com/users/6057371/okuoub"
        },
        "is_answered": true,
        "view_count": 26,
        "accepted_answer_id": 68193597,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1625051576,
        "creation_date": 1625051004,
        "question_id": 68193558,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values",
        "title": "pandas group many columns to one column where every cell is a list of values",
        "body": "<p>I have the dataframe</p>\n<pre><code>df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n</code></pre>\n<p>And I want to group all columns to a single list that will be the only columns, so I will get:</p>\n<pre><code>df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n</code></pre>\n<p>(Shape of df was change from (3,5) to (3,1))</p>\n<p>What is the best way to do this?</p>\n",
        "answer_body": "<p>Try:</p>\n<pre><code>#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n</code></pre>\n",
        "question_body": "<p>I have the dataframe</p>\n<pre><code>df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n</code></pre>\n<p>And I want to group all columns to a single list that will be the only columns, so I will get:</p>\n<pre><code>df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n</code></pre>\n<p>(Shape of df was change from (3,5) to (3,1))</p>\n<p>What is the best way to do this?</p>\n",
        "formatted_input": {
            "qid": 68193558,
            "link": "https://stackoverflow.com/questions/68193558/pandas-group-many-columns-to-one-column-where-every-cell-is-a-list-of-values",
            "question": {
                "title": "pandas group many columns to one column where every cell is a list of values",
                "ques_desc": "I have the dataframe And I want to group all columns to a single list that will be the only columns, so I will get: (Shape of df was change from (3,5) to (3,1)) What is the best way to do this? "
            },
            "io": [
                "df = \nc1 c2 c3 c4 c5\n1.  2. 3. 1. 5\n8.  2. 1. 3. 8\n4.  9. 1  2. 3\n",
                "df = \n    l\n[1,2,3,1,5]\n[8,2,1,3,8]\n[4,9,1,2,3]\n"
            ],
            "answer": {
                "ans_desc": "Try: ",
                "code": [
                    "#best way:\ndf['l']=df.values.tolist()\n#OR\ndf['l']=df.to_numpy().tolist()\n\n\n#another way:\ndf['l']=df.agg(list,1)\n#OR\ndf['l']=df.apply(list,1)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 15759786,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/002b02baee3603027b66ed2361da99e2?s=128&d=identicon&r=PG&f=1",
            "display_name": "big sad",
            "link": "https://stackoverflow.com/users/15759786/big-sad"
        },
        "is_answered": true,
        "view_count": 27,
        "accepted_answer_id": 68193581,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1625051129,
        "creation_date": 1625050872,
        "question_id": 68193521,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame",
        "title": "Concatenate values and column names in a data frame to create a new data frame",
        "body": "<p>I have the following data frame(<code>df1</code>):</p>\n<pre><code>  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n</code></pre>\n<p>I need to derive the data frame(<code>df2</code>) from <code>df1</code> such that column 1 of <code>d2</code> will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of <code>d2</code> will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate.   :</p>\n<pre><code>      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n</code></pre>\n<p>I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process?</p>\n<p>Below is the code I have used</p>\n<pre><code>d = {'Value': ['a','b','c','d','e'],'col1': ['aa','ba','ca','da','ea'], 'col2' : ['ab','bb','cb','db','eb'],'col3': ['ac','bc','cc','dc','ec']}\ndf1 = pd.DataFrame(data = d)\n\n# Repeat every value is Value column 3 times.\nX = df1['Value'].repeat(4).reset_index(drop=True)\n\n# Create separate series with Col 1, Col 2, Col 3 names.\nY = pd.Series(df1.columns[1:])\n\n# Repeated series Y to the length of data df1\nYY = pd.Series(np.tile(Y.values, len(df1)))\n\n# Create the first column by concatenating X and YY\nfirst_column_1 = X + &quot;_&quot; + YY\n\nZ = df1.set_index('Value')\nZZ = np.ravel(Z.values)\n\n#Create 2nd column from ZZ\nsecond_column = pd.Series(ZZ)\n\n#Create df2\ndf2 = pd.DataFrame([first_column, second_column]).T\n</code></pre>\n",
        "answer_body": "<p>Try:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = df.melt(&quot;Value&quot;, value_name=&quot;Col 1&quot;)\nx.Value += &quot;_&quot; + x.variable\nx = x.drop(columns=&quot;variable&quot;)\nprint(x)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>     Value Col 1\n0   a_col1    aa\n1   b_col1    ba\n2   c_col1    ca\n3   d_col1    da\n4   e_col1    ea\n5   a_col2    ab\n6   b_col2    bb\n7   c_col2    cb\n8   d_col2    db\n9   e_col2    eb\n10  a_col3    ac\n11  b_col3    bc\n12  c_col3    cc\n13  d_col3    dc\n14  e_col3    ec\n</code></pre>\n<hr />\n<p>Optionally, you can sort values afterwards:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = x.sort_values(by=&quot;Value&quot;).reset_index(drop=True)\nprint(x)\n\n     Value Col 1\n0   a_col1    aa\n1   a_col2    ab\n2   a_col3    ac\n3   b_col1    ba\n4   b_col2    bb\n5   b_col3    bc\n6   c_col1    ca\n7   c_col2    cb\n8   c_col3    cc\n9   d_col1    da\n10  d_col2    db\n11  d_col3    dc\n12  e_col1    ea\n13  e_col2    eb\n14  e_col3    ec\n</code></pre>\n",
        "question_body": "<p>I have the following data frame(<code>df1</code>):</p>\n<pre><code>  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n</code></pre>\n<p>I need to derive the data frame(<code>df2</code>) from <code>df1</code> such that column 1 of <code>d2</code> will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of <code>d2</code> will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate.   :</p>\n<pre><code>      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n</code></pre>\n<p>I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process?</p>\n<p>Below is the code I have used</p>\n<pre><code>d = {'Value': ['a','b','c','d','e'],'col1': ['aa','ba','ca','da','ea'], 'col2' : ['ab','bb','cb','db','eb'],'col3': ['ac','bc','cc','dc','ec']}\ndf1 = pd.DataFrame(data = d)\n\n# Repeat every value is Value column 3 times.\nX = df1['Value'].repeat(4).reset_index(drop=True)\n\n# Create separate series with Col 1, Col 2, Col 3 names.\nY = pd.Series(df1.columns[1:])\n\n# Repeated series Y to the length of data df1\nYY = pd.Series(np.tile(Y.values, len(df1)))\n\n# Create the first column by concatenating X and YY\nfirst_column_1 = X + &quot;_&quot; + YY\n\nZ = df1.set_index('Value')\nZZ = np.ravel(Z.values)\n\n#Create 2nd column from ZZ\nsecond_column = pd.Series(ZZ)\n\n#Create df2\ndf2 = pd.DataFrame([first_column, second_column]).T\n</code></pre>\n",
        "formatted_input": {
            "qid": 68193521,
            "link": "https://stackoverflow.com/questions/68193521/concatenate-values-and-column-names-in-a-data-frame-to-create-a-new-data-frame",
            "question": {
                "title": "Concatenate values and column names in a data frame to create a new data frame",
                "ques_desc": "I have the following data frame(): I need to derive the data frame() from such that column 1 of will have concatenated raw values of Value column with column names of Col 1 to Col 3. Column 2 of will have the raw value corresponding to each concatenated column name, Below is the sample which require to generate. : I have followed the below steps to derive df2 from df1. But this process seems a bit long. Any recommendations on shortening the process? Below is the code I have used "
            },
            "io": [
                "  Value col1 col2 col3\n0     a   aa   ab   ac\n1     b   ba   bb   bc\n2     c   ca   cb   cc\n3     d   da   db   dc\n4     e   ea   eb   ec\n",
                "      Value Col 1\n0   a_Col 1    aa\n1   a_Col 2    ab\n2   a_Col 3    ac\n3   b_Col 1    ba\n4   b_Col 2    bb\n5   b_Col 3    bc\n6   c_Col 1    ca\n7   c_Col 2    cb\n8   c_Col 3    cc\n9   d_Col 1    da\n10  d_Col 2    db\n11  d_Col 3    dc\n12  e_Col 1    ea\n13  e_Col 2    eb\n14  e_Col 3    ec\n"
            ],
            "answer": {
                "ans_desc": "Try: Prints: Optionally, you can sort values afterwards: ",
                "code": [
                    "x = df.melt(\"Value\", value_name=\"Col 1\")\nx.Value += \"_\" + x.variable\nx = x.drop(columns=\"variable\")\nprint(x)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe",
            "data-structures"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 9079043,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/974eaeafce0b16f35dce0911a26ade93?s=128&d=identicon&r=PG&f=1",
            "display_name": "Isha",
            "link": "https://stackoverflow.com/users/9079043/isha"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 68174753,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624955956,
        "creation_date": 1624953161,
        "last_edit_date": 1624953363,
        "question_id": 68174614,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
        "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
        "body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "answer_body": "<p>The problem here is, when you call apply on <code>axis=1</code>, pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; s = pd.Series([1, 2, 3.2])\ns\n0    1.0\n1    2.0\n2    3.2\ndtype: float64\n</code></pre>\n<p>As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to :</p>\n<pre><code>df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n</code></pre>\n<p>There's already an issue <a href=\"https://github.com/pandas-dev/pandas/issues/23230\" rel=\"nofollow noreferrer\">DataFrame.apply unintuitively changes int to float because of another column</a> on github for this upcasting behavior of pandas <code>apply</code>.\nSo, one possible option for you is as I have mentioned in the comment, to call <code>to_json</code> on the entire dataframe as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt;df.to_json(orient='index')\n'{&quot;0&quot;:{&quot;A&quot;:2,&quot;B&quot;:6,&quot;C&quot;:5,&quot;D&quot;:8.0},&quot;1&quot;:{&quot;A&quot;:6,&quot;B&quot;:11,&quot;C&quot;:2,&quot;D&quot;:3.6},&quot;2&quot;:{&quot;A&quot;:1,&quot;B&quot;:5,&quot;C&quot;:7,&quot;D&quot;:5.2}}'\n</code></pre>\n<p>A working solution for you may be using python's <code>json</code> module alongwith <code>DataFrame.to_json()</code>, but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['total'] = list(map(json.dumps, [*json.loads(df.to_json(orient='index')).values()]))\n</code></pre>\n<p><strong>OUTPUT:</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>   A   B  C    D                                total\n0  2   6  5  8.0   {&quot;A&quot;: 2, &quot;B&quot;: 6, &quot;C&quot;: 5, &quot;D&quot;: 8.0}\n1  6  11  2  3.6  {&quot;A&quot;: 6, &quot;B&quot;: 11, &quot;C&quot;: 2, &quot;D&quot;: 3.6}\n2  1   5  7  5.2   {&quot;A&quot;: 1, &quot;B&quot;: 5, &quot;C&quot;: 7, &quot;D&quot;: 5.2}\n</code></pre>\n",
        "question_body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "formatted_input": {
            "qid": 68174614,
            "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
            "question": {
                "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
                "ques_desc": "I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? "
            },
            "io": [
                "A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n",
                "{\"A\":2.0, \"B\":6.0, \"C\":5.0, \"D\":8.0}\n{\"A\":6.0, \"B\":11.0, \"C\":2.0, \"D\":3.6}\n{\"A\":1.0, \"B\":5.0, \"C\":7.0, \"D\":5.2}\n"
            ],
            "answer": {
                "ans_desc": "The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: ",
                "code": [
                    "df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 4149213,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/vBcjv.jpg?s=128&g=1",
            "display_name": "openwater",
            "link": "https://stackoverflow.com/users/4149213/openwater"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 68174309,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624951870,
        "creation_date": 1624950821,
        "last_edit_date": 1624951870,
        "question_id": 68174113,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
        "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
        "body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "answer_body": "<p>A simpler solution would be to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.GroupBy.apply.html\" rel=\"nofollow noreferrer\"><code>apply</code></a> a custom function on each group. In this case, we can define a function <code>reclass</code> that obtains the correct bins and ids and then uses <code>pd.cut</code>:</p>\n<pre><code>def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n</code></pre>\n<p>Result:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "formatted_input": {
            "qid": 68174113,
            "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
            "question": {
                "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
                "ques_desc": "I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? "
            },
            "io": [
                "bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n",
                "ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n"
            ],
            "answer": {
                "ans_desc": "A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: ",
                "code": [
                    "def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 121,
            "user_id": 2643948,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/3423e43b576ab333117f731daef5aad9?s=128&d=identicon&r=PG&f=1",
            "display_name": "RebeccaKennedy",
            "link": "https://stackoverflow.com/users/2643948/rebeccakennedy"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 68150849,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624808663,
        "creation_date": 1624788441,
        "question_id": 68150020,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
        "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
        "body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "answer_body": "<h3>Numpy approach</h3>\n<p>We can define a function <code>first_value</code> which takes a <code>2D</code> array and <code>offset</code> (n) as input arguments and returns <code>1D</code> array. Basically, for each row it returns the <code>nth</code> value after the first <code>non-nan</code> value</p>\n<pre><code>def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i &gt;= arr.shape[1])] = np.nan\n    return vals\n</code></pre>\n<h3>Pandas approach</h3>\n<p>We can <code>stack</code> the dataframe to reshape then group the dataframe on <code>level=0</code> and aggregate using <code>nth</code>, then <code>reindex</code> to conform the index of aggregated frame according to original frame</p>\n<pre><code>def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n</code></pre>\n<h3>Sample run</h3>\n<pre><code>&gt;&gt;&gt; first_valid(df, 0)\nDate\n1     25.0\n2     29.0\n3     33.0\n4     31.0\n5     30.0\n6     35.0\n7     31.0\n8     33.0\n9     26.0\n10    27.0\n11    35.0\n12    33.0\n13    28.0\n14    25.0\n15    25.0\n16    26.0\n17    34.0\n18    28.0\n19    34.0\n20    28.0\ndtype: float64\n\n\n&gt;&gt;&gt; first_valid(df, 1)\nDate\n1      NaN\n2      NaN\n3      NaN\n4     35.0\n5     34.0\n6     34.0\n7     26.0\n8     25.0\n9     31.0\n10    26.0\n11    25.0\n12    35.0\n13    25.0\n14    25.0\n15    26.0\n16    31.0\n17    29.0\n18    29.0\n19    26.0\n20    30.0\ndtype: float64\n\n&gt;&gt;&gt; first_valid(df, 2)\nDate\n1      NaN\n2      NaN\n3      NaN\n4      NaN\n5      NaN\n6      NaN\n7      NaN\n8     31.0\n9      NaN\n10    28.0\n11    29.0\n12    28.0\n13    35.0\n14    28.0\n15    31.0\n16    27.0\n17    25.0\n18    31.0\n19     NaN\n20     NaN\ndtype: float64\n</code></pre>\n<h3>Performance</h3>\n<pre><code># Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p><strong>Numpy based approach is approximately <code>300x</code> faster than the <code>OP's</code> given approach</strong> while pandas based approach is approximately <code>22x</code> faster</p>\n",
        "question_body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "formatted_input": {
            "qid": 68150020,
            "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
            "question": {
                "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
                "ques_desc": "I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. "
            },
            "io": [
                "f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n",
                "f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n"
            ],
            "answer": {
                "ans_desc": "Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster ",
                "code": [
                    "def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan\n    return vals\n",
                    "def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n",
                    "# Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 51,
            "user_id": 10082987,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/141965c0e67ba4408bdb2267fd7c04f2?s=128&d=identicon&r=PG&f=1",
            "display_name": "syedmfk",
            "link": "https://stackoverflow.com/users/10082987/syedmfk"
        },
        "is_answered": true,
        "view_count": 9367,
        "accepted_answer_id": 57033774,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1624643356,
        "creation_date": 1563168421,
        "last_edit_date": 1563169986,
        "question_id": 57033657,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
        "title": "How to Extract Month Name and Year from Date column of DataFrame",
        "body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "answer_body": "<p>Cast you date from object to actual datetime and use dt to access what you need.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n</code></pre>\n",
        "question_body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "formatted_input": {
            "qid": 57033657,
            "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
            "question": {
                "title": "How to Extract Month Name and Year from Date column of DataFrame",
                "ques_desc": "I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. "
            },
            "io": [
                "45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n",
                "45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n"
            ],
            "answer": {
                "ans_desc": "Cast you date from object to actual datetime and use dt to access what you need. ",
                "code": [
                    "import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 183,
            "user_id": 12939325,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-ylhVvK83HnQ/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdDak9Ir2cS6IzUxpxSksMsp-B7tA/photo.jpg?sz=128",
            "display_name": "Steak",
            "link": "https://stackoverflow.com/users/12939325/steak"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68107510,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1624488621,
        "creation_date": 1624484332,
        "last_edit_date": 1624488621,
        "question_id": 68107298,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
        "title": "How to Create a Correlation Dataframe from already related data",
        "body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "answer_body": "<p>I assume that the pairs are unique, i.e. if there's <code>English - Spanish</code>, there won't be a <code>Spanish - English</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code># Rename the columns. Because working with columns\n# named like numbers is a pain in the butt\ndf.columns = ['lang1', 'lang2', 'corr']\n\n# Create the opposite pairs\ntmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)\n\n# Create the identity pairs\nall_lang = np.unique(df[['lang1', 'lang2']])\ntmp2 = pd.DataFrame({\n    'lang1': all_lang,\n    'lang2': all_lang,\n    'corr': 1.0\n})\n\n# Concatenate all 3 together and beat the data frame into shape\nresult = (\n    pd.concat([df, tmp1, tmp2], axis=0)\n        .set_index(['lang1', 'lang2'])\n        .unstack()\n        .droplevel(0, axis=1)\n        .rename_axis(index=[None], columns=[None])\n)\n</code></pre>\n",
        "question_body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "formatted_input": {
            "qid": 68107298,
            "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
            "question": {
                "title": "How to Create a Correlation Dataframe from already related data",
                "ques_desc": "I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? "
            },
            "io": [
                "    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n",
                "        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n"
            ],
            "answer": {
                "ans_desc": "I assume that the pairs are unique, i.e. if there's , there won't be a . ",
                "code": [
                    "# Rename the columns. Because working with columns\n# named like numbers is a pain in the butt\ndf.columns = ['lang1', 'lang2', 'corr']\n\n# Create the opposite pairs\ntmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)\n\n# Create the identity pairs\nall_lang = np.unique(df[['lang1', 'lang2']])\ntmp2 = pd.DataFrame({\n    'lang1': all_lang,\n    'lang2': all_lang,\n    'corr': 1.0\n})\n\n# Concatenate all 3 together and beat the data frame into shape\nresult = (\n    pd.concat([df, tmp1, tmp2], axis=0)\n        .set_index(['lang1', 'lang2'])\n        .unstack()\n        .droplevel(0, axis=1)\n        .rename_axis(index=[None], columns=[None])\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16124075,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxaHbBnxKKjuXo_wOrkhGQHbT5P4fY54mYRG-h6=k-s128",
            "display_name": "hang",
            "link": "https://stackoverflow.com/users/16124075/hang"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 68098622,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624446318,
        "creation_date": 1624444563,
        "question_id": 68098150,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
        "title": "set some rule to groupby in pandas",
        "body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "answer_body": "<p>IIUC:</p>\n<p>try:</p>\n<pre><code>c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') &amp; c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n</code></pre>\n",
        "question_body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "formatted_input": {
            "qid": 68098150,
            "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
            "question": {
                "title": "set some rule to groupby in pandas",
                "ques_desc": "I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have \"dup by\" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. "
            },
            "io": [
                "1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n",
                "1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n"
            ],
            "answer": {
                "ans_desc": "IIUC: try: ",
                "code": [
                    "c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 13920381,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgbR0vHurgfWnrlCJifphJzuFAkbDbJV3Ls033L6g=k-s128",
            "display_name": "dinn_",
            "link": "https://stackoverflow.com/users/13920381/dinn"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68090682,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624397667,
        "creation_date": 1624394378,
        "question_id": 68090463,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
        "title": "Merging more than two columns of the same dataframe in pandas",
        "body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "answer_body": "<p>Input data:</p>\n<pre><code>&gt;&gt;&gt; df\n    VAR 1  VAR 2  VAR 3    GROUP  ANOTHER\n0     NaN    NaN    3.0   [0-10]  another\n1     1.0    NaN    3.0   [0-10]  another\n2     1.0    NaN    3.0   [0-10]  another\n3     1.0    2.0    NaN   [0-10]  another\n4     NaN    2.0    NaN   [0-10]  another\n5     3.0    NaN    3.0  [10-20]  another\n6     3.0    1.0    NaN  [10-20]  another\n7     NaN    1.0    NaN  [10-20]  another\n8     NaN    2.0    NaN  [10-20]  another\n9     NaN    NaN    2.0  [10-20]  another\n10    NaN    NaN    2.0  [10-20]  another\n</code></pre>\n<p>You can use <code>melt</code>. To fully understand, you can execute the code line by line (<code>df.melt(...)</code>, <code>df.melt(...).dropna()</code>, <code>df.melt(...).dropna.sort_values(...)</code> and so on):</p>\n<pre><code>id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n</code></pre>\n<p>Result output:</p>\n<pre><code>&gt;&gt;&gt; out\n    VAR_MERGED    GROUP  ANOTHER\n0          1.0   [0-10]  another\n1          1.0   [0-10]  another\n2          1.0   [0-10]  another\n3          2.0   [0-10]  another\n4          2.0   [0-10]  another\n5          3.0   [0-10]  another\n6          3.0   [0-10]  another\n7          3.0   [0-10]  another\n8          1.0  [10-20]  another\n9          1.0  [10-20]  another\n10         2.0  [10-20]  another\n11         2.0  [10-20]  another\n12         2.0  [10-20]  another\n13         3.0  [10-20]  another\n14         3.0  [10-20]  another\n15         3.0  [10-20]  another\n</code></pre>\n",
        "question_body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "formatted_input": {
            "qid": 68090463,
            "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
            "question": {
                "title": "Merging more than two columns of the same dataframe in pandas",
                "ques_desc": "Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? "
            },
            "io": [
                "VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n",
                "VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n"
            ],
            "answer": {
                "ans_desc": "Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: ",
                "code": [
                    "id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "vectorization"
        ],
        "owner": {
            "reputation": 1433,
            "user_id": 4263878,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/c95a66d30180a95353e7e655d0020f51?s=128&d=identicon&r=PG&f=1",
            "display_name": "MJS",
            "link": "https://stackoverflow.com/users/4263878/mjs"
        },
        "is_answered": true,
        "view_count": 3872,
        "accepted_answer_id": 33130915,
        "answer_count": 2,
        "score": 7,
        "last_activity_date": 1623919516,
        "creation_date": 1444839526,
        "last_edit_date": 1444840857,
        "question_id": 33130586,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
        "title": "python pandas - creating a column which keeps a running count of consecutive values",
        "body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "answer_body": "<p>You can use the compare-cumsum-groupby pattern (which I <em>really</em> need to getting around to writing up for the documentation), with a final <code>cumcount</code>:</p>\n\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"binary\": [0,1,1,1,0,0,1,1,0]})\n&gt;&gt;&gt; df[\"consec\"] = df[\"binary\"].groupby((df[\"binary\"] == 0).cumsum()).cumcount()\n&gt;&gt;&gt; df\n   binary  consec\n0       0       0\n1       1       1\n2       1       2\n3       1       3\n4       0       0\n5       0       0\n6       1       1\n7       1       2\n8       0       0\n</code></pre>\n\n<hr>\n\n<p>This works because first we get the positions where we want to reset the counter:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n</code></pre>\n\n<p>The cumulative sum of these gives us a different id for each group:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0).cumsum()\n0    1\n1    1\n2    1\n3    1\n4    2\n5    3\n6    3\n7    3\n8    4\nName: binary, dtype: int64\n</code></pre>\n\n<p>And then we can pass this to <code>groupby</code> and use <code>cumcount</code> to get an increasing index in each group.</p>\n",
        "question_body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "formatted_input": {
            "qid": 33130586,
            "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
            "question": {
                "title": "python pandas - creating a column which keeps a running count of consecutive values",
                "ques_desc": "I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. "
            },
            "io": [
                ".    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n",
                ".  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n"
            ],
            "answer": {
                "ans_desc": "You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. ",
                "code": [
                    ">>> (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "pyspark-dataframes"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9735423,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "learningtocode",
            "link": "https://stackoverflow.com/users/9735423/learningtocode"
        },
        "is_answered": true,
        "view_count": 575,
        "accepted_answer_id": 62099641,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1623911798,
        "creation_date": 1590822619,
        "last_edit_date": 1590926848,
        "question_id": 62099066,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
        "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
        "body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "answer_body": "<p>Here you go:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n</code></pre>\n\n<p>Check <code>output.xlsx</code> for the output.</p>\n",
        "question_body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "formatted_input": {
            "qid": 62099066,
            "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
            "question": {
                "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
                "ques_desc": "I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: "
            },
            "io": [
                "(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n",
                "(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n"
            ],
            "answer": {
                "ans_desc": "Here you go: Check for the output. ",
                "code": [
                    "import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe",
            "data-structures"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 9079043,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/974eaeafce0b16f35dce0911a26ade93?s=128&d=identicon&r=PG&f=1",
            "display_name": "Isha",
            "link": "https://stackoverflow.com/users/9079043/isha"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 68174753,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1624955956,
        "creation_date": 1624953161,
        "last_edit_date": 1624953363,
        "question_id": 68174614,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
        "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
        "body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "answer_body": "<p>The problem here is, when you call apply on <code>axis=1</code>, pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; s = pd.Series([1, 2, 3.2])\ns\n0    1.0\n1    2.0\n2    3.2\ndtype: float64\n</code></pre>\n<p>As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to :</p>\n<pre><code>df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n</code></pre>\n<p>There's already an issue <a href=\"https://github.com/pandas-dev/pandas/issues/23230\" rel=\"nofollow noreferrer\">DataFrame.apply unintuitively changes int to float because of another column</a> on github for this upcasting behavior of pandas <code>apply</code>.\nSo, one possible option for you is as I have mentioned in the comment, to call <code>to_json</code> on the entire dataframe as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt;df.to_json(orient='index')\n'{&quot;0&quot;:{&quot;A&quot;:2,&quot;B&quot;:6,&quot;C&quot;:5,&quot;D&quot;:8.0},&quot;1&quot;:{&quot;A&quot;:6,&quot;B&quot;:11,&quot;C&quot;:2,&quot;D&quot;:3.6},&quot;2&quot;:{&quot;A&quot;:1,&quot;B&quot;:5,&quot;C&quot;:7,&quot;D&quot;:5.2}}'\n</code></pre>\n<p>A working solution for you may be using python's <code>json</code> module alongwith <code>DataFrame.to_json()</code>, but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['total'] = list(map(json.dumps, [*json.loads(df.to_json(orient='index')).values()]))\n</code></pre>\n<p><strong>OUTPUT:</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>   A   B  C    D                                total\n0  2   6  5  8.0   {&quot;A&quot;: 2, &quot;B&quot;: 6, &quot;C&quot;: 5, &quot;D&quot;: 8.0}\n1  6  11  2  3.6  {&quot;A&quot;: 6, &quot;B&quot;: 11, &quot;C&quot;: 2, &quot;D&quot;: 3.6}\n2  1   5  7  5.2   {&quot;A&quot;: 1, &quot;B&quot;: 5, &quot;C&quot;: 7, &quot;D&quot;: 5.2}\n</code></pre>\n",
        "question_body": "<p>I have the following DataFrame:\ndf :</p>\n<pre><code>A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n</code></pre>\n<p>to convert to JSON , I write following snippet:</p>\n<pre><code>df['total'] = df.apply(lambda i: i.to_json(), axis=1)\n</code></pre>\n<p>I get following output:</p>\n<p>total</p>\n<pre><code>{&quot;A&quot;:2.0, &quot;B&quot;:6.0, &quot;C&quot;:5.0, &quot;D&quot;:8.0}\n{&quot;A&quot;:6.0, &quot;B&quot;:11.0, &quot;C&quot;:2.0, &quot;D&quot;:3.6}\n{&quot;A&quot;:1.0, &quot;B&quot;:5.0, &quot;C&quot;:7.0, &quot;D&quot;:5.2}\n</code></pre>\n<p>Why is that extra .0 is added to the result ?\nHow do I remove that extra .0 ?</p>\n",
        "formatted_input": {
            "qid": 68174614,
            "link": "https://stackoverflow.com/questions/68174614/why-does-it-add-0-to-the-value-while-converting-dataframe-columns-to-json",
            "question": {
                "title": "Why does it add .0 to the value while converting Dataframe columns to JSON",
                "ques_desc": "I have the following DataFrame: df : to convert to JSON , I write following snippet: I get following output: total Why is that extra .0 is added to the result ? How do I remove that extra .0 ? "
            },
            "io": [
                "A   B   C   D\n2   6   5   8.0\n6   11  2   3.6 \n1   5   7   5.2\n",
                "{\"A\":2.0, \"B\":6.0, \"C\":5.0, \"D\":8.0}\n{\"A\":6.0, \"B\":11.0, \"C\":2.0, \"D\":3.6}\n{\"A\":1.0, \"B\":5.0, \"C\":7.0, \"D\":5.2}\n"
            ],
            "answer": {
                "ans_desc": "The problem here is, when you call apply on , pandas creates a Series out of it and upcasts the values because it is a single Series. For example consider following Series: As you can see, the entire series is converted to float because integer type can not hold all the values for the above series, similar is the case when you call apply on axis=1, it is same to : There's already an issue DataFrame.apply unintuitively changes int to float because of another column on github for this upcasting behavior of pandas . So, one possible option for you is as I have mentioned in the comment, to call on the entire dataframe as: A working solution for you may be using python's module alongwith , but remember, it does the same thing twice so it may be a bit slow for a large dataframes, however, you will get the data in the rquired format: OUTPUT: ",
                "code": [
                    "df.iloc[0]\nA    2.0\nB    6.0\nC    5.0\nD    8.0\nName: 0, dtype: float64\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 23,
            "user_id": 4149213,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/vBcjv.jpg?s=128&g=1",
            "display_name": "openwater",
            "link": "https://stackoverflow.com/users/4149213/openwater"
        },
        "is_answered": true,
        "view_count": 36,
        "accepted_answer_id": 68174309,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624951870,
        "creation_date": 1624950821,
        "last_edit_date": 1624951870,
        "question_id": 68174113,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
        "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
        "body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "answer_body": "<p>A simpler solution would be to use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\" rel=\"nofollow noreferrer\"><code>groupby</code></a> and <a href=\"https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.core.groupby.GroupBy.apply.html\" rel=\"nofollow noreferrer\"><code>apply</code></a> a custom function on each group. In this case, we can define a function <code>reclass</code> that obtains the correct bins and ids and then uses <code>pd.cut</code>:</p>\n<pre><code>def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n</code></pre>\n<p>Result:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n</code></pre>\n",
        "question_body": "<p>I have a pandas dataframe as follows:</p>\n<pre><code>df = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\n\n    polyid  value\n0        1   0.56\n1        1   0.59\n2        1   0.62\n3        1   0.83\n4        2   0.85\n5        2   0.01\n6        2   0.79\n7        3   0.37\n8        3   0.99\n9        3   0.48\n10       3   0.55\n11       3   0.06\n</code></pre>\n<p>I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately:</p>\n<pre><code>bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n</code></pre>\n<p>And one with the ids with which I want to label the resulting bins:</p>\n<pre><code>ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n</code></pre>\n<p>I tried to get <a href=\"https://stackoverflow.com/a/49382340/4149213\">this</a> answer to work for my use case. I could only come up with applying <code>pd.cut</code> on each 'polyid' subset and then <code>pd.concat</code> all subsets again back to one dataframe:</p>\n<pre><code>import pandas as pd\n\ndef reclass_df_dic(df, bins_dic, names_dic, bin_key_col, val_col, name_col):\n    df_lst = []\n    for key in df[bin_key_col].unique():\n        bins = bins_dic[key]\n        names = names_dic[key]\n        sub_df = df[df[bin_key_col] == key]\n        sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n        df_lst.append(sub_df)\n    return(pd.concat(df_lst))\n\ndf = pd.DataFrame(data = [[1,0.56],[1,0.59],[1,0.62],[1,0.83],[2,0.85],[2,0.01],[2,0.79],[3,0.37],[3,0.99],[3,0.48],[3,0.55],[3,0.06]],columns=['polyid','value'])\nbins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\nids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n\ndf = reclass_df_dic(df, bins_dic, ids_dic, 'polyid', 'value', 'id')\n\n</code></pre>\n<p>This results in my desired output:</p>\n<pre><code>    polyid  value  id\n0        1   0.56   1\n1        1   0.59   1\n2        1   0.62   2\n3        1   0.83   3\n4        2   0.85   2\n5        2   0.01   1\n6        2   0.79   2\n7        3   0.37   1\n8        3   0.99   3\n9        3   0.48   1\n10       3   0.55   2\n11       3   0.06   1\n\n</code></pre>\n<p>However, the line:</p>\n<pre><code>sub_df[name_col] = pd.cut(df[val_col], bins, labels=names)\n</code></pre>\n<p>raises the warning:</p>\n<blockquote>\n<p>A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead</p>\n</blockquote>\n<p>that I am unable to solve with using <code>.loc</code>. Also, I guess there generally is a more efficient way of doing this without having to loop over each category?</p>\n",
        "formatted_input": {
            "qid": 68174113,
            "link": "https://stackoverflow.com/questions/68174113/map-numeric-data-into-bins-in-pandas-dataframe-for-seperate-groups-using-diction",
            "question": {
                "title": "Map numeric data into bins in Pandas dataframe for seperate groups using dictionaries",
                "ques_desc": "I have a pandas dataframe as follows: I need to reclassify the 'value' column separately for each 'polyid'. For the reclassification, I have two dictionaries. One with the bins that contain the information on how I want to cut the 'values' for each 'polyid' separately: And one with the ids with which I want to label the resulting bins: I tried to get this answer to work for my use case. I could only come up with applying on each 'polyid' subset and then all subsets again back to one dataframe: This results in my desired output: However, the line: raises the warning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead that I am unable to solve with using . Also, I guess there generally is a more efficient way of doing this without having to loop over each category? "
            },
            "io": [
                "bins_dic = {1:[0,0.6,0.8,1], 2:[0,0.2,0.9,1], 3:[0,0.5,0.6,1]}\n",
                "ids_dic = {1:[1,2,3], 2:[1,2,3], 3:[1,2,3]}\n"
            ],
            "answer": {
                "ans_desc": "A simpler solution would be to use and a custom function on each group. In this case, we can define a function that obtains the correct bins and ids and then uses : Result: ",
                "code": [
                    "def reclass(group, name):\n    bins = bins_dic[name]\n    ids = ids_dic[name]\n    return pd.cut(group, bins, labels=ids)\n    \ndf['id'] = df.groupby('polyid')['value'].apply(lambda x: reclass(x, x.name))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 121,
            "user_id": 2643948,
            "user_type": "registered",
            "accept_rate": 50,
            "profile_image": "https://www.gravatar.com/avatar/3423e43b576ab333117f731daef5aad9?s=128&d=identicon&r=PG&f=1",
            "display_name": "RebeccaKennedy",
            "link": "https://stackoverflow.com/users/2643948/rebeccakennedy"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 68150849,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624808663,
        "creation_date": 1624788441,
        "question_id": 68150020,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
        "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
        "body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "answer_body": "<h3>Numpy approach</h3>\n<p>We can define a function <code>first_value</code> which takes a <code>2D</code> array and <code>offset</code> (n) as input arguments and returns <code>1D</code> array. Basically, for each row it returns the <code>nth</code> value after the first <code>non-nan</code> value</p>\n<pre><code>def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i &gt;= arr.shape[1])] = np.nan\n    return vals\n</code></pre>\n<h3>Pandas approach</h3>\n<p>We can <code>stack</code> the dataframe to reshape then group the dataframe on <code>level=0</code> and aggregate using <code>nth</code>, then <code>reindex</code> to conform the index of aggregated frame according to original frame</p>\n<pre><code>def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n</code></pre>\n<h3>Sample run</h3>\n<pre><code>&gt;&gt;&gt; first_valid(df, 0)\nDate\n1     25.0\n2     29.0\n3     33.0\n4     31.0\n5     30.0\n6     35.0\n7     31.0\n8     33.0\n9     26.0\n10    27.0\n11    35.0\n12    33.0\n13    28.0\n14    25.0\n15    25.0\n16    26.0\n17    34.0\n18    28.0\n19    34.0\n20    28.0\ndtype: float64\n\n\n&gt;&gt;&gt; first_valid(df, 1)\nDate\n1      NaN\n2      NaN\n3      NaN\n4     35.0\n5     34.0\n6     34.0\n7     26.0\n8     25.0\n9     31.0\n10    26.0\n11    25.0\n12    35.0\n13    25.0\n14    25.0\n15    26.0\n16    31.0\n17    29.0\n18    29.0\n19    26.0\n20    30.0\ndtype: float64\n\n&gt;&gt;&gt; first_valid(df, 2)\nDate\n1      NaN\n2      NaN\n3      NaN\n4      NaN\n5      NaN\n6      NaN\n7      NaN\n8     31.0\n9      NaN\n10    28.0\n11    29.0\n12    28.0\n13    35.0\n14    28.0\n15    31.0\n16    27.0\n17    25.0\n18    31.0\n19     NaN\n20     NaN\ndtype: float64\n</code></pre>\n<h3>Performance</h3>\n<pre><code># Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre>\n<p><strong>Numpy based approach is approximately <code>300x</code> faster than the <code>OP's</code> given approach</strong> while pandas based approach is approximately <code>22x</code> faster</p>\n",
        "question_body": "<p>I have the following <code>pandas</code> <code>df</code>:</p>\n<pre><code>| Date | GB | US | CA | AU | SG | DE | FR |\n| ---- | -- | -- | -- | -- | -- | -- | -- |\n| 1    | 25 |    |    |    |    |    |    |\n| 2    | 29 |    |    |    |    |    |    |\n| 3    | 33 |    |    |    |    |    |    |\n| 4    | 31 | 35 |    |    |    |    |    |\n| 5    | 30 | 34 |    |    |    |    |    |\n| 6    |    | 35 | 34 |    |    |    |    |\n| 7    |    | 31 | 26 |    |    |    |    |\n| 8    |    | 33 | 25 | 31 |    |    |    |\n| 9    |    |    | 26 | 31 |    |    |    |\n| 10   |    |    | 27 | 26 | 28 |    |    |\n| 11   |    |    | 35 | 25 | 29 |    |    |\n| 12   |    |    |    | 33 | 35 | 28 |    |\n| 13   |    |    |    | 28 | 25 | 35 |    |\n| 14   |    |    |    | 25 | 25 | 28 |    |\n| 15   |    |    |    | 25 | 26 | 31 | 25 |\n| 16   |    |    |    |    | 26 | 31 | 27 |\n| 17   |    |    |    |    | 34 | 29 | 25 |\n| 18   |    |    |    |    | 28 | 29 | 31 |\n| 19   |    |    |    |    |    | 34 | 26 |\n| 20   |    |    |    |    |    | 28 | 30 |\n</code></pre>\n<p>I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use <code>numpy</code> (see <a href=\"https://stackoverflow.com/questions/68068102/getting-the-nearest-values-to-the-left-in-a-pandas-column/\">Getting the nearest values to the left in a pandas column</a>) and that is where I am struggling.</p>\n<p>Essentialy, I want my function <code>f</code> which takes an argument <code>int(offset)</code>, to capture the first non <code>nan</code> value for each row from the left, and return the whole thing as a <code>numpy</code> array/vector so that:</p>\n<pre><code>f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n</code></pre>\n<p>As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. <code>offset=0</code> then returns the first value (in that array) and <code>offset=1</code> will return the second value intersected and so on.</p>\n<p>Therefore:</p>\n<pre><code>f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n</code></pre>\n<p>The <code>pandas</code> solution proposed in the post above is very effective:</p>\n<pre><code>def f(df, offset=0):\n    x = df.iloc[:, 0:].apply(lambda x: sorted(x, key=pd.isna)[offset], axis=1)\n    return x\n\nprint(f(df, 1))\n</code></pre>\n<p>However this is very slow with larger iterations. I have tried this with <code>np.apply_along_axis</code> and its even slower!</p>\n<p>Is there a fatser way with <code>numpy</code> vectorization?</p>\n<p>Many thanks.</p>\n",
        "formatted_input": {
            "qid": 68150020,
            "link": "https://stackoverflow.com/questions/68150020/getting-first-second-third-value-in-row-of-numpy-array-after-nan-using-vector",
            "question": {
                "title": "Getting first/second/third... value in row of numpy array after nan using vectorization",
                "ques_desc": "I have the following : I have partly acomplished what I am trying to do here using Pandas alone but the process takes ages so I am having to use (see Getting the nearest values to the left in a pandas column) and that is where I am struggling. Essentialy, I want my function which takes an argument , to capture the first non value for each row from the left, and return the whole thing as a array/vector so that: As I have described in the other post, its best to imagine a horizontal line being drawn from the left for each row, and returning the values intersected by that line as an array. then returns the first value (in that array) and will return the second value intersected and so on. Therefore: The solution proposed in the post above is very effective: However this is very slow with larger iterations. I have tried this with and its even slower! Is there a fatser way with vectorization? Many thanks. "
            },
            "io": [
                "f(offset=0)\n\n\n| 0  | 1  |\n| -- | -- |\n| 1  | 25 |\n| 2  | 29 |\n| 3  | 33 |\n| 4  | 31 |\n| 5  | 30 |\n| 6  | 35 |\n| 7  | 31 |\n| 8  | 33 |\n| 9  | 26 |\n| 10 | 27 |\n| 11 | 35 |\n| 12 | 33 |\n| 13 | 28 |\n| 14 | 25 |\n| 15 | 25 |\n| 16 | 26 |\n| 17 | 34 |\n| 18 | 28 |\n| 19 | 34 |\n| 20 | 28 |\n",
                "f(offset=1)\n\n| 0  | 1   |\n| -- | --- |\n| 1  | nan |\n| 2  | nan |\n| 3  | nan |\n| 4  | 35  |\n| 5  | 34  |\n| 6  | 34  |\n| 7  | 26  |\n| 8  | 25  |\n| 9  | 31  |\n| 10 | 26  |\n| 11 | 25  |\n| 12 | 35  |\n| 13 | 25  |\n| 14 | 25  |\n| 15 | 26  |\n| 16 | 31  |\n| 17 | 29  |\n| 18 | 29  |\n| 19 | 26  |\n| 20 | 30  |\n"
            ],
            "answer": {
                "ans_desc": "Numpy approach We can define a function which takes a array and (n) as input arguments and returns array. Basically, for each row it returns the value after the first value Pandas approach We can the dataframe to reshape then group the dataframe on and aggregate using , then to conform the index of aggregated frame according to original frame Sample run Performance Numpy based approach is approximately faster than the given approach while pandas based approach is approximately faster ",
                "code": [
                    "def first_valid(arr, offset=0):\n    m = ~np.isnan(arr)\n    i =  m.argmax(axis=1) + offset\n    iy = np.clip(i, 0, arr.shape[1] - 1)\n\n    vals = arr[np.r_[:arr.shape[0]], iy]\n    vals[(~m.any(1)) | (i >= arr.shape[1])] = np.nan\n    return vals\n",
                    "def first_valid(df, offset=0):\n    return df.stack().groupby(level=0)\\\n                     .nth(offset).reindex(df.index)\n",
                    "# Sample dataframe for testing purpose\ndf_test = pd.concat([df] * 10000, ignore_index=True)\n\n%%timeit # Numpy approach\n_ = first_valid(df_test.to_numpy(), 1)\n# 6.9 ms \u00b1 212 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n%%timeit # Pandas approach\n_ = first_valid(df_test, 1)\n# 90 ms \u00b1 867 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n%%timeit # OP's approach\n_ = f(df_test, 1)\n# 2.03 s \u00b1 183 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 51,
            "user_id": 10082987,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/141965c0e67ba4408bdb2267fd7c04f2?s=128&d=identicon&r=PG&f=1",
            "display_name": "syedmfk",
            "link": "https://stackoverflow.com/users/10082987/syedmfk"
        },
        "is_answered": true,
        "view_count": 9367,
        "accepted_answer_id": 57033774,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1624643356,
        "creation_date": 1563168421,
        "last_edit_date": 1563169986,
        "question_id": 57033657,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
        "title": "How to Extract Month Name and Year from Date column of DataFrame",
        "body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "answer_body": "<p>Cast you date from object to actual datetime and use dt to access what you need.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n</code></pre>\n",
        "question_body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "formatted_input": {
            "qid": 57033657,
            "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
            "question": {
                "title": "How to Extract Month Name and Year from Date column of DataFrame",
                "ques_desc": "I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. "
            },
            "io": [
                "45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n",
                "45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n"
            ],
            "answer": {
                "ans_desc": "Cast you date from object to actual datetime and use dt to access what you need. ",
                "code": [
                    "import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 183,
            "user_id": 12939325,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-ylhVvK83HnQ/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdDak9Ir2cS6IzUxpxSksMsp-B7tA/photo.jpg?sz=128",
            "display_name": "Steak",
            "link": "https://stackoverflow.com/users/12939325/steak"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68107510,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1624488621,
        "creation_date": 1624484332,
        "last_edit_date": 1624488621,
        "question_id": 68107298,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
        "title": "How to Create a Correlation Dataframe from already related data",
        "body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "answer_body": "<p>I assume that the pairs are unique, i.e. if there's <code>English - Spanish</code>, there won't be a <code>Spanish - English</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code># Rename the columns. Because working with columns\n# named like numbers is a pain in the butt\ndf.columns = ['lang1', 'lang2', 'corr']\n\n# Create the opposite pairs\ntmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)\n\n# Create the identity pairs\nall_lang = np.unique(df[['lang1', 'lang2']])\ntmp2 = pd.DataFrame({\n    'lang1': all_lang,\n    'lang2': all_lang,\n    'corr': 1.0\n})\n\n# Concatenate all 3 together and beat the data frame into shape\nresult = (\n    pd.concat([df, tmp1, tmp2], axis=0)\n        .set_index(['lang1', 'lang2'])\n        .unstack()\n        .droplevel(0, axis=1)\n        .rename_axis(index=[None], columns=[None])\n)\n</code></pre>\n",
        "question_body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "formatted_input": {
            "qid": 68107298,
            "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
            "question": {
                "title": "How to Create a Correlation Dataframe from already related data",
                "ques_desc": "I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? "
            },
            "io": [
                "    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n",
                "        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n"
            ],
            "answer": {
                "ans_desc": "I assume that the pairs are unique, i.e. if there's , there won't be a . ",
                "code": [
                    "# Rename the columns. Because working with columns\n# named like numbers is a pain in the butt\ndf.columns = ['lang1', 'lang2', 'corr']\n\n# Create the opposite pairs\ntmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)\n\n# Create the identity pairs\nall_lang = np.unique(df[['lang1', 'lang2']])\ntmp2 = pd.DataFrame({\n    'lang1': all_lang,\n    'lang2': all_lang,\n    'corr': 1.0\n})\n\n# Concatenate all 3 together and beat the data frame into shape\nresult = (\n    pd.concat([df, tmp1, tmp2], axis=0)\n        .set_index(['lang1', 'lang2'])\n        .unstack()\n        .droplevel(0, axis=1)\n        .rename_axis(index=[None], columns=[None])\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16124075,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxaHbBnxKKjuXo_wOrkhGQHbT5P4fY54mYRG-h6=k-s128",
            "display_name": "hang",
            "link": "https://stackoverflow.com/users/16124075/hang"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 68098622,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624446318,
        "creation_date": 1624444563,
        "question_id": 68098150,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
        "title": "set some rule to groupby in pandas",
        "body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "answer_body": "<p>IIUC:</p>\n<p>try:</p>\n<pre><code>c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') &amp; c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n</code></pre>\n",
        "question_body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "formatted_input": {
            "qid": 68098150,
            "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
            "question": {
                "title": "set some rule to groupby in pandas",
                "ques_desc": "I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have \"dup by\" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. "
            },
            "io": [
                "1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n",
                "1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n"
            ],
            "answer": {
                "ans_desc": "IIUC: try: ",
                "code": [
                    "c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 13920381,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgbR0vHurgfWnrlCJifphJzuFAkbDbJV3Ls033L6g=k-s128",
            "display_name": "dinn_",
            "link": "https://stackoverflow.com/users/13920381/dinn"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68090682,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624397667,
        "creation_date": 1624394378,
        "question_id": 68090463,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
        "title": "Merging more than two columns of the same dataframe in pandas",
        "body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "answer_body": "<p>Input data:</p>\n<pre><code>&gt;&gt;&gt; df\n    VAR 1  VAR 2  VAR 3    GROUP  ANOTHER\n0     NaN    NaN    3.0   [0-10]  another\n1     1.0    NaN    3.0   [0-10]  another\n2     1.0    NaN    3.0   [0-10]  another\n3     1.0    2.0    NaN   [0-10]  another\n4     NaN    2.0    NaN   [0-10]  another\n5     3.0    NaN    3.0  [10-20]  another\n6     3.0    1.0    NaN  [10-20]  another\n7     NaN    1.0    NaN  [10-20]  another\n8     NaN    2.0    NaN  [10-20]  another\n9     NaN    NaN    2.0  [10-20]  another\n10    NaN    NaN    2.0  [10-20]  another\n</code></pre>\n<p>You can use <code>melt</code>. To fully understand, you can execute the code line by line (<code>df.melt(...)</code>, <code>df.melt(...).dropna()</code>, <code>df.melt(...).dropna.sort_values(...)</code> and so on):</p>\n<pre><code>id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n</code></pre>\n<p>Result output:</p>\n<pre><code>&gt;&gt;&gt; out\n    VAR_MERGED    GROUP  ANOTHER\n0          1.0   [0-10]  another\n1          1.0   [0-10]  another\n2          1.0   [0-10]  another\n3          2.0   [0-10]  another\n4          2.0   [0-10]  another\n5          3.0   [0-10]  another\n6          3.0   [0-10]  another\n7          3.0   [0-10]  another\n8          1.0  [10-20]  another\n9          1.0  [10-20]  another\n10         2.0  [10-20]  another\n11         2.0  [10-20]  another\n12         2.0  [10-20]  another\n13         3.0  [10-20]  another\n14         3.0  [10-20]  another\n15         3.0  [10-20]  another\n</code></pre>\n",
        "question_body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "formatted_input": {
            "qid": 68090463,
            "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
            "question": {
                "title": "Merging more than two columns of the same dataframe in pandas",
                "ques_desc": "Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? "
            },
            "io": [
                "VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n",
                "VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n"
            ],
            "answer": {
                "ans_desc": "Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: ",
                "code": [
                    "id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "vectorization"
        ],
        "owner": {
            "reputation": 1433,
            "user_id": 4263878,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/c95a66d30180a95353e7e655d0020f51?s=128&d=identicon&r=PG&f=1",
            "display_name": "MJS",
            "link": "https://stackoverflow.com/users/4263878/mjs"
        },
        "is_answered": true,
        "view_count": 3872,
        "accepted_answer_id": 33130915,
        "answer_count": 2,
        "score": 7,
        "last_activity_date": 1623919516,
        "creation_date": 1444839526,
        "last_edit_date": 1444840857,
        "question_id": 33130586,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
        "title": "python pandas - creating a column which keeps a running count of consecutive values",
        "body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "answer_body": "<p>You can use the compare-cumsum-groupby pattern (which I <em>really</em> need to getting around to writing up for the documentation), with a final <code>cumcount</code>:</p>\n\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"binary\": [0,1,1,1,0,0,1,1,0]})\n&gt;&gt;&gt; df[\"consec\"] = df[\"binary\"].groupby((df[\"binary\"] == 0).cumsum()).cumcount()\n&gt;&gt;&gt; df\n   binary  consec\n0       0       0\n1       1       1\n2       1       2\n3       1       3\n4       0       0\n5       0       0\n6       1       1\n7       1       2\n8       0       0\n</code></pre>\n\n<hr>\n\n<p>This works because first we get the positions where we want to reset the counter:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n</code></pre>\n\n<p>The cumulative sum of these gives us a different id for each group:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0).cumsum()\n0    1\n1    1\n2    1\n3    1\n4    2\n5    3\n6    3\n7    3\n8    4\nName: binary, dtype: int64\n</code></pre>\n\n<p>And then we can pass this to <code>groupby</code> and use <code>cumcount</code> to get an increasing index in each group.</p>\n",
        "question_body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "formatted_input": {
            "qid": 33130586,
            "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
            "question": {
                "title": "python pandas - creating a column which keeps a running count of consecutive values",
                "ques_desc": "I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. "
            },
            "io": [
                ".    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n",
                ".  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n"
            ],
            "answer": {
                "ans_desc": "You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. ",
                "code": [
                    ">>> (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "pyspark-dataframes"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9735423,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "learningtocode",
            "link": "https://stackoverflow.com/users/9735423/learningtocode"
        },
        "is_answered": true,
        "view_count": 575,
        "accepted_answer_id": 62099641,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1623911798,
        "creation_date": 1590822619,
        "last_edit_date": 1590926848,
        "question_id": 62099066,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
        "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
        "body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "answer_body": "<p>Here you go:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n</code></pre>\n\n<p>Check <code>output.xlsx</code> for the output.</p>\n",
        "question_body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "formatted_input": {
            "qid": 62099066,
            "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
            "question": {
                "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
                "ques_desc": "I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: "
            },
            "io": [
                "(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n",
                "(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n"
            ],
            "answer": {
                "ans_desc": "Here you go: Check for the output. ",
                "code": [
                    "import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 10310,
            "user_id": 1334761,
            "user_type": "registered",
            "accept_rate": 62,
            "profile_image": "https://www.gravatar.com/avatar/e98df35e5a847e4adee98cb544e5bf6e?s=128&d=identicon&r=PG",
            "display_name": "Jjang",
            "link": "https://stackoverflow.com/users/1334761/jjang"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 67990087,
        "answer_count": 5,
        "score": 1,
        "last_activity_date": 1623830508,
        "creation_date": 1623772927,
        "last_edit_date": 1623830508,
        "question_id": 67989744,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
        "title": "Pandas replacing values in a column by values in another column",
        "body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "answer_body": "<p>Have a look at Jeremy Z answer on this post, for further explanation on solution <a href=\"https://stackoverflow.com/a/55631906/16235276\">https://stackoverflow.com/a/55631906/16235276</a></p>\n<pre><code>df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n</code></pre>\n",
        "question_body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "formatted_input": {
            "qid": 67989744,
            "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
            "question": {
                "title": "Pandas replacing values in a column by values in another column",
                "ques_desc": "Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: "
            },
            "io": [
                "    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n",
                "    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n"
            ],
            "answer": {
                "ans_desc": "Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 ",
                "code": [
                    "df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16117705,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b2f44431a658e5622efe2e5d2d6a84b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "WorkingPerson",
            "link": "https://stackoverflow.com/users/16117705/workingperson"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67987417,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623764393,
        "creation_date": 1623761108,
        "last_edit_date": 1623763345,
        "question_id": 67986537,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
        "title": "How do I check for conflict between columns in a pandas dataframe?",
        "body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "answer_body": "<p>IIUC, try:</p>\n<pre><code>df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      Yes\n1  Item2      6.0      7.0        5      Yes\n2  Item3      NaN      NaN        5      nan\n3  Item4      5.0      5.0        5      nan\n4  Item5      5.0      NaN        5      nan\n</code></pre>\n",
        "question_body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "formatted_input": {
            "qid": 67986537,
            "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
            "question": {
                "title": "How do I check for conflict between columns in a pandas dataframe?",
                "ques_desc": "I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. "
            },
            "io": [
                "    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n",
                "    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n"
            ],
            "answer": {
                "ans_desc": "IIUC, try: Output: ",
                "code": [
                    "df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 117,
            "user_id": 8581989,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/S7DTo.jpg?s=128&g=1",
            "display_name": "Steve Dallas",
            "link": "https://stackoverflow.com/users/8581989/steve-dallas"
        },
        "is_answered": true,
        "view_count": 11295,
        "accepted_answer_id": 46125692,
        "answer_count": 3,
        "score": 8,
        "last_activity_date": 1623741805,
        "creation_date": 1504905625,
        "last_edit_date": 1527649748,
        "question_id": 46124699,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
        "title": "Splitting a dataframe into separate CSV files",
        "body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "answer_body": "<p>Why not just groupby <code>Column1</code> and save each group?</p>\n\n<pre><code>df = df.sort_values('Column1').assign(NewColumn='NewColumnValue')\nprint(df)\n\n   Column1  Column2       NewColumn\n0        1    93644  NewColumnValue\n5        1    19593  NewColumnValue\n6        1    12707  NewColumnValue\n1        2    63246  NewColumnValue\n7        2    53480  NewColumnValue\n2        3    47790  NewColumnValue\n3        3    39644  NewColumnValue\n4        3    32585  NewColumnValue\n</code></pre>\n\n<hr>\n\n<pre><code>for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n</code></pre>\n\n<p>Thanks to Unatiel for the <a href=\"https://stackoverflow.com/questions/46124699/splitting-csv-based-on-content-of-one-column-after-having-done-some-other-manipu/46125692?noredirect=1#comment79214220_46125692\">improvement</a>. <code>header=False</code> will not write headers and <code>index_label=False</code> will not write an index column.</p>\n\n<p>This creates 3 files:</p>\n\n<pre><code>1.csv\n2.csv\n3.csv\n</code></pre>\n\n<p>Each having data corresponding to each <code>Column1</code> group.</p>\n",
        "question_body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "formatted_input": {
            "qid": 46124699,
            "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
            "question": {
                "title": "Splitting a dataframe into separate CSV files",
                "ques_desc": "I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. "
            },
            "io": [
                "+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n",
                "+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n"
            ],
            "answer": {
                "ans_desc": "Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. ",
                "code": [
                    "for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 51,
            "user_id": 10082987,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/141965c0e67ba4408bdb2267fd7c04f2?s=128&d=identicon&r=PG&f=1",
            "display_name": "syedmfk",
            "link": "https://stackoverflow.com/users/10082987/syedmfk"
        },
        "is_answered": true,
        "view_count": 9367,
        "accepted_answer_id": 57033774,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1624643356,
        "creation_date": 1563168421,
        "last_edit_date": 1563169986,
        "question_id": 57033657,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
        "title": "How to Extract Month Name and Year from Date column of DataFrame",
        "body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "answer_body": "<p>Cast you date from object to actual datetime and use dt to access what you need.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n</code></pre>\n",
        "question_body": "<p>I have the following DF</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n</code></pre>\n\n<p>I want to extract the month name and year in a simple way in the following format:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n</code></pre>\n\n<p>I have used the <code>df.Date.dt.to_period(\"M\")</code> which return <code>\"2018-01\"</code> format.</p>\n",
        "formatted_input": {
            "qid": 57033657,
            "link": "https://stackoverflow.com/questions/57033657/how-to-extract-month-name-and-year-from-date-column-of-dataframe",
            "question": {
                "title": "How to Extract Month Name and Year from Date column of DataFrame",
                "ques_desc": "I have the following DF I want to extract the month name and year in a simple way in the following format: I have used the which return format. "
            },
            "io": [
                "45    2018-01-01\n73    2018-02-08\n74    2018-02-08\n75    2018-02-08\n76    2018-02-08\n",
                "45    Jan-2018\n73    Feb-2018\n74    Feb-2018\n75    Feb-2018\n76    Feb-2018\n"
            ],
            "answer": {
                "ans_desc": "Cast you date from object to actual datetime and use dt to access what you need. ",
                "code": [
                    "import pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08']})\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# You can format your date as you wish\ndf['Mon_Year'] = df['Date'].dt.strftime('%b-%Y')\n\n# the result is object/string unlike `.dt.to_period('M')` that retains datetime data type.\n\nprint(df['Mon_Year'])\n\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 183,
            "user_id": 12939325,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-ylhVvK83HnQ/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdDak9Ir2cS6IzUxpxSksMsp-B7tA/photo.jpg?sz=128",
            "display_name": "Steak",
            "link": "https://stackoverflow.com/users/12939325/steak"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 68107510,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1624488621,
        "creation_date": 1624484332,
        "last_edit_date": 1624488621,
        "question_id": 68107298,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
        "title": "How to Create a Correlation Dataframe from already related data",
        "body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "answer_body": "<p>I assume that the pairs are unique, i.e. if there's <code>English - Spanish</code>, there won't be a <code>Spanish - English</code>.</p>\n<pre class=\"lang-py prettyprint-override\"><code># Rename the columns. Because working with columns\n# named like numbers is a pain in the butt\ndf.columns = ['lang1', 'lang2', 'corr']\n\n# Create the opposite pairs\ntmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)\n\n# Create the identity pairs\nall_lang = np.unique(df[['lang1', 'lang2']])\ntmp2 = pd.DataFrame({\n    'lang1': all_lang,\n    'lang2': all_lang,\n    'corr': 1.0\n})\n\n# Concatenate all 3 together and beat the data frame into shape\nresult = (\n    pd.concat([df, tmp1, tmp2], axis=0)\n        .set_index(['lang1', 'lang2'])\n        .unstack()\n        .droplevel(0, axis=1)\n        .rename_axis(index=[None], columns=[None])\n)\n</code></pre>\n",
        "question_body": "<p>I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity:</p>\n<pre><code>    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n</code></pre>\n<p>I would like to create a correlation dataframe such as:</p>\n<pre><code>        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n</code></pre>\n<p>To create the first dataframe, I ran:</p>\n<pre><code>pairing_list = [[&quot;English&quot;,&quot;Spanish&quot;,0.5],[&quot;English&quot;,&quot;Russian&quot;,0.15]]\ndf = pd.DataFrame(pairing_list)\n</code></pre>\n<p>I have tried:</p>\n<pre><code>df.corr()\n</code></pre>\n<p>Which returns:</p>\n<pre><code>        2\n2       1.0\n</code></pre>\n<p>I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related).</p>\n<p>To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns.</p>\n<p>How could I use Python / Pandas to do this?</p>\n",
        "formatted_input": {
            "qid": 68107298,
            "link": "https://stackoverflow.com/questions/68107298/how-to-create-a-correlation-dataframe-from-already-related-data",
            "question": {
                "title": "How to Create a Correlation Dataframe from already related data",
                "ques_desc": "I have a data frame of language similarity. Here is a small snippet that's been edited for simplicity: I would like to create a correlation dataframe such as: To create the first dataframe, I ran: I have tried: Which returns: I have looked at other similar questions but it seems that the data for use in .corr() is by itself (ie: my data here is already a correlation between the two columns, whereas the examples I have seen are not yet such related). To clarify: the data presented is already the similarity between the two languages, and thus is not some value associated with one language alone; it is for the pair listed in the columns. How could I use Python / Pandas to do this? "
            },
            "io": [
                "    0       1       2\n0   English Spanish 0.50\n1   English Russian 0.15\n",
                "        English Spanish Russian\nEnglish 1       0.5     0.15\nSpanish 0.5     1       -\nRussian 0.15    -       1\n"
            ],
            "answer": {
                "ans_desc": "I assume that the pairs are unique, i.e. if there's , there won't be a . ",
                "code": [
                    "# Rename the columns. Because working with columns\n# named like numbers is a pain in the butt\ndf.columns = ['lang1', 'lang2', 'corr']\n\n# Create the opposite pairs\ntmp1 = df.copy().set_axis(['lang2', 'lang1', 'corr'], axis=1)\n\n# Create the identity pairs\nall_lang = np.unique(df[['lang1', 'lang2']])\ntmp2 = pd.DataFrame({\n    'lang1': all_lang,\n    'lang2': all_lang,\n    'corr': 1.0\n})\n\n# Concatenate all 3 together and beat the data frame into shape\nresult = (\n    pd.concat([df, tmp1, tmp2], axis=0)\n        .set_index(['lang1', 'lang2'])\n        .unstack()\n        .droplevel(0, axis=1)\n        .rename_axis(index=[None], columns=[None])\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "datetime"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16124075,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJxaHbBnxKKjuXo_wOrkhGQHbT5P4fY54mYRG-h6=k-s128",
            "display_name": "hang",
            "link": "https://stackoverflow.com/users/16124075/hang"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 68098622,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1624446318,
        "creation_date": 1624444563,
        "question_id": 68098150,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
        "title": "set some rule to groupby in pandas",
        "body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "answer_body": "<p>IIUC:</p>\n<p>try:</p>\n<pre><code>c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') &amp; c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n</code></pre>\n",
        "question_body": "<p>I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have &quot;dup by&quot; before I groupby the datetime.</p>\n<p>There is my code:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;sample.csv&quot;,delimiter='|')\n\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df.groupby(df['VIP_ID'])['datetime'].max()\nmost_recent_date= most_recent_date.rename(&quot;most_recent_date&quot;)\ndf = df.join(most_recent_date, on=&quot;VIP_ID&quot;)\n\ndf['both'] = np.where(\n       ((df['keep'] == 'same tier')&amp;(dup == 'yes')),\n          df['VIP_ID']+df['datetime'].astype(str),\n         df['ID']\n)\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\n\n\n\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\nprint(df)\n</code></pre>\n<p>And this code make all keep column become 'dup by'.</p>\n<p>example csv:</p>\n<pre><code>ID|VIP_ID|TIER|datatime|keep\n1|F08210020403|GO|2014-05-17 00:00:00|same tier\n2|F08210020403|GO|2014-04-18 00:00:00|same tier\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than\ngroupby remain rows.</p>\n<p>This is my output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>expect output:</p>\n<pre><code>1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n</code></pre>\n<p>Any help would be very much appreciated.</p>\n",
        "formatted_input": {
            "qid": 68098150,
            "link": "https://stackoverflow.com/questions/68098150/set-some-rule-to-groupby-in-pandas",
            "question": {
                "title": "set some rule to groupby in pandas",
                "ques_desc": "I need to set some rule to groupby in pandas. I hope I can ignore the rows if ['keep'] column have \"dup by\" before I groupby the datetime. There is my code: And this code make all keep column become 'dup by'. example csv: Because 2016-05-10 00:00:00 is the max datetime by F08210020403, all keep columns will show dup by F08210020403.I hope I can set some rules about if keep contain 'dup', ignore this row. After than groupby remain rows. This is my output: expect output: Any help would be very much appreciated. "
            },
            "io": [
                "1|F08210020403|GO|2014-05-17 00:00:00|dup by F08210020403\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n",
                "1|F08210020403|GO|2014-05-17 00:00:00|yes\n2|F08210020403|GO|2014-04-18 00:00:00|dup by F08210020403\n3|F08210020403|FO||dup by F08210020403\n4|F08210020403|FO||dup by F08210020403\n5|F08210020403|FO|2016-09-18 00:00:00|dup by F08210020403\n6|F08210020403|FO|2016-05-10 00:00:00|dup by F08210020403\n7|F08210020403|FO||dup by F08210020403\n8|F08210020403|FO||dup by F08210020403\n"
            ],
            "answer": {
                "ans_desc": "IIUC: try: ",
                "code": [
                    "c=df['keep'].str.contains('dup by')\n#created a condition which check if 'keep' column contains 'dup by' or not\ndf['datetime'] = pd.to_datetime(df['datetime'],errors = 'coerce')\nmost_recent_date = df[~c].groupby(df['VIP_ID'])['datetime'].max()\n#excluded those rows in groupby where 'keep' contains 'dup by'\ndf['most_recent_date']=df['VIP_ID'].map(most_recent_date)\ndf['both'] = np.where((df['keep'] == 'same tier') & c,df['VIP_ID']+df['datetime'].astype(str),df['ID'])\ndf['keep'] = np.where(\n    df['keep'] != 'same tier',df['keep'],\n    (np.where(\n         df['most_recent_date'] == df['datetime'],\n         'yes',\n         'dup by ' + df['VIP_ID'].astype(str)))\n)\ndf.loc[df.duplicated(subset=['both'], keep = False),'keep'] = 'same time'\ndf = df.drop(columns = ['both','most_recent_date'])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 13920381,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgbR0vHurgfWnrlCJifphJzuFAkbDbJV3Ls033L6g=k-s128",
            "display_name": "dinn_",
            "link": "https://stackoverflow.com/users/13920381/dinn"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68090682,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624397667,
        "creation_date": 1624394378,
        "question_id": 68090463,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
        "title": "Merging more than two columns of the same dataframe in pandas",
        "body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "answer_body": "<p>Input data:</p>\n<pre><code>&gt;&gt;&gt; df\n    VAR 1  VAR 2  VAR 3    GROUP  ANOTHER\n0     NaN    NaN    3.0   [0-10]  another\n1     1.0    NaN    3.0   [0-10]  another\n2     1.0    NaN    3.0   [0-10]  another\n3     1.0    2.0    NaN   [0-10]  another\n4     NaN    2.0    NaN   [0-10]  another\n5     3.0    NaN    3.0  [10-20]  another\n6     3.0    1.0    NaN  [10-20]  another\n7     NaN    1.0    NaN  [10-20]  another\n8     NaN    2.0    NaN  [10-20]  another\n9     NaN    NaN    2.0  [10-20]  another\n10    NaN    NaN    2.0  [10-20]  another\n</code></pre>\n<p>You can use <code>melt</code>. To fully understand, you can execute the code line by line (<code>df.melt(...)</code>, <code>df.melt(...).dropna()</code>, <code>df.melt(...).dropna.sort_values(...)</code> and so on):</p>\n<pre><code>id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n</code></pre>\n<p>Result output:</p>\n<pre><code>&gt;&gt;&gt; out\n    VAR_MERGED    GROUP  ANOTHER\n0          1.0   [0-10]  another\n1          1.0   [0-10]  another\n2          1.0   [0-10]  another\n3          2.0   [0-10]  another\n4          2.0   [0-10]  another\n5          3.0   [0-10]  another\n6          3.0   [0-10]  another\n7          3.0   [0-10]  another\n8          1.0  [10-20]  another\n9          1.0  [10-20]  another\n10         2.0  [10-20]  another\n11         2.0  [10-20]  another\n12         2.0  [10-20]  another\n13         3.0  [10-20]  another\n14         3.0  [10-20]  another\n15         3.0  [10-20]  another\n</code></pre>\n",
        "question_body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "formatted_input": {
            "qid": 68090463,
            "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
            "question": {
                "title": "Merging more than two columns of the same dataframe in pandas",
                "ques_desc": "Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? "
            },
            "io": [
                "VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n",
                "VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n"
            ],
            "answer": {
                "ans_desc": "Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: ",
                "code": [
                    "id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "vectorization"
        ],
        "owner": {
            "reputation": 1433,
            "user_id": 4263878,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/c95a66d30180a95353e7e655d0020f51?s=128&d=identicon&r=PG&f=1",
            "display_name": "MJS",
            "link": "https://stackoverflow.com/users/4263878/mjs"
        },
        "is_answered": true,
        "view_count": 3872,
        "accepted_answer_id": 33130915,
        "answer_count": 2,
        "score": 7,
        "last_activity_date": 1623919516,
        "creation_date": 1444839526,
        "last_edit_date": 1444840857,
        "question_id": 33130586,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
        "title": "python pandas - creating a column which keeps a running count of consecutive values",
        "body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "answer_body": "<p>You can use the compare-cumsum-groupby pattern (which I <em>really</em> need to getting around to writing up for the documentation), with a final <code>cumcount</code>:</p>\n\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"binary\": [0,1,1,1,0,0,1,1,0]})\n&gt;&gt;&gt; df[\"consec\"] = df[\"binary\"].groupby((df[\"binary\"] == 0).cumsum()).cumcount()\n&gt;&gt;&gt; df\n   binary  consec\n0       0       0\n1       1       1\n2       1       2\n3       1       3\n4       0       0\n5       0       0\n6       1       1\n7       1       2\n8       0       0\n</code></pre>\n\n<hr>\n\n<p>This works because first we get the positions where we want to reset the counter:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n</code></pre>\n\n<p>The cumulative sum of these gives us a different id for each group:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0).cumsum()\n0    1\n1    1\n2    1\n3    1\n4    2\n5    3\n6    3\n7    3\n8    4\nName: binary, dtype: int64\n</code></pre>\n\n<p>And then we can pass this to <code>groupby</code> and use <code>cumcount</code> to get an increasing index in each group.</p>\n",
        "question_body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "formatted_input": {
            "qid": 33130586,
            "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
            "question": {
                "title": "python pandas - creating a column which keeps a running count of consecutive values",
                "ques_desc": "I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. "
            },
            "io": [
                ".    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n",
                ".  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n"
            ],
            "answer": {
                "ans_desc": "You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. ",
                "code": [
                    ">>> (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "pyspark-dataframes"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9735423,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "learningtocode",
            "link": "https://stackoverflow.com/users/9735423/learningtocode"
        },
        "is_answered": true,
        "view_count": 575,
        "accepted_answer_id": 62099641,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1623911798,
        "creation_date": 1590822619,
        "last_edit_date": 1590926848,
        "question_id": 62099066,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
        "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
        "body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "answer_body": "<p>Here you go:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n</code></pre>\n\n<p>Check <code>output.xlsx</code> for the output.</p>\n",
        "question_body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "formatted_input": {
            "qid": 62099066,
            "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
            "question": {
                "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
                "ques_desc": "I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: "
            },
            "io": [
                "(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n",
                "(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n"
            ],
            "answer": {
                "ans_desc": "Here you go: Check for the output. ",
                "code": [
                    "import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 10310,
            "user_id": 1334761,
            "user_type": "registered",
            "accept_rate": 62,
            "profile_image": "https://www.gravatar.com/avatar/e98df35e5a847e4adee98cb544e5bf6e?s=128&d=identicon&r=PG",
            "display_name": "Jjang",
            "link": "https://stackoverflow.com/users/1334761/jjang"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 67990087,
        "answer_count": 5,
        "score": 1,
        "last_activity_date": 1623830508,
        "creation_date": 1623772927,
        "last_edit_date": 1623830508,
        "question_id": 67989744,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
        "title": "Pandas replacing values in a column by values in another column",
        "body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "answer_body": "<p>Have a look at Jeremy Z answer on this post, for further explanation on solution <a href=\"https://stackoverflow.com/a/55631906/16235276\">https://stackoverflow.com/a/55631906/16235276</a></p>\n<pre><code>df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n</code></pre>\n",
        "question_body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "formatted_input": {
            "qid": 67989744,
            "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
            "question": {
                "title": "Pandas replacing values in a column by values in another column",
                "ques_desc": "Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: "
            },
            "io": [
                "    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n",
                "    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n"
            ],
            "answer": {
                "ans_desc": "Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 ",
                "code": [
                    "df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16117705,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b2f44431a658e5622efe2e5d2d6a84b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "WorkingPerson",
            "link": "https://stackoverflow.com/users/16117705/workingperson"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67987417,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623764393,
        "creation_date": 1623761108,
        "last_edit_date": 1623763345,
        "question_id": 67986537,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
        "title": "How do I check for conflict between columns in a pandas dataframe?",
        "body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "answer_body": "<p>IIUC, try:</p>\n<pre><code>df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      Yes\n1  Item2      6.0      7.0        5      Yes\n2  Item3      NaN      NaN        5      nan\n3  Item4      5.0      5.0        5      nan\n4  Item5      5.0      NaN        5      nan\n</code></pre>\n",
        "question_body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "formatted_input": {
            "qid": 67986537,
            "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
            "question": {
                "title": "How do I check for conflict between columns in a pandas dataframe?",
                "ques_desc": "I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. "
            },
            "io": [
                "    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n",
                "    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n"
            ],
            "answer": {
                "ans_desc": "IIUC, try: Output: ",
                "code": [
                    "df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 117,
            "user_id": 8581989,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/S7DTo.jpg?s=128&g=1",
            "display_name": "Steve Dallas",
            "link": "https://stackoverflow.com/users/8581989/steve-dallas"
        },
        "is_answered": true,
        "view_count": 11295,
        "accepted_answer_id": 46125692,
        "answer_count": 3,
        "score": 8,
        "last_activity_date": 1623741805,
        "creation_date": 1504905625,
        "last_edit_date": 1527649748,
        "question_id": 46124699,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
        "title": "Splitting a dataframe into separate CSV files",
        "body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "answer_body": "<p>Why not just groupby <code>Column1</code> and save each group?</p>\n\n<pre><code>df = df.sort_values('Column1').assign(NewColumn='NewColumnValue')\nprint(df)\n\n   Column1  Column2       NewColumn\n0        1    93644  NewColumnValue\n5        1    19593  NewColumnValue\n6        1    12707  NewColumnValue\n1        2    63246  NewColumnValue\n7        2    53480  NewColumnValue\n2        3    47790  NewColumnValue\n3        3    39644  NewColumnValue\n4        3    32585  NewColumnValue\n</code></pre>\n\n<hr>\n\n<pre><code>for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n</code></pre>\n\n<p>Thanks to Unatiel for the <a href=\"https://stackoverflow.com/questions/46124699/splitting-csv-based-on-content-of-one-column-after-having-done-some-other-manipu/46125692?noredirect=1#comment79214220_46125692\">improvement</a>. <code>header=False</code> will not write headers and <code>index_label=False</code> will not write an index column.</p>\n\n<p>This creates 3 files:</p>\n\n<pre><code>1.csv\n2.csv\n3.csv\n</code></pre>\n\n<p>Each having data corresponding to each <code>Column1</code> group.</p>\n",
        "question_body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "formatted_input": {
            "qid": 46124699,
            "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
            "question": {
                "title": "Splitting a dataframe into separate CSV files",
                "ques_desc": "I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. "
            },
            "io": [
                "+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n",
                "+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n"
            ],
            "answer": {
                "ans_desc": "Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. ",
                "code": [
                    "for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 9235000,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1652134728176732/picture?type=large",
            "display_name": "Amruth Anand",
            "link": "https://stackoverflow.com/users/9235000/amruth-anand"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 67919490,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623322230,
        "creation_date": 1623321043,
        "question_id": 67919385,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
        "title": "Find unique column values out of two different Dataframes",
        "body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "answer_body": "<p>TRY:</p>\n<pre><code>unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n</code></pre>\n<p><em>NOTE</em> : Replace <code>0</code> in <code>subset= [0]</code> with the first column name.</p>\n",
        "question_body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "formatted_input": {
            "qid": 67919385,
            "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
            "question": {
                "title": "Find unique column values out of two different Dataframes",
                "ques_desc": "How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read "
            },
            "io": [
                "67      Hij\n14      Xyz \n87      Pqr\n",
                "43      Def\n67      Lmn\n14      Xyz\n"
            ],
            "answer": {
                "ans_desc": "TRY: NOTE : Replace in with the first column name. ",
                "code": [
                    "unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 67917741,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623316126,
        "creation_date": 1623314044,
        "last_edit_date": 1623314922,
        "question_id": 67917573,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
        "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
        "body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "answer_body": "<p>Here is one way:</p>\n<pre><code>minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, &quot;-&quot;)\n</code></pre>\n<p>where we first flip the <code>df</code> over the columns, look where it is not <code>NaN</code> and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put <code>&quot;-&quot;</code> so we use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html\" rel=\"nofollow noreferrer\"><code>mask</code></a> method to put minus signs there,</p>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n    L1   D1   L2   D2 L3\n0  1.0  ABC  1.1  4.1  -\n1  NaN  NaN  1.7    -  -\n2  NaN  4.1    -    -  -\n3  NaN  1.8  3.2  PQR  -\n4  NaN  NaN  1.6    -  -\n</code></pre>\n",
        "question_body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "formatted_input": {
            "qid": 67917573,
            "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
            "question": {
                "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
                "ques_desc": "I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! "
            },
            "io": [
                " L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n",
                " L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n"
            ],
            "answer": {
                "ans_desc": "Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get ",
                "code": [
                    "minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, \"-\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "merge"
        ],
        "owner": {
            "reputation": 37,
            "user_id": 13920381,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgbR0vHurgfWnrlCJifphJzuFAkbDbJV3Ls033L6g=k-s128",
            "display_name": "dinn_",
            "link": "https://stackoverflow.com/users/13920381/dinn"
        },
        "is_answered": true,
        "view_count": 29,
        "accepted_answer_id": 68090682,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1624397667,
        "creation_date": 1624394378,
        "question_id": 68090463,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
        "title": "Merging more than two columns of the same dataframe in pandas",
        "body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "answer_body": "<p>Input data:</p>\n<pre><code>&gt;&gt;&gt; df\n    VAR 1  VAR 2  VAR 3    GROUP  ANOTHER\n0     NaN    NaN    3.0   [0-10]  another\n1     1.0    NaN    3.0   [0-10]  another\n2     1.0    NaN    3.0   [0-10]  another\n3     1.0    2.0    NaN   [0-10]  another\n4     NaN    2.0    NaN   [0-10]  another\n5     3.0    NaN    3.0  [10-20]  another\n6     3.0    1.0    NaN  [10-20]  another\n7     NaN    1.0    NaN  [10-20]  another\n8     NaN    2.0    NaN  [10-20]  another\n9     NaN    NaN    2.0  [10-20]  another\n10    NaN    NaN    2.0  [10-20]  another\n</code></pre>\n<p>You can use <code>melt</code>. To fully understand, you can execute the code line by line (<code>df.melt(...)</code>, <code>df.melt(...).dropna()</code>, <code>df.melt(...).dropna.sort_values(...)</code> and so on):</p>\n<pre><code>id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n</code></pre>\n<p>Result output:</p>\n<pre><code>&gt;&gt;&gt; out\n    VAR_MERGED    GROUP  ANOTHER\n0          1.0   [0-10]  another\n1          1.0   [0-10]  another\n2          1.0   [0-10]  another\n3          2.0   [0-10]  another\n4          2.0   [0-10]  another\n5          3.0   [0-10]  another\n6          3.0   [0-10]  another\n7          3.0   [0-10]  another\n8          1.0  [10-20]  another\n9          1.0  [10-20]  another\n10         2.0  [10-20]  another\n11         2.0  [10-20]  another\n12         2.0  [10-20]  another\n13         3.0  [10-20]  another\n14         3.0  [10-20]  another\n15         3.0  [10-20]  another\n</code></pre>\n",
        "question_body": "<p>Trying to reorganise the below dataframe so that <code>[VAR]</code> 1-3 are merged in numeric order along <code>[GROUP]</code> column</p>\n<pre><code>VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n</code></pre>\n<p>Trying to get this as the final result:</p>\n<pre><code>VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n</code></pre>\n<p>I've tried to use <code>df['VAR_MERGED'] = df[['VAR 1' 'VAR 2' 'VAR 3']].agg('-'.join, axis=1)</code> but get error about expected str, but values in <code>VAR</code> columns are all float but not sure why this would need string values?</p>\n",
        "formatted_input": {
            "qid": 68090463,
            "link": "https://stackoverflow.com/questions/68090463/merging-more-than-two-columns-of-the-same-dataframe-in-pandas",
            "question": {
                "title": "Merging more than two columns of the same dataframe in pandas",
                "ques_desc": "Trying to reorganise the below dataframe so that 1-3 are merged in numeric order along column Trying to get this as the final result: I've tried to use but get error about expected str, but values in columns are all float but not sure why this would need string values? "
            },
            "io": [
                "VAR 1   VAR 2   VAR 3   GROUP \n                3   [0-10]\n1               3   [0-10]\n1               3   [0-10]\n1       2           [0-10]\n        2           [0-10]\n3              3    [10-20]\n3       1           [10-20]\n        1           [10-20]\n        2           [10-20]\n               2    [10-20]\n               2    [10-20]\n",
                "VAR_MERGED  GROUP \n1           [0-10]\n1           [0-10]\n1           [0-10]\n2           [0-10]\n2           [0-10]\n3           [0-10]\n3           [0-10]\n3           [0-10]\n1           [10-20]\n1           [10-20]\n2           [10-20]\n2           [10-20]\n2           [10-20]\n3           [10-20]\n3           [10-20]\n3           [10-20]\n"
            ],
            "answer": {
                "ans_desc": "Input data: You can use . To fully understand, you can execute the code line by line (, , and so on): Result output: ",
                "code": [
                    "id_vars = df.columns[~df.columns.str.startswith('VAR')]\n\nout = df.melt(id_vars, value_name='VAR_MERGED') \\\n        .dropna() \\\n        .sort_values(['GROUP', 'VAR_MERGED']) \\\n        .reset_index(drop=True) \\\n        [['VAR_MERGED'] + id_vars]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "vectorization"
        ],
        "owner": {
            "reputation": 1433,
            "user_id": 4263878,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/c95a66d30180a95353e7e655d0020f51?s=128&d=identicon&r=PG&f=1",
            "display_name": "MJS",
            "link": "https://stackoverflow.com/users/4263878/mjs"
        },
        "is_answered": true,
        "view_count": 3872,
        "accepted_answer_id": 33130915,
        "answer_count": 2,
        "score": 7,
        "last_activity_date": 1623919516,
        "creation_date": 1444839526,
        "last_edit_date": 1444840857,
        "question_id": 33130586,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
        "title": "python pandas - creating a column which keeps a running count of consecutive values",
        "body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "answer_body": "<p>You can use the compare-cumsum-groupby pattern (which I <em>really</em> need to getting around to writing up for the documentation), with a final <code>cumcount</code>:</p>\n\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"binary\": [0,1,1,1,0,0,1,1,0]})\n&gt;&gt;&gt; df[\"consec\"] = df[\"binary\"].groupby((df[\"binary\"] == 0).cumsum()).cumcount()\n&gt;&gt;&gt; df\n   binary  consec\n0       0       0\n1       1       1\n2       1       2\n3       1       3\n4       0       0\n5       0       0\n6       1       1\n7       1       2\n8       0       0\n</code></pre>\n\n<hr>\n\n<p>This works because first we get the positions where we want to reset the counter:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n</code></pre>\n\n<p>The cumulative sum of these gives us a different id for each group:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0).cumsum()\n0    1\n1    1\n2    1\n3    1\n4    2\n5    3\n6    3\n7    3\n8    4\nName: binary, dtype: int64\n</code></pre>\n\n<p>And then we can pass this to <code>groupby</code> and use <code>cumcount</code> to get an increasing index in each group.</p>\n",
        "question_body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "formatted_input": {
            "qid": 33130586,
            "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
            "question": {
                "title": "python pandas - creating a column which keeps a running count of consecutive values",
                "ques_desc": "I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. "
            },
            "io": [
                ".    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n",
                ".  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n"
            ],
            "answer": {
                "ans_desc": "You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. ",
                "code": [
                    ">>> (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "pyspark-dataframes"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9735423,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "learningtocode",
            "link": "https://stackoverflow.com/users/9735423/learningtocode"
        },
        "is_answered": true,
        "view_count": 575,
        "accepted_answer_id": 62099641,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1623911798,
        "creation_date": 1590822619,
        "last_edit_date": 1590926848,
        "question_id": 62099066,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
        "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
        "body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "answer_body": "<p>Here you go:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n</code></pre>\n\n<p>Check <code>output.xlsx</code> for the output.</p>\n",
        "question_body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "formatted_input": {
            "qid": 62099066,
            "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
            "question": {
                "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
                "ques_desc": "I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: "
            },
            "io": [
                "(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n",
                "(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n"
            ],
            "answer": {
                "ans_desc": "Here you go: Check for the output. ",
                "code": [
                    "import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 10310,
            "user_id": 1334761,
            "user_type": "registered",
            "accept_rate": 62,
            "profile_image": "https://www.gravatar.com/avatar/e98df35e5a847e4adee98cb544e5bf6e?s=128&d=identicon&r=PG",
            "display_name": "Jjang",
            "link": "https://stackoverflow.com/users/1334761/jjang"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 67990087,
        "answer_count": 5,
        "score": 1,
        "last_activity_date": 1623830508,
        "creation_date": 1623772927,
        "last_edit_date": 1623830508,
        "question_id": 67989744,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
        "title": "Pandas replacing values in a column by values in another column",
        "body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "answer_body": "<p>Have a look at Jeremy Z answer on this post, for further explanation on solution <a href=\"https://stackoverflow.com/a/55631906/16235276\">https://stackoverflow.com/a/55631906/16235276</a></p>\n<pre><code>df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n</code></pre>\n",
        "question_body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "formatted_input": {
            "qid": 67989744,
            "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
            "question": {
                "title": "Pandas replacing values in a column by values in another column",
                "ques_desc": "Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: "
            },
            "io": [
                "    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n",
                "    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n"
            ],
            "answer": {
                "ans_desc": "Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 ",
                "code": [
                    "df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16117705,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b2f44431a658e5622efe2e5d2d6a84b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "WorkingPerson",
            "link": "https://stackoverflow.com/users/16117705/workingperson"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67987417,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623764393,
        "creation_date": 1623761108,
        "last_edit_date": 1623763345,
        "question_id": 67986537,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
        "title": "How do I check for conflict between columns in a pandas dataframe?",
        "body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "answer_body": "<p>IIUC, try:</p>\n<pre><code>df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      Yes\n1  Item2      6.0      7.0        5      Yes\n2  Item3      NaN      NaN        5      nan\n3  Item4      5.0      5.0        5      nan\n4  Item5      5.0      NaN        5      nan\n</code></pre>\n",
        "question_body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "formatted_input": {
            "qid": 67986537,
            "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
            "question": {
                "title": "How do I check for conflict between columns in a pandas dataframe?",
                "ques_desc": "I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. "
            },
            "io": [
                "    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n",
                "    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n"
            ],
            "answer": {
                "ans_desc": "IIUC, try: Output: ",
                "code": [
                    "df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 117,
            "user_id": 8581989,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/S7DTo.jpg?s=128&g=1",
            "display_name": "Steve Dallas",
            "link": "https://stackoverflow.com/users/8581989/steve-dallas"
        },
        "is_answered": true,
        "view_count": 11295,
        "accepted_answer_id": 46125692,
        "answer_count": 3,
        "score": 8,
        "last_activity_date": 1623741805,
        "creation_date": 1504905625,
        "last_edit_date": 1527649748,
        "question_id": 46124699,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
        "title": "Splitting a dataframe into separate CSV files",
        "body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "answer_body": "<p>Why not just groupby <code>Column1</code> and save each group?</p>\n\n<pre><code>df = df.sort_values('Column1').assign(NewColumn='NewColumnValue')\nprint(df)\n\n   Column1  Column2       NewColumn\n0        1    93644  NewColumnValue\n5        1    19593  NewColumnValue\n6        1    12707  NewColumnValue\n1        2    63246  NewColumnValue\n7        2    53480  NewColumnValue\n2        3    47790  NewColumnValue\n3        3    39644  NewColumnValue\n4        3    32585  NewColumnValue\n</code></pre>\n\n<hr>\n\n<pre><code>for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n</code></pre>\n\n<p>Thanks to Unatiel for the <a href=\"https://stackoverflow.com/questions/46124699/splitting-csv-based-on-content-of-one-column-after-having-done-some-other-manipu/46125692?noredirect=1#comment79214220_46125692\">improvement</a>. <code>header=False</code> will not write headers and <code>index_label=False</code> will not write an index column.</p>\n\n<p>This creates 3 files:</p>\n\n<pre><code>1.csv\n2.csv\n3.csv\n</code></pre>\n\n<p>Each having data corresponding to each <code>Column1</code> group.</p>\n",
        "question_body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "formatted_input": {
            "qid": 46124699,
            "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
            "question": {
                "title": "Splitting a dataframe into separate CSV files",
                "ques_desc": "I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. "
            },
            "io": [
                "+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n",
                "+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n"
            ],
            "answer": {
                "ans_desc": "Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. ",
                "code": [
                    "for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 9235000,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1652134728176732/picture?type=large",
            "display_name": "Amruth Anand",
            "link": "https://stackoverflow.com/users/9235000/amruth-anand"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 67919490,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623322230,
        "creation_date": 1623321043,
        "question_id": 67919385,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
        "title": "Find unique column values out of two different Dataframes",
        "body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "answer_body": "<p>TRY:</p>\n<pre><code>unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n</code></pre>\n<p><em>NOTE</em> : Replace <code>0</code> in <code>subset= [0]</code> with the first column name.</p>\n",
        "question_body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "formatted_input": {
            "qid": 67919385,
            "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
            "question": {
                "title": "Find unique column values out of two different Dataframes",
                "ques_desc": "How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read "
            },
            "io": [
                "67      Hij\n14      Xyz \n87      Pqr\n",
                "43      Def\n67      Lmn\n14      Xyz\n"
            ],
            "answer": {
                "ans_desc": "TRY: NOTE : Replace in with the first column name. ",
                "code": [
                    "unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 67917741,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623316126,
        "creation_date": 1623314044,
        "last_edit_date": 1623314922,
        "question_id": 67917573,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
        "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
        "body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "answer_body": "<p>Here is one way:</p>\n<pre><code>minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, &quot;-&quot;)\n</code></pre>\n<p>where we first flip the <code>df</code> over the columns, look where it is not <code>NaN</code> and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put <code>&quot;-&quot;</code> so we use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html\" rel=\"nofollow noreferrer\"><code>mask</code></a> method to put minus signs there,</p>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n    L1   D1   L2   D2 L3\n0  1.0  ABC  1.1  4.1  -\n1  NaN  NaN  1.7    -  -\n2  NaN  4.1    -    -  -\n3  NaN  1.8  3.2  PQR  -\n4  NaN  NaN  1.6    -  -\n</code></pre>\n",
        "question_body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "formatted_input": {
            "qid": 67917573,
            "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
            "question": {
                "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
                "ques_desc": "I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! "
            },
            "io": [
                " L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n",
                " L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n"
            ],
            "answer": {
                "ans_desc": "Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get ",
                "code": [
                    "minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, \"-\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 67910764,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1623312939,
        "creation_date": 1623267501,
        "question_id": 67910688,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
        "title": "move column above and delete rows in pandas python dataframe",
        "body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "answer_body": "<p>You can shift back each column by the number of preceding missing values which is found with <code>first_valid_index</code>:</p>\n<pre><code>df.apply(lambda s: s.shift(-s.first_valid_index()))\n</code></pre>\n<p>to get</p>\n<pre><code>     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1  NaN  NaN  c.2  d.2  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n</code></pre>\n<p>To drop the rows full of <code>NaN</code>s and fill the rest with empty string:</p>\n<pre><code>out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=&quot;all&quot;)\n         .fillna(&quot;&quot;))\n</code></pre>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1            c.2  d.2\n</code></pre>\n<hr>\n<p>note: this assumes your index is <code>0..N-1</code>; so if it's not, you can store it beforehand and then restore back:</p>\n<pre><code>index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=&quot;all&quot;)\n        .fillna(&quot;&quot;))\ndf.index = index[:len(df)]\n</code></pre>\n<hr>\n<p>To make the pulling up specific to some columns:</p>\n<pre><code>def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n</code></pre>\n",
        "question_body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "formatted_input": {
            "qid": 67910688,
            "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
            "question": {
                "title": "move column above and delete rows in pandas python dataframe",
                "ques_desc": "I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be "
            },
            "io": [
                "A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n",
                "A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n"
            ],
            "answer": {
                "ans_desc": "You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: ",
                "code": [
                    "out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=\"all\")\n         .fillna(\"\"))\n",
                    "index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=\"all\")\n        .fillna(\"\"))\ndf.index = index[:len(df)]\n",
                    "def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 166,
            "user_id": 16154762,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgA5126u43jbFmhmVe4Tv7Rq8KecxLwFi0ibsEU=k-s128",
            "display_name": "Vivek Singh",
            "link": "https://stackoverflow.com/users/16154762/vivek-singh"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 67905751,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1623247633,
        "creation_date": 1623247407,
        "last_edit_date": 1623247633,
        "question_id": 67905723,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
        "title": "Replicating the DataFrame row in a special manner",
        "body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "answer_body": "<p>Try with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html#pandas-series-str-split\" rel=\"noreferrer\"><code>str.split</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html#pandas-dataframe-explode\" rel=\"noreferrer\"><code>DataFrame.explode</code></a>:</p>\n<pre><code>df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n</code></pre>\n<pre><code>  col1      mob_no  col3\n0    a  9382949201    45\n0    a  3245622535    45\n1    b  8383459345    67\n1    b  4325562678    67\n2    c  8976247543    89\n2    c  1827472398    89\n3    d  7844329432    09\n</code></pre>\n",
        "question_body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "formatted_input": {
            "qid": 67905723,
            "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
            "question": {
                "title": "Replicating the DataFrame row in a special manner",
                "ques_desc": "I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: "
            },
            "io": [
                "col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n",
                "col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n"
            ],
            "answer": {
                "ans_desc": "Try with + : ",
                "code": [
                    "df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "dictionary"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9168000,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4f8b0ca12d7cf4951fffba8f055251cb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Manglu",
            "link": "https://stackoverflow.com/users/9168000/manglu"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67882312,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623132901,
        "creation_date": 1623131314,
        "question_id": 67882067,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
        "title": "Dictionary making for a transportation model from a Dataframe",
        "body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "answer_body": "<pre><code>df1 = df.set_index([&quot;Unnamed: 0&quot;, &quot;Unnamed: 1&quot;])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[&quot;demand&quot;].T.squeeze().dropna().iteritems())\nM = dict(plants[&quot;capacity&quot;].iteritems())\nI = list(plants.drop(columns=&quot;capacity&quot;).columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=&quot;capacity&quot;).stack().iteritems())\n</code></pre>\n<pre><code>&gt;&gt;&gt; d\n{'c1': 80.0, 'c2': 270.0, 'c3': 250.0, 'c4': 160.0, 'c5': 180.0}\n\n&gt;&gt;&gt; M\n{'p1': 500.0, 'p2': 500.0, 'p3': 500.0}\n\n&gt;&gt;&gt; I\n['c1', 'c2', 'c3', 'c4', 'c5']\n\n&gt;&gt;&gt; J\n['p1', 'p2', 'p3']\n\n&gt;&gt;&gt; cost\n{('p1', 'c1'): 4,\n ('p1', 'c2'): 5,\n ('p1', 'c3'): 6,\n ('p1', 'c4'): 8,\n ('p1', 'c5'): 10,\n ('p2', 'c1'): 6,\n ('p2', 'c2'): 4,\n ('p2', 'c3'): 3,\n ('p2', 'c4'): 5,\n ('p2', 'c5'): 8,\n ('p3', 'c1'): 9,\n ('p3', 'c2'): 7,\n ('p3', 'c3'): 4,\n ('p3', 'c4'): 2,\n ('p3', 'c5'): 4}\n</code></pre>\n",
        "question_body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "formatted_input": {
            "qid": 67882067,
            "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
            "question": {
                "title": "Dictionary making for a transportation model from a Dataframe",
                "ques_desc": "I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. "
            },
            "io": [
                " d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n",
                "  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "df1 = df.set_index([\"Unnamed: 0\", \"Unnamed: 1\"])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[\"demand\"].T.squeeze().dropna().iteritems())\nM = dict(plants[\"capacity\"].iteritems())\nI = list(plants.drop(columns=\"capacity\").columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=\"capacity\").stack().iteritems())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "vectorization"
        ],
        "owner": {
            "reputation": 1433,
            "user_id": 4263878,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/c95a66d30180a95353e7e655d0020f51?s=128&d=identicon&r=PG&f=1",
            "display_name": "MJS",
            "link": "https://stackoverflow.com/users/4263878/mjs"
        },
        "is_answered": true,
        "view_count": 3872,
        "accepted_answer_id": 33130915,
        "answer_count": 2,
        "score": 7,
        "last_activity_date": 1623919516,
        "creation_date": 1444839526,
        "last_edit_date": 1444840857,
        "question_id": 33130586,
        "content_license": "CC BY-SA 3.0",
        "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
        "title": "python pandas - creating a column which keeps a running count of consecutive values",
        "body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "answer_body": "<p>You can use the compare-cumsum-groupby pattern (which I <em>really</em> need to getting around to writing up for the documentation), with a final <code>cumcount</code>:</p>\n\n<pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"binary\": [0,1,1,1,0,0,1,1,0]})\n&gt;&gt;&gt; df[\"consec\"] = df[\"binary\"].groupby((df[\"binary\"] == 0).cumsum()).cumcount()\n&gt;&gt;&gt; df\n   binary  consec\n0       0       0\n1       1       1\n2       1       2\n3       1       3\n4       0       0\n5       0       0\n6       1       1\n7       1       2\n8       0       0\n</code></pre>\n\n<hr>\n\n<p>This works because first we get the positions where we want to reset the counter:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n</code></pre>\n\n<p>The cumulative sum of these gives us a different id for each group:</p>\n\n<pre><code>&gt;&gt;&gt; (df[\"binary\"] == 0).cumsum()\n0    1\n1    1\n2    1\n3    1\n4    2\n5    3\n6    3\n7    3\n8    4\nName: binary, dtype: int64\n</code></pre>\n\n<p>And then we can pass this to <code>groupby</code> and use <code>cumcount</code> to get an increasing index in each group.</p>\n",
        "question_body": "<p>I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like:</p>\n\n<pre><code>.    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n</code></pre>\n\n<p>However, this...</p>\n\n<pre><code>df['consec'][df['binary']==1] = df['consec'].shift(1) + df['binary']\n</code></pre>\n\n<p>results in this...</p>\n\n<pre><code>.  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n</code></pre>\n\n<p>I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help.</p>\n",
        "formatted_input": {
            "qid": 33130586,
            "link": "https://stackoverflow.com/questions/33130586/python-pandas-creating-a-column-which-keeps-a-running-count-of-consecutive-val",
            "question": {
                "title": "python pandas - creating a column which keeps a running count of consecutive values",
                "ques_desc": "I am trying to create a column (\u201cconsec\u201d) which will keep a running count of consecutive values in another (\u201cbinary\u201d) without using loop. This is what the desired outcome would look like: However, this... results in this... I see other posts which use grouping or sorting, but unfortunately, I don't see how that could work for me. Thanks in advance for your help. "
            },
            "io": [
                ".    binary consec\n1       0      0\n2       1      1\n3       1      2\n4       1      3\n5       1      4\n5       0      0\n6       1      1\n7       1      2\n8       0      0\n",
                ".  binary   consec\n0     1       NaN\n1     1       1\n2     1       1\n3     0       0\n4     1       1\n5     0       0\n6     1       1\n7     1       1\n8     1       1\n9     0       0\n"
            ],
            "answer": {
                "ans_desc": "You can use the compare-cumsum-groupby pattern (which I really need to getting around to writing up for the documentation), with a final : This works because first we get the positions where we want to reset the counter: The cumulative sum of these gives us a different id for each group: And then we can pass this to and use to get an increasing index in each group. ",
                "code": [
                    ">>> (df[\"binary\"] == 0)\n0     True\n1    False\n2    False\n3    False\n4     True\n5     True\n6    False\n7    False\n8     True\nName: binary, dtype: bool\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "pyspark-dataframes"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9735423,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
            "display_name": "learningtocode",
            "link": "https://stackoverflow.com/users/9735423/learningtocode"
        },
        "is_answered": true,
        "view_count": 575,
        "accepted_answer_id": 62099641,
        "answer_count": 2,
        "score": 1,
        "last_activity_date": 1623911798,
        "creation_date": 1590822619,
        "last_edit_date": 1590926848,
        "question_id": 62099066,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
        "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
        "body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "answer_body": "<p>Here you go:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n</code></pre>\n\n<p>Check <code>output.xlsx</code> for the output.</p>\n",
        "question_body": "<p>I am not good in python please forgive me for this question but I need to create a function which does the following thing:</p>\n\n<ol>\n<li>Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name.</li>\n<li>The columns' values should be concatenated and checked if there is no duplicate value.</li>\n<li>if the concat value has a duplicate then it should be told as yes/No in another column.</li>\n<li>all the dataframes then should be written into a single workbook as different worksheets inside.\nvalues inside () are columns for better understanding</li>\n</ol>\n\n<p>example:</p>\n\n<p>sheet1</p>\n\n<pre><code>(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(c) (d) (concate) (is duplicate)\nc1  d1  c1_d1     no\nc2  d2  c2_d2     no\n</code></pre>\n\n<p>sheet2</p>\n\n<pre><code>(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>(e) (f) (concat) (has duplicate)\ne1 f1 e1_f1 yes\ne2 f2 e2_f2 no\ne4 f4 e4_f4 no\ne4 f5 e4_f5 no\n</code></pre>\n",
        "formatted_input": {
            "qid": 62099066,
            "link": "https://stackoverflow.com/questions/62099066/is-there-a-way-to-read-multiple-excel-tab-sheets-from-single-xlsx-to-multiple-da",
            "question": {
                "title": "is there a way to read multiple excel tab/sheets from single xlsx to multiple dataframes with each dataframe named with sheet name?",
                "ques_desc": "I am not good in python please forgive me for this question but I need to create a function which does the following thing: Create multiple data frames from multiple excel tab/sheet present in a single xlsx file and be named on the sheet name. The columns' values should be concatenated and checked if there is no duplicate value. if the concat value has a duplicate then it should be told as yes/No in another column. all the dataframes then should be written into a single workbook as different worksheets inside. values inside () are columns for better understanding example: sheet1 result: sheet2 result: "
            },
            "io": [
                "(a) (b) (c) (d)\na1  b1  c1  d1\na2  b2  c2  d2\n",
                "(a) (b) (e) (f)\na3  b3  e1  f1\na4  b4  e1  f1\na5  b5  e2  f2\na6  b6  e4  f4\na7  a8  e4  f5\n"
            ],
            "answer": {
                "ans_desc": "Here you go: Check for the output. ",
                "code": [
                    "import pandas as pd\nfrom pandas import ExcelWriter\n\ndef detect_duplicate(group):\n    group['is_duplicate'] = ['No'] + ['Yes'] * (len(group) - 1)\n    return group\n\nwith ExcelWriter('output.xlsx') as output:\n    for sheet_name, df in pd.read_excel('input.xlsx', sheet_name=None).items():\n        df = df.drop(['a', 'b'], axis=1)\n        df['concat'] = df.apply(lambda row: '_'.join(row), axis=1)\n        df = df.groupby(['concat']).apply(detect_duplicate)\n        df = df.drop_duplicates(keep='last', subset=['concat'])\n        df.to_excel(output, sheet_name=sheet_name, index=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 10310,
            "user_id": 1334761,
            "user_type": "registered",
            "accept_rate": 62,
            "profile_image": "https://www.gravatar.com/avatar/e98df35e5a847e4adee98cb544e5bf6e?s=128&d=identicon&r=PG",
            "display_name": "Jjang",
            "link": "https://stackoverflow.com/users/1334761/jjang"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 67990087,
        "answer_count": 5,
        "score": 1,
        "last_activity_date": 1623830508,
        "creation_date": 1623772927,
        "last_edit_date": 1623830508,
        "question_id": 67989744,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
        "title": "Pandas replacing values in a column by values in another column",
        "body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "answer_body": "<p>Have a look at Jeremy Z answer on this post, for further explanation on solution <a href=\"https://stackoverflow.com/a/55631906/16235276\">https://stackoverflow.com/a/55631906/16235276</a></p>\n<pre><code>df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n</code></pre>\n",
        "question_body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "formatted_input": {
            "qid": 67989744,
            "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
            "question": {
                "title": "Pandas replacing values in a column by values in another column",
                "ques_desc": "Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: "
            },
            "io": [
                "    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n",
                "    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n"
            ],
            "answer": {
                "ans_desc": "Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 ",
                "code": [
                    "df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16117705,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b2f44431a658e5622efe2e5d2d6a84b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "WorkingPerson",
            "link": "https://stackoverflow.com/users/16117705/workingperson"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67987417,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623764393,
        "creation_date": 1623761108,
        "last_edit_date": 1623763345,
        "question_id": 67986537,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
        "title": "How do I check for conflict between columns in a pandas dataframe?",
        "body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "answer_body": "<p>IIUC, try:</p>\n<pre><code>df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      Yes\n1  Item2      6.0      7.0        5      Yes\n2  Item3      NaN      NaN        5      nan\n3  Item4      5.0      5.0        5      nan\n4  Item5      5.0      NaN        5      nan\n</code></pre>\n",
        "question_body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "formatted_input": {
            "qid": 67986537,
            "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
            "question": {
                "title": "How do I check for conflict between columns in a pandas dataframe?",
                "ques_desc": "I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. "
            },
            "io": [
                "    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n",
                "    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n"
            ],
            "answer": {
                "ans_desc": "IIUC, try: Output: ",
                "code": [
                    "df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 117,
            "user_id": 8581989,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/S7DTo.jpg?s=128&g=1",
            "display_name": "Steve Dallas",
            "link": "https://stackoverflow.com/users/8581989/steve-dallas"
        },
        "is_answered": true,
        "view_count": 11295,
        "accepted_answer_id": 46125692,
        "answer_count": 3,
        "score": 8,
        "last_activity_date": 1623741805,
        "creation_date": 1504905625,
        "last_edit_date": 1527649748,
        "question_id": 46124699,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
        "title": "Splitting a dataframe into separate CSV files",
        "body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "answer_body": "<p>Why not just groupby <code>Column1</code> and save each group?</p>\n\n<pre><code>df = df.sort_values('Column1').assign(NewColumn='NewColumnValue')\nprint(df)\n\n   Column1  Column2       NewColumn\n0        1    93644  NewColumnValue\n5        1    19593  NewColumnValue\n6        1    12707  NewColumnValue\n1        2    63246  NewColumnValue\n7        2    53480  NewColumnValue\n2        3    47790  NewColumnValue\n3        3    39644  NewColumnValue\n4        3    32585  NewColumnValue\n</code></pre>\n\n<hr>\n\n<pre><code>for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n</code></pre>\n\n<p>Thanks to Unatiel for the <a href=\"https://stackoverflow.com/questions/46124699/splitting-csv-based-on-content-of-one-column-after-having-done-some-other-manipu/46125692?noredirect=1#comment79214220_46125692\">improvement</a>. <code>header=False</code> will not write headers and <code>index_label=False</code> will not write an index column.</p>\n\n<p>This creates 3 files:</p>\n\n<pre><code>1.csv\n2.csv\n3.csv\n</code></pre>\n\n<p>Each having data corresponding to each <code>Column1</code> group.</p>\n",
        "question_body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "formatted_input": {
            "qid": 46124699,
            "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
            "question": {
                "title": "Splitting a dataframe into separate CSV files",
                "ques_desc": "I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. "
            },
            "io": [
                "+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n",
                "+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n"
            ],
            "answer": {
                "ans_desc": "Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. ",
                "code": [
                    "for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 9235000,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1652134728176732/picture?type=large",
            "display_name": "Amruth Anand",
            "link": "https://stackoverflow.com/users/9235000/amruth-anand"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 67919490,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623322230,
        "creation_date": 1623321043,
        "question_id": 67919385,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
        "title": "Find unique column values out of two different Dataframes",
        "body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "answer_body": "<p>TRY:</p>\n<pre><code>unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n</code></pre>\n<p><em>NOTE</em> : Replace <code>0</code> in <code>subset= [0]</code> with the first column name.</p>\n",
        "question_body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "formatted_input": {
            "qid": 67919385,
            "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
            "question": {
                "title": "Find unique column values out of two different Dataframes",
                "ques_desc": "How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read "
            },
            "io": [
                "67      Hij\n14      Xyz \n87      Pqr\n",
                "43      Def\n67      Lmn\n14      Xyz\n"
            ],
            "answer": {
                "ans_desc": "TRY: NOTE : Replace in with the first column name. ",
                "code": [
                    "unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 67917741,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623316126,
        "creation_date": 1623314044,
        "last_edit_date": 1623314922,
        "question_id": 67917573,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
        "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
        "body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "answer_body": "<p>Here is one way:</p>\n<pre><code>minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, &quot;-&quot;)\n</code></pre>\n<p>where we first flip the <code>df</code> over the columns, look where it is not <code>NaN</code> and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put <code>&quot;-&quot;</code> so we use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html\" rel=\"nofollow noreferrer\"><code>mask</code></a> method to put minus signs there,</p>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n    L1   D1   L2   D2 L3\n0  1.0  ABC  1.1  4.1  -\n1  NaN  NaN  1.7    -  -\n2  NaN  4.1    -    -  -\n3  NaN  1.8  3.2  PQR  -\n4  NaN  NaN  1.6    -  -\n</code></pre>\n",
        "question_body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "formatted_input": {
            "qid": 67917573,
            "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
            "question": {
                "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
                "ques_desc": "I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! "
            },
            "io": [
                " L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n",
                " L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n"
            ],
            "answer": {
                "ans_desc": "Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get ",
                "code": [
                    "minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, \"-\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 67910764,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1623312939,
        "creation_date": 1623267501,
        "question_id": 67910688,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
        "title": "move column above and delete rows in pandas python dataframe",
        "body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "answer_body": "<p>You can shift back each column by the number of preceding missing values which is found with <code>first_valid_index</code>:</p>\n<pre><code>df.apply(lambda s: s.shift(-s.first_valid_index()))\n</code></pre>\n<p>to get</p>\n<pre><code>     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1  NaN  NaN  c.2  d.2  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n</code></pre>\n<p>To drop the rows full of <code>NaN</code>s and fill the rest with empty string:</p>\n<pre><code>out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=&quot;all&quot;)\n         .fillna(&quot;&quot;))\n</code></pre>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1            c.2  d.2\n</code></pre>\n<hr>\n<p>note: this assumes your index is <code>0..N-1</code>; so if it's not, you can store it beforehand and then restore back:</p>\n<pre><code>index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=&quot;all&quot;)\n        .fillna(&quot;&quot;))\ndf.index = index[:len(df)]\n</code></pre>\n<hr>\n<p>To make the pulling up specific to some columns:</p>\n<pre><code>def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n</code></pre>\n",
        "question_body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "formatted_input": {
            "qid": 67910688,
            "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
            "question": {
                "title": "move column above and delete rows in pandas python dataframe",
                "ques_desc": "I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be "
            },
            "io": [
                "A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n",
                "A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n"
            ],
            "answer": {
                "ans_desc": "You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: ",
                "code": [
                    "out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=\"all\")\n         .fillna(\"\"))\n",
                    "index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=\"all\")\n        .fillna(\"\"))\ndf.index = index[:len(df)]\n",
                    "def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 166,
            "user_id": 16154762,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgA5126u43jbFmhmVe4Tv7Rq8KecxLwFi0ibsEU=k-s128",
            "display_name": "Vivek Singh",
            "link": "https://stackoverflow.com/users/16154762/vivek-singh"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 67905751,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1623247633,
        "creation_date": 1623247407,
        "last_edit_date": 1623247633,
        "question_id": 67905723,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
        "title": "Replicating the DataFrame row in a special manner",
        "body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "answer_body": "<p>Try with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html#pandas-series-str-split\" rel=\"noreferrer\"><code>str.split</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html#pandas-dataframe-explode\" rel=\"noreferrer\"><code>DataFrame.explode</code></a>:</p>\n<pre><code>df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n</code></pre>\n<pre><code>  col1      mob_no  col3\n0    a  9382949201    45\n0    a  3245622535    45\n1    b  8383459345    67\n1    b  4325562678    67\n2    c  8976247543    89\n2    c  1827472398    89\n3    d  7844329432    09\n</code></pre>\n",
        "question_body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "formatted_input": {
            "qid": 67905723,
            "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
            "question": {
                "title": "Replicating the DataFrame row in a special manner",
                "ques_desc": "I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: "
            },
            "io": [
                "col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n",
                "col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n"
            ],
            "answer": {
                "ans_desc": "Try with + : ",
                "code": [
                    "df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "dictionary"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9168000,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4f8b0ca12d7cf4951fffba8f055251cb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Manglu",
            "link": "https://stackoverflow.com/users/9168000/manglu"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67882312,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623132901,
        "creation_date": 1623131314,
        "question_id": 67882067,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
        "title": "Dictionary making for a transportation model from a Dataframe",
        "body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "answer_body": "<pre><code>df1 = df.set_index([&quot;Unnamed: 0&quot;, &quot;Unnamed: 1&quot;])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[&quot;demand&quot;].T.squeeze().dropna().iteritems())\nM = dict(plants[&quot;capacity&quot;].iteritems())\nI = list(plants.drop(columns=&quot;capacity&quot;).columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=&quot;capacity&quot;).stack().iteritems())\n</code></pre>\n<pre><code>&gt;&gt;&gt; d\n{'c1': 80.0, 'c2': 270.0, 'c3': 250.0, 'c4': 160.0, 'c5': 180.0}\n\n&gt;&gt;&gt; M\n{'p1': 500.0, 'p2': 500.0, 'p3': 500.0}\n\n&gt;&gt;&gt; I\n['c1', 'c2', 'c3', 'c4', 'c5']\n\n&gt;&gt;&gt; J\n['p1', 'p2', 'p3']\n\n&gt;&gt;&gt; cost\n{('p1', 'c1'): 4,\n ('p1', 'c2'): 5,\n ('p1', 'c3'): 6,\n ('p1', 'c4'): 8,\n ('p1', 'c5'): 10,\n ('p2', 'c1'): 6,\n ('p2', 'c2'): 4,\n ('p2', 'c3'): 3,\n ('p2', 'c4'): 5,\n ('p2', 'c5'): 8,\n ('p3', 'c1'): 9,\n ('p3', 'c2'): 7,\n ('p3', 'c3'): 4,\n ('p3', 'c4'): 2,\n ('p3', 'c5'): 4}\n</code></pre>\n",
        "question_body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "formatted_input": {
            "qid": 67882067,
            "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
            "question": {
                "title": "Dictionary making for a transportation model from a Dataframe",
                "ques_desc": "I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. "
            },
            "io": [
                " d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n",
                "  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "df1 = df.set_index([\"Unnamed: 0\", \"Unnamed: 1\"])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[\"demand\"].T.squeeze().dropna().iteritems())\nM = dict(plants[\"capacity\"].iteritems())\nI = list(plants.drop(columns=\"capacity\").columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=\"capacity\").stack().iteritems())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 352,
            "user_id": 8147508,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dbf0ab4fedaca38f5f4ae3c2344f9b95?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dev Ananth",
            "link": "https://stackoverflow.com/users/8147508/dev-ananth"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67871572,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1623073970,
        "creation_date": 1623063317,
        "last_edit_date": 1623065349,
        "question_id": 67870323,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
        "title": "How to get list of previous n values of a column conditionally in DataFrame?",
        "body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "answer_body": "<p>Since rolling only supports production of numeric values, this has to be a work around.</p>\n<p>Try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html#pandas-dataframe-sort-values\" rel=\"nofollow noreferrer\"><code>sort_values</code></a> first then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html\" rel=\"nofollow noreferrer\"><code>groupby rolling</code></a> on window + 1 and strip off the last element:</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n5         1      5         [15]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n11        1     12   [5, 20, 8]\n1         2      0           []\n3         2     30          [0]\n7         2      7      [0, 30]\n10        2      9   [0, 30, 7]\n2         3     18           []\n4         3     17         [18]\n6         4      9           []\n</code></pre>\n<p>Then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html#pandas-dataframe-sort-index\" rel=\"nofollow noreferrer\"><code>sort_index</code></a> to restore the initial order:</p>\n<pre><code>df = df.sort_index()\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [0, 30]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n10        2      9   [0, 30, 7]\n11        1     12   [5, 20, 8]\n</code></pre>\n<p>(Optional use extended slicing to reverse the lists and get elements in same order as expected output above):</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [30, 0]\n8         1     20      [5, 15]\n9         1      8  [20, 5, 15]\n10        2      9   [7, 30, 0]\n11        1     12   [8, 20, 5]\n</code></pre>\n<hr/>\n<p>Complete Working Example:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'Subject': [1, 2, 3, 2, 3, 1, 4, 2, 1, 1, 2, 1],\n    'Score': [15, 0, 18, 30, 17, 5, 9, 7, 20, 8, 9, 12]\n})\n\nwindow = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\ndf = df.sort_index()\n\nprint(df)\n</code></pre>\n",
        "question_body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "formatted_input": {
            "qid": 67870323,
            "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
            "question": {
                "title": "How to get list of previous n values of a column conditionally in DataFrame?",
                "ques_desc": "My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? "
            },
            "io": [
                "Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n",
                "Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n"
            ],
            "answer": {
                "ans_desc": "Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: ",
                "code": [
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n",
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "indexing"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 8970043,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/uL3Er.jpg?s=128&g=1",
            "display_name": "Futurex",
            "link": "https://stackoverflow.com/users/8970043/futurex"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 67870693,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623064916,
        "creation_date": 1623064484,
        "question_id": 67870585,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
        "title": "Python dataframe create index column based on other id column",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "answer_body": "<p>Try <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.ngroup.html#pandas-core-groupby-groupby-ngroup\" rel=\"nofollow noreferrer\"><code>groupby ngroup</code></a> + 1 :</p>\n<pre><code>df['ID_2'] = df.groupby('ID').ngroup() + 1\n</code></pre>\n<p>Or with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rank.html#pandas-series-rank\" rel=\"nofollow noreferrer\"><code>Rank</code></a>:</p>\n<pre><code>df['ID_2'] = df['ID'].rank(method='dense').astype(int)\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>                 ID   Price  ID_2\n0  000afb96ded6677c  1514.5     1\n1  000afb96ded6677c    13.0     1\n2  000afb96ded6677c   611.0     1\n3  000afb96ded6677c   723.0     1\n4  000afb96ded6677c  2065.0     1\n5  ffea14e87a4e1269  2286.0     2\n6  ffea14e87a4e1269  1150.0     2\n7  ffea14e87a4e1269    80.0     2\n8  fff455057ad492da   650.0     3\n9  fff5fc66c1fd66c2   450.0     4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67870585,
            "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
            "question": {
                "title": "Python dataframe create index column based on other id column",
                "ques_desc": "I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: "
            },
            "io": [
                "ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n",
                "ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n"
            ],
            "answer": {
                "ans_desc": "Try + 1 : Or with : : ",
                "code": [
                    "df['ID_2'] = df.groupby('ID').ngroup() + 1\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16132607,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/234e69866fa5f5a2e96b28a4fd12597b?s=128&d=identicon&r=PG&f=1",
            "display_name": "homeboykeroro",
            "link": "https://stackoverflow.com/users/16132607/homeboykeroro"
        },
        "is_answered": true,
        "view_count": 85,
        "accepted_answer_id": 67857571,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1622976694,
        "creation_date": 1622966815,
        "last_edit_date": 1622970680,
        "question_id": 67856992,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
        "title": "Element wise numeric comparison in Pandas dataframe column value with list",
        "body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "answer_body": "<p>Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise).</p>\n<p>You can use <code>apply</code>:</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n</code></pre>\n<hr />\n<pre><code>res = df2.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<p><strong>res:</strong></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\">A</th>\n<th style=\"text-align: center;\">B</th>\n<th style=\"text-align: center;\">C</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\"></td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">2</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[False, False, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n</tr>\n</tbody>\n</table>\n</div><h2>Update</h2>\n<p>(Complete Solution Based on the data you've provided):</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<h2>Time Comparison:</h2>\n<p><strong>Method 1 (Nk03's method1):</strong></p>\n<blockquote>\n<p>CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms</p>\n</blockquote>\n<p><strong>Method 2 (Nk03's method2):</strong></p>\n<blockquote>\n<p>CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms</p>\n</blockquote>\n<p><strong>Method 3 (Using numpy based comparison):</strong></p>\n<blockquote>\n<p>CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms</p>\n</blockquote>\n<p><strong>Nk03's Updated and Optimized Solution:</strong></p>\n<blockquote>\n<p>CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms</p>\n</blockquote>\n",
        "question_body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "formatted_input": {
            "qid": 67856992,
            "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
            "question": {
                "title": "Element wise numeric comparison in Pandas dataframe column value with list",
                "ques_desc": "I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: "
            },
            "io": [
                "  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n",
                "  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n"
            ],
            "answer": {
                "ans_desc": "Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms ",
                "code": [
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n",
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 10566774,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-8T7iqZlWN8Y/AAAAAAAAAAI/AAAAAAAACu0/hiVsQlXBjzQ/photo.jpg?sz=128",
            "display_name": "Salvatore Nedia",
            "link": "https://stackoverflow.com/users/10566774/salvatore-nedia"
        },
        "is_answered": true,
        "view_count": 107,
        "accepted_answer_id": 67845512,
        "answer_count": 4,
        "score": 5,
        "last_activity_date": 1622859671,
        "creation_date": 1622854122,
        "last_edit_date": 1622854820,
        "question_id": 67845362,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
        "title": "Sort pandas df subset of rows (within a group) by specific column",
        "body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "answer_body": "<p>I think it should be as simple as this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.sort_values([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;])\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "formatted_input": {
            "qid": 67845362,
            "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
            "question": {
                "title": "Sort pandas df subset of rows (within a group) by specific column",
                "ques_desc": "I have the following dataframe let\u2019s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? "
            },
            "io": [
                "\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n",
                "\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n"
            ],
            "answer": {
                "ans_desc": "I think it should be as simple as this: ",
                "code": [
                    "df = df.sort_values([\"A\", \"B\", \"C\", \"D\"])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 12785115,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b3484fd202fb5624044a4ed7b5181fab?s=128&d=identicon&r=PG&f=1",
            "display_name": "RamboJ",
            "link": "https://stackoverflow.com/users/12785115/ramboj"
        },
        "is_answered": true,
        "view_count": 90,
        "accepted_answer_id": 61625210,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1622820639,
        "creation_date": 1588722398,
        "question_id": 61624957,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
        "title": "Creating a table in pandas from json",
        "body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "answer_body": "<p>You need to massage the data a little bit to get what you want.</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: x['website'][0])\n    .to_frame('website')\n)\n\n    website\n1   https://bitcoin.org/\n2   https://litecoin.org/\n</code></pre>\n\n<p>To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. </p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n</code></pre>\n\n<p>To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls'</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n</code></pre>\n",
        "question_body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 61624957,
            "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
            "question": {
                "title": "Creating a table in pandas from json",
                "ques_desc": "I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! "
            },
            "io": [
                "0     website\n0  1  NaN\n1  2  NaN \n",
                "0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n"
            ],
            "answer": {
                "ans_desc": "You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' ",
                "code": [
                    "(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n",
                    "(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 10310,
            "user_id": 1334761,
            "user_type": "registered",
            "accept_rate": 62,
            "profile_image": "https://www.gravatar.com/avatar/e98df35e5a847e4adee98cb544e5bf6e?s=128&d=identicon&r=PG",
            "display_name": "Jjang",
            "link": "https://stackoverflow.com/users/1334761/jjang"
        },
        "is_answered": true,
        "view_count": 68,
        "accepted_answer_id": 67990087,
        "answer_count": 5,
        "score": 1,
        "last_activity_date": 1623830508,
        "creation_date": 1623772927,
        "last_edit_date": 1623830508,
        "question_id": 67989744,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
        "title": "Pandas replacing values in a column by values in another column",
        "body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "answer_body": "<p>Have a look at Jeremy Z answer on this post, for further explanation on solution <a href=\"https://stackoverflow.com/a/55631906/16235276\">https://stackoverflow.com/a/55631906/16235276</a></p>\n<pre><code>df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n</code></pre>\n",
        "question_body": "<p>Let's say I have the following dataframe X (ppid is unique):</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n</code></pre>\n<p>I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids:</p>\n<pre><code>    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n</code></pre>\n<p>I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get:</p>\n<pre><code>    ppid  col2 ...\n1   'id1'  '5'\n2   'id2'  '6'\n3   'id3'  '3' # didn't change, as there's no match\n...\n</code></pre>\n",
        "formatted_input": {
            "qid": 67989744,
            "link": "https://stackoverflow.com/questions/67989744/pandas-replacing-values-in-a-column-by-values-in-another-column",
            "question": {
                "title": "Pandas replacing values in a column by values in another column",
                "ques_desc": "Let's say I have the following dataframe X (ppid is unique): I have another dataframe which serves as a mapping. ppid is same as above and unique, however it might not contain all X's ppids: I would like to use the mapping dataframe to switch col2 in dataframe X according to where the ppids are equal (in reality, they're multiple columns which are unique together), to get: "
            },
            "io": [
                "    ppid  col2 ...\n1   'id1'  '1'\n2   'id2'  '2'\n3   'id3'  '3'\n...\n",
                "    ppid  val\n1   'id1' '5'\n2   'id2' '6'\n"
            ],
            "answer": {
                "ans_desc": "Have a look at Jeremy Z answer on this post, for further explanation on solution https://stackoverflow.com/a/55631906/16235276 ",
                "code": [
                    "df1 = df1.set_index('ppid')\ndf2 = df2.set_index('ppid')\ndf1.update(df2)\ndf1.reset_index(inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16117705,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b2f44431a658e5622efe2e5d2d6a84b0?s=128&d=identicon&r=PG&f=1",
            "display_name": "WorkingPerson",
            "link": "https://stackoverflow.com/users/16117705/workingperson"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67987417,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623764393,
        "creation_date": 1623761108,
        "last_edit_date": 1623763345,
        "question_id": 67986537,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
        "title": "How do I check for conflict between columns in a pandas dataframe?",
        "body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "answer_body": "<p>IIUC, try:</p>\n<pre><code>df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      Yes\n1  Item2      6.0      7.0        5      Yes\n2  Item3      NaN      NaN        5      nan\n3  Item4      5.0      5.0        5      nan\n4  Item5      5.0      NaN        5      nan\n</code></pre>\n",
        "question_body": "<p>I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as:</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ninp = [\n    {&quot;Item&quot;: &quot;Item1&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: 6, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item2&quot;, &quot;Local A&quot;: 6, &quot;Local B&quot;: 7, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item3&quot;, &quot;Local A&quot;: np.nan, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item4&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: 5, &quot;Local C&quot;: 5},\n    {&quot;Item&quot;: &quot;Item5&quot;, &quot;Local A&quot;: 5, &quot;Local B&quot;: np.nan, &quot;Local C&quot;: 5},\n]\ndf = pd.DataFrame(inp)\nprint(df)\n\n</code></pre>\n<p>Output:</p>\n<pre><code>    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n</code></pre>\n<p>My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty).</p>\n<p>Ideal Output:</p>\n<pre><code>    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n</code></pre>\n<p>In order to do that I decided to build a filter that checks if the three sources are non-null and if  they are different.</p>\n<p>I built the filters for the three other cases consisting of two values being available for an index.</p>\n<pre><code>condition1 = (\n    df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()\n) &amp; ~(df[&quot;Local A&quot;] == df[&quot;Local B&quot;] == df[&quot;Local C&quot;])\n\ncondition2 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local B&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local B&quot;]\n)\n\ncondition3 = (df[&quot;Local B&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local B&quot;] == df[&quot;Local C&quot;]\n)\n\ncondition4 = (df[&quot;Local A&quot;].notnull() &amp; df[&quot;Local C&quot;].notnull()) &amp; ~(\n    df[&quot;Local A&quot;] == df[&quot;Local C&quot;]\n)\n\n\ndf.loc[condition1 | condition2 | condition3 | condition4, &quot;Conflict&quot;] = &quot;yes&quot;\n</code></pre>\n<p>This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script:</p>\n<p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>\n<p>I've seen this a few times and was able to find the cause, but I just can't figure this one out.\nIt seems that I'm comparing Bool series instead of individual cases like I want to.</p>\n",
        "formatted_input": {
            "qid": 67986537,
            "link": "https://stackoverflow.com/questions/67986537/how-do-i-check-for-conflict-between-columns-in-a-pandas-dataframe",
            "question": {
                "title": "How do I check for conflict between columns in a pandas dataframe?",
                "ques_desc": "I'm working on a Dataframe which contains multiple possible values from three different sources for a single item, which is in the index, such as: Output: My goal is to create a column which specifies if there is conflict between sources when there are multiple non-null values for an index (some cells are empty). Ideal Output: In order to do that I decided to build a filter that checks if the three sources are non-null and if they are different. I built the filters for the three other cases consisting of two values being available for an index. This solution of enumerating the different possible outcomes is not very elegant but I wasn't able to find a simpler alternative. Moreover, I get the following error while running the script: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). I've seen this a few times and was able to find the cause, but I just can't figure this one out. It seems that I'm comparing Bool series instead of individual cases like I want to. "
            },
            "io": [
                "    Item  Local A  Local B  Local C\n0  Item1      NaN      6.0        5\n1  Item2      6.0      7.0        5\n2  Item3      NaN      NaN        5\n3  Item4      5.0      5.0        5\n4  Item5      5.0      NaN        5\n",
                "    Item  Local A  Local B  Local C Conflict\n0  Item1      NaN      6.0        5      yes\n1  Item2      6.0      7.0        5      yes\n2  Item3      NaN      NaN        5      NaN\n3  Item4      5.0      5.0        5      NaN\n4  Item5      5.0      NaN        5      NaN\n"
            ],
            "answer": {
                "ans_desc": "IIUC, try: Output: ",
                "code": [
                    "df['Conflict'] = np.where((df.iloc[:, 1:].nunique(axis=1) != 1),'Yes',np.nan)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "group-by",
            "pandas-groupby"
        ],
        "owner": {
            "reputation": 117,
            "user_id": 8581989,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/S7DTo.jpg?s=128&g=1",
            "display_name": "Steve Dallas",
            "link": "https://stackoverflow.com/users/8581989/steve-dallas"
        },
        "is_answered": true,
        "view_count": 11295,
        "accepted_answer_id": 46125692,
        "answer_count": 3,
        "score": 8,
        "last_activity_date": 1623741805,
        "creation_date": 1504905625,
        "last_edit_date": 1527649748,
        "question_id": 46124699,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
        "title": "Splitting a dataframe into separate CSV files",
        "body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "answer_body": "<p>Why not just groupby <code>Column1</code> and save each group?</p>\n\n<pre><code>df = df.sort_values('Column1').assign(NewColumn='NewColumnValue')\nprint(df)\n\n   Column1  Column2       NewColumn\n0        1    93644  NewColumnValue\n5        1    19593  NewColumnValue\n6        1    12707  NewColumnValue\n1        2    63246  NewColumnValue\n7        2    53480  NewColumnValue\n2        3    47790  NewColumnValue\n3        3    39644  NewColumnValue\n4        3    32585  NewColumnValue\n</code></pre>\n\n<hr>\n\n<pre><code>for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n</code></pre>\n\n<p>Thanks to Unatiel for the <a href=\"https://stackoverflow.com/questions/46124699/splitting-csv-based-on-content-of-one-column-after-having-done-some-other-manipu/46125692?noredirect=1#comment79214220_46125692\">improvement</a>. <code>header=False</code> will not write headers and <code>index_label=False</code> will not write an index column.</p>\n\n<p>This creates 3 files:</p>\n\n<pre><code>1.csv\n2.csv\n3.csv\n</code></pre>\n\n<p>Each having data corresponding to each <code>Column1</code> group.</p>\n",
        "question_body": "<p>I have a fairly large csv, looking like this:</p>\n\n<pre><code>+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n</code></pre>\n\n<p>My intent is to </p>\n\n<ol>\n<li>Add a new column</li>\n<li>Insert a specific value into that column, 'NewColumnValue', on each row of the csv</li>\n<li>Sort the file based on the value in Column1</li>\n<li>Split the original CSV into new files based on the contents of 'Column1', removing the header</li>\n</ol>\n\n<p>For example, I want to end up with multiple files that look like:</p>\n\n<pre><code>+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n</code></pre>\n\n<p>I have managed to do this using separate .py files:</p>\n\n<p>Step1</p>\n\n<pre><code># -*- coding: utf-8 -*-\nimport pandas as pd\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\ndf.to_csv('ready.csv', index=False, header=False)\n</code></pre>\n\n<p>Step2</p>\n\n<pre><code>import csv\nfrom itertools import groupby\nfor key, rows in groupby(csv.reader(open(\"ready.csv\")),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>But I'd really like to learn how to accomplish everything in a single .py file.  I tried this: </p>\n\n<pre><code># -*- coding: utf-8 -*-\n#This processes a large CSV file.  \n#It will dd a new column, populate the new column with a uniform piece of data for each row, sort the CSV, and remove headers\n#Then it will split the single large CSV into multiple CSVs based on the value in column 0 \nimport pandas as pd\nimport csv\nfrom itertools import groupby\ndf = pd.read_csv('source.csv')\ndf = df.sort_values('Column1')\ndf['NewColumn'] = 'NewColumnValue'\nfor key, rows in groupby(csv.reader((df)),\n                         lambda row: row[0]):\n    with open(\"%s.csv\" % key, \"w\") as output:\n        for row in rows:\n            output.write(\",\".join(row) + \"\\n\")\n</code></pre>\n\n<p>but instead of working as intended, it's giving me multiple CSVs named after each column header.</p>\n\n<p>Is that happening because I removed the header row when I used separate .py files and I'm not doing it here?  I'm not really certain what operation I need to do when splitting the files to remove the header.</p>\n",
        "formatted_input": {
            "qid": 46124699,
            "link": "https://stackoverflow.com/questions/46124699/splitting-a-dataframe-into-separate-csv-files",
            "question": {
                "title": "Splitting a dataframe into separate CSV files",
                "ques_desc": "I have a fairly large csv, looking like this: My intent is to Add a new column Insert a specific value into that column, 'NewColumnValue', on each row of the csv Sort the file based on the value in Column1 Split the original CSV into new files based on the contents of 'Column1', removing the header For example, I want to end up with multiple files that look like: I have managed to do this using separate .py files: Step1 Step2 But I'd really like to learn how to accomplish everything in a single .py file. I tried this: but instead of working as intended, it's giving me multiple CSVs named after each column header. Is that happening because I removed the header row when I used separate .py files and I'm not doing it here? I'm not really certain what operation I need to do when splitting the files to remove the header. "
            },
            "io": [
                "+---------+---------+\n| Column1 | Column2 |\n+---------+---------+\n|       1 |   93644 |\n|       2 |   63246 |\n|       3 |   47790 |\n|       3 |   39644 |\n|       3 |   32585 |\n|       1 |   19593 |\n|       1 |   12707 |\n|       2 |   53480 |\n+---------+---------+\n",
                "+---+-------+----------------+\n| 1 | 19593 | NewColumnValue |\n| 1 | 93644 | NewColumnValue |\n| 1 | 12707 | NewColumnValue |\n+---+-------+----------------+\n\n+---+-------+-----------------+\n| 2 | 63246 | NewColumnValue |\n| 2 | 53480 | NewColumnValue |\n+---+-------+-----------------+\n\n+---+-------+-----------------+\n| 3 | 47790 | NewColumnValue |\n| 3 | 39644 | NewColumnValue |\n| 3 | 32585 | NewColumnValue |\n+---+-------+-----------------+\n"
            ],
            "answer": {
                "ans_desc": "Why not just groupby and save each group? Thanks to Unatiel for the improvement. will not write headers and will not write an index column. This creates 3 files: Each having data corresponding to each group. ",
                "code": [
                    "for i, g in df.groupby('Column1'):\n    g.to_csv('{}.csv'.format(i), header=False, index_label=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 9235000,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1652134728176732/picture?type=large",
            "display_name": "Amruth Anand",
            "link": "https://stackoverflow.com/users/9235000/amruth-anand"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 67919490,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623322230,
        "creation_date": 1623321043,
        "question_id": 67919385,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
        "title": "Find unique column values out of two different Dataframes",
        "body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "answer_body": "<p>TRY:</p>\n<pre><code>unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n</code></pre>\n<p><em>NOTE</em> : Replace <code>0</code> in <code>subset= [0]</code> with the first column name.</p>\n",
        "question_body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "formatted_input": {
            "qid": 67919385,
            "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
            "question": {
                "title": "Find unique column values out of two different Dataframes",
                "ques_desc": "How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read "
            },
            "io": [
                "67      Hij\n14      Xyz \n87      Pqr\n",
                "43      Def\n67      Lmn\n14      Xyz\n"
            ],
            "answer": {
                "ans_desc": "TRY: NOTE : Replace in with the first column name. ",
                "code": [
                    "unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 67917741,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623316126,
        "creation_date": 1623314044,
        "last_edit_date": 1623314922,
        "question_id": 67917573,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
        "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
        "body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "answer_body": "<p>Here is one way:</p>\n<pre><code>minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, &quot;-&quot;)\n</code></pre>\n<p>where we first flip the <code>df</code> over the columns, look where it is not <code>NaN</code> and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put <code>&quot;-&quot;</code> so we use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html\" rel=\"nofollow noreferrer\"><code>mask</code></a> method to put minus signs there,</p>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n    L1   D1   L2   D2 L3\n0  1.0  ABC  1.1  4.1  -\n1  NaN  NaN  1.7    -  -\n2  NaN  4.1    -    -  -\n3  NaN  1.8  3.2  PQR  -\n4  NaN  NaN  1.6    -  -\n</code></pre>\n",
        "question_body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "formatted_input": {
            "qid": 67917573,
            "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
            "question": {
                "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
                "ques_desc": "I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! "
            },
            "io": [
                " L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n",
                " L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n"
            ],
            "answer": {
                "ans_desc": "Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get ",
                "code": [
                    "minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, \"-\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 67910764,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1623312939,
        "creation_date": 1623267501,
        "question_id": 67910688,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
        "title": "move column above and delete rows in pandas python dataframe",
        "body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "answer_body": "<p>You can shift back each column by the number of preceding missing values which is found with <code>first_valid_index</code>:</p>\n<pre><code>df.apply(lambda s: s.shift(-s.first_valid_index()))\n</code></pre>\n<p>to get</p>\n<pre><code>     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1  NaN  NaN  c.2  d.2  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n</code></pre>\n<p>To drop the rows full of <code>NaN</code>s and fill the rest with empty string:</p>\n<pre><code>out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=&quot;all&quot;)\n         .fillna(&quot;&quot;))\n</code></pre>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1            c.2  d.2\n</code></pre>\n<hr>\n<p>note: this assumes your index is <code>0..N-1</code>; so if it's not, you can store it beforehand and then restore back:</p>\n<pre><code>index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=&quot;all&quot;)\n        .fillna(&quot;&quot;))\ndf.index = index[:len(df)]\n</code></pre>\n<hr>\n<p>To make the pulling up specific to some columns:</p>\n<pre><code>def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n</code></pre>\n",
        "question_body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "formatted_input": {
            "qid": 67910688,
            "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
            "question": {
                "title": "move column above and delete rows in pandas python dataframe",
                "ques_desc": "I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be "
            },
            "io": [
                "A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n",
                "A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n"
            ],
            "answer": {
                "ans_desc": "You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: ",
                "code": [
                    "out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=\"all\")\n         .fillna(\"\"))\n",
                    "index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=\"all\")\n        .fillna(\"\"))\ndf.index = index[:len(df)]\n",
                    "def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 166,
            "user_id": 16154762,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgA5126u43jbFmhmVe4Tv7Rq8KecxLwFi0ibsEU=k-s128",
            "display_name": "Vivek Singh",
            "link": "https://stackoverflow.com/users/16154762/vivek-singh"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 67905751,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1623247633,
        "creation_date": 1623247407,
        "last_edit_date": 1623247633,
        "question_id": 67905723,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
        "title": "Replicating the DataFrame row in a special manner",
        "body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "answer_body": "<p>Try with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html#pandas-series-str-split\" rel=\"noreferrer\"><code>str.split</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html#pandas-dataframe-explode\" rel=\"noreferrer\"><code>DataFrame.explode</code></a>:</p>\n<pre><code>df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n</code></pre>\n<pre><code>  col1      mob_no  col3\n0    a  9382949201    45\n0    a  3245622535    45\n1    b  8383459345    67\n1    b  4325562678    67\n2    c  8976247543    89\n2    c  1827472398    89\n3    d  7844329432    09\n</code></pre>\n",
        "question_body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "formatted_input": {
            "qid": 67905723,
            "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
            "question": {
                "title": "Replicating the DataFrame row in a special manner",
                "ques_desc": "I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: "
            },
            "io": [
                "col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n",
                "col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n"
            ],
            "answer": {
                "ans_desc": "Try with + : ",
                "code": [
                    "df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "dictionary"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9168000,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4f8b0ca12d7cf4951fffba8f055251cb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Manglu",
            "link": "https://stackoverflow.com/users/9168000/manglu"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67882312,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623132901,
        "creation_date": 1623131314,
        "question_id": 67882067,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
        "title": "Dictionary making for a transportation model from a Dataframe",
        "body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "answer_body": "<pre><code>df1 = df.set_index([&quot;Unnamed: 0&quot;, &quot;Unnamed: 1&quot;])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[&quot;demand&quot;].T.squeeze().dropna().iteritems())\nM = dict(plants[&quot;capacity&quot;].iteritems())\nI = list(plants.drop(columns=&quot;capacity&quot;).columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=&quot;capacity&quot;).stack().iteritems())\n</code></pre>\n<pre><code>&gt;&gt;&gt; d\n{'c1': 80.0, 'c2': 270.0, 'c3': 250.0, 'c4': 160.0, 'c5': 180.0}\n\n&gt;&gt;&gt; M\n{'p1': 500.0, 'p2': 500.0, 'p3': 500.0}\n\n&gt;&gt;&gt; I\n['c1', 'c2', 'c3', 'c4', 'c5']\n\n&gt;&gt;&gt; J\n['p1', 'p2', 'p3']\n\n&gt;&gt;&gt; cost\n{('p1', 'c1'): 4,\n ('p1', 'c2'): 5,\n ('p1', 'c3'): 6,\n ('p1', 'c4'): 8,\n ('p1', 'c5'): 10,\n ('p2', 'c1'): 6,\n ('p2', 'c2'): 4,\n ('p2', 'c3'): 3,\n ('p2', 'c4'): 5,\n ('p2', 'c5'): 8,\n ('p3', 'c1'): 9,\n ('p3', 'c2'): 7,\n ('p3', 'c3'): 4,\n ('p3', 'c4'): 2,\n ('p3', 'c5'): 4}\n</code></pre>\n",
        "question_body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "formatted_input": {
            "qid": 67882067,
            "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
            "question": {
                "title": "Dictionary making for a transportation model from a Dataframe",
                "ques_desc": "I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. "
            },
            "io": [
                " d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n",
                "  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "df1 = df.set_index([\"Unnamed: 0\", \"Unnamed: 1\"])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[\"demand\"].T.squeeze().dropna().iteritems())\nM = dict(plants[\"capacity\"].iteritems())\nI = list(plants.drop(columns=\"capacity\").columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=\"capacity\").stack().iteritems())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 352,
            "user_id": 8147508,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dbf0ab4fedaca38f5f4ae3c2344f9b95?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dev Ananth",
            "link": "https://stackoverflow.com/users/8147508/dev-ananth"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67871572,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1623073970,
        "creation_date": 1623063317,
        "last_edit_date": 1623065349,
        "question_id": 67870323,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
        "title": "How to get list of previous n values of a column conditionally in DataFrame?",
        "body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "answer_body": "<p>Since rolling only supports production of numeric values, this has to be a work around.</p>\n<p>Try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html#pandas-dataframe-sort-values\" rel=\"nofollow noreferrer\"><code>sort_values</code></a> first then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html\" rel=\"nofollow noreferrer\"><code>groupby rolling</code></a> on window + 1 and strip off the last element:</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n5         1      5         [15]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n11        1     12   [5, 20, 8]\n1         2      0           []\n3         2     30          [0]\n7         2      7      [0, 30]\n10        2      9   [0, 30, 7]\n2         3     18           []\n4         3     17         [18]\n6         4      9           []\n</code></pre>\n<p>Then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html#pandas-dataframe-sort-index\" rel=\"nofollow noreferrer\"><code>sort_index</code></a> to restore the initial order:</p>\n<pre><code>df = df.sort_index()\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [0, 30]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n10        2      9   [0, 30, 7]\n11        1     12   [5, 20, 8]\n</code></pre>\n<p>(Optional use extended slicing to reverse the lists and get elements in same order as expected output above):</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [30, 0]\n8         1     20      [5, 15]\n9         1      8  [20, 5, 15]\n10        2      9   [7, 30, 0]\n11        1     12   [8, 20, 5]\n</code></pre>\n<hr/>\n<p>Complete Working Example:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'Subject': [1, 2, 3, 2, 3, 1, 4, 2, 1, 1, 2, 1],\n    'Score': [15, 0, 18, 30, 17, 5, 9, 7, 20, 8, 9, 12]\n})\n\nwindow = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\ndf = df.sort_index()\n\nprint(df)\n</code></pre>\n",
        "question_body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "formatted_input": {
            "qid": 67870323,
            "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
            "question": {
                "title": "How to get list of previous n values of a column conditionally in DataFrame?",
                "ques_desc": "My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? "
            },
            "io": [
                "Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n",
                "Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n"
            ],
            "answer": {
                "ans_desc": "Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: ",
                "code": [
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n",
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "indexing"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 8970043,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/uL3Er.jpg?s=128&g=1",
            "display_name": "Futurex",
            "link": "https://stackoverflow.com/users/8970043/futurex"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 67870693,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623064916,
        "creation_date": 1623064484,
        "question_id": 67870585,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
        "title": "Python dataframe create index column based on other id column",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "answer_body": "<p>Try <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.ngroup.html#pandas-core-groupby-groupby-ngroup\" rel=\"nofollow noreferrer\"><code>groupby ngroup</code></a> + 1 :</p>\n<pre><code>df['ID_2'] = df.groupby('ID').ngroup() + 1\n</code></pre>\n<p>Or with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rank.html#pandas-series-rank\" rel=\"nofollow noreferrer\"><code>Rank</code></a>:</p>\n<pre><code>df['ID_2'] = df['ID'].rank(method='dense').astype(int)\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>                 ID   Price  ID_2\n0  000afb96ded6677c  1514.5     1\n1  000afb96ded6677c    13.0     1\n2  000afb96ded6677c   611.0     1\n3  000afb96ded6677c   723.0     1\n4  000afb96ded6677c  2065.0     1\n5  ffea14e87a4e1269  2286.0     2\n6  ffea14e87a4e1269  1150.0     2\n7  ffea14e87a4e1269    80.0     2\n8  fff455057ad492da   650.0     3\n9  fff5fc66c1fd66c2   450.0     4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67870585,
            "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
            "question": {
                "title": "Python dataframe create index column based on other id column",
                "ques_desc": "I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: "
            },
            "io": [
                "ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n",
                "ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n"
            ],
            "answer": {
                "ans_desc": "Try + 1 : Or with : : ",
                "code": [
                    "df['ID_2'] = df.groupby('ID').ngroup() + 1\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16132607,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/234e69866fa5f5a2e96b28a4fd12597b?s=128&d=identicon&r=PG&f=1",
            "display_name": "homeboykeroro",
            "link": "https://stackoverflow.com/users/16132607/homeboykeroro"
        },
        "is_answered": true,
        "view_count": 85,
        "accepted_answer_id": 67857571,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1622976694,
        "creation_date": 1622966815,
        "last_edit_date": 1622970680,
        "question_id": 67856992,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
        "title": "Element wise numeric comparison in Pandas dataframe column value with list",
        "body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "answer_body": "<p>Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise).</p>\n<p>You can use <code>apply</code>:</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n</code></pre>\n<hr />\n<pre><code>res = df2.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<p><strong>res:</strong></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\">A</th>\n<th style=\"text-align: center;\">B</th>\n<th style=\"text-align: center;\">C</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\"></td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">2</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[False, False, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n</tr>\n</tbody>\n</table>\n</div><h2>Update</h2>\n<p>(Complete Solution Based on the data you've provided):</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<h2>Time Comparison:</h2>\n<p><strong>Method 1 (Nk03's method1):</strong></p>\n<blockquote>\n<p>CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms</p>\n</blockquote>\n<p><strong>Method 2 (Nk03's method2):</strong></p>\n<blockquote>\n<p>CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms</p>\n</blockquote>\n<p><strong>Method 3 (Using numpy based comparison):</strong></p>\n<blockquote>\n<p>CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms</p>\n</blockquote>\n<p><strong>Nk03's Updated and Optimized Solution:</strong></p>\n<blockquote>\n<p>CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms</p>\n</blockquote>\n",
        "question_body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "formatted_input": {
            "qid": 67856992,
            "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
            "question": {
                "title": "Element wise numeric comparison in Pandas dataframe column value with list",
                "ques_desc": "I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: "
            },
            "io": [
                "  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n",
                "  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n"
            ],
            "answer": {
                "ans_desc": "Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms ",
                "code": [
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n",
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 10566774,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-8T7iqZlWN8Y/AAAAAAAAAAI/AAAAAAAACu0/hiVsQlXBjzQ/photo.jpg?sz=128",
            "display_name": "Salvatore Nedia",
            "link": "https://stackoverflow.com/users/10566774/salvatore-nedia"
        },
        "is_answered": true,
        "view_count": 107,
        "accepted_answer_id": 67845512,
        "answer_count": 4,
        "score": 5,
        "last_activity_date": 1622859671,
        "creation_date": 1622854122,
        "last_edit_date": 1622854820,
        "question_id": 67845362,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
        "title": "Sort pandas df subset of rows (within a group) by specific column",
        "body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "answer_body": "<p>I think it should be as simple as this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.sort_values([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;])\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "formatted_input": {
            "qid": 67845362,
            "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
            "question": {
                "title": "Sort pandas df subset of rows (within a group) by specific column",
                "ques_desc": "I have the following dataframe let\u2019s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? "
            },
            "io": [
                "\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n",
                "\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n"
            ],
            "answer": {
                "ans_desc": "I think it should be as simple as this: ",
                "code": [
                    "df = df.sort_values([\"A\", \"B\", \"C\", \"D\"])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 12785115,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b3484fd202fb5624044a4ed7b5181fab?s=128&d=identicon&r=PG&f=1",
            "display_name": "RamboJ",
            "link": "https://stackoverflow.com/users/12785115/ramboj"
        },
        "is_answered": true,
        "view_count": 90,
        "accepted_answer_id": 61625210,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1622820639,
        "creation_date": 1588722398,
        "question_id": 61624957,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
        "title": "Creating a table in pandas from json",
        "body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "answer_body": "<p>You need to massage the data a little bit to get what you want.</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: x['website'][0])\n    .to_frame('website')\n)\n\n    website\n1   https://bitcoin.org/\n2   https://litecoin.org/\n</code></pre>\n\n<p>To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. </p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n</code></pre>\n\n<p>To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls'</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n</code></pre>\n",
        "question_body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 61624957,
            "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
            "question": {
                "title": "Creating a table in pandas from json",
                "ques_desc": "I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! "
            },
            "io": [
                "0     website\n0  1  NaN\n1  2  NaN \n",
                "0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n"
            ],
            "answer": {
                "ans_desc": "You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' ",
                "code": [
                    "(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n",
                    "(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 345,
            "user_id": 5426588,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://www.gravatar.com/avatar/068913af46eb9b38c9ffb53ba219c7c9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Baig",
            "link": "https://stackoverflow.com/users/5426588/baig"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 67822589,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1622728725,
        "creation_date": 1622726951,
        "last_edit_date": 1622727694,
        "question_id": 67822403,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
        "title": "Replicate dataframe n number of times and increment another column by 1",
        "body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "answer_body": "<p>Something along these lines should work:</p>\n<pre><code>df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "formatted_input": {
            "qid": 67822403,
            "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
            "question": {
                "title": "Replicate dataframe n number of times and increment another column by 1",
                "ques_desc": "I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. "
            },
            "io": [
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n",
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n"
            ],
            "answer": {
                "ans_desc": "Something along these lines should work: ",
                "code": [
                    "df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 9235000,
            "user_type": "registered",
            "profile_image": "https://graph.facebook.com/1652134728176732/picture?type=large",
            "display_name": "Amruth Anand",
            "link": "https://stackoverflow.com/users/9235000/amruth-anand"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 67919490,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623322230,
        "creation_date": 1623321043,
        "question_id": 67919385,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
        "title": "Find unique column values out of two different Dataframes",
        "body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "answer_body": "<p>TRY:</p>\n<pre><code>unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n</code></pre>\n<p><em>NOTE</em> : Replace <code>0</code> in <code>subset= [0]</code> with the first column name.</p>\n",
        "question_body": "<p>How to find unique values of first column out of DF1 &amp; DF2</p>\n<p>DF1</p>\n<pre><code>67      Hij\n14      Xyz \n87      Pqr\n</code></pre>\n<p>DF2</p>\n<pre><code>43      Def\n67      Lmn\n14      Xyz\n</code></pre>\n<p>Output</p>\n<pre><code>87      Pqr\n43      Def\n</code></pre>\n<p>This is how Read</p>\n<pre><code>import pandas as pd\ndf1 = pd.read_csv('Sample1.csv', sep='|')\ndf2 = pd.read_csv('sample2.csv', sep='|')\n</code></pre>\n",
        "formatted_input": {
            "qid": 67919385,
            "link": "https://stackoverflow.com/questions/67919385/find-unique-column-values-out-of-two-different-dataframes",
            "question": {
                "title": "Find unique column values out of two different Dataframes",
                "ques_desc": "How to find unique values of first column out of DF1 & DF2 DF1 DF2 Output This is how Read "
            },
            "io": [
                "67      Hij\n14      Xyz \n87      Pqr\n",
                "43      Def\n67      Lmn\n14      Xyz\n"
            ],
            "answer": {
                "ans_desc": "TRY: NOTE : Replace in with the first column name. ",
                "code": [
                    "unique_df = pd.concat([df1, df2]).drop_duplicates(subset=[0], keep=False)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 34,
        "accepted_answer_id": 67917741,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1623316126,
        "creation_date": 1623314044,
        "last_edit_date": 1623314922,
        "question_id": 67917573,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
        "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
        "body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "answer_body": "<p>Here is one way:</p>\n<pre><code>minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, &quot;-&quot;)\n</code></pre>\n<p>where we first flip the <code>df</code> over the columns, look where it is not <code>NaN</code> and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put <code>&quot;-&quot;</code> so we use <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html\" rel=\"nofollow noreferrer\"><code>mask</code></a> method to put minus signs there,</p>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n    L1   D1   L2   D2 L3\n0  1.0  ABC  1.1  4.1  -\n1  NaN  NaN  1.7    -  -\n2  NaN  4.1    -    -  -\n3  NaN  1.8  3.2  PQR  -\n4  NaN  NaN  1.6    -  -\n</code></pre>\n",
        "question_body": "<p>I have a dataframe</p>\n<pre><code> L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n</code></pre>\n<p>I want to replace all the NaN with '-' (only when the value in any column is last value in that row)</p>\n<p>so basically my desired output will be</p>\n<pre><code> L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n</code></pre>\n<p>Can someone help, Thank you in advance!</p>\n",
        "formatted_input": {
            "qid": 67917573,
            "link": "https://stackoverflow.com/questions/67917573/replace-nan-with-sign-only-in-specefic-condition-python-pandas",
            "question": {
                "title": "replace NaN with &#39;-&#39; sign only in specefic condition ,Python-Pandas",
                "ques_desc": "I have a dataframe I want to replace all the NaN with '-' (only when the value in any column is last value in that row) so basically my desired output will be Can someone help, Thank you in advance! "
            },
            "io": [
                " L1      D1     L2      D2         L3\n 1.0    ABC     1.1     4.1        NaN\n NaN    NaN     1.7     NaN        NaN\n NaN    4.1     NaN     NaN        NaN\n NaN    1.8     3.2     PQR        NaN\n NaN    NaN     1.6     NaN        NaN\n",
                " L1      D1      L2      D2         L3\n 1.0    ABC     1.1     4.1        -\n NaN    NaN     1.7     -          -\n NaN    4.1      -      -          -\n NaN    1.8     3.2     PQR        -\n NaN    NaN     1.6     -          -\n\n"
            ],
            "answer": {
                "ans_desc": "Here is one way: where we first flip the over the columns, look where it is not and take the cumulative sum. If the cumulative sum equals 0, those places are where we should put so we use method to put minus signs there, to get ",
                "code": [
                    "minus_mask = df.loc[:, ::-1].notna().cumsum(axis=1).eq(0)\nout = df.mask(minus_mask, \"-\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 67910764,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1623312939,
        "creation_date": 1623267501,
        "question_id": 67910688,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
        "title": "move column above and delete rows in pandas python dataframe",
        "body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "answer_body": "<p>You can shift back each column by the number of preceding missing values which is found with <code>first_valid_index</code>:</p>\n<pre><code>df.apply(lambda s: s.shift(-s.first_valid_index()))\n</code></pre>\n<p>to get</p>\n<pre><code>     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1  NaN  NaN  c.2  d.2  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n</code></pre>\n<p>To drop the rows full of <code>NaN</code>s and fill the rest with empty string:</p>\n<pre><code>out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=&quot;all&quot;)\n         .fillna(&quot;&quot;))\n</code></pre>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1            c.2  d.2\n</code></pre>\n<hr>\n<p>note: this assumes your index is <code>0..N-1</code>; so if it's not, you can store it beforehand and then restore back:</p>\n<pre><code>index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=&quot;all&quot;)\n        .fillna(&quot;&quot;))\ndf.index = index[:len(df)]\n</code></pre>\n<hr>\n<p>To make the pulling up specific to some columns:</p>\n<pre><code>def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n</code></pre>\n",
        "question_body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "formatted_input": {
            "qid": 67910688,
            "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
            "question": {
                "title": "move column above and delete rows in pandas python dataframe",
                "ques_desc": "I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be "
            },
            "io": [
                "A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n",
                "A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n"
            ],
            "answer": {
                "ans_desc": "You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: ",
                "code": [
                    "out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=\"all\")\n         .fillna(\"\"))\n",
                    "index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=\"all\")\n        .fillna(\"\"))\ndf.index = index[:len(df)]\n",
                    "def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 166,
            "user_id": 16154762,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgA5126u43jbFmhmVe4Tv7Rq8KecxLwFi0ibsEU=k-s128",
            "display_name": "Vivek Singh",
            "link": "https://stackoverflow.com/users/16154762/vivek-singh"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 67905751,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1623247633,
        "creation_date": 1623247407,
        "last_edit_date": 1623247633,
        "question_id": 67905723,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
        "title": "Replicating the DataFrame row in a special manner",
        "body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "answer_body": "<p>Try with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html#pandas-series-str-split\" rel=\"noreferrer\"><code>str.split</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html#pandas-dataframe-explode\" rel=\"noreferrer\"><code>DataFrame.explode</code></a>:</p>\n<pre><code>df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n</code></pre>\n<pre><code>  col1      mob_no  col3\n0    a  9382949201    45\n0    a  3245622535    45\n1    b  8383459345    67\n1    b  4325562678    67\n2    c  8976247543    89\n2    c  1827472398    89\n3    d  7844329432    09\n</code></pre>\n",
        "question_body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "formatted_input": {
            "qid": 67905723,
            "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
            "question": {
                "title": "Replicating the DataFrame row in a special manner",
                "ques_desc": "I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: "
            },
            "io": [
                "col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n",
                "col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n"
            ],
            "answer": {
                "ans_desc": "Try with + : ",
                "code": [
                    "df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "dictionary"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9168000,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4f8b0ca12d7cf4951fffba8f055251cb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Manglu",
            "link": "https://stackoverflow.com/users/9168000/manglu"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67882312,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623132901,
        "creation_date": 1623131314,
        "question_id": 67882067,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
        "title": "Dictionary making for a transportation model from a Dataframe",
        "body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "answer_body": "<pre><code>df1 = df.set_index([&quot;Unnamed: 0&quot;, &quot;Unnamed: 1&quot;])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[&quot;demand&quot;].T.squeeze().dropna().iteritems())\nM = dict(plants[&quot;capacity&quot;].iteritems())\nI = list(plants.drop(columns=&quot;capacity&quot;).columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=&quot;capacity&quot;).stack().iteritems())\n</code></pre>\n<pre><code>&gt;&gt;&gt; d\n{'c1': 80.0, 'c2': 270.0, 'c3': 250.0, 'c4': 160.0, 'c5': 180.0}\n\n&gt;&gt;&gt; M\n{'p1': 500.0, 'p2': 500.0, 'p3': 500.0}\n\n&gt;&gt;&gt; I\n['c1', 'c2', 'c3', 'c4', 'c5']\n\n&gt;&gt;&gt; J\n['p1', 'p2', 'p3']\n\n&gt;&gt;&gt; cost\n{('p1', 'c1'): 4,\n ('p1', 'c2'): 5,\n ('p1', 'c3'): 6,\n ('p1', 'c4'): 8,\n ('p1', 'c5'): 10,\n ('p2', 'c1'): 6,\n ('p2', 'c2'): 4,\n ('p2', 'c3'): 3,\n ('p2', 'c4'): 5,\n ('p2', 'c5'): 8,\n ('p3', 'c1'): 9,\n ('p3', 'c2'): 7,\n ('p3', 'c3'): 4,\n ('p3', 'c4'): 2,\n ('p3', 'c5'): 4}\n</code></pre>\n",
        "question_body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "formatted_input": {
            "qid": 67882067,
            "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
            "question": {
                "title": "Dictionary making for a transportation model from a Dataframe",
                "ques_desc": "I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. "
            },
            "io": [
                " d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n",
                "  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "df1 = df.set_index([\"Unnamed: 0\", \"Unnamed: 1\"])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[\"demand\"].T.squeeze().dropna().iteritems())\nM = dict(plants[\"capacity\"].iteritems())\nI = list(plants.drop(columns=\"capacity\").columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=\"capacity\").stack().iteritems())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 352,
            "user_id": 8147508,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dbf0ab4fedaca38f5f4ae3c2344f9b95?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dev Ananth",
            "link": "https://stackoverflow.com/users/8147508/dev-ananth"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67871572,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1623073970,
        "creation_date": 1623063317,
        "last_edit_date": 1623065349,
        "question_id": 67870323,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
        "title": "How to get list of previous n values of a column conditionally in DataFrame?",
        "body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "answer_body": "<p>Since rolling only supports production of numeric values, this has to be a work around.</p>\n<p>Try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html#pandas-dataframe-sort-values\" rel=\"nofollow noreferrer\"><code>sort_values</code></a> first then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html\" rel=\"nofollow noreferrer\"><code>groupby rolling</code></a> on window + 1 and strip off the last element:</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n5         1      5         [15]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n11        1     12   [5, 20, 8]\n1         2      0           []\n3         2     30          [0]\n7         2      7      [0, 30]\n10        2      9   [0, 30, 7]\n2         3     18           []\n4         3     17         [18]\n6         4      9           []\n</code></pre>\n<p>Then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html#pandas-dataframe-sort-index\" rel=\"nofollow noreferrer\"><code>sort_index</code></a> to restore the initial order:</p>\n<pre><code>df = df.sort_index()\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [0, 30]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n10        2      9   [0, 30, 7]\n11        1     12   [5, 20, 8]\n</code></pre>\n<p>(Optional use extended slicing to reverse the lists and get elements in same order as expected output above):</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [30, 0]\n8         1     20      [5, 15]\n9         1      8  [20, 5, 15]\n10        2      9   [7, 30, 0]\n11        1     12   [8, 20, 5]\n</code></pre>\n<hr/>\n<p>Complete Working Example:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'Subject': [1, 2, 3, 2, 3, 1, 4, 2, 1, 1, 2, 1],\n    'Score': [15, 0, 18, 30, 17, 5, 9, 7, 20, 8, 9, 12]\n})\n\nwindow = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\ndf = df.sort_index()\n\nprint(df)\n</code></pre>\n",
        "question_body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "formatted_input": {
            "qid": 67870323,
            "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
            "question": {
                "title": "How to get list of previous n values of a column conditionally in DataFrame?",
                "ques_desc": "My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? "
            },
            "io": [
                "Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n",
                "Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n"
            ],
            "answer": {
                "ans_desc": "Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: ",
                "code": [
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n",
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "indexing"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 8970043,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/uL3Er.jpg?s=128&g=1",
            "display_name": "Futurex",
            "link": "https://stackoverflow.com/users/8970043/futurex"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 67870693,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623064916,
        "creation_date": 1623064484,
        "question_id": 67870585,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
        "title": "Python dataframe create index column based on other id column",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "answer_body": "<p>Try <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.ngroup.html#pandas-core-groupby-groupby-ngroup\" rel=\"nofollow noreferrer\"><code>groupby ngroup</code></a> + 1 :</p>\n<pre><code>df['ID_2'] = df.groupby('ID').ngroup() + 1\n</code></pre>\n<p>Or with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rank.html#pandas-series-rank\" rel=\"nofollow noreferrer\"><code>Rank</code></a>:</p>\n<pre><code>df['ID_2'] = df['ID'].rank(method='dense').astype(int)\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>                 ID   Price  ID_2\n0  000afb96ded6677c  1514.5     1\n1  000afb96ded6677c    13.0     1\n2  000afb96ded6677c   611.0     1\n3  000afb96ded6677c   723.0     1\n4  000afb96ded6677c  2065.0     1\n5  ffea14e87a4e1269  2286.0     2\n6  ffea14e87a4e1269  1150.0     2\n7  ffea14e87a4e1269    80.0     2\n8  fff455057ad492da   650.0     3\n9  fff5fc66c1fd66c2   450.0     4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67870585,
            "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
            "question": {
                "title": "Python dataframe create index column based on other id column",
                "ques_desc": "I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: "
            },
            "io": [
                "ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n",
                "ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n"
            ],
            "answer": {
                "ans_desc": "Try + 1 : Or with : : ",
                "code": [
                    "df['ID_2'] = df.groupby('ID').ngroup() + 1\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16132607,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/234e69866fa5f5a2e96b28a4fd12597b?s=128&d=identicon&r=PG&f=1",
            "display_name": "homeboykeroro",
            "link": "https://stackoverflow.com/users/16132607/homeboykeroro"
        },
        "is_answered": true,
        "view_count": 85,
        "accepted_answer_id": 67857571,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1622976694,
        "creation_date": 1622966815,
        "last_edit_date": 1622970680,
        "question_id": 67856992,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
        "title": "Element wise numeric comparison in Pandas dataframe column value with list",
        "body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "answer_body": "<p>Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise).</p>\n<p>You can use <code>apply</code>:</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n</code></pre>\n<hr />\n<pre><code>res = df2.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<p><strong>res:</strong></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\">A</th>\n<th style=\"text-align: center;\">B</th>\n<th style=\"text-align: center;\">C</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\"></td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">2</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[False, False, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n</tr>\n</tbody>\n</table>\n</div><h2>Update</h2>\n<p>(Complete Solution Based on the data you've provided):</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<h2>Time Comparison:</h2>\n<p><strong>Method 1 (Nk03's method1):</strong></p>\n<blockquote>\n<p>CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms</p>\n</blockquote>\n<p><strong>Method 2 (Nk03's method2):</strong></p>\n<blockquote>\n<p>CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms</p>\n</blockquote>\n<p><strong>Method 3 (Using numpy based comparison):</strong></p>\n<blockquote>\n<p>CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms</p>\n</blockquote>\n<p><strong>Nk03's Updated and Optimized Solution:</strong></p>\n<blockquote>\n<p>CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms</p>\n</blockquote>\n",
        "question_body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "formatted_input": {
            "qid": 67856992,
            "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
            "question": {
                "title": "Element wise numeric comparison in Pandas dataframe column value with list",
                "ques_desc": "I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: "
            },
            "io": [
                "  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n",
                "  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n"
            ],
            "answer": {
                "ans_desc": "Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms ",
                "code": [
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n",
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 10566774,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-8T7iqZlWN8Y/AAAAAAAAAAI/AAAAAAAACu0/hiVsQlXBjzQ/photo.jpg?sz=128",
            "display_name": "Salvatore Nedia",
            "link": "https://stackoverflow.com/users/10566774/salvatore-nedia"
        },
        "is_answered": true,
        "view_count": 107,
        "accepted_answer_id": 67845512,
        "answer_count": 4,
        "score": 5,
        "last_activity_date": 1622859671,
        "creation_date": 1622854122,
        "last_edit_date": 1622854820,
        "question_id": 67845362,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
        "title": "Sort pandas df subset of rows (within a group) by specific column",
        "body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "answer_body": "<p>I think it should be as simple as this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.sort_values([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;])\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "formatted_input": {
            "qid": 67845362,
            "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
            "question": {
                "title": "Sort pandas df subset of rows (within a group) by specific column",
                "ques_desc": "I have the following dataframe let\u2019s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? "
            },
            "io": [
                "\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n",
                "\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n"
            ],
            "answer": {
                "ans_desc": "I think it should be as simple as this: ",
                "code": [
                    "df = df.sort_values([\"A\", \"B\", \"C\", \"D\"])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 12785115,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b3484fd202fb5624044a4ed7b5181fab?s=128&d=identicon&r=PG&f=1",
            "display_name": "RamboJ",
            "link": "https://stackoverflow.com/users/12785115/ramboj"
        },
        "is_answered": true,
        "view_count": 90,
        "accepted_answer_id": 61625210,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1622820639,
        "creation_date": 1588722398,
        "question_id": 61624957,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
        "title": "Creating a table in pandas from json",
        "body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "answer_body": "<p>You need to massage the data a little bit to get what you want.</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: x['website'][0])\n    .to_frame('website')\n)\n\n    website\n1   https://bitcoin.org/\n2   https://litecoin.org/\n</code></pre>\n\n<p>To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. </p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n</code></pre>\n\n<p>To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls'</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n</code></pre>\n",
        "question_body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 61624957,
            "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
            "question": {
                "title": "Creating a table in pandas from json",
                "ques_desc": "I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! "
            },
            "io": [
                "0     website\n0  1  NaN\n1  2  NaN \n",
                "0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n"
            ],
            "answer": {
                "ans_desc": "You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' ",
                "code": [
                    "(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n",
                    "(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 345,
            "user_id": 5426588,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://www.gravatar.com/avatar/068913af46eb9b38c9ffb53ba219c7c9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Baig",
            "link": "https://stackoverflow.com/users/5426588/baig"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 67822589,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1622728725,
        "creation_date": 1622726951,
        "last_edit_date": 1622727694,
        "question_id": 67822403,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
        "title": "Replicate dataframe n number of times and increment another column by 1",
        "body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "answer_body": "<p>Something along these lines should work:</p>\n<pre><code>df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "formatted_input": {
            "qid": 67822403,
            "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
            "question": {
                "title": "Replicate dataframe n number of times and increment another column by 1",
                "ques_desc": "I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. "
            },
            "io": [
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n",
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n"
            ],
            "answer": {
                "ans_desc": "Something along these lines should work: ",
                "code": [
                    "df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16067894,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a6cf989fce509918dc286c22d013c519?s=128&d=identicon&r=PG&f=1",
            "display_name": "lasse3434",
            "link": "https://stackoverflow.com/users/16067894/lasse3434"
        },
        "is_answered": true,
        "view_count": 74,
        "accepted_answer_id": 67782978,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1622554424,
        "creation_date": 1622521690,
        "last_edit_date": 1622521880,
        "question_id": 67782727,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
        "title": "Python - Delete lines from dataframe (pandas)",
        "body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "answer_body": "<p>Make a clean dataframe and <strong>keep</strong> values you want:</p>\n<pre><code>data[['1', '2']] = data[['1', '2']].replace({&quot;///&quot;: np.nan, &quot;,&quot;: &quot;.&quot;}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)]\n</code></pre>\n<pre><code>&gt;&gt;&gt; data\n               0      1      2\n0  9783630876672  12.35   2.62\n1  9783423282789  11.67   6.07\n2  9783833879500  17.25  12.40\n3  9783898798822   6.91   1.16\n4  9783453281417  12.93   2.84\n5  9783630876672  12.35   4.08\n6  9783423282789  11.67   6.07\n7  9783833879500  17.25   9.94\n8  9783898798822   6.91   2.96\n9  9783453281417  12.93   2.68\n</code></pre>\n<p><strong>Comments:</strong></p>\n<p><strong>1st line:</strong></p>\n<ul>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#different-choices-for-indexing\" rel=\"nofollow noreferrer\"><code>data[['1', '2']]</code></a> select columns named '1' and '2'</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\" rel=\"nofollow noreferrer\"><code>replace</code></a> change existing values ('///' and ',') by new ones ('nan' and '.')</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\" rel=\"nofollow noreferrer\"><code>astype(float)</code></a> convert your string columns to real numbers (float) since your dataframe is cleaned.</li>\n</ul>\n<p><strong>2nd line:</strong></p>\n<ul>\n<li><code>data.loc[...]</code> locate something in your dataframe</li>\n<li><code>data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)</code>: in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row.</li>\n</ul>\n",
        "question_body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "formatted_input": {
            "qid": 67782727,
            "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
            "question": {
                "title": "Python - Delete lines from dataframe (pandas)",
                "ques_desc": "I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. "
            },
            "io": [
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n",
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n"
            ],
            "answer": {
                "ans_desc": "Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. ",
                "code": [
                    "data[['1', '2']] = data[['1', '2']].replace({\"///\": np.nan, \",\": \".\"}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[\"1\", \"2\"]].ge(1.).all(axis=\"columns\")]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "regex",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 185,
            "user_id": 13196248,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/--wvokyexUe4/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJMneagY7ciQcziCtlVV3rVrRFH57A/photo.jpg?sz=128",
            "display_name": "Charles",
            "link": "https://stackoverflow.com/users/13196248/charles"
        },
        "is_answered": true,
        "view_count": 69,
        "accepted_answer_id": 67770609,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1622454286,
        "creation_date": 1622448681,
        "question_id": 67770056,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
        "title": "How to create columns from a string in a dataframe?",
        "body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "answer_body": "<pre><code>ndf = (df[&quot;long string&quot;]\n         .str.extractall(r&quot;(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)&quot;)\n         .droplevel(&quot;match&quot;)\n         .set_index(0, append=True)\n         .set_axis([&quot;tra&quot;, &quot;la&quot;], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(&quot;-&quot;.join)\n</code></pre>\n<ul>\n<li>Extract the desired parts with a <a href=\"https://regex101.com/r/F3EKrX/1\" rel=\"nofollow noreferrer\">regex</a></li>\n<li>Drop the index level induced by <code>extractall</code> called <code>match</code></li>\n<li>Append the <code>ha-hi-ho</code> matches as the index (<code>0</code> is first capturing group)</li>\n<li>Rename the remaining columns <code>tra</code> and <code>la</code></li>\n<li>Unstack the <code>ha-hi-ho</code> index to the columns</li>\n<li>Swap the <code>ha-hi-ho</code> and <code>tra-la</code> levels' order in columns so that <code>ha-hi-ho</code> is upper</li>\n<li>Lastly join these levels of columns' names with a hyphen</li>\n</ul>\n<p>to get</p>\n<pre><code>  ha-tra hi-tra ho-tra ha-la hi-la ho-la\n0      1      1      1     2     2     2\n1      1      1      1     2     2     2\n2      1      1      1     2     2     2\n</code></pre>\n",
        "question_body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "formatted_input": {
            "qid": 67770056,
            "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
            "question": {
                "title": "How to create columns from a string in a dataframe?",
                "ques_desc": "WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. "
            },
            "io": [
                "    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n",
                "    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n"
            ],
            "answer": {
                "ans_desc": " Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get ",
                "code": [
                    "ndf = (df[\"long string\"]\n         .str.extractall(r\"(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)\")\n         .droplevel(\"match\")\n         .set_index(0, append=True)\n         .set_axis([\"tra\", \"la\"], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(\"-\".join)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "python-2.7"
        ],
        "owner": {
            "reputation": 127,
            "user_id": 15452168,
            "user_type": "registered",
            "profile_image": "https://lh6.googleusercontent.com/-LZAfG23GD2c/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclxz8BiX4ffYV0jPcUQZay2Hh_XBg/s96-c/photo.jpg?sz=128",
            "display_name": "sdave",
            "link": "https://stackoverflow.com/users/15452168/sdave"
        },
        "is_answered": true,
        "view_count": 46,
        "accepted_answer_id": 67910764,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1623312939,
        "creation_date": 1623267501,
        "question_id": 67910688,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
        "title": "move column above and delete rows in pandas python dataframe",
        "body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "answer_body": "<p>You can shift back each column by the number of preceding missing values which is found with <code>first_valid_index</code>:</p>\n<pre><code>df.apply(lambda s: s.shift(-s.first_valid_index()))\n</code></pre>\n<p>to get</p>\n<pre><code>     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1  NaN  NaN  c.2  d.2  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n</code></pre>\n<p>To drop the rows full of <code>NaN</code>s and fill the rest with empty string:</p>\n<pre><code>out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=&quot;all&quot;)\n         .fillna(&quot;&quot;))\n</code></pre>\n<p>to get</p>\n<pre><code>&gt;&gt;&gt; out\n\n     A    B    C    D    E    F    G    H\n0  a.1  b.1  c.1  d.1  e.1  f.1  g.1  h.1\n1            c.2  d.2\n</code></pre>\n<hr>\n<p>note: this assumes your index is <code>0..N-1</code>; so if it's not, you can store it beforehand and then restore back:</p>\n<pre><code>index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=&quot;all&quot;)\n        .fillna(&quot;&quot;))\ndf.index = index[:len(df)]\n</code></pre>\n<hr>\n<p>To make the pulling up specific to some columns:</p>\n<pre><code>def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n</code></pre>\n",
        "question_body": "<p>I have a data frame df like this</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n</code></pre>\n<p>Create the sample DataFrame</p>\n<pre><code>from io import StringIO\n\ns = &quot;&quot;&quot;A,B,C,D,E,F,G,H\na.1,b.1,,,,,,\n,,c.1,d.1,,,,\n,,c.2,d.2,e.1,f.1,,\n,,,,,,g.1,h.1&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(s))\n</code></pre>\n<p>I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help.</p>\n<p>my desired results would be</p>\n<pre><code>A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n</code></pre>\n",
        "formatted_input": {
            "qid": 67910688,
            "link": "https://stackoverflow.com/questions/67910688/move-column-above-and-delete-rows-in-pandas-python-dataframe",
            "question": {
                "title": "move column above and delete rows in pandas python dataframe",
                "ques_desc": "I have a data frame df like this Create the sample DataFrame I want to remove these extra spaces and I want dataframe to start from the top row. Can anyone help. my desired results would be "
            },
            "io": [
                "A        B        C        D        E        F        G        H\na.1      b.1     \n                  \n                  c.1      d.1 \n                  c.2      d.2           e.1      f.1 \n                                                      \n\n                                                     g.1       h.1\n  \n\n\n",
                "A        B        C        D        E        F        G        H\na.1      b.1      c.1      d.1      e.1      f.1      g.1       h.1\n                  c.2      d.2                                                   \n"
            ],
            "answer": {
                "ans_desc": "You can shift back each column by the number of preceding missing values which is found with : to get To drop the rows full of s and fill the rest with empty string: to get note: this assumes your index is ; so if it's not, you can store it beforehand and then restore back: To make the pulling up specific to some columns: ",
                "code": [
                    "out = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n         .dropna(how=\"all\")\n         .fillna(\"\"))\n",
                    "index = df.index\ndf = df.reset_index(drop=True)\ndf = (df.apply(lambda s: s.shift(-s.first_valid_index()))\n        .dropna(how=\"all\")\n        .fillna(\"\"))\ndf.index = index[:len(df)]\n",
                    "def pull_up(s):\n    # this will be a column number; `s.name` is the column name\n    col_index = df.columns.get_indexer([s.name])\n\n   # for example: if `col_index` is either 7 or 8, pull by 4\n   if col_index in (7, 8):\n       return s.shift(-4)\n   else:\n       # otherwise, pull as much\n       return s.shift(-s.first_valid_index())\n\n# applying\ndf.apply(pull_up)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 166,
            "user_id": 16154762,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgA5126u43jbFmhmVe4Tv7Rq8KecxLwFi0ibsEU=k-s128",
            "display_name": "Vivek Singh",
            "link": "https://stackoverflow.com/users/16154762/vivek-singh"
        },
        "is_answered": true,
        "view_count": 59,
        "accepted_answer_id": 67905751,
        "answer_count": 1,
        "score": 4,
        "last_activity_date": 1623247633,
        "creation_date": 1623247407,
        "last_edit_date": 1623247633,
        "question_id": 67905723,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
        "title": "Replicating the DataFrame row in a special manner",
        "body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "answer_body": "<p>Try with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html#pandas-series-str-split\" rel=\"noreferrer\"><code>str.split</code></a> + <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html#pandas-dataframe-explode\" rel=\"noreferrer\"><code>DataFrame.explode</code></a>:</p>\n<pre><code>df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n</code></pre>\n<pre><code>  col1      mob_no  col3\n0    a  9382949201    45\n0    a  3245622535    45\n1    b  8383459345    67\n1    b  4325562678    67\n2    c  8976247543    89\n2    c  1827472398    89\n3    d  7844329432    09\n</code></pre>\n",
        "question_body": "<p>I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help</p>\n<p>Input:</p>\n<p>df</p>\n<pre><code>col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n</code></pre>\n<p>Expected output:</p>\n<pre><code>col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n</code></pre>\n",
        "formatted_input": {
            "qid": 67905723,
            "link": "https://stackoverflow.com/questions/67905723/replicating-the-dataframe-row-in-a-special-manner",
            "question": {
                "title": "Replicating the DataFrame row in a special manner",
                "ques_desc": "I want to replicate data frame rows by splitting the contact number, I'm trying several ways but unable to do so. Please help Input: df Expected output: "
            },
            "io": [
                "col1        mob_no             col3\n a    9382949201/3245622535    45\n b    8383459345/4325562678    67\n c    8976247543/1827472398    89\n d    7844329432               09\n",
                "col1    mob_no      col3\n a    9382949201     45\n a    3245622535     45\n b    8383459345     67\n b    4325562678     67\n c    8976247543     89\n c    1827472398     89\n d    7844329432     09\n"
            ],
            "answer": {
                "ans_desc": "Try with + : ",
                "code": [
                    "df['mob_no'] = df['mob_no'].str.split('/')\ndf = df.explode('mob_no')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "dictionary"
        ],
        "owner": {
            "reputation": 167,
            "user_id": 9168000,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4f8b0ca12d7cf4951fffba8f055251cb?s=128&d=identicon&r=PG&f=1",
            "display_name": "Manglu",
            "link": "https://stackoverflow.com/users/9168000/manglu"
        },
        "is_answered": true,
        "view_count": 31,
        "accepted_answer_id": 67882312,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623132901,
        "creation_date": 1623131314,
        "question_id": 67882067,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
        "title": "Dictionary making for a transportation model from a Dataframe",
        "body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "answer_body": "<pre><code>df1 = df.set_index([&quot;Unnamed: 0&quot;, &quot;Unnamed: 1&quot;])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[&quot;demand&quot;].T.squeeze().dropna().iteritems())\nM = dict(plants[&quot;capacity&quot;].iteritems())\nI = list(plants.drop(columns=&quot;capacity&quot;).columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=&quot;capacity&quot;).stack().iteritems())\n</code></pre>\n<pre><code>&gt;&gt;&gt; d\n{'c1': 80.0, 'c2': 270.0, 'c3': 250.0, 'c4': 160.0, 'c5': 180.0}\n\n&gt;&gt;&gt; M\n{'p1': 500.0, 'p2': 500.0, 'p3': 500.0}\n\n&gt;&gt;&gt; I\n['c1', 'c2', 'c3', 'c4', 'c5']\n\n&gt;&gt;&gt; J\n['p1', 'p2', 'p3']\n\n&gt;&gt;&gt; cost\n{('p1', 'c1'): 4,\n ('p1', 'c2'): 5,\n ('p1', 'c3'): 6,\n ('p1', 'c4'): 8,\n ('p1', 'c5'): 10,\n ('p2', 'c1'): 6,\n ('p2', 'c2'): 4,\n ('p2', 'c3'): 3,\n ('p2', 'c4'): 5,\n ('p2', 'c5'): 8,\n ('p3', 'c1'): 9,\n ('p3', 'c2'): 7,\n ('p3', 'c3'): 4,\n ('p3', 'c4'): 2,\n ('p3', 'c5'): 4}\n</code></pre>\n",
        "question_body": "<p>I have this Dataframe for a transportation problem.</p>\n<pre><code>Unnamed: 0 Unnamed: 1    c1   c2   c3   c4   c5  capacity\n0        NaN         p1   4    5    6    8   10     500.0\n1        NaN         p2   6    4    3    5    8     500.0\n2        NaN         p3   9    7    4    2    4     500.0\n3     demand        NaN  80  270  250  160  180       NaN\n</code></pre>\n<p>I have changed the column name like this,</p>\n<pre><code>df.columns = ['Demand', 'Plant', 'c1', 'c2', 'c3', 'c4', 'c5', 'capacity']\n</code></pre>\n<p>I want to make a dictionary like this,</p>\n<pre><code> d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n</code></pre>\n<p>For 1st case, I have used the following code,</p>\n<pre><code>  M = df.set_index('Plant')['capacity'].to_dict()\n</code></pre>\n<p>It is giving me,</p>\n<pre><code>  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n</code></pre>\n<p>I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN.</p>\n",
        "formatted_input": {
            "qid": 67882067,
            "link": "https://stackoverflow.com/questions/67882067/dictionary-making-for-a-transportation-model-from-a-dataframe",
            "question": {
                "title": "Dictionary making for a transportation model from a Dataframe",
                "ques_desc": "I have this Dataframe for a transportation problem. I have changed the column name like this, I want to make a dictionary like this, For 1st case, I have used the following code, It is giving me, I don't want any NaN value. Please help to find this total dictionary (d, M and cost) in a generic way without a NaN. "
            },
            "io": [
                " d = {c1:80, c2:270, c3:250, c4:160, c5:180}  # customer demand\n M = {p1:500, p2:500, p3:500}               # factory capacity\n I = [c1,c2,c3,c4,c5]                         # Customers\n J = [p1,p2,p3]                             # Factories\n cost = {(p1,c1):4,    (p1,c2):5,    (p1,c3):6,\n (p1,c4):8,    (p1,c5):10, ......\n  } \n",
                "  {'p1': 500.0, 'p2': 500.0, 'p3': 500.0, nan: nan}  \n"
            ],
            "answer": {
                "ans_desc": " ",
                "code": [
                    "df1 = df.set_index([\"Unnamed: 0\", \"Unnamed: 1\"])\nplants = df1.loc[np.NaN]  # remove demand from dataframe\n\nd = dict(df1.loc[\"demand\"].T.squeeze().dropna().iteritems())\nM = dict(plants[\"capacity\"].iteritems())\nI = list(plants.drop(columns=\"capacity\").columns)\nJ = list(plants.index)\ncost = dict(plants.drop(columns=\"capacity\").stack().iteritems())\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 352,
            "user_id": 8147508,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dbf0ab4fedaca38f5f4ae3c2344f9b95?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dev Ananth",
            "link": "https://stackoverflow.com/users/8147508/dev-ananth"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67871572,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1623073970,
        "creation_date": 1623063317,
        "last_edit_date": 1623065349,
        "question_id": 67870323,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
        "title": "How to get list of previous n values of a column conditionally in DataFrame?",
        "body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "answer_body": "<p>Since rolling only supports production of numeric values, this has to be a work around.</p>\n<p>Try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html#pandas-dataframe-sort-values\" rel=\"nofollow noreferrer\"><code>sort_values</code></a> first then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html\" rel=\"nofollow noreferrer\"><code>groupby rolling</code></a> on window + 1 and strip off the last element:</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n5         1      5         [15]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n11        1     12   [5, 20, 8]\n1         2      0           []\n3         2     30          [0]\n7         2      7      [0, 30]\n10        2      9   [0, 30, 7]\n2         3     18           []\n4         3     17         [18]\n6         4      9           []\n</code></pre>\n<p>Then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html#pandas-dataframe-sort-index\" rel=\"nofollow noreferrer\"><code>sort_index</code></a> to restore the initial order:</p>\n<pre><code>df = df.sort_index()\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [0, 30]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n10        2      9   [0, 30, 7]\n11        1     12   [5, 20, 8]\n</code></pre>\n<p>(Optional use extended slicing to reverse the lists and get elements in same order as expected output above):</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [30, 0]\n8         1     20      [5, 15]\n9         1      8  [20, 5, 15]\n10        2      9   [7, 30, 0]\n11        1     12   [8, 20, 5]\n</code></pre>\n<hr/>\n<p>Complete Working Example:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'Subject': [1, 2, 3, 2, 3, 1, 4, 2, 1, 1, 2, 1],\n    'Score': [15, 0, 18, 30, 17, 5, 9, 7, 20, 8, 9, 12]\n})\n\nwindow = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\ndf = df.sort_index()\n\nprint(df)\n</code></pre>\n",
        "question_body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "formatted_input": {
            "qid": 67870323,
            "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
            "question": {
                "title": "How to get list of previous n values of a column conditionally in DataFrame?",
                "ques_desc": "My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? "
            },
            "io": [
                "Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n",
                "Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n"
            ],
            "answer": {
                "ans_desc": "Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: ",
                "code": [
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n",
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "indexing"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 8970043,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/uL3Er.jpg?s=128&g=1",
            "display_name": "Futurex",
            "link": "https://stackoverflow.com/users/8970043/futurex"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 67870693,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623064916,
        "creation_date": 1623064484,
        "question_id": 67870585,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
        "title": "Python dataframe create index column based on other id column",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "answer_body": "<p>Try <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.ngroup.html#pandas-core-groupby-groupby-ngroup\" rel=\"nofollow noreferrer\"><code>groupby ngroup</code></a> + 1 :</p>\n<pre><code>df['ID_2'] = df.groupby('ID').ngroup() + 1\n</code></pre>\n<p>Or with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rank.html#pandas-series-rank\" rel=\"nofollow noreferrer\"><code>Rank</code></a>:</p>\n<pre><code>df['ID_2'] = df['ID'].rank(method='dense').astype(int)\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>                 ID   Price  ID_2\n0  000afb96ded6677c  1514.5     1\n1  000afb96ded6677c    13.0     1\n2  000afb96ded6677c   611.0     1\n3  000afb96ded6677c   723.0     1\n4  000afb96ded6677c  2065.0     1\n5  ffea14e87a4e1269  2286.0     2\n6  ffea14e87a4e1269  1150.0     2\n7  ffea14e87a4e1269    80.0     2\n8  fff455057ad492da   650.0     3\n9  fff5fc66c1fd66c2   450.0     4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67870585,
            "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
            "question": {
                "title": "Python dataframe create index column based on other id column",
                "ques_desc": "I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: "
            },
            "io": [
                "ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n",
                "ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n"
            ],
            "answer": {
                "ans_desc": "Try + 1 : Or with : : ",
                "code": [
                    "df['ID_2'] = df.groupby('ID').ngroup() + 1\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16132607,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/234e69866fa5f5a2e96b28a4fd12597b?s=128&d=identicon&r=PG&f=1",
            "display_name": "homeboykeroro",
            "link": "https://stackoverflow.com/users/16132607/homeboykeroro"
        },
        "is_answered": true,
        "view_count": 85,
        "accepted_answer_id": 67857571,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1622976694,
        "creation_date": 1622966815,
        "last_edit_date": 1622970680,
        "question_id": 67856992,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
        "title": "Element wise numeric comparison in Pandas dataframe column value with list",
        "body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "answer_body": "<p>Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise).</p>\n<p>You can use <code>apply</code>:</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n</code></pre>\n<hr />\n<pre><code>res = df2.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<p><strong>res:</strong></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\">A</th>\n<th style=\"text-align: center;\">B</th>\n<th style=\"text-align: center;\">C</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\"></td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">2</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[False, False, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n</tr>\n</tbody>\n</table>\n</div><h2>Update</h2>\n<p>(Complete Solution Based on the data you've provided):</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<h2>Time Comparison:</h2>\n<p><strong>Method 1 (Nk03's method1):</strong></p>\n<blockquote>\n<p>CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms</p>\n</blockquote>\n<p><strong>Method 2 (Nk03's method2):</strong></p>\n<blockquote>\n<p>CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms</p>\n</blockquote>\n<p><strong>Method 3 (Using numpy based comparison):</strong></p>\n<blockquote>\n<p>CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms</p>\n</blockquote>\n<p><strong>Nk03's Updated and Optimized Solution:</strong></p>\n<blockquote>\n<p>CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms</p>\n</blockquote>\n",
        "question_body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "formatted_input": {
            "qid": 67856992,
            "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
            "question": {
                "title": "Element wise numeric comparison in Pandas dataframe column value with list",
                "ques_desc": "I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: "
            },
            "io": [
                "  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n",
                "  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n"
            ],
            "answer": {
                "ans_desc": "Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms ",
                "code": [
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n",
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 10566774,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-8T7iqZlWN8Y/AAAAAAAAAAI/AAAAAAAACu0/hiVsQlXBjzQ/photo.jpg?sz=128",
            "display_name": "Salvatore Nedia",
            "link": "https://stackoverflow.com/users/10566774/salvatore-nedia"
        },
        "is_answered": true,
        "view_count": 107,
        "accepted_answer_id": 67845512,
        "answer_count": 4,
        "score": 5,
        "last_activity_date": 1622859671,
        "creation_date": 1622854122,
        "last_edit_date": 1622854820,
        "question_id": 67845362,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
        "title": "Sort pandas df subset of rows (within a group) by specific column",
        "body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "answer_body": "<p>I think it should be as simple as this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.sort_values([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;])\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "formatted_input": {
            "qid": 67845362,
            "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
            "question": {
                "title": "Sort pandas df subset of rows (within a group) by specific column",
                "ques_desc": "I have the following dataframe let\u2019s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? "
            },
            "io": [
                "\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n",
                "\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n"
            ],
            "answer": {
                "ans_desc": "I think it should be as simple as this: ",
                "code": [
                    "df = df.sort_values([\"A\", \"B\", \"C\", \"D\"])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 12785115,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b3484fd202fb5624044a4ed7b5181fab?s=128&d=identicon&r=PG&f=1",
            "display_name": "RamboJ",
            "link": "https://stackoverflow.com/users/12785115/ramboj"
        },
        "is_answered": true,
        "view_count": 90,
        "accepted_answer_id": 61625210,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1622820639,
        "creation_date": 1588722398,
        "question_id": 61624957,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
        "title": "Creating a table in pandas from json",
        "body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "answer_body": "<p>You need to massage the data a little bit to get what you want.</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: x['website'][0])\n    .to_frame('website')\n)\n\n    website\n1   https://bitcoin.org/\n2   https://litecoin.org/\n</code></pre>\n\n<p>To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. </p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n</code></pre>\n\n<p>To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls'</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n</code></pre>\n",
        "question_body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 61624957,
            "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
            "question": {
                "title": "Creating a table in pandas from json",
                "ques_desc": "I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! "
            },
            "io": [
                "0     website\n0  1  NaN\n1  2  NaN \n",
                "0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n"
            ],
            "answer": {
                "ans_desc": "You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' ",
                "code": [
                    "(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n",
                    "(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 345,
            "user_id": 5426588,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://www.gravatar.com/avatar/068913af46eb9b38c9ffb53ba219c7c9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Baig",
            "link": "https://stackoverflow.com/users/5426588/baig"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 67822589,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1622728725,
        "creation_date": 1622726951,
        "last_edit_date": 1622727694,
        "question_id": 67822403,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
        "title": "Replicate dataframe n number of times and increment another column by 1",
        "body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "answer_body": "<p>Something along these lines should work:</p>\n<pre><code>df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "formatted_input": {
            "qid": 67822403,
            "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
            "question": {
                "title": "Replicate dataframe n number of times and increment another column by 1",
                "ques_desc": "I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. "
            },
            "io": [
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n",
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n"
            ],
            "answer": {
                "ans_desc": "Something along these lines should work: ",
                "code": [
                    "df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16067894,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a6cf989fce509918dc286c22d013c519?s=128&d=identicon&r=PG&f=1",
            "display_name": "lasse3434",
            "link": "https://stackoverflow.com/users/16067894/lasse3434"
        },
        "is_answered": true,
        "view_count": 74,
        "accepted_answer_id": 67782978,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1622554424,
        "creation_date": 1622521690,
        "last_edit_date": 1622521880,
        "question_id": 67782727,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
        "title": "Python - Delete lines from dataframe (pandas)",
        "body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "answer_body": "<p>Make a clean dataframe and <strong>keep</strong> values you want:</p>\n<pre><code>data[['1', '2']] = data[['1', '2']].replace({&quot;///&quot;: np.nan, &quot;,&quot;: &quot;.&quot;}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)]\n</code></pre>\n<pre><code>&gt;&gt;&gt; data\n               0      1      2\n0  9783630876672  12.35   2.62\n1  9783423282789  11.67   6.07\n2  9783833879500  17.25  12.40\n3  9783898798822   6.91   1.16\n4  9783453281417  12.93   2.84\n5  9783630876672  12.35   4.08\n6  9783423282789  11.67   6.07\n7  9783833879500  17.25   9.94\n8  9783898798822   6.91   2.96\n9  9783453281417  12.93   2.68\n</code></pre>\n<p><strong>Comments:</strong></p>\n<p><strong>1st line:</strong></p>\n<ul>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#different-choices-for-indexing\" rel=\"nofollow noreferrer\"><code>data[['1', '2']]</code></a> select columns named '1' and '2'</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\" rel=\"nofollow noreferrer\"><code>replace</code></a> change existing values ('///' and ',') by new ones ('nan' and '.')</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\" rel=\"nofollow noreferrer\"><code>astype(float)</code></a> convert your string columns to real numbers (float) since your dataframe is cleaned.</li>\n</ul>\n<p><strong>2nd line:</strong></p>\n<ul>\n<li><code>data.loc[...]</code> locate something in your dataframe</li>\n<li><code>data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)</code>: in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row.</li>\n</ul>\n",
        "question_body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "formatted_input": {
            "qid": 67782727,
            "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
            "question": {
                "title": "Python - Delete lines from dataframe (pandas)",
                "ques_desc": "I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. "
            },
            "io": [
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n",
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n"
            ],
            "answer": {
                "ans_desc": "Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. ",
                "code": [
                    "data[['1', '2']] = data[['1', '2']].replace({\"///\": np.nan, \",\": \".\"}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[\"1\", \"2\"]].ge(1.).all(axis=\"columns\")]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "regex",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 185,
            "user_id": 13196248,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/--wvokyexUe4/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJMneagY7ciQcziCtlVV3rVrRFH57A/photo.jpg?sz=128",
            "display_name": "Charles",
            "link": "https://stackoverflow.com/users/13196248/charles"
        },
        "is_answered": true,
        "view_count": 69,
        "accepted_answer_id": 67770609,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1622454286,
        "creation_date": 1622448681,
        "question_id": 67770056,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
        "title": "How to create columns from a string in a dataframe?",
        "body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "answer_body": "<pre><code>ndf = (df[&quot;long string&quot;]\n         .str.extractall(r&quot;(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)&quot;)\n         .droplevel(&quot;match&quot;)\n         .set_index(0, append=True)\n         .set_axis([&quot;tra&quot;, &quot;la&quot;], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(&quot;-&quot;.join)\n</code></pre>\n<ul>\n<li>Extract the desired parts with a <a href=\"https://regex101.com/r/F3EKrX/1\" rel=\"nofollow noreferrer\">regex</a></li>\n<li>Drop the index level induced by <code>extractall</code> called <code>match</code></li>\n<li>Append the <code>ha-hi-ho</code> matches as the index (<code>0</code> is first capturing group)</li>\n<li>Rename the remaining columns <code>tra</code> and <code>la</code></li>\n<li>Unstack the <code>ha-hi-ho</code> index to the columns</li>\n<li>Swap the <code>ha-hi-ho</code> and <code>tra-la</code> levels' order in columns so that <code>ha-hi-ho</code> is upper</li>\n<li>Lastly join these levels of columns' names with a hyphen</li>\n</ul>\n<p>to get</p>\n<pre><code>  ha-tra hi-tra ho-tra ha-la hi-la ho-la\n0      1      1      1     2     2     2\n1      1      1      1     2     2     2\n2      1      1      1     2     2     2\n</code></pre>\n",
        "question_body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "formatted_input": {
            "qid": 67770056,
            "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
            "question": {
                "title": "How to create columns from a string in a dataframe?",
                "ques_desc": "WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. "
            },
            "io": [
                "    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n",
                "    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n"
            ],
            "answer": {
                "ans_desc": " Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get ",
                "code": [
                    "ndf = (df[\"long string\"]\n         .str.extractall(r\"(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)\")\n         .droplevel(\"match\")\n         .set_index(0, append=True)\n         .set_axis([\"tra\", \"la\"], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(\"-\".join)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 15286348,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a1dfedef68132d2be690d3ce84d9aff5?s=128&d=identicon&r=PG",
            "display_name": "lynch1972",
            "link": "https://stackoverflow.com/users/15286348/lynch1972"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 67697000,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621987482,
        "creation_date": 1621985797,
        "question_id": 67696814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
        "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
        "body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "answer_body": "<p>Tr this. This would depend on how large your data is.</p>\n<pre><code>\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n</code></pre>\n",
        "question_body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 67696814,
            "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
            "question": {
                "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
                "ques_desc": "I have this code with a dictionary \"dict\": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! "
            },
            "io": [
                "                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n",
                "        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n"
            ],
            "answer": {
                "ans_desc": "Tr this. This would depend on how large your data is. ",
                "code": [
                    "\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 352,
            "user_id": 8147508,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/dbf0ab4fedaca38f5f4ae3c2344f9b95?s=128&d=identicon&r=PG&f=1",
            "display_name": "Dev Ananth",
            "link": "https://stackoverflow.com/users/8147508/dev-ananth"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67871572,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1623073970,
        "creation_date": 1623063317,
        "last_edit_date": 1623065349,
        "question_id": 67870323,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
        "title": "How to get list of previous n values of a column conditionally in DataFrame?",
        "body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "answer_body": "<p>Since rolling only supports production of numeric values, this has to be a work around.</p>\n<p>Try <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html#pandas-dataframe-sort-values\" rel=\"nofollow noreferrer\"><code>sort_values</code></a> first then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html\" rel=\"nofollow noreferrer\"><code>groupby rolling</code></a> on window + 1 and strip off the last element:</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n5         1      5         [15]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n11        1     12   [5, 20, 8]\n1         2      0           []\n3         2     30          [0]\n7         2      7      [0, 30]\n10        2      9   [0, 30, 7]\n2         3     18           []\n4         3     17         [18]\n6         4      9           []\n</code></pre>\n<p>Then <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html#pandas-dataframe-sort-index\" rel=\"nofollow noreferrer\"><code>sort_index</code></a> to restore the initial order:</p>\n<pre><code>df = df.sort_index()\n</code></pre>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [0, 30]\n8         1     20      [15, 5]\n9         1      8  [15, 5, 20]\n10        2      9   [0, 30, 7]\n11        1     12   [5, 20, 8]\n</code></pre>\n<p>(Optional use extended slicing to reverse the lists and get elements in same order as expected output above):</p>\n<pre><code>window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>    Subject  Score     Previous\n0         1     15           []\n1         2      0           []\n2         3     18           []\n3         2     30          [0]\n4         3     17         [18]\n5         1      5         [15]\n6         4      9           []\n7         2      7      [30, 0]\n8         1     20      [5, 15]\n9         1      8  [20, 5, 15]\n10        2      9   [7, 30, 0]\n11        1     12   [8, 20, 5]\n</code></pre>\n<hr/>\n<p>Complete Working Example:</p>\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'Subject': [1, 2, 3, 2, 3, 1, 4, 2, 1, 1, 2, 1],\n    'Score': [15, 0, 18, 30, 17, 5, 9, 7, 20, 8, 9, 12]\n})\n\nwindow = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\ndf = df.sort_index()\n\nprint(df)\n</code></pre>\n",
        "question_body": "<p>My dataframe looks like below:</p>\n<pre><code>Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n</code></pre>\n<p>I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below:</p>\n<pre><code>Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n</code></pre>\n<p>Below code rolls all record not grouped by Subject</p>\n<pre><code>df['Previous']= [x.values.tolist()[:-1] for x in points['Score'].rolling(4)]\n</code></pre>\n<p>How do I get the above expected result?</p>\n",
        "formatted_input": {
            "qid": 67870323,
            "link": "https://stackoverflow.com/questions/67870323/how-to-get-list-of-previous-n-values-of-a-column-conditionally-in-dataframe",
            "question": {
                "title": "How to get list of previous n values of a column conditionally in DataFrame?",
                "ques_desc": "My dataframe looks like below: I want to get the previous 3 scores for each record grouped by Subject as a list in new column like below: Below code rolls all record not grouped by Subject How do I get the above expected result? "
            },
            "io": [
                "Subject     Score\n    1       15\n    2       0\n    3       18\n    2       30\n    3       17\n    1       5\n    4       9\n    2       7\n    1       20\n    1       8\n    2       9\n    1       12\n",
                "Subject   Score Previous\n1       15      []\n2       0       []\n3       18      []\n2       30      [0]\n3       17      [18]\n1       5       [15]\n4       9       []\n2       7       [30,0]\n1       20      [5,15]\n1       8       [20,5,15]\n2       9       [7,30,0]\n1       12      [8,20,5]\n"
            ],
            "answer": {
                "ans_desc": "Since rolling only supports production of numeric values, this has to be a work around. Try first then on window + 1 and strip off the last element: Then to restore the initial order: (Optional use extended slicing to reverse the lists and get elements in same order as expected output above): : Complete Working Example: ",
                "code": [
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [\n    x.agg(list)[:-1] for x in df.groupby('Subject')['Score'].rolling(window + 1)\n]\n",
                    "window = 3\ndf = df.sort_values('Subject')\ndf['Previous'] = [x.agg(list)[-2::-1]\n                  for x in df.groupby('Subject')['Score'].rolling(window + 1)]\ndf = df.sort_index()\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy",
            "indexing"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 8970043,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/uL3Er.jpg?s=128&g=1",
            "display_name": "Futurex",
            "link": "https://stackoverflow.com/users/8970043/futurex"
        },
        "is_answered": true,
        "view_count": 24,
        "accepted_answer_id": 67870693,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1623064916,
        "creation_date": 1623064484,
        "question_id": 67870585,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
        "title": "Python dataframe create index column based on other id column",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "answer_body": "<p>Try <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.ngroup.html#pandas-core-groupby-groupby-ngroup\" rel=\"nofollow noreferrer\"><code>groupby ngroup</code></a> + 1 :</p>\n<pre><code>df['ID_2'] = df.groupby('ID').ngroup() + 1\n</code></pre>\n<p>Or with <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rank.html#pandas-series-rank\" rel=\"nofollow noreferrer\"><code>Rank</code></a>:</p>\n<pre><code>df['ID_2'] = df['ID'].rank(method='dense').astype(int)\n</code></pre>\n<p><code>df</code>:</p>\n<pre><code>                 ID   Price  ID_2\n0  000afb96ded6677c  1514.5     1\n1  000afb96ded6677c    13.0     1\n2  000afb96ded6677c   611.0     1\n3  000afb96ded6677c   723.0     1\n4  000afb96ded6677c  2065.0     1\n5  ffea14e87a4e1269  2286.0     2\n6  ffea14e87a4e1269  1150.0     2\n7  ffea14e87a4e1269    80.0     2\n8  fff455057ad492da   650.0     3\n9  fff5fc66c1fd66c2   450.0     4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n</code></pre>\n<p>I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below:</p>\n<pre><code>ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67870585,
            "link": "https://stackoverflow.com/questions/67870585/python-dataframe-create-index-column-based-on-other-id-column",
            "question": {
                "title": "Python dataframe create index column based on other id column",
                "ques_desc": "I have a dataframe like this: I need an ID column which iterates from 1 to however many rows there are but i need it to be like in the code below: "
            },
            "io": [
                "ID                  Price\n000afb96ded6677c    1514.5\n000afb96ded6677c    13.0\n000afb96ded6677c    611.0\n000afb96ded6677c    723.0\n000afb96ded6677c    2065.0\nffea14e87a4e1269    2286.0\nffea14e87a4e1269    1150.0\nffea14e87a4e1269    80.0\nfff455057ad492da    650.0\nfff5fc66c1fd66c2    450.0\n",
                "ID                  Price    ID 2\n000afb96ded6677c    1514.5   1\n000afb96ded6677c    13.0     1\n000afb96ded6677c    611.0    1\n000afb96ded6677c    723.0    1\n000afb96ded6677c    2065.0   1\nffea14e87a4e1269    2286.0   2\nffea14e87a4e1269    1150.0   2\nffea14e87a4e1269    80.0     2\nfff455057ad492da    650.0    3\nfff5fc66c1fd66c2    450.0    4\n"
            ],
            "answer": {
                "ans_desc": "Try + 1 : Or with : : ",
                "code": [
                    "df['ID_2'] = df.groupby('ID').ngroup() + 1\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 16132607,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/234e69866fa5f5a2e96b28a4fd12597b?s=128&d=identicon&r=PG&f=1",
            "display_name": "homeboykeroro",
            "link": "https://stackoverflow.com/users/16132607/homeboykeroro"
        },
        "is_answered": true,
        "view_count": 85,
        "accepted_answer_id": 67857571,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1622976694,
        "creation_date": 1622966815,
        "last_edit_date": 1622970680,
        "question_id": 67856992,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
        "title": "Element wise numeric comparison in Pandas dataframe column value with list",
        "body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "answer_body": "<p>Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise).</p>\n<p>You can use <code>apply</code>:</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n</code></pre>\n<hr />\n<pre><code>res = df2.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<p><strong>res:</strong></p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th style=\"text-align: center;\"></th>\n<th style=\"text-align: center;\">A</th>\n<th style=\"text-align: center;\">B</th>\n<th style=\"text-align: center;\">C</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\"></td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n<td style=\"text-align: center;\">Result</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">0</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">1</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n<td style=\"text-align: center;\">[True, False, False]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">2</td>\n<td style=\"text-align: center;\">[True, True, True]</td>\n<td style=\"text-align: center;\">[False, False, False]</td>\n<td style=\"text-align: center;\">[True, True, False]</td>\n</tr>\n</tbody>\n</table>\n</div><h2>Update</h2>\n<p>(Complete Solution Based on the data you've provided):</p>\n<pre><code>def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x &gt;= min_val) &amp; (x &lt;= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n</code></pre>\n<hr />\n<h2>Time Comparison:</h2>\n<p><strong>Method 1 (Nk03's method1):</strong></p>\n<blockquote>\n<p>CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms</p>\n</blockquote>\n<p><strong>Method 2 (Nk03's method2):</strong></p>\n<blockquote>\n<p>CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms</p>\n</blockquote>\n<p><strong>Method 3 (Using numpy based comparison):</strong></p>\n<blockquote>\n<p>CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms</p>\n</blockquote>\n<p><strong>Nk03's Updated and Optimized Solution:</strong></p>\n<blockquote>\n<p>CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms</p>\n</blockquote>\n",
        "question_body": "<p>I have 3 pandas multiindex column dataframes</p>\n<p>dataframe 1(minimum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Min  |  Min  |  Min |\n  |-------|-------|------|\n0 | 26.47 | 17.31 | 1.26 |\n1 | 27.23 | 14.38 | 1.36 |\n2 | 27.23 | 18.88 | 1.28 |\n</code></pre>\n<p>dataframe 2 (value used to compare with)</p>\n<p>row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray</p>\n<pre><code>  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n</code></pre>\n<p>dataframe 3(maximum value):</p>\n<pre><code>  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n</code></pre>\n<p>Expected result:</p>\n<pre><code>  |          A          |           B           |          C           |\n  |        Result       |          Result       |         Result       |\n  |---------------------|-----------------------|----------------------|\n0 | [True, True, False] |  [True, True, False]  | [True, True, True]   |\n1 | [True, True, True]  | [True, False, False]  | [True, False, False] |\n2 | [True, True, True]  | [False, False, False] | [True, True, False]  |\n</code></pre>\n<p>I'd like to perform element wise comparison in this way:</p>\n<pre><code>min &lt;= each element in ndarray &lt;= max\n</code></pre>\n<p>i.e</p>\n<pre><code>for row 0:\n\n26.47 &lt;= [27.58,28.37,28.73] &lt;= 28.68\n\n17.31 &lt;= [17.31, 18.42, 18.72] &lt;= 18.42\n\n1.26 &lt;= [1.36, 1.28, 1.27] &lt;= 1.37\n</code></pre>\n<p>and so on</p>\n<p>I tried <code>( datafram2 &gt;= dataframe3 ) &amp; ( datafram2 &lt;= datafram3 )</code> but not work.</p>\n<p>What's the simplest way and fastest way to compute the result?</p>\n<p>Example dataframe code:</p>\n<pre><code>min_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Min' ] ] )\nval_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Val' ] ] )\nmax_columns = pd.MultiIndex.from_product( [ [ 'A', 'B', 'C' ], [ 'Max' ] ] )\n\nmin_df = pd.DataFrame( [ [ 26.47, 17.31, 1.26 ], [ 27.23, 14.38, 1.36 ], [ 27.23, 18.88, 1.28 ] ], columns=min_columns )\nval_df = pd.DataFrame( [ [ [ 27.58, 28.37, 28.73 ], [ 17.31, 18.42, 18.72], [1.36, 1.28, 1.27 ] ] ] , columns=val_columns )\nmax_df = pd.DataFrame( [ [ 28.68, 18.42, 1.37 ], [ 29.50, 17.31, 1.47 ], [ 29.87, 20.45, 1.39 ] ] , columns=max_columns )\n</code></pre>\n",
        "formatted_input": {
            "qid": 67856992,
            "link": "https://stackoverflow.com/questions/67856992/element-wise-numeric-comparison-in-pandas-dataframe-column-value-with-list",
            "question": {
                "title": "Element wise numeric comparison in Pandas dataframe column value with list",
                "ques_desc": "I have 3 pandas multiindex column dataframes dataframe 1(minimum value): dataframe 2 (value used to compare with) row 0, row 1 and row 2 are the same, I extend the dataframe to three row for comparison with min and max dataframe. Value in each dataframe cell is ndarray dataframe 3(maximum value): Expected result: I'd like to perform element wise comparison in this way: i.e and so on I tried but not work. What's the simplest way and fastest way to compute the result? Example dataframe code: "
            },
            "io": [
                "  |          A          |           B           |          C         |\n  |         Val         |          Val          |         Val        |\n  |---------------------|-----------------------|--------------------|\n0 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n1 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n2 | [27.58,28.37,28.73] | [17.31, 18.42, 18.72] | [1.36, 1.28, 1.27] |\n",
                "  |  A    |   B   |  C   |\n  |  Max  |  Max  |  Max |\n  |-------|-------|------|\n0 | 28.68 | 18.42 | 1.37 |\n1 | 29.50 | 17.31 | 1.47 |\n2 | 29.87 | 20.45 | 1.39 |\n"
            ],
            "answer": {
                "ans_desc": "Just turn the column values into NumPy arrays. and simply treat it as an array comparing problem (row wise). You can use : res: A B C Result Result Result 0 [True, True, False] [True, True, False] [True, True, True] 1 [True, True, True] [True, False, False] [True, False, False] 2 [True, True, True] [False, False, False] [True, True, False] Update (Complete Solution Based on the data you've provided): Time Comparison: Method 1 (Nk03's method1): CPU times: user 19.5 ms, sys: 0 ns, total: 19.5 ms Wall time: 18.9 ms Method 2 (Nk03's method2): CPU times: user 23 ms, sys: 102 \u00b5s, total: 23.1 ms Wall time: 21.9 ms Method 3 (Using numpy based comparison): CPU times: user 8.76 ms, sys: 26 \u00b5s, total: 8.79 ms Wall time: 8.91 ms Nk03's Updated and Optimized Solution: CPU times: user 16 ms, sys: 0 ns, total: 16 ms Wall time: 15.5 ms ",
                "code": [
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = df1[pd.IndexSlice[col]].to_numpy()\n    max_val = df3[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n",
                    "def bool_check(row):\n    col = row.name[0]\n    min_val = min_df[pd.IndexSlice[col]].to_numpy()\n    max_val = max_df[pd.IndexSlice[col]].to_numpy()\n    x = np.array(row.tolist())\n    return list((x >= min_val) & (x <= max_val))\n\nres = val_df.apply(bool_check,axis=0).rename(columns={'Val':'Result'})\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 129,
            "user_id": 10566774,
            "user_type": "registered",
            "profile_image": "https://lh5.googleusercontent.com/-8T7iqZlWN8Y/AAAAAAAAAAI/AAAAAAAACu0/hiVsQlXBjzQ/photo.jpg?sz=128",
            "display_name": "Salvatore Nedia",
            "link": "https://stackoverflow.com/users/10566774/salvatore-nedia"
        },
        "is_answered": true,
        "view_count": 107,
        "accepted_answer_id": 67845512,
        "answer_count": 4,
        "score": 5,
        "last_activity_date": 1622859671,
        "creation_date": 1622854122,
        "last_edit_date": 1622854820,
        "question_id": 67845362,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
        "title": "Sort pandas df subset of rows (within a group) by specific column",
        "body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "answer_body": "<p>I think it should be as simple as this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df = df.sort_values([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;])\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe let\u2019s say:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n</code></pre>\n<p>And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case)</p>\n<p>The expected output would be:</p>\n<p>df</p>\n<pre><code>\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n</code></pre>\n<p>Any help for this kind of operation?</p>\n",
        "formatted_input": {
            "qid": 67845362,
            "link": "https://stackoverflow.com/questions/67845362/sort-pandas-df-subset-of-rows-within-a-group-by-specific-column",
            "question": {
                "title": "Sort pandas df subset of rows (within a group) by specific column",
                "ques_desc": "I have the following dataframe let\u2019s say: df And I would like to sort it based on col D for each sub row (that has for example same cols A,B and C in this case) The expected output would be: df Any help for this kind of operation? "
            },
            "io": [
                "\nA B C D E\nz k s 7 d\nz k s 6 l\nx t r 2 e\nx t r 1 x\nu c r 8 f\nu c r 9 h\ny t s 5 l\ny t s 2 o\n",
                "\nA B C D E\nz k s 6 l\nz k s 7 d\nx t r 1 x\nx t r 2 e\nu c r 8 f\nu c r 9 h\ny t s 2 o\ny t s 5 l\n"
            ],
            "answer": {
                "ans_desc": "I think it should be as simple as this: ",
                "code": [
                    "df = df.sort_values([\"A\", \"B\", \"C\", \"D\"])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "json",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 15,
            "user_id": 12785115,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/b3484fd202fb5624044a4ed7b5181fab?s=128&d=identicon&r=PG&f=1",
            "display_name": "RamboJ",
            "link": "https://stackoverflow.com/users/12785115/ramboj"
        },
        "is_answered": true,
        "view_count": 90,
        "accepted_answer_id": 61625210,
        "answer_count": 1,
        "score": -1,
        "last_activity_date": 1622820639,
        "creation_date": 1588722398,
        "question_id": 61624957,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
        "title": "Creating a table in pandas from json",
        "body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "answer_body": "<p>You need to massage the data a little bit to get what you want.</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: x['website'][0])\n    .to_frame('website')\n)\n\n    website\n1   https://bitcoin.org/\n2   https://litecoin.org/\n</code></pre>\n\n<p>To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. </p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n</code></pre>\n\n<p>To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls'</p>\n\n<pre><code>(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n</code></pre>\n",
        "question_body": "<p>I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request:</p>\n\n<p><code>data = {'status': {'timestamp': '2020-05-05T21:34:45.057Z', 'error_code': 0, 'error_message': None, 'elapsed': 8, 'credit_count': 1, 'notice': None}, 'data': {'1': {'urls': {'website': ['https://bitcoin.org/'], 'technical_doc': ['https://bitcoin.org/bitcoin.pdf'], 'twitter': [], 'reddit': ['https://reddit.com/r/bitcoin'], 'message_board': ['https://bitcointalk.org'], 'announcement': [], 'chat': [], 'explorer': ['https://blockchain.coinmarketcap.com/chain/bitcoin', 'https://blockchain.info/', 'https://live.blockcypher.com/btc/', 'https://blockchair.com/bitcoin', 'https://explorer.viabtc.com/btc'], 'source_code': ['https://github.com/bitcoin/']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/1.png', 'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'description': 'Bitcoin (BTC) is a consensus network that enables a new payment system and a completely digital currency. Powered by its users, it is a peer to peer payment network that requires no central authority to operate. On October 31st, 2008, an individual or group of individuals operating under the pseudonym \"Satoshi Nakamoto\" published the Bitcoin Whitepaper and described it as: \"a purely peer-to-peer version of electronic cash, which would allow online payments to be sent directly from one party to another without going through a financial institution.\"', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}, '2': {'urls': {'website': ['https://litecoin.org/'], 'technical_doc': [], 'twitter': ['https://twitter.com/LitecoinProject'], 'reddit': ['https://reddit.com/r/litecoin'], 'message_board': ['https://litecointalk.io/', 'https://litecoin-foundation.org/'], 'announcement': ['https://bitcointalk.org/index.php?topic=47417.0'], 'chat': ['https://telegram.me/litecoin'], 'explorer': ['https://blockchair.com/litecoin', 'https://chainz.cryptoid.info/ltc/', 'http://explorer.litecoin.net/chain/Litecoin', 'https://ltc.tokenview.com/en', 'https://explorer.viabtc.com/ltc'], 'source_code': ['https://github.com/litecoin-project/litecoin']}, 'logo': 'https://s2.coinmarketcap.com/static/img/coins/64x64/2.png', 'id': 2, 'name': 'Litecoin', 'symbol': 'LTC', 'slug': 'litecoin', 'description': 'Litecoin is a peer-to-peer cryptocurrency created by Charlie Lee. It was created based on the Bitcoin protocol but differs in terms of the hashing algorithm used. Litecoin uses the memory intensive Scrypt proof of work mining algorithm. Scrypt allows consumer-grade hardware such as GPU to mine those coins.', 'notice': None, 'date_added': '2013-04-28T00:00:00.000Z', 'tags': ['mineable'], 'tag-names': ['Mineable'], 'tag-groups': ['APPLICATION'], 'is_hidden': 0, 'platform': None, 'category': 'coin'}}}</code></p>\n\n<p>the code:</p>\n\n<pre><code>result = pd.json_normalize(data,'data',['website'], errors='ignore')\nprint(result)\n\n</code></pre>\n\n<p>the output:</p>\n\n<pre><code>0     website\n0  1  NaN\n1  2  NaN \n</code></pre>\n\n<p>What I am trying to achieve is something like that:</p>\n\n<pre><code>0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n</code></pre>\n\n<p>I've tried so many things and really frustrated. Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 61624957,
            "link": "https://stackoverflow.com/questions/61624957/creating-a-table-in-pandas-from-json",
            "question": {
                "title": "Creating a table in pandas from json",
                "ques_desc": "I am really stuck in creating a table from nested json. The json output from a Coinmarketcap API request: the code: the output: What I am trying to achieve is something like that: I've tried so many things and really frustrated. Thanks in advance! "
            },
            "io": [
                "0     website\n0  1  NaN\n1  2  NaN \n",
                "0     website\n0  1  https://bitcoin.org/\n1  2  https://litecoin.org/\n"
            ],
            "answer": {
                "ans_desc": "You need to massage the data a little bit to get what you want. To get technical_doc, you can do something similar. The reason you got error is because for some elements there is no tech doc. To get logo, you need to loc 'logo' instead of 'urls' as it's at the same level as 'urls' ",
                "code": [
                    "(\n    pd.DataFrame(data['data'])\n    .loc['urls']\n    .apply(lambda x: (x['technical_doc']+[''])[0])\n    .to_frame('website')\n)\n",
                    "(\n    pd.DataFrame(data['data'])\n    .loc['logo']\n    .to_frame('logo')\n)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 345,
            "user_id": 5426588,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://www.gravatar.com/avatar/068913af46eb9b38c9ffb53ba219c7c9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Baig",
            "link": "https://stackoverflow.com/users/5426588/baig"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 67822589,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1622728725,
        "creation_date": 1622726951,
        "last_edit_date": 1622727694,
        "question_id": 67822403,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
        "title": "Replicate dataframe n number of times and increment another column by 1",
        "body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "answer_body": "<p>Something along these lines should work:</p>\n<pre><code>df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "formatted_input": {
            "qid": 67822403,
            "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
            "question": {
                "title": "Replicate dataframe n number of times and increment another column by 1",
                "ques_desc": "I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. "
            },
            "io": [
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n",
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n"
            ],
            "answer": {
                "ans_desc": "Something along these lines should work: ",
                "code": [
                    "df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16067894,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a6cf989fce509918dc286c22d013c519?s=128&d=identicon&r=PG&f=1",
            "display_name": "lasse3434",
            "link": "https://stackoverflow.com/users/16067894/lasse3434"
        },
        "is_answered": true,
        "view_count": 74,
        "accepted_answer_id": 67782978,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1622554424,
        "creation_date": 1622521690,
        "last_edit_date": 1622521880,
        "question_id": 67782727,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
        "title": "Python - Delete lines from dataframe (pandas)",
        "body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "answer_body": "<p>Make a clean dataframe and <strong>keep</strong> values you want:</p>\n<pre><code>data[['1', '2']] = data[['1', '2']].replace({&quot;///&quot;: np.nan, &quot;,&quot;: &quot;.&quot;}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)]\n</code></pre>\n<pre><code>&gt;&gt;&gt; data\n               0      1      2\n0  9783630876672  12.35   2.62\n1  9783423282789  11.67   6.07\n2  9783833879500  17.25  12.40\n3  9783898798822   6.91   1.16\n4  9783453281417  12.93   2.84\n5  9783630876672  12.35   4.08\n6  9783423282789  11.67   6.07\n7  9783833879500  17.25   9.94\n8  9783898798822   6.91   2.96\n9  9783453281417  12.93   2.68\n</code></pre>\n<p><strong>Comments:</strong></p>\n<p><strong>1st line:</strong></p>\n<ul>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#different-choices-for-indexing\" rel=\"nofollow noreferrer\"><code>data[['1', '2']]</code></a> select columns named '1' and '2'</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\" rel=\"nofollow noreferrer\"><code>replace</code></a> change existing values ('///' and ',') by new ones ('nan' and '.')</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\" rel=\"nofollow noreferrer\"><code>astype(float)</code></a> convert your string columns to real numbers (float) since your dataframe is cleaned.</li>\n</ul>\n<p><strong>2nd line:</strong></p>\n<ul>\n<li><code>data.loc[...]</code> locate something in your dataframe</li>\n<li><code>data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)</code>: in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row.</li>\n</ul>\n",
        "question_body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "formatted_input": {
            "qid": 67782727,
            "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
            "question": {
                "title": "Python - Delete lines from dataframe (pandas)",
                "ques_desc": "I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. "
            },
            "io": [
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n",
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n"
            ],
            "answer": {
                "ans_desc": "Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. ",
                "code": [
                    "data[['1', '2']] = data[['1', '2']].replace({\"///\": np.nan, \",\": \".\"}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[\"1\", \"2\"]].ge(1.).all(axis=\"columns\")]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "regex",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 185,
            "user_id": 13196248,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/--wvokyexUe4/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJMneagY7ciQcziCtlVV3rVrRFH57A/photo.jpg?sz=128",
            "display_name": "Charles",
            "link": "https://stackoverflow.com/users/13196248/charles"
        },
        "is_answered": true,
        "view_count": 69,
        "accepted_answer_id": 67770609,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1622454286,
        "creation_date": 1622448681,
        "question_id": 67770056,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
        "title": "How to create columns from a string in a dataframe?",
        "body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "answer_body": "<pre><code>ndf = (df[&quot;long string&quot;]\n         .str.extractall(r&quot;(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)&quot;)\n         .droplevel(&quot;match&quot;)\n         .set_index(0, append=True)\n         .set_axis([&quot;tra&quot;, &quot;la&quot;], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(&quot;-&quot;.join)\n</code></pre>\n<ul>\n<li>Extract the desired parts with a <a href=\"https://regex101.com/r/F3EKrX/1\" rel=\"nofollow noreferrer\">regex</a></li>\n<li>Drop the index level induced by <code>extractall</code> called <code>match</code></li>\n<li>Append the <code>ha-hi-ho</code> matches as the index (<code>0</code> is first capturing group)</li>\n<li>Rename the remaining columns <code>tra</code> and <code>la</code></li>\n<li>Unstack the <code>ha-hi-ho</code> index to the columns</li>\n<li>Swap the <code>ha-hi-ho</code> and <code>tra-la</code> levels' order in columns so that <code>ha-hi-ho</code> is upper</li>\n<li>Lastly join these levels of columns' names with a hyphen</li>\n</ul>\n<p>to get</p>\n<pre><code>  ha-tra hi-tra ho-tra ha-la hi-la ho-la\n0      1      1      1     2     2     2\n1      1      1      1     2     2     2\n2      1      1      1     2     2     2\n</code></pre>\n",
        "question_body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "formatted_input": {
            "qid": 67770056,
            "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
            "question": {
                "title": "How to create columns from a string in a dataframe?",
                "ques_desc": "WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. "
            },
            "io": [
                "    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n",
                "    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n"
            ],
            "answer": {
                "ans_desc": " Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get ",
                "code": [
                    "ndf = (df[\"long string\"]\n         .str.extractall(r\"(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)\")\n         .droplevel(\"match\")\n         .set_index(0, append=True)\n         .set_axis([\"tra\", \"la\"], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(\"-\".join)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 15286348,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a1dfedef68132d2be690d3ce84d9aff5?s=128&d=identicon&r=PG",
            "display_name": "lynch1972",
            "link": "https://stackoverflow.com/users/15286348/lynch1972"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 67697000,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621987482,
        "creation_date": 1621985797,
        "question_id": 67696814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
        "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
        "body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "answer_body": "<p>Tr this. This would depend on how large your data is.</p>\n<pre><code>\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n</code></pre>\n",
        "question_body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 67696814,
            "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
            "question": {
                "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
                "ques_desc": "I have this code with a dictionary \"dict\": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! "
            },
            "io": [
                "                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n",
                "        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n"
            ],
            "answer": {
                "ans_desc": "Tr this. This would depend on how large your data is. ",
                "code": [
                    "\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 548,
            "user_id": 5560529,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/dEQgB.jpg?s=128&g=1",
            "display_name": "Peter",
            "link": "https://stackoverflow.com/users/5560529/peter"
        },
        "is_answered": true,
        "view_count": 71,
        "accepted_answer_id": 67672293,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1621860569,
        "creation_date": 1621859819,
        "question_id": 67672199,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
        "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
        "body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "answer_body": "<p>I think you just need <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> with boolean indexing on <code>df1</code>.</p>\n<pre><code>pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n</code></pre>\n<p>The <code>df1[df1['C']==43]]</code> part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2.</p>\n<p>Output:</p>\n<pre><code>    A   B   C\n0   4   c   12\n1   5   d   19\n2   2   b   43\n</code></pre>\n",
        "question_body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "formatted_input": {
            "qid": 67672199,
            "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
            "question": {
                "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
                "ques_desc": "Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? "
            },
            "io": [
                "A    B    C\n4    c    12\n5    d    19\n2    b    43\n",
                "    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n"
            ],
            "answer": {
                "ans_desc": "I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: ",
                "code": [
                    "pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "reputation": 31,
            "user_id": 12537783,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4dec20a3cd9053e9e8a1751561acb545?s=128&d=identicon&r=PG&f=1",
            "display_name": "tester559",
            "link": "https://stackoverflow.com/users/12537783/tester559"
        },
        "is_answered": true,
        "view_count": 33,
        "closed_date": 1621790691,
        "accepted_answer_id": 67662500,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621790367,
        "creation_date": 1621789806,
        "question_id": 67662415,
        "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
        "closed_reason": "Duplicate",
        "title": "Working with list inside a Pandas dataframe",
        "body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "answer_body": "<p>I'd iterate through each element in <code>Column1</code>, get its length, save it in a list and then assign it to a new <code>Column2</code>. This would be summarized with:</p>\n<pre><code>df['Column2'] = [len(x) for x in df['Column1']]\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "formatted_input": {
            "qid": 67662415,
            "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
            "question": {
                "title": "Working with list inside a Pandas dataframe",
                "ques_desc": "I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? "
            },
            "io": [
                "ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n",
                "ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n"
            ],
            "answer": {
                "ans_desc": "I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: ",
                "code": [
                    "df['Column2'] = [len(x) for x in df['Column1']]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 345,
            "user_id": 5426588,
            "user_type": "registered",
            "accept_rate": 86,
            "profile_image": "https://www.gravatar.com/avatar/068913af46eb9b38c9ffb53ba219c7c9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Baig",
            "link": "https://stackoverflow.com/users/5426588/baig"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 67822589,
        "answer_count": 4,
        "score": 0,
        "last_activity_date": 1622728725,
        "creation_date": 1622726951,
        "last_edit_date": 1622727694,
        "question_id": 67822403,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
        "title": "Replicate dataframe n number of times and increment another column by 1",
        "body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "answer_body": "<p>Something along these lines should work:</p>\n<pre><code>df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n</code></pre>\n<p>I want to achieve something like below:</p>\n<pre><code>S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n</code></pre>\n<p>In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe.</p>\n",
        "formatted_input": {
            "qid": 67822403,
            "link": "https://stackoverflow.com/questions/67822403/replicate-dataframe-n-number-of-times-and-increment-another-column-by-1",
            "question": {
                "title": "Replicate dataframe n number of times and increment another column by 1",
                "ques_desc": "I have a dataframe with more than thousand rows and approx 10 columns. I want to replicate the entire dataframe 20 times and increment index column with each dataframe replication. For example I want to achieve something like below: In the above example S/No column is incrementing once end of dataframe is reached not sure if I need to use group by function in order to achieve the above. Have checked few other thread but can only find incrementing values with each row but not based on complete dataframe. "
            },
            "io": [
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n",
                "S/No Column1 Column2 Column3\n1      123     abc     2.20\n1      234     bcd     1.19\n1      345     cde     1.22\n2      123     abc     2.20\n2      234     bcd     1.19\n2      345     cde     1.22\n3      123     abc     2.20\n3      234     bcd     1.19\n3      345     cde     1.22\n"
            ],
            "answer": {
                "ans_desc": "Something along these lines should work: ",
                "code": [
                    "df1=df.copy()\nfor i in range(1,20):\n    df['S/No']=df['S/No']+1\n    df1=pd.concat([df1,df])\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16067894,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a6cf989fce509918dc286c22d013c519?s=128&d=identicon&r=PG&f=1",
            "display_name": "lasse3434",
            "link": "https://stackoverflow.com/users/16067894/lasse3434"
        },
        "is_answered": true,
        "view_count": 74,
        "accepted_answer_id": 67782978,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1622554424,
        "creation_date": 1622521690,
        "last_edit_date": 1622521880,
        "question_id": 67782727,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
        "title": "Python - Delete lines from dataframe (pandas)",
        "body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "answer_body": "<p>Make a clean dataframe and <strong>keep</strong> values you want:</p>\n<pre><code>data[['1', '2']] = data[['1', '2']].replace({&quot;///&quot;: np.nan, &quot;,&quot;: &quot;.&quot;}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)]\n</code></pre>\n<pre><code>&gt;&gt;&gt; data\n               0      1      2\n0  9783630876672  12.35   2.62\n1  9783423282789  11.67   6.07\n2  9783833879500  17.25  12.40\n3  9783898798822   6.91   1.16\n4  9783453281417  12.93   2.84\n5  9783630876672  12.35   4.08\n6  9783423282789  11.67   6.07\n7  9783833879500  17.25   9.94\n8  9783898798822   6.91   2.96\n9  9783453281417  12.93   2.68\n</code></pre>\n<p><strong>Comments:</strong></p>\n<p><strong>1st line:</strong></p>\n<ul>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#different-choices-for-indexing\" rel=\"nofollow noreferrer\"><code>data[['1', '2']]</code></a> select columns named '1' and '2'</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\" rel=\"nofollow noreferrer\"><code>replace</code></a> change existing values ('///' and ',') by new ones ('nan' and '.')</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\" rel=\"nofollow noreferrer\"><code>astype(float)</code></a> convert your string columns to real numbers (float) since your dataframe is cleaned.</li>\n</ul>\n<p><strong>2nd line:</strong></p>\n<ul>\n<li><code>data.loc[...]</code> locate something in your dataframe</li>\n<li><code>data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)</code>: in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row.</li>\n</ul>\n",
        "question_body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "formatted_input": {
            "qid": 67782727,
            "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
            "question": {
                "title": "Python - Delete lines from dataframe (pandas)",
                "ques_desc": "I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. "
            },
            "io": [
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n",
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n"
            ],
            "answer": {
                "ans_desc": "Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. ",
                "code": [
                    "data[['1', '2']] = data[['1', '2']].replace({\"///\": np.nan, \",\": \".\"}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[\"1\", \"2\"]].ge(1.).all(axis=\"columns\")]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "regex",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 185,
            "user_id": 13196248,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/--wvokyexUe4/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJMneagY7ciQcziCtlVV3rVrRFH57A/photo.jpg?sz=128",
            "display_name": "Charles",
            "link": "https://stackoverflow.com/users/13196248/charles"
        },
        "is_answered": true,
        "view_count": 69,
        "accepted_answer_id": 67770609,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1622454286,
        "creation_date": 1622448681,
        "question_id": 67770056,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
        "title": "How to create columns from a string in a dataframe?",
        "body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "answer_body": "<pre><code>ndf = (df[&quot;long string&quot;]\n         .str.extractall(r&quot;(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)&quot;)\n         .droplevel(&quot;match&quot;)\n         .set_index(0, append=True)\n         .set_axis([&quot;tra&quot;, &quot;la&quot;], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(&quot;-&quot;.join)\n</code></pre>\n<ul>\n<li>Extract the desired parts with a <a href=\"https://regex101.com/r/F3EKrX/1\" rel=\"nofollow noreferrer\">regex</a></li>\n<li>Drop the index level induced by <code>extractall</code> called <code>match</code></li>\n<li>Append the <code>ha-hi-ho</code> matches as the index (<code>0</code> is first capturing group)</li>\n<li>Rename the remaining columns <code>tra</code> and <code>la</code></li>\n<li>Unstack the <code>ha-hi-ho</code> index to the columns</li>\n<li>Swap the <code>ha-hi-ho</code> and <code>tra-la</code> levels' order in columns so that <code>ha-hi-ho</code> is upper</li>\n<li>Lastly join these levels of columns' names with a hyphen</li>\n</ul>\n<p>to get</p>\n<pre><code>  ha-tra hi-tra ho-tra ha-la hi-la ho-la\n0      1      1      1     2     2     2\n1      1      1      1     2     2     2\n2      1      1      1     2     2     2\n</code></pre>\n",
        "question_body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "formatted_input": {
            "qid": 67770056,
            "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
            "question": {
                "title": "How to create columns from a string in a dataframe?",
                "ques_desc": "WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. "
            },
            "io": [
                "    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n",
                "    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n"
            ],
            "answer": {
                "ans_desc": " Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get ",
                "code": [
                    "ndf = (df[\"long string\"]\n         .str.extractall(r\"(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)\")\n         .droplevel(\"match\")\n         .set_index(0, append=True)\n         .set_axis([\"tra\", \"la\"], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(\"-\".join)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 15286348,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a1dfedef68132d2be690d3ce84d9aff5?s=128&d=identicon&r=PG",
            "display_name": "lynch1972",
            "link": "https://stackoverflow.com/users/15286348/lynch1972"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 67697000,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621987482,
        "creation_date": 1621985797,
        "question_id": 67696814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
        "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
        "body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "answer_body": "<p>Tr this. This would depend on how large your data is.</p>\n<pre><code>\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n</code></pre>\n",
        "question_body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 67696814,
            "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
            "question": {
                "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
                "ques_desc": "I have this code with a dictionary \"dict\": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! "
            },
            "io": [
                "                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n",
                "        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n"
            ],
            "answer": {
                "ans_desc": "Tr this. This would depend on how large your data is. ",
                "code": [
                    "\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 548,
            "user_id": 5560529,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/dEQgB.jpg?s=128&g=1",
            "display_name": "Peter",
            "link": "https://stackoverflow.com/users/5560529/peter"
        },
        "is_answered": true,
        "view_count": 71,
        "accepted_answer_id": 67672293,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1621860569,
        "creation_date": 1621859819,
        "question_id": 67672199,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
        "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
        "body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "answer_body": "<p>I think you just need <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> with boolean indexing on <code>df1</code>.</p>\n<pre><code>pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n</code></pre>\n<p>The <code>df1[df1['C']==43]]</code> part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2.</p>\n<p>Output:</p>\n<pre><code>    A   B   C\n0   4   c   12\n1   5   d   19\n2   2   b   43\n</code></pre>\n",
        "question_body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "formatted_input": {
            "qid": 67672199,
            "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
            "question": {
                "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
                "ques_desc": "Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? "
            },
            "io": [
                "A    B    C\n4    c    12\n5    d    19\n2    b    43\n",
                "    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n"
            ],
            "answer": {
                "ans_desc": "I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: ",
                "code": [
                    "pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "reputation": 31,
            "user_id": 12537783,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4dec20a3cd9053e9e8a1751561acb545?s=128&d=identicon&r=PG&f=1",
            "display_name": "tester559",
            "link": "https://stackoverflow.com/users/12537783/tester559"
        },
        "is_answered": true,
        "view_count": 33,
        "closed_date": 1621790691,
        "accepted_answer_id": 67662500,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621790367,
        "creation_date": 1621789806,
        "question_id": 67662415,
        "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
        "closed_reason": "Duplicate",
        "title": "Working with list inside a Pandas dataframe",
        "body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "answer_body": "<p>I'd iterate through each element in <code>Column1</code>, get its length, save it in a list and then assign it to a new <code>Column2</code>. This would be summarized with:</p>\n<pre><code>df['Column2'] = [len(x) for x in df['Column1']]\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "formatted_input": {
            "qid": 67662415,
            "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
            "question": {
                "title": "Working with list inside a Pandas dataframe",
                "ques_desc": "I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? "
            },
            "io": [
                "ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n",
                "ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n"
            ],
            "answer": {
                "ans_desc": "I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: ",
                "code": [
                    "df['Column2'] = [len(x) for x in df['Column1']]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 214,
            "user_id": 15313738,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/d6SVY.png?s=128&g=1",
            "display_name": "ganbaa",
            "link": "https://stackoverflow.com/users/15313738/ganbaa"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 67595963,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1621394938,
        "creation_date": 1621388779,
        "last_edit_date": 1621390133,
        "question_id": 67595888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
        "title": "how to extract each numbers from pandas string column to list?",
        "body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "answer_body": "<p>You can use <code>re</code> to find all the occurrences of the numbers either integer or float.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[['11.2', '17', '21'], ['25.2', '4.1', '53', '17', '78'], ['121.1', '14'], ['12']]\n</code></pre>\n<p>If you want, you can type cast them to <code>float</code>/<code>int</code> checking if the extracted string has <code>.</code> in them, something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[[11.2, 17, 21], [25.2, 4.1, 53, 17, 78], [121.1, 14], [12]]\n</code></pre>\n<p>As pointed by @Uts, we can directly call <code>findall</code> over <code>Series.str</code> as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA, listB, listC, listD = df.Column_A.str.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;)\n</code></pre>\n",
        "question_body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "formatted_input": {
            "qid": 67595888,
            "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
            "question": {
                "title": "how to extract each numbers from pandas string column to list?",
                "ques_desc": "How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: "
            },
            "io": [
                "Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n",
                "listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n"
            ],
            "answer": {
                "ans_desc": "You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: ",
                "code": [
                    "df['Column_A'].apply(lambda x: re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n",
                    "listA, listB, listC, listD = df.Column_A.str.findall(r\"[-+]?\\d*\\.\\d+|\\d+\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 25,
            "user_id": 16067894,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a6cf989fce509918dc286c22d013c519?s=128&d=identicon&r=PG&f=1",
            "display_name": "lasse3434",
            "link": "https://stackoverflow.com/users/16067894/lasse3434"
        },
        "is_answered": true,
        "view_count": 74,
        "accepted_answer_id": 67782978,
        "answer_count": 3,
        "score": 1,
        "last_activity_date": 1622554424,
        "creation_date": 1622521690,
        "last_edit_date": 1622521880,
        "question_id": 67782727,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
        "title": "Python - Delete lines from dataframe (pandas)",
        "body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "answer_body": "<p>Make a clean dataframe and <strong>keep</strong> values you want:</p>\n<pre><code>data[['1', '2']] = data[['1', '2']].replace({&quot;///&quot;: np.nan, &quot;,&quot;: &quot;.&quot;}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)]\n</code></pre>\n<pre><code>&gt;&gt;&gt; data\n               0      1      2\n0  9783630876672  12.35   2.62\n1  9783423282789  11.67   6.07\n2  9783833879500  17.25  12.40\n3  9783898798822   6.91   1.16\n4  9783453281417  12.93   2.84\n5  9783630876672  12.35   4.08\n6  9783423282789  11.67   6.07\n7  9783833879500  17.25   9.94\n8  9783898798822   6.91   2.96\n9  9783453281417  12.93   2.68\n</code></pre>\n<p><strong>Comments:</strong></p>\n<p><strong>1st line:</strong></p>\n<ul>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#different-choices-for-indexing\" rel=\"nofollow noreferrer\"><code>data[['1', '2']]</code></a> select columns named '1' and '2'</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\" rel=\"nofollow noreferrer\"><code>replace</code></a> change existing values ('///' and ',') by new ones ('nan' and '.')</li>\n<li><a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html\" rel=\"nofollow noreferrer\"><code>astype(float)</code></a> convert your string columns to real numbers (float) since your dataframe is cleaned.</li>\n</ul>\n<p><strong>2nd line:</strong></p>\n<ul>\n<li><code>data.loc[...]</code> locate something in your dataframe</li>\n<li><code>data[[&quot;1&quot;, &quot;2&quot;]].ge(1.).all(axis=&quot;columns&quot;)</code>: in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row.</li>\n</ul>\n",
        "question_body": "<p>I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea?</p>\n<p>My Code:</p>\n<pre><code>    import pandas as pd\n\ndef join():\n\n    open_momox_xlsx = &quot;momox_ergebnisse.xlsx&quot;\n    open_rebuy_xlsx = &quot;rebuy_ergebnisse.xlsx&quot;\n\n    rebuy_xlsx = pd.read_excel(open_rebuy_xlsx)\n    momox_xlsx = pd.read_excel(open_momox_xlsx)\n\n    rebuy_data = rebuy_xlsx[['ReBuy']]\n    isbn_data = rebuy_xlsx[['ISBN']]\n    momox_data = momox_xlsx[['Momox']]\n\n    dataframe = pd.DataFrame =({'ISBN': isbn_data, 'Rebuy': rebuy_data, 'Momox': momox_data})\n    data = pd.concat(dataframe,axis=1, ignore_index=True)\n\n    c=0\n    #print(data[0])\n    while c &lt; len(data):\n\n        if data[1][c] and data[2][c] == '///':\n            data.drop(index=c)\n        elif data[1][c] and data[2][c] &lt; '1':\n            data.drop(index=c)\n        elif data[1][c] or data[2][c] &lt; '1' and data[1][c] or data[2][c] == '///' :\n            data.drop(index=c)\n        c=c+1\n    print(data)\n</code></pre>\n<p>Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n</code></pre>\n<p>Wanted Output:</p>\n<pre><code>                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n</code></pre>\n<p>the if-statement seems to work properly, but the data.drop does not do what it should..</p>\n",
        "formatted_input": {
            "qid": 67782727,
            "link": "https://stackoverflow.com/questions/67782727/python-delete-lines-from-dataframe-pandas",
            "question": {
                "title": "Python - Delete lines from dataframe (pandas)",
                "ques_desc": "I am trying to delete certain information from a data frame, but the 'delete-command' (.drop) does not work like it should anyone got an idea? My Code: Output: Wanted Output: the if-statement seems to work properly, but the data.drop does not do what it should.. "
            },
            "io": [
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n10     3927905909    ///    ///\n11     3872948210    ///   0.15\n12  9783293003781    ///   0.15\n13  9783423246842    ///    ///\n14  9783423247146    ///    ///\n15  9783423246934    ///    ///\n16     387294116x    ///    ///\n17  9783935597456   0,16   0.15\n18  9783423204545    ///    ///\n",
                "                0      1      2\n0   9783630876672  12,35   2.62\n1   9783423282789  11,67   6.07\n2   9783833879500  17,25  12.40\n3   9783898798822   6,91   1.16\n4   9783453281417  12,93   2.84\n5   9783630876672  12,35   4.08\n6   9783423282789  11,67   6.07\n7   9783833879500  17,25   9.94\n8   9783898798822   6,91   2.96\n9   9783453281417  12,93   2.68\n"
            ],
            "answer": {
                "ans_desc": "Make a clean dataframe and keep values you want: Comments: 1st line: select columns named '1' and '2' change existing values ('///' and ',') by new ones ('nan' and '.') convert your string columns to real numbers (float) since your dataframe is cleaned. 2nd line: locate something in your dataframe : in columns '1' and '2', search values 'greater than or equal 1 and it must be true for 'all columns' of the row. ",
                "code": [
                    "data[['1', '2']] = data[['1', '2']].replace({\"///\": np.nan, \",\": \".\"}, regex=True)\n                                   .astype(float)\ndata = data.loc[data[[\"1\", \"2\"]].ge(1.).all(axis=\"columns\")]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "regex",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 185,
            "user_id": 13196248,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/--wvokyexUe4/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJMneagY7ciQcziCtlVV3rVrRFH57A/photo.jpg?sz=128",
            "display_name": "Charles",
            "link": "https://stackoverflow.com/users/13196248/charles"
        },
        "is_answered": true,
        "view_count": 69,
        "accepted_answer_id": 67770609,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1622454286,
        "creation_date": 1622448681,
        "question_id": 67770056,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
        "title": "How to create columns from a string in a dataframe?",
        "body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "answer_body": "<pre><code>ndf = (df[&quot;long string&quot;]\n         .str.extractall(r&quot;(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)&quot;)\n         .droplevel(&quot;match&quot;)\n         .set_index(0, append=True)\n         .set_axis([&quot;tra&quot;, &quot;la&quot;], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(&quot;-&quot;.join)\n</code></pre>\n<ul>\n<li>Extract the desired parts with a <a href=\"https://regex101.com/r/F3EKrX/1\" rel=\"nofollow noreferrer\">regex</a></li>\n<li>Drop the index level induced by <code>extractall</code> called <code>match</code></li>\n<li>Append the <code>ha-hi-ho</code> matches as the index (<code>0</code> is first capturing group)</li>\n<li>Rename the remaining columns <code>tra</code> and <code>la</code></li>\n<li>Unstack the <code>ha-hi-ho</code> index to the columns</li>\n<li>Swap the <code>ha-hi-ho</code> and <code>tra-la</code> levels' order in columns so that <code>ha-hi-ho</code> is upper</li>\n<li>Lastly join these levels of columns' names with a hyphen</li>\n</ul>\n<p>to get</p>\n<pre><code>  ha-tra hi-tra ho-tra ha-la hi-la ho-la\n0      1      1      1     2     2     2\n1      1      1      1     2     2     2\n2      1      1      1     2     2     2\n</code></pre>\n",
        "question_body": "<p>WHAT I HAVE:</p>\n<pre><code>import pandas as pd\ninp = [{'long string':'ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho: (tra: 1 la: 2)'}, \n{'long string':'ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2)'}]\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n</code></pre>\n<p>WHAT I WANT</p>\n<pre><code>inp = {'ha-tra':['1', '1', '1'], 'ha-la':['2', '2', '2'], 'hi-tra':['1', '1', '1'], 'hi-la':['2', '2', '2'],'ho-tra':['1', '1', '1'], 'ho-la':['2', '2', '2']}\ndf = pd.DataFrame(inp)\ndf\n</code></pre>\n<p>GIVES</p>\n<pre><code>    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n</code></pre>\n<p>CONTEXT</p>\n<p>From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar.</p>\n",
        "formatted_input": {
            "qid": 67770056,
            "link": "https://stackoverflow.com/questions/67770056/how-to-create-columns-from-a-string-in-a-dataframe",
            "question": {
                "title": "How to create columns from a string in a dataframe?",
                "ques_desc": "WHAT I HAVE: GIVES WHAT I WANT GIVES CONTEXT From a large string, I want to get each combination of (ha hi ho) and (tra la), and get the scores related to those combinations from the string. The problem is that the order of (ha hi ho) is not similar. "
            },
            "io": [
                "    long string\n0   ha: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ho...\n1   hi: (tra: 1 la: 2) \\n ha: (tra: 1 la: 2) \\n ho...\n2   ho: (tra: 1 la: 2) \\n hi: (tra: 1 la: 2) \\n ha...\n",
                "    ha-tra  ha-la   hi-tra  hi-la   ho-tra  ho-la\n0   1       2       1       2       1       2\n1   1       2       1       2       1       2\n2   1       2       1       2       1       2\n"
            ],
            "answer": {
                "ans_desc": " Extract the desired parts with a regex Drop the index level induced by called Append the matches as the index ( is first capturing group) Rename the remaining columns and Unstack the index to the columns Swap the and levels' order in columns so that is upper Lastly join these levels of columns' names with a hyphen to get ",
                "code": [
                    "ndf = (df[\"long string\"]\n         .str.extractall(r\"(ha|hi|ho):\\s\\((?:tra|la):\\s(\\d+)\\s(?:tra|la):\\s(\\d+)\\)\")\n         .droplevel(\"match\")\n         .set_index(0, append=True)\n         .set_axis([\"tra\", \"la\"], axis=1)\n         .unstack()\n         .swaplevel(axis=1))\nndf.columns = ndf.columns.map(\"-\".join)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 15286348,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a1dfedef68132d2be690d3ce84d9aff5?s=128&d=identicon&r=PG",
            "display_name": "lynch1972",
            "link": "https://stackoverflow.com/users/15286348/lynch1972"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 67697000,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621987482,
        "creation_date": 1621985797,
        "question_id": 67696814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
        "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
        "body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "answer_body": "<p>Tr this. This would depend on how large your data is.</p>\n<pre><code>\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n</code></pre>\n",
        "question_body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 67696814,
            "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
            "question": {
                "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
                "ques_desc": "I have this code with a dictionary \"dict\": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! "
            },
            "io": [
                "                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n",
                "        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n"
            ],
            "answer": {
                "ans_desc": "Tr this. This would depend on how large your data is. ",
                "code": [
                    "\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 548,
            "user_id": 5560529,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/dEQgB.jpg?s=128&g=1",
            "display_name": "Peter",
            "link": "https://stackoverflow.com/users/5560529/peter"
        },
        "is_answered": true,
        "view_count": 71,
        "accepted_answer_id": 67672293,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1621860569,
        "creation_date": 1621859819,
        "question_id": 67672199,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
        "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
        "body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "answer_body": "<p>I think you just need <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> with boolean indexing on <code>df1</code>.</p>\n<pre><code>pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n</code></pre>\n<p>The <code>df1[df1['C']==43]]</code> part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2.</p>\n<p>Output:</p>\n<pre><code>    A   B   C\n0   4   c   12\n1   5   d   19\n2   2   b   43\n</code></pre>\n",
        "question_body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "formatted_input": {
            "qid": 67672199,
            "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
            "question": {
                "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
                "ques_desc": "Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? "
            },
            "io": [
                "A    B    C\n4    c    12\n5    d    19\n2    b    43\n",
                "    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n"
            ],
            "answer": {
                "ans_desc": "I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: ",
                "code": [
                    "pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "reputation": 31,
            "user_id": 12537783,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4dec20a3cd9053e9e8a1751561acb545?s=128&d=identicon&r=PG&f=1",
            "display_name": "tester559",
            "link": "https://stackoverflow.com/users/12537783/tester559"
        },
        "is_answered": true,
        "view_count": 33,
        "closed_date": 1621790691,
        "accepted_answer_id": 67662500,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621790367,
        "creation_date": 1621789806,
        "question_id": 67662415,
        "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
        "closed_reason": "Duplicate",
        "title": "Working with list inside a Pandas dataframe",
        "body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "answer_body": "<p>I'd iterate through each element in <code>Column1</code>, get its length, save it in a list and then assign it to a new <code>Column2</code>. This would be summarized with:</p>\n<pre><code>df['Column2'] = [len(x) for x in df['Column1']]\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "formatted_input": {
            "qid": 67662415,
            "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
            "question": {
                "title": "Working with list inside a Pandas dataframe",
                "ques_desc": "I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? "
            },
            "io": [
                "ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n",
                "ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n"
            ],
            "answer": {
                "ans_desc": "I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: ",
                "code": [
                    "df['Column2'] = [len(x) for x in df['Column1']]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 214,
            "user_id": 15313738,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/d6SVY.png?s=128&g=1",
            "display_name": "ganbaa",
            "link": "https://stackoverflow.com/users/15313738/ganbaa"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 67595963,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1621394938,
        "creation_date": 1621388779,
        "last_edit_date": 1621390133,
        "question_id": 67595888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
        "title": "how to extract each numbers from pandas string column to list?",
        "body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "answer_body": "<p>You can use <code>re</code> to find all the occurrences of the numbers either integer or float.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[['11.2', '17', '21'], ['25.2', '4.1', '53', '17', '78'], ['121.1', '14'], ['12']]\n</code></pre>\n<p>If you want, you can type cast them to <code>float</code>/<code>int</code> checking if the extracted string has <code>.</code> in them, something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[[11.2, 17, 21], [25.2, 4.1, 53, 17, 78], [121.1, 14], [12]]\n</code></pre>\n<p>As pointed by @Uts, we can directly call <code>findall</code> over <code>Series.str</code> as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA, listB, listC, listD = df.Column_A.str.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;)\n</code></pre>\n",
        "question_body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "formatted_input": {
            "qid": 67595888,
            "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
            "question": {
                "title": "how to extract each numbers from pandas string column to list?",
                "ques_desc": "How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: "
            },
            "io": [
                "Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n",
                "listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n"
            ],
            "answer": {
                "ans_desc": "You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: ",
                "code": [
                    "df['Column_A'].apply(lambda x: re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n",
                    "listA, listB, listC, listD = df.Column_A.str.findall(r\"[-+]?\\d*\\.\\d+|\\d+\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2159,
            "user_id": 11901732,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/RiNDt.jpg?s=128&g=1",
            "display_name": "nilsinelabore",
            "link": "https://stackoverflow.com/users/11901732/nilsinelabore"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 67580063,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1621313364,
        "creation_date": 1621312500,
        "question_id": 67580031,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
        "title": "Groupby, counts in ranges and spread in Pandas",
        "body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "answer_body": "<p>You can use <code>pivot table</code>:</p>\n<pre><code>df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n</code></pre>\n<p><code>OUTPUT</code>:</p>\n<pre><code>bins  (0, 10]  (10, 20]  (20, 30]\nb                                \nAAA         1         0         1\nBBB         1         1         0\nCCC         0         2         0\n</code></pre>\n",
        "question_body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 67580031,
            "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
            "question": {
                "title": "Groupby, counts in ranges and spread in Pandas",
                "ques_desc": "I want to group by \"\" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then \"transpose\" the dataframe and making the ranges new columns Expected output: "
            },
            "io": [
                "\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n",
                "    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n"
            ],
            "answer": {
                "ans_desc": "You can use : : ",
                "code": [
                    "df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 328,
            "user_id": 2030915,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/e99080a01128b3308d0c34114ff8470c?s=128&d=identicon&r=PG",
            "display_name": "Just_Some_Guy",
            "link": "https://stackoverflow.com/users/2030915/just-some-guy"
        },
        "is_answered": true,
        "view_count": 57,
        "accepted_answer_id": 67561636,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1621218414,
        "creation_date": 1621198511,
        "last_edit_date": 1621198851,
        "question_id": 67561501,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
        "title": "splitting list in dataframe columns to separate columns",
        "body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "answer_body": "<p>Here is a way using <code>applymap</code> and <code>map</code>:</p>\n<pre><code>df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n</code></pre>\n",
        "question_body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "formatted_input": {
            "qid": 67561501,
            "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
            "question": {
                "title": "splitting list in dataframe columns to separate columns",
                "ques_desc": "my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success "
            },
            "io": [
                "    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n",
                "   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n"
            ],
            "answer": {
                "ans_desc": "Here is a way using and : ",
                "code": [
                    "df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "dictionary"
        ],
        "owner": {
            "reputation": 3,
            "user_id": 15286348,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/a1dfedef68132d2be690d3ce84d9aff5?s=128&d=identicon&r=PG",
            "display_name": "lynch1972",
            "link": "https://stackoverflow.com/users/15286348/lynch1972"
        },
        "is_answered": true,
        "view_count": 42,
        "accepted_answer_id": 67697000,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621987482,
        "creation_date": 1621985797,
        "question_id": 67696814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
        "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
        "body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "answer_body": "<p>Tr this. This would depend on how large your data is.</p>\n<pre><code>\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n</code></pre>\n",
        "question_body": "<p>I have this code with a dictionary &quot;dict&quot;:</p>\n<pre><code>\nimport pandas as pd\n\ndict = {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\n\ndf = pd.DataFrame.from_dict(dict, orient='index')\nprint(df)\n</code></pre>\n<p>The result is:</p>\n<pre><code>                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n</code></pre>\n<p>But what I want is:</p>\n<pre><code>        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n</code></pre>\n<p>I would like to obtain this, without using loops in python, and by using pandas.\nCan anyone help me out?</p>\n<p>Thanks in advance!</p>\n",
        "formatted_input": {
            "qid": 67696814,
            "link": "https://stackoverflow.com/questions/67696814/convert-dictionary-with-sub-list-of-dictionaries-into-pandas-dataframe",
            "question": {
                "title": "Convert dictionary with sub-list of dictionaries into pandas dataframe",
                "ques_desc": "I have this code with a dictionary \"dict\": The result is: But what I want is: I would like to obtain this, without using loops in python, and by using pandas. Can anyone help me out? Thanks in advance! "
            },
            "io": [
                "                                                 0\n2000  {'team': 'Manchester United', 'points': '91'}\n2001  {'team': 'Manchester United', 'points': '80'}\n2002            {'team': 'Arsenal', 'points': '87'}\n\n",
                "        team                  points\n2000    Manchester United     91\n2001    Manchester United     80\n2002    Arsenal               87\n\n"
            ],
            "answer": {
                "ans_desc": "Tr this. This would depend on how large your data is. ",
                "code": [
                    "\ndiction =  {\n    '2000': [{'team': 'Manchester United', 'points': '91'}],\n    '2001': [{'team': 'Manchester United', 'points': '80'}],\n    '2002': [{'team': 'Arsenal', 'points': '87'}]\n}\ntransformed_dict= {x:d for x,y in diction.items() for d in y }\ndf = pd.DataFrame(transformed_dict)\ndf.T\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 548,
            "user_id": 5560529,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/dEQgB.jpg?s=128&g=1",
            "display_name": "Peter",
            "link": "https://stackoverflow.com/users/5560529/peter"
        },
        "is_answered": true,
        "view_count": 71,
        "accepted_answer_id": 67672293,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1621860569,
        "creation_date": 1621859819,
        "question_id": 67672199,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
        "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
        "body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "answer_body": "<p>I think you just need <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> with boolean indexing on <code>df1</code>.</p>\n<pre><code>pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n</code></pre>\n<p>The <code>df1[df1['C']==43]]</code> part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2.</p>\n<p>Output:</p>\n<pre><code>    A   B   C\n0   4   c   12\n1   5   d   19\n2   2   b   43\n</code></pre>\n",
        "question_body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "formatted_input": {
            "qid": 67672199,
            "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
            "question": {
                "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
                "ques_desc": "Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? "
            },
            "io": [
                "A    B    C\n4    c    12\n5    d    19\n2    b    43\n",
                "    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n"
            ],
            "answer": {
                "ans_desc": "I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: ",
                "code": [
                    "pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "reputation": 31,
            "user_id": 12537783,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4dec20a3cd9053e9e8a1751561acb545?s=128&d=identicon&r=PG&f=1",
            "display_name": "tester559",
            "link": "https://stackoverflow.com/users/12537783/tester559"
        },
        "is_answered": true,
        "view_count": 33,
        "closed_date": 1621790691,
        "accepted_answer_id": 67662500,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621790367,
        "creation_date": 1621789806,
        "question_id": 67662415,
        "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
        "closed_reason": "Duplicate",
        "title": "Working with list inside a Pandas dataframe",
        "body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "answer_body": "<p>I'd iterate through each element in <code>Column1</code>, get its length, save it in a list and then assign it to a new <code>Column2</code>. This would be summarized with:</p>\n<pre><code>df['Column2'] = [len(x) for x in df['Column1']]\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "formatted_input": {
            "qid": 67662415,
            "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
            "question": {
                "title": "Working with list inside a Pandas dataframe",
                "ques_desc": "I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? "
            },
            "io": [
                "ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n",
                "ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n"
            ],
            "answer": {
                "ans_desc": "I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: ",
                "code": [
                    "df['Column2'] = [len(x) for x in df['Column1']]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 214,
            "user_id": 15313738,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/d6SVY.png?s=128&g=1",
            "display_name": "ganbaa",
            "link": "https://stackoverflow.com/users/15313738/ganbaa"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 67595963,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1621394938,
        "creation_date": 1621388779,
        "last_edit_date": 1621390133,
        "question_id": 67595888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
        "title": "how to extract each numbers from pandas string column to list?",
        "body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "answer_body": "<p>You can use <code>re</code> to find all the occurrences of the numbers either integer or float.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[['11.2', '17', '21'], ['25.2', '4.1', '53', '17', '78'], ['121.1', '14'], ['12']]\n</code></pre>\n<p>If you want, you can type cast them to <code>float</code>/<code>int</code> checking if the extracted string has <code>.</code> in them, something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[[11.2, 17, 21], [25.2, 4.1, 53, 17, 78], [121.1, 14], [12]]\n</code></pre>\n<p>As pointed by @Uts, we can directly call <code>findall</code> over <code>Series.str</code> as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA, listB, listC, listD = df.Column_A.str.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;)\n</code></pre>\n",
        "question_body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "formatted_input": {
            "qid": 67595888,
            "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
            "question": {
                "title": "how to extract each numbers from pandas string column to list?",
                "ques_desc": "How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: "
            },
            "io": [
                "Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n",
                "listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n"
            ],
            "answer": {
                "ans_desc": "You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: ",
                "code": [
                    "df['Column_A'].apply(lambda x: re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n",
                    "listA, listB, listC, listD = df.Column_A.str.findall(r\"[-+]?\\d*\\.\\d+|\\d+\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2159,
            "user_id": 11901732,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/RiNDt.jpg?s=128&g=1",
            "display_name": "nilsinelabore",
            "link": "https://stackoverflow.com/users/11901732/nilsinelabore"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 67580063,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1621313364,
        "creation_date": 1621312500,
        "question_id": 67580031,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
        "title": "Groupby, counts in ranges and spread in Pandas",
        "body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "answer_body": "<p>You can use <code>pivot table</code>:</p>\n<pre><code>df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n</code></pre>\n<p><code>OUTPUT</code>:</p>\n<pre><code>bins  (0, 10]  (10, 20]  (20, 30]\nb                                \nAAA         1         0         1\nBBB         1         1         0\nCCC         0         2         0\n</code></pre>\n",
        "question_body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 67580031,
            "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
            "question": {
                "title": "Groupby, counts in ranges and spread in Pandas",
                "ques_desc": "I want to group by \"\" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then \"transpose\" the dataframe and making the ranges new columns Expected output: "
            },
            "io": [
                "\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n",
                "    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n"
            ],
            "answer": {
                "ans_desc": "You can use : : ",
                "code": [
                    "df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 328,
            "user_id": 2030915,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/e99080a01128b3308d0c34114ff8470c?s=128&d=identicon&r=PG",
            "display_name": "Just_Some_Guy",
            "link": "https://stackoverflow.com/users/2030915/just-some-guy"
        },
        "is_answered": true,
        "view_count": 57,
        "accepted_answer_id": 67561636,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1621218414,
        "creation_date": 1621198511,
        "last_edit_date": 1621198851,
        "question_id": 67561501,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
        "title": "splitting list in dataframe columns to separate columns",
        "body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "answer_body": "<p>Here is a way using <code>applymap</code> and <code>map</code>:</p>\n<pre><code>df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n</code></pre>\n",
        "question_body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "formatted_input": {
            "qid": 67561501,
            "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
            "question": {
                "title": "splitting list in dataframe columns to separate columns",
                "ques_desc": "my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success "
            },
            "io": [
                "    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n",
                "   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n"
            ],
            "answer": {
                "ans_desc": "Here is a way using and : ",
                "code": [
                    "df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 403,
            "user_id": 12076197,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/228f4f1d534ab4bc69cbae003c708bf2?s=128&d=identicon&r=PG&f=1",
            "display_name": "dmd7",
            "link": "https://stackoverflow.com/users/12076197/dmd7"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67506886,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1620834577,
        "creation_date": 1620833933,
        "question_id": 67506798,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
        "title": "Reformat Dataframe / Add rows when condition is met",
        "body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "answer_body": "<p>Assuming you only have positive integers in <code>'ColB'</code> You can re-create the DataFrame from scratch using <code>np.repeat</code>. The repeat takes care of the duplication, so we can assign ColB = 1.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n</code></pre>\n<p>Alternatively, if you have a non-duplicated Index, you can repeat that and use <code>loc</code> to get the repitition. Useful when you have more than a single column you want to repeat:</p>\n<pre><code>df = (df.loc[df.index.repeat(df.ColB)]\n        .assign(ColB=1))\n</code></pre>\n<hr />\n<pre><code>         ColA  ColB\n0  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n2  2021-03-10     1\n2  2021-03-10     1\n3  2021-03-10     1\n4  2021-03-10     1\n4  2021-03-10     1\n5  2021-03-11     1\n5  2021-03-11     1\n</code></pre>\n",
        "question_body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "formatted_input": {
            "qid": 67506798,
            "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
            "question": {
                "title": "Reformat Dataframe / Add rows when condition is met",
                "ques_desc": "I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only \"1's\". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! "
            },
            "io": [
                "   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n",
                "   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n"
            ],
            "answer": {
                "ans_desc": "Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 548,
            "user_id": 5560529,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/dEQgB.jpg?s=128&g=1",
            "display_name": "Peter",
            "link": "https://stackoverflow.com/users/5560529/peter"
        },
        "is_answered": true,
        "view_count": 71,
        "accepted_answer_id": 67672293,
        "answer_count": 3,
        "score": 0,
        "last_activity_date": 1621860569,
        "creation_date": 1621859819,
        "question_id": 67672199,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
        "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
        "body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "answer_body": "<p>I think you just need <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.concat.html\" rel=\"nofollow noreferrer\"><code>concat</code></a> with boolean indexing on <code>df1</code>.</p>\n<pre><code>pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n</code></pre>\n<p>The <code>df1[df1['C']==43]]</code> part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2.</p>\n<p>Output:</p>\n<pre><code>    A   B   C\n0   4   c   12\n1   5   d   19\n2   2   b   43\n</code></pre>\n",
        "question_body": "<p>Assume I have the following two pandas DataFrames:</p>\n<pre><code>df1 = pd.DataFrame({&quot;A&quot;: [1, 2, 3],\n                    &quot;B&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],\n                    &quot;C&quot;: [7, 43, 15]})\n\ndf2 = pd.DataFrame({&quot;A&quot;: [4, 5],\n                    &quot;B&quot;: [&quot;c&quot;, &quot;d&quot;],\n                    &quot;C&quot;: [12, 19]})\n</code></pre>\n<p>Now, I want to iterate over the rows in <code>df1</code>, and if a certain condition is met for that row, add the row to <code>df2</code>.</p>\n<p>For example:</p>\n<pre><code>for i, row in df1.iterrows():\n    if row[&quot;C&quot;] == 43:\n        df2 = pd.concat([row, df2])\n\ndf2.head()\n</code></pre>\n<p>Should give me output:</p>\n<pre><code>A    B    C\n4    c    12\n5    d    19\n2    b    43\n</code></pre>\n<p>But instead I get an output where the column names of the DataFrames appear in the rows:</p>\n<pre><code>    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n</code></pre>\n<p>How to solve this?</p>\n",
        "formatted_input": {
            "qid": 67672199,
            "link": "https://stackoverflow.com/questions/67672199/how-to-concat-the-row-output-of-iterrows-to-another-pandas-dataframe-with-the-sa",
            "question": {
                "title": "How to concat the row output of iterrows to another pandas DataFrame with the same columns?",
                "ques_desc": "Assume I have the following two pandas DataFrames: Now, I want to iterate over the rows in , and if a certain condition is met for that row, add the row to . For example: Should give me output: But instead I get an output where the column names of the DataFrames appear in the rows: How to solve this? "
            },
            "io": [
                "A    B    C\n4    c    12\n5    d    19\n2    b    43\n",
                "    0   A   B   C\nA   2   NaN     NaN     NaN\nB   b   NaN     NaN     NaN\nC   43  NaN     NaN     NaN\n0   NaN     4.0     c   12.0\n1   NaN     5.0     d   19.0\n"
            ],
            "answer": {
                "ans_desc": "I think you just need with boolean indexing on . The part takes a slice of df1 based on the condition the column C being equal to 43 and concats it to df2. Output: ",
                "code": [
                    "pd.concat([df2, df1[df1['C'] == 43]], ignore_index=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "list",
            "dataframe"
        ],
        "owner": {
            "reputation": 31,
            "user_id": 12537783,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/4dec20a3cd9053e9e8a1751561acb545?s=128&d=identicon&r=PG&f=1",
            "display_name": "tester559",
            "link": "https://stackoverflow.com/users/12537783/tester559"
        },
        "is_answered": true,
        "view_count": 33,
        "closed_date": 1621790691,
        "accepted_answer_id": 67662500,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1621790367,
        "creation_date": 1621789806,
        "question_id": 67662415,
        "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
        "closed_reason": "Duplicate",
        "title": "Working with list inside a Pandas dataframe",
        "body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "answer_body": "<p>I'd iterate through each element in <code>Column1</code>, get its length, save it in a list and then assign it to a new <code>Column2</code>. This would be summarized with:</p>\n<pre><code>df['Column2'] = [len(x) for x in df['Column1']]\n</code></pre>\n",
        "question_body": "<p>I have the following dataframe -</p>\n<pre><code>ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n</code></pre>\n<p>I want a column which gives the length of the list in column1. Result should look like -</p>\n<pre><code>ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n</code></pre>\n<p>I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 -</p>\n<pre><code>df1 = df.assign(Column2 = lambda x: (len(x['Column1'])))\n</code></pre>\n<p>Can someone please help me out here?</p>\n",
        "formatted_input": {
            "qid": 67662415,
            "link": "https://stackoverflow.com/questions/67662415/working-with-list-inside-a-pandas-dataframe",
            "question": {
                "title": "Working with list inside a Pandas dataframe",
                "ques_desc": "I have the following dataframe - I want a column which gives the length of the list in column1. Result should look like - I tried using lambda function but it is not giving the number of rows in the data frame as every entry in column 2 - Can someone please help me out here? "
            },
            "io": [
                "ID | Column1 |\n0  |  []     | \n1  |  [1,2]  | \n2  |  []     |\n",
                "ID | Column1 | Column2 |\n0  |  []     |   0     |\n1  |  [1,2]  |   2     |\n2  |  []     |   0     |\n"
            ],
            "answer": {
                "ans_desc": "I'd iterate through each element in , get its length, save it in a list and then assign it to a new . This would be summarized with: ",
                "code": [
                    "df['Column2'] = [len(x) for x in df['Column1']]\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 214,
            "user_id": 15313738,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/d6SVY.png?s=128&g=1",
            "display_name": "ganbaa",
            "link": "https://stackoverflow.com/users/15313738/ganbaa"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 67595963,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1621394938,
        "creation_date": 1621388779,
        "last_edit_date": 1621390133,
        "question_id": 67595888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
        "title": "how to extract each numbers from pandas string column to list?",
        "body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "answer_body": "<p>You can use <code>re</code> to find all the occurrences of the numbers either integer or float.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[['11.2', '17', '21'], ['25.2', '4.1', '53', '17', '78'], ['121.1', '14'], ['12']]\n</code></pre>\n<p>If you want, you can type cast them to <code>float</code>/<code>int</code> checking if the extracted string has <code>.</code> in them, something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[[11.2, 17, 21], [25.2, 4.1, 53, 17, 78], [121.1, 14], [12]]\n</code></pre>\n<p>As pointed by @Uts, we can directly call <code>findall</code> over <code>Series.str</code> as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA, listB, listC, listD = df.Column_A.str.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;)\n</code></pre>\n",
        "question_body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "formatted_input": {
            "qid": 67595888,
            "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
            "question": {
                "title": "how to extract each numbers from pandas string column to list?",
                "ques_desc": "How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: "
            },
            "io": [
                "Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n",
                "listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n"
            ],
            "answer": {
                "ans_desc": "You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: ",
                "code": [
                    "df['Column_A'].apply(lambda x: re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n",
                    "listA, listB, listC, listD = df.Column_A.str.findall(r\"[-+]?\\d*\\.\\d+|\\d+\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2159,
            "user_id": 11901732,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/RiNDt.jpg?s=128&g=1",
            "display_name": "nilsinelabore",
            "link": "https://stackoverflow.com/users/11901732/nilsinelabore"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 67580063,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1621313364,
        "creation_date": 1621312500,
        "question_id": 67580031,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
        "title": "Groupby, counts in ranges and spread in Pandas",
        "body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "answer_body": "<p>You can use <code>pivot table</code>:</p>\n<pre><code>df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n</code></pre>\n<p><code>OUTPUT</code>:</p>\n<pre><code>bins  (0, 10]  (10, 20]  (20, 30]\nb                                \nAAA         1         0         1\nBBB         1         1         0\nCCC         0         2         0\n</code></pre>\n",
        "question_body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 67580031,
            "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
            "question": {
                "title": "Groupby, counts in ranges and spread in Pandas",
                "ques_desc": "I want to group by \"\" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then \"transpose\" the dataframe and making the ranges new columns Expected output: "
            },
            "io": [
                "\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n",
                "    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n"
            ],
            "answer": {
                "ans_desc": "You can use : : ",
                "code": [
                    "df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 328,
            "user_id": 2030915,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/e99080a01128b3308d0c34114ff8470c?s=128&d=identicon&r=PG",
            "display_name": "Just_Some_Guy",
            "link": "https://stackoverflow.com/users/2030915/just-some-guy"
        },
        "is_answered": true,
        "view_count": 57,
        "accepted_answer_id": 67561636,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1621218414,
        "creation_date": 1621198511,
        "last_edit_date": 1621198851,
        "question_id": 67561501,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
        "title": "splitting list in dataframe columns to separate columns",
        "body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "answer_body": "<p>Here is a way using <code>applymap</code> and <code>map</code>:</p>\n<pre><code>df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n</code></pre>\n",
        "question_body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "formatted_input": {
            "qid": 67561501,
            "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
            "question": {
                "title": "splitting list in dataframe columns to separate columns",
                "ques_desc": "my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success "
            },
            "io": [
                "    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n",
                "   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n"
            ],
            "answer": {
                "ans_desc": "Here is a way using and : ",
                "code": [
                    "df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 403,
            "user_id": 12076197,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/228f4f1d534ab4bc69cbae003c708bf2?s=128&d=identicon&r=PG&f=1",
            "display_name": "dmd7",
            "link": "https://stackoverflow.com/users/12076197/dmd7"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67506886,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1620834577,
        "creation_date": 1620833933,
        "question_id": 67506798,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
        "title": "Reformat Dataframe / Add rows when condition is met",
        "body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "answer_body": "<p>Assuming you only have positive integers in <code>'ColB'</code> You can re-create the DataFrame from scratch using <code>np.repeat</code>. The repeat takes care of the duplication, so we can assign ColB = 1.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n</code></pre>\n<p>Alternatively, if you have a non-duplicated Index, you can repeat that and use <code>loc</code> to get the repitition. Useful when you have more than a single column you want to repeat:</p>\n<pre><code>df = (df.loc[df.index.repeat(df.ColB)]\n        .assign(ColB=1))\n</code></pre>\n<hr />\n<pre><code>         ColA  ColB\n0  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n2  2021-03-10     1\n2  2021-03-10     1\n3  2021-03-10     1\n4  2021-03-10     1\n4  2021-03-10     1\n5  2021-03-11     1\n5  2021-03-11     1\n</code></pre>\n",
        "question_body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "formatted_input": {
            "qid": 67506798,
            "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
            "question": {
                "title": "Reformat Dataframe / Add rows when condition is met",
                "ques_desc": "I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only \"1's\". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! "
            },
            "io": [
                "   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n",
                "   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n"
            ],
            "answer": {
                "ans_desc": "Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 214,
            "user_id": 15313738,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/d6SVY.png?s=128&g=1",
            "display_name": "ganbaa",
            "link": "https://stackoverflow.com/users/15313738/ganbaa"
        },
        "is_answered": true,
        "view_count": 79,
        "accepted_answer_id": 67595963,
        "answer_count": 2,
        "score": 3,
        "last_activity_date": 1621394938,
        "creation_date": 1621388779,
        "last_edit_date": 1621390133,
        "question_id": 67595888,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
        "title": "how to extract each numbers from pandas string column to list?",
        "body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "answer_body": "<p>You can use <code>re</code> to find all the occurrences of the numbers either integer or float.</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[['11.2', '17', '21'], ['25.2', '4.1', '53', '17', '78'], ['121.1', '14'], ['12']]\n</code></pre>\n<p>If you want, you can type cast them to <code>float</code>/<code>int</code> checking if the extracted string has <code>.</code> in them, something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df['Column_A'].apply(lambda x: re.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;, x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n</code></pre>\n<p><strong>OUTPUT</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>[[11.2, 17, 21], [25.2, 4.1, 53, 17, 78], [121.1, 14], [12]]\n</code></pre>\n<p>As pointed by @Uts, we can directly call <code>findall</code> over <code>Series.str</code> as:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA, listB, listC, listD = df.Column_A.str.findall(r&quot;[-+]?\\d*\\.\\d+|\\d+&quot;)\n</code></pre>\n",
        "question_body": "<p>How to do that?</p>\n<p>I have pandas dataframe looks like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n</code></pre>\n<p>I need to transfer this each row to separated list:</p>\n<pre class=\"lang-py prettyprint-override\"><code>listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n</code></pre>\n",
        "formatted_input": {
            "qid": 67595888,
            "link": "https://stackoverflow.com/questions/67595888/how-to-extract-each-numbers-from-pandas-string-column-to-list",
            "question": {
                "title": "how to extract each numbers from pandas string column to list?",
                "ques_desc": "How to do that? I have pandas dataframe looks like: I need to transfer this each row to separated list: "
            },
            "io": [
                "Column_A\n11.2 some text 17 some text 21\nsome text 25.2 4.1 some text 53 17 78\n121.1 bla bla bla 14 some text\n12 some text\n",
                "listA[0] = 11.2 listA[1] = 17 listA[2] = 21\nlistB[0] = 25.2 listB[1] = 4.1 listB[2] = 53 listB[3] = 17 listB[4] = 78\nlistC[0] = 121.1 listC[1] = 14\nlistD[0] = 12\n"
            ],
            "answer": {
                "ans_desc": "You can use to find all the occurrences of the numbers either integer or float. OUTPUT: If you want, you can type cast them to / checking if the extracted string has in them, something like this: OUTPUT: As pointed by @Uts, we can directly call over as: ",
                "code": [
                    "df['Column_A'].apply(lambda x: re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)).map(lambda x: [int(i) if '.' not in i else float(i) for i in x]).tolist()\n",
                    "listA, listB, listC, listD = df.Column_A.str.findall(r\"[-+]?\\d*\\.\\d+|\\d+\")\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2159,
            "user_id": 11901732,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/RiNDt.jpg?s=128&g=1",
            "display_name": "nilsinelabore",
            "link": "https://stackoverflow.com/users/11901732/nilsinelabore"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 67580063,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1621313364,
        "creation_date": 1621312500,
        "question_id": 67580031,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
        "title": "Groupby, counts in ranges and spread in Pandas",
        "body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "answer_body": "<p>You can use <code>pivot table</code>:</p>\n<pre><code>df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n</code></pre>\n<p><code>OUTPUT</code>:</p>\n<pre><code>bins  (0, 10]  (10, 20]  (20, 30]\nb                                \nAAA         1         0         1\nBBB         1         1         0\nCCC         0         2         0\n</code></pre>\n",
        "question_body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 67580031,
            "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
            "question": {
                "title": "Groupby, counts in ranges and spread in Pandas",
                "ques_desc": "I want to group by \"\" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then \"transpose\" the dataframe and making the ranges new columns Expected output: "
            },
            "io": [
                "\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n",
                "    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n"
            ],
            "answer": {
                "ans_desc": "You can use : : ",
                "code": [
                    "df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 328,
            "user_id": 2030915,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/e99080a01128b3308d0c34114ff8470c?s=128&d=identicon&r=PG",
            "display_name": "Just_Some_Guy",
            "link": "https://stackoverflow.com/users/2030915/just-some-guy"
        },
        "is_answered": true,
        "view_count": 57,
        "accepted_answer_id": 67561636,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1621218414,
        "creation_date": 1621198511,
        "last_edit_date": 1621198851,
        "question_id": 67561501,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
        "title": "splitting list in dataframe columns to separate columns",
        "body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "answer_body": "<p>Here is a way using <code>applymap</code> and <code>map</code>:</p>\n<pre><code>df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n</code></pre>\n",
        "question_body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "formatted_input": {
            "qid": 67561501,
            "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
            "question": {
                "title": "splitting list in dataframe columns to separate columns",
                "ques_desc": "my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success "
            },
            "io": [
                "    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n",
                "   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n"
            ],
            "answer": {
                "ans_desc": "Here is a way using and : ",
                "code": [
                    "df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 403,
            "user_id": 12076197,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/228f4f1d534ab4bc69cbae003c708bf2?s=128&d=identicon&r=PG&f=1",
            "display_name": "dmd7",
            "link": "https://stackoverflow.com/users/12076197/dmd7"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67506886,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1620834577,
        "creation_date": 1620833933,
        "question_id": 67506798,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
        "title": "Reformat Dataframe / Add rows when condition is met",
        "body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "answer_body": "<p>Assuming you only have positive integers in <code>'ColB'</code> You can re-create the DataFrame from scratch using <code>np.repeat</code>. The repeat takes care of the duplication, so we can assign ColB = 1.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n</code></pre>\n<p>Alternatively, if you have a non-duplicated Index, you can repeat that and use <code>loc</code> to get the repitition. Useful when you have more than a single column you want to repeat:</p>\n<pre><code>df = (df.loc[df.index.repeat(df.ColB)]\n        .assign(ColB=1))\n</code></pre>\n<hr />\n<pre><code>         ColA  ColB\n0  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n2  2021-03-10     1\n2  2021-03-10     1\n3  2021-03-10     1\n4  2021-03-10     1\n4  2021-03-10     1\n5  2021-03-11     1\n5  2021-03-11     1\n</code></pre>\n",
        "question_body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "formatted_input": {
            "qid": 67506798,
            "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
            "question": {
                "title": "Reformat Dataframe / Add rows when condition is met",
                "ques_desc": "I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only \"1's\". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! "
            },
            "io": [
                "   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n",
                "   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n"
            ],
            "answer": {
                "ans_desc": "Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 2159,
            "user_id": 11901732,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/RiNDt.jpg?s=128&g=1",
            "display_name": "nilsinelabore",
            "link": "https://stackoverflow.com/users/11901732/nilsinelabore"
        },
        "is_answered": true,
        "view_count": 32,
        "accepted_answer_id": 67580063,
        "answer_count": 2,
        "score": -1,
        "last_activity_date": 1621313364,
        "creation_date": 1621312500,
        "question_id": 67580031,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
        "title": "Groupby, counts in ranges and spread in Pandas",
        "body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "answer_body": "<p>You can use <code>pivot table</code>:</p>\n<pre><code>df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n</code></pre>\n<p><code>OUTPUT</code>:</p>\n<pre><code>bins  (0, 10]  (10, 20]  (20, 30]\nb                                \nAAA         1         0         1\nBBB         1         1         0\nCCC         0         2         0\n</code></pre>\n",
        "question_body": "<p>I want to group <code>df</code> by &quot;<code>b</code>&quot; and count the number of items in different ranges.</p>\n<p>I tried:</p>\n<pre><code>np.random.seed(2)\ndf = pd.DataFrame({&quot;a&quot;: np.random.random_integers(1, high=50, size=10), &quot;b&quot;: ['AAA', 'BBB', 'AAA', 'BBB', 'AAA', 'BBB', 'CCC', 'CCC', 'AAA', 'AAA']})\nranges = [0,10,20, 30 ]\ndf.groupby(pd.cut(df.a, ranges)).agg({'a':'count', 'b':'first'})\n</code></pre>\n<p>which returned:</p>\n<pre><code>\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n</code></pre>\n<p>But I want to groupby <code>b</code> thus making it the index, then &quot;transpose&quot; the dataframe and making the ranges new columns Expected output:</p>\n<pre><code>    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n</code></pre>\n",
        "formatted_input": {
            "qid": 67580031,
            "link": "https://stackoverflow.com/questions/67580031/groupby-counts-in-ranges-and-spread-in-pandas",
            "question": {
                "title": "Groupby, counts in ranges and spread in Pandas",
                "ques_desc": "I want to group by \"\" and count the number of items in different ranges. I tried: which returned: But I want to groupby thus making it the index, then \"transpose\" the dataframe and making the ranges new columns Expected output: "
            },
            "io": [
                "\n           a    b\n   a        \n(0, 10]     2   BBB\n(10, 20]    3   BBB\n(20, 30]    1   AAA\n",
                "    (0, 10]   (10, 20]   (20, 30]\n \nAAA    0          0         1      \nBBB    2          3         0\n\n"
            ],
            "answer": {
                "ans_desc": "You can use : : ",
                "code": [
                    "df = df.assign(bins = pd.cut(df.a, bins=ranges)).pivot_table(index='b', columns='bins', values='a', aggfunc='count')\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 328,
            "user_id": 2030915,
            "user_type": "registered",
            "accept_rate": 84,
            "profile_image": "https://www.gravatar.com/avatar/e99080a01128b3308d0c34114ff8470c?s=128&d=identicon&r=PG",
            "display_name": "Just_Some_Guy",
            "link": "https://stackoverflow.com/users/2030915/just-some-guy"
        },
        "is_answered": true,
        "view_count": 57,
        "accepted_answer_id": 67561636,
        "answer_count": 4,
        "score": 2,
        "last_activity_date": 1621218414,
        "creation_date": 1621198511,
        "last_edit_date": 1621198851,
        "question_id": 67561501,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
        "title": "splitting list in dataframe columns to separate columns",
        "body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "answer_body": "<p>Here is a way using <code>applymap</code> and <code>map</code>:</p>\n<pre><code>df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n</code></pre>\n",
        "question_body": "<p>my data frame looks like as follows</p>\n<pre><code>    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n</code></pre>\n<p>I need to make it look like:</p>\n<pre><code>   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n</code></pre>\n<hr />\n<p>My code</p>\n<pre><code>import pandas as pd\n\nd = {'col1':[[1,'a'],[2,'b'],[3,'c']],\n     'col2':[[1,'a1'],[2,'b1'],[3,'c1']],\n     'col3':[[1,'a2'],[2,'b2'],[3,'c2']]}\n\ndf = pd.DataFrame.from_dict(d)\n</code></pre>\n<p>so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success</p>\n",
        "formatted_input": {
            "qid": 67561501,
            "link": "https://stackoverflow.com/questions/67561501/splitting-list-in-dataframe-columns-to-separate-columns",
            "question": {
                "title": "splitting list in dataframe columns to separate columns",
                "ques_desc": "my data frame looks like as follows I need to make it look like: My code so far I have tried using apply(pd.Series) and iterating through a for loop to reassign the values and have not had success "
            },
            "io": [
                "    col1     col2     col3\n0  [1, a]  [1, a1]  [1, a2]\n1  [2, b]  [2, b1]  [2, b2]\n2  [3, c]  [3, c1]  [3, c2]\n",
                "   col1     col2     col3  col4\n0  a         a1      a2    1\n1  b         b1      b2    2\n2  c         c1      c2    3\n"
            ],
            "answer": {
                "ans_desc": "Here is a way using and : ",
                "code": [
                    "df.applymap(lambda x: x[-1]).assign(col4 = df['col1'].map(lambda x: x[0]))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 403,
            "user_id": 12076197,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/228f4f1d534ab4bc69cbae003c708bf2?s=128&d=identicon&r=PG&f=1",
            "display_name": "dmd7",
            "link": "https://stackoverflow.com/users/12076197/dmd7"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67506886,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1620834577,
        "creation_date": 1620833933,
        "question_id": 67506798,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
        "title": "Reformat Dataframe / Add rows when condition is met",
        "body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "answer_body": "<p>Assuming you only have positive integers in <code>'ColB'</code> You can re-create the DataFrame from scratch using <code>np.repeat</code>. The repeat takes care of the duplication, so we can assign ColB = 1.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n</code></pre>\n<p>Alternatively, if you have a non-duplicated Index, you can repeat that and use <code>loc</code> to get the repitition. Useful when you have more than a single column you want to repeat:</p>\n<pre><code>df = (df.loc[df.index.repeat(df.ColB)]\n        .assign(ColB=1))\n</code></pre>\n<hr />\n<pre><code>         ColA  ColB\n0  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n2  2021-03-10     1\n2  2021-03-10     1\n3  2021-03-10     1\n4  2021-03-10     1\n4  2021-03-10     1\n5  2021-03-11     1\n5  2021-03-11     1\n</code></pre>\n",
        "question_body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "formatted_input": {
            "qid": 67506798,
            "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
            "question": {
                "title": "Reformat Dataframe / Add rows when condition is met",
                "ques_desc": "I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only \"1's\". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! "
            },
            "io": [
                "   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n",
                "   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n"
            ],
            "answer": {
                "ans_desc": "Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 15817395,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyRFG42bJfN7WzpcIAmho4ZvAc1tTi_zySFK3_C=k-s128",
            "display_name": "Bernd Blase",
            "link": "https://stackoverflow.com/users/15817395/bernd-blase"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 67361868,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1619999183,
        "creation_date": 1619996303,
        "question_id": 67361824,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
        "title": "Convert dataframe objects to float by iterating over columns",
        "body": "<p>I want to convert data in Pandas.Series by iterating over Series</p>\n<p>DataFrame df looks like</p>\n<pre><code>   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n</code></pre>\n<p>'%' and '-' only values should be removed. Desired result:</p>\n<pre><code>   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n</code></pre>\n<p>If I call</p>\n<pre><code>df['c1']= df['c1'].str.replace('%', '')\ndf['c1']= df['c1'].str.replace('-%', '0')\ndf['c1']= df['c1'].astype(float)\n</code></pre>\n<p>it works.</p>\n<p>But if I try to iterate it does not:</p>\n<pre><code>col= [] \ncol.append(df['c1'])\ncol.append(df['c2'])\n\nfor i  in (col):\n    i = i.str.replace('%', '',regex=True)\n    i = i.str.replace('-$', '0',regex=True)\n</code></pre>\n<p>Thanks in advance</p>\n",
        "answer_body": "<p>EDIT: Improved regex</p>\n<pre><code># Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n</code></pre>\n<p>Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals.</p>\n<p><strong>Output</strong></p>\n<pre><code>    c1      c2\n0   0.0     75.0\n1   -5.5    65.8\n2   0.0     6.9\n</code></pre>\n<p><strong>Explanation</strong></p>\n<p>We can use regex over complete df, to replace the required symbols, we are replacing <code>%</code> with empty string and if a row consists of <code>-</code> at the end then replace it with 0.0.</p>\n",
        "question_body": "<p>I want to convert data in Pandas.Series by iterating over Series</p>\n<p>DataFrame df looks like</p>\n<pre><code>   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n</code></pre>\n<p>'%' and '-' only values should be removed. Desired result:</p>\n<pre><code>   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n</code></pre>\n<p>If I call</p>\n<pre><code>df['c1']= df['c1'].str.replace('%', '')\ndf['c1']= df['c1'].str.replace('-%', '0')\ndf['c1']= df['c1'].astype(float)\n</code></pre>\n<p>it works.</p>\n<p>But if I try to iterate it does not:</p>\n<pre><code>col= [] \ncol.append(df['c1'])\ncol.append(df['c2'])\n\nfor i  in (col):\n    i = i.str.replace('%', '',regex=True)\n    i = i.str.replace('-$', '0',regex=True)\n</code></pre>\n<p>Thanks in advance</p>\n",
        "formatted_input": {
            "qid": 67361824,
            "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
            "question": {
                "title": "Convert dataframe objects to float by iterating over columns",
                "ques_desc": "I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance "
            },
            "io": [
                "   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n",
                "   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n"
            ],
            "answer": {
                "ans_desc": "EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. ",
                "code": [
                    "# Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-apply"
        ],
        "owner": {
            "reputation": 777,
            "user_id": 4014825,
            "user_type": "registered",
            "accept_rate": 90,
            "profile_image": "https://www.gravatar.com/avatar/28ddcf15e4e55ae03685fcecee3c54f9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Inthu",
            "link": "https://stackoverflow.com/users/4014825/inthu"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 67358000,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1619969264,
        "creation_date": 1619967653,
        "last_edit_date": 1619968454,
        "question_id": 67357814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan",
        "title": "Python pandas dataframe apply result of function to multiple columns where NaN",
        "body": "<p>I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN.</p>\n<pre><code>def calculate(x):\n    return 1, 2\n\ndf = pd.DataFrame({'x':['a', 'b', 'c', 'd', 'e', 'f'], 'y':[np.NaN, np.NaN, np.NaN, 'a1', 'b2', 'c3'], 'z':[np.NaN, np.NaN, np.NaN, 'a2', 'b1', 'c4']})\n\n x    y    z\n0  a  NaN  NaN\n1  b  NaN  NaN\n2  c  NaN  NaN\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n\nmask = (df.isnull().any(axis=1))\n\ndf[['y', 'z']] = df[mask].apply(calculate, axis=1, result_type='expand')\n</code></pre>\n<p>However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong.</p>\n<pre><code>    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n</code></pre>\n<p>If the mask is inverted I get the following result:</p>\n<pre><code>df[['y', 'z']] = df[~mask].apply(calculate, axis=1, result_type='expand')\n    x   y   z\n0   a   NaN NaN\n1   b   NaN NaN\n2   c   NaN NaN\n3   d   1.0 2.0\n4   e   1.0 2.0\n5   f   1.0 2.0\n</code></pre>\n<p>Expected result:</p>\n<pre><code>   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n</code></pre>\n",
        "answer_body": "<p>you can fillna after calculating for the full dataframe and <code>set_axis</code></p>\n<pre><code>out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n</code></pre>\n<hr />\n<pre><code>print(out)\n\n   x   y   z\n0  a   1   2\n1  b   1   2\n2  c   1   2\n3  d  a1  a2\n4  e  b2  b1\n5  f  c3  c4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN.</p>\n<pre><code>def calculate(x):\n    return 1, 2\n\ndf = pd.DataFrame({'x':['a', 'b', 'c', 'd', 'e', 'f'], 'y':[np.NaN, np.NaN, np.NaN, 'a1', 'b2', 'c3'], 'z':[np.NaN, np.NaN, np.NaN, 'a2', 'b1', 'c4']})\n\n x    y    z\n0  a  NaN  NaN\n1  b  NaN  NaN\n2  c  NaN  NaN\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n\nmask = (df.isnull().any(axis=1))\n\ndf[['y', 'z']] = df[mask].apply(calculate, axis=1, result_type='expand')\n</code></pre>\n<p>However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong.</p>\n<pre><code>    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n</code></pre>\n<p>If the mask is inverted I get the following result:</p>\n<pre><code>df[['y', 'z']] = df[~mask].apply(calculate, axis=1, result_type='expand')\n    x   y   z\n0   a   NaN NaN\n1   b   NaN NaN\n2   c   NaN NaN\n3   d   1.0 2.0\n4   e   1.0 2.0\n5   f   1.0 2.0\n</code></pre>\n<p>Expected result:</p>\n<pre><code>   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67357814,
            "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan",
            "question": {
                "title": "Python pandas dataframe apply result of function to multiple columns where NaN",
                "ques_desc": "I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result: "
            },
            "io": [
                "    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n",
                "   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n"
            ],
            "answer": {
                "ans_desc": "you can fillna after calculating for the full dataframe and ",
                "code": [
                    "out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 403,
            "user_id": 12076197,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/228f4f1d534ab4bc69cbae003c708bf2?s=128&d=identicon&r=PG&f=1",
            "display_name": "dmd7",
            "link": "https://stackoverflow.com/users/12076197/dmd7"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67506886,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1620834577,
        "creation_date": 1620833933,
        "question_id": 67506798,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
        "title": "Reformat Dataframe / Add rows when condition is met",
        "body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "answer_body": "<p>Assuming you only have positive integers in <code>'ColB'</code> You can re-create the DataFrame from scratch using <code>np.repeat</code>. The repeat takes care of the duplication, so we can assign ColB = 1.</p>\n<pre><code>import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n</code></pre>\n<p>Alternatively, if you have a non-duplicated Index, you can repeat that and use <code>loc</code> to get the repitition. Useful when you have more than a single column you want to repeat:</p>\n<pre><code>df = (df.loc[df.index.repeat(df.ColB)]\n        .assign(ColB=1))\n</code></pre>\n<hr />\n<pre><code>         ColA  ColB\n0  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n1  2021-03-09     1\n2  2021-03-10     1\n2  2021-03-10     1\n3  2021-03-10     1\n4  2021-03-10     1\n4  2021-03-10     1\n5  2021-03-11     1\n5  2021-03-11     1\n</code></pre>\n",
        "question_body": "<p>I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only &quot;1's&quot;. If the  value is greater than one, then add length of rows equal to the number thats &gt; 1, while keeping ColA sorted by date asc. Example below:</p>\n<p>Original DF:</p>\n<pre><code>   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n</code></pre>\n<p>Desired DF</p>\n<pre><code>   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n</code></pre>\n<p>any suggestions are much appreciated!</p>\n",
        "formatted_input": {
            "qid": 67506798,
            "link": "https://stackoverflow.com/questions/67506798/reformat-dataframe-add-rows-when-condition-is-met",
            "question": {
                "title": "Reformat Dataframe / Add rows when condition is met",
                "ques_desc": "I'm looking to add dataframe rows and edit a column when a condition is met. I want Column B to be only \"1's\". If the value is greater than one, then add length of rows equal to the number thats > 1, while keeping ColA sorted by date asc. Example below: Original DF: Desired DF any suggestions are much appreciated! "
            },
            "io": [
                "   ColA        ColB\n2021-03-09       1\n2021-03-09       3\n2021-03-10       2\n2021-03-10       1\n2021-03-10       2\n2021-03-11       2\n",
                "   ColA         ColB\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-09       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-10       1\n2021-03-11       1\n2021-03-11       1\n"
            ],
            "answer": {
                "ans_desc": "Assuming you only have positive integers in You can re-create the DataFrame from scratch using . The repeat takes care of the duplication, so we can assign ColB = 1. Alternatively, if you have a non-duplicated Index, you can repeat that and use to get the repitition. Useful when you have more than a single column you want to repeat: ",
                "code": [
                    "import pandas as pd\nimport numpy as np\n\ndf = (pd.DataFrame(np.repeat(df.ColA, df.ColB))\n        .assign(ColB=1))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 15817395,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyRFG42bJfN7WzpcIAmho4ZvAc1tTi_zySFK3_C=k-s128",
            "display_name": "Bernd Blase",
            "link": "https://stackoverflow.com/users/15817395/bernd-blase"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 67361868,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1619999183,
        "creation_date": 1619996303,
        "question_id": 67361824,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
        "title": "Convert dataframe objects to float by iterating over columns",
        "body": "<p>I want to convert data in Pandas.Series by iterating over Series</p>\n<p>DataFrame df looks like</p>\n<pre><code>   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n</code></pre>\n<p>'%' and '-' only values should be removed. Desired result:</p>\n<pre><code>   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n</code></pre>\n<p>If I call</p>\n<pre><code>df['c1']= df['c1'].str.replace('%', '')\ndf['c1']= df['c1'].str.replace('-%', '0')\ndf['c1']= df['c1'].astype(float)\n</code></pre>\n<p>it works.</p>\n<p>But if I try to iterate it does not:</p>\n<pre><code>col= [] \ncol.append(df['c1'])\ncol.append(df['c2'])\n\nfor i  in (col):\n    i = i.str.replace('%', '',regex=True)\n    i = i.str.replace('-$', '0',regex=True)\n</code></pre>\n<p>Thanks in advance</p>\n",
        "answer_body": "<p>EDIT: Improved regex</p>\n<pre><code># Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n</code></pre>\n<p>Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals.</p>\n<p><strong>Output</strong></p>\n<pre><code>    c1      c2\n0   0.0     75.0\n1   -5.5    65.8\n2   0.0     6.9\n</code></pre>\n<p><strong>Explanation</strong></p>\n<p>We can use regex over complete df, to replace the required symbols, we are replacing <code>%</code> with empty string and if a row consists of <code>-</code> at the end then replace it with 0.0.</p>\n",
        "question_body": "<p>I want to convert data in Pandas.Series by iterating over Series</p>\n<p>DataFrame df looks like</p>\n<pre><code>   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n</code></pre>\n<p>'%' and '-' only values should be removed. Desired result:</p>\n<pre><code>   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n</code></pre>\n<p>If I call</p>\n<pre><code>df['c1']= df['c1'].str.replace('%', '')\ndf['c1']= df['c1'].str.replace('-%', '0')\ndf['c1']= df['c1'].astype(float)\n</code></pre>\n<p>it works.</p>\n<p>But if I try to iterate it does not:</p>\n<pre><code>col= [] \ncol.append(df['c1'])\ncol.append(df['c2'])\n\nfor i  in (col):\n    i = i.str.replace('%', '',regex=True)\n    i = i.str.replace('-$', '0',regex=True)\n</code></pre>\n<p>Thanks in advance</p>\n",
        "formatted_input": {
            "qid": 67361824,
            "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
            "question": {
                "title": "Convert dataframe objects to float by iterating over columns",
                "ques_desc": "I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance "
            },
            "io": [
                "   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n",
                "   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n"
            ],
            "answer": {
                "ans_desc": "EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. ",
                "code": [
                    "# Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-apply"
        ],
        "owner": {
            "reputation": 777,
            "user_id": 4014825,
            "user_type": "registered",
            "accept_rate": 90,
            "profile_image": "https://www.gravatar.com/avatar/28ddcf15e4e55ae03685fcecee3c54f9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Inthu",
            "link": "https://stackoverflow.com/users/4014825/inthu"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 67358000,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1619969264,
        "creation_date": 1619967653,
        "last_edit_date": 1619968454,
        "question_id": 67357814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan",
        "title": "Python pandas dataframe apply result of function to multiple columns where NaN",
        "body": "<p>I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN.</p>\n<pre><code>def calculate(x):\n    return 1, 2\n\ndf = pd.DataFrame({'x':['a', 'b', 'c', 'd', 'e', 'f'], 'y':[np.NaN, np.NaN, np.NaN, 'a1', 'b2', 'c3'], 'z':[np.NaN, np.NaN, np.NaN, 'a2', 'b1', 'c4']})\n\n x    y    z\n0  a  NaN  NaN\n1  b  NaN  NaN\n2  c  NaN  NaN\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n\nmask = (df.isnull().any(axis=1))\n\ndf[['y', 'z']] = df[mask].apply(calculate, axis=1, result_type='expand')\n</code></pre>\n<p>However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong.</p>\n<pre><code>    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n</code></pre>\n<p>If the mask is inverted I get the following result:</p>\n<pre><code>df[['y', 'z']] = df[~mask].apply(calculate, axis=1, result_type='expand')\n    x   y   z\n0   a   NaN NaN\n1   b   NaN NaN\n2   c   NaN NaN\n3   d   1.0 2.0\n4   e   1.0 2.0\n5   f   1.0 2.0\n</code></pre>\n<p>Expected result:</p>\n<pre><code>   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n</code></pre>\n",
        "answer_body": "<p>you can fillna after calculating for the full dataframe and <code>set_axis</code></p>\n<pre><code>out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n</code></pre>\n<hr />\n<pre><code>print(out)\n\n   x   y   z\n0  a   1   2\n1  b   1   2\n2  c   1   2\n3  d  a1  a2\n4  e  b2  b1\n5  f  c3  c4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN.</p>\n<pre><code>def calculate(x):\n    return 1, 2\n\ndf = pd.DataFrame({'x':['a', 'b', 'c', 'd', 'e', 'f'], 'y':[np.NaN, np.NaN, np.NaN, 'a1', 'b2', 'c3'], 'z':[np.NaN, np.NaN, np.NaN, 'a2', 'b1', 'c4']})\n\n x    y    z\n0  a  NaN  NaN\n1  b  NaN  NaN\n2  c  NaN  NaN\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n\nmask = (df.isnull().any(axis=1))\n\ndf[['y', 'z']] = df[mask].apply(calculate, axis=1, result_type='expand')\n</code></pre>\n<p>However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong.</p>\n<pre><code>    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n</code></pre>\n<p>If the mask is inverted I get the following result:</p>\n<pre><code>df[['y', 'z']] = df[~mask].apply(calculate, axis=1, result_type='expand')\n    x   y   z\n0   a   NaN NaN\n1   b   NaN NaN\n2   c   NaN NaN\n3   d   1.0 2.0\n4   e   1.0 2.0\n5   f   1.0 2.0\n</code></pre>\n<p>Expected result:</p>\n<pre><code>   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67357814,
            "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan",
            "question": {
                "title": "Python pandas dataframe apply result of function to multiple columns where NaN",
                "ques_desc": "I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result: "
            },
            "io": [
                "    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n",
                "   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n"
            ],
            "answer": {
                "ans_desc": "you can fillna after calculating for the full dataframe and ",
                "code": [
                    "out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 361,
            "user_id": 15492238,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5bb9116f49e2fd92ba919ad7f1fd1141?s=128&d=identicon&r=PG&f=1",
            "display_name": "tj judge ",
            "link": "https://stackoverflow.com/users/15492238/tj-judge"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67336086,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1619843858,
        "creation_date": 1619793380,
        "last_edit_date": 1619843858,
        "question_id": 67335759,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python",
        "title": "Checking if column headers match PYTHON",
        "body": "<p>I have two dataframes:</p>\n<p>df1:</p>\n<pre><code>      ID  Open High Low  \n       1  64   66   52   \n</code></pre>\n<p>df2</p>\n<pre><code>      ID Open High  Volume\n      1   33   45   30043\n</code></pre>\n<p>I want to write a function that checks if the column headers are matching/the same as columns in df1.</p>\n<p>IF not we get a message telling us what column is missing.</p>\n<p>Example of the message given these dataframes:</p>\n<pre><code>  &quot;The column 'Low' is not selected in df2. The column 'Volume' is not selected in df1' \n</code></pre>\n<p>I want a generalized code that can work for any given dataframe.</p>\n<p>Is this possible on python?</p>\n",
        "answer_body": "<p>You can have access to the column names via <code>.columns</code> and then use set operations to check what you want:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        &quot;ID&quot;: [1],\n        &quot;Open&quot;: [64],\n        &quot;High&quot;: [66],\n        &quot;Low&quot;: [52]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        &quot;ID&quot;: [1],\n        &quot;Open&quot;: [33],\n        &quot;High&quot;: [45],\n        &quot;Volume&quot;: [30043]\n    }\n)\n\ndf1_columns = set(df1.columns)\ndf2_columns = set(df2.columns)\n\ncommon_columns = df1_columns &amp; df2_columns\n\ndf1_columns_only = df1_columns - common_columns\ndf2_columns_only = df2_columns - common_columns\n\nprint(&quot;Columns only available in df1&quot;, df1_columns_only)\nprint(&quot;Columns only available in df2&quot;, df2_columns_only)\n</code></pre>\n<p>And it gives the expected output:</p>\n<pre><code>Columns only available in df1 {'Low'}\nColumns only available in df2 {'Volume'}\n</code></pre>\n",
        "question_body": "<p>I have two dataframes:</p>\n<p>df1:</p>\n<pre><code>      ID  Open High Low  \n       1  64   66   52   \n</code></pre>\n<p>df2</p>\n<pre><code>      ID Open High  Volume\n      1   33   45   30043\n</code></pre>\n<p>I want to write a function that checks if the column headers are matching/the same as columns in df1.</p>\n<p>IF not we get a message telling us what column is missing.</p>\n<p>Example of the message given these dataframes:</p>\n<pre><code>  &quot;The column 'Low' is not selected in df2. The column 'Volume' is not selected in df1' \n</code></pre>\n<p>I want a generalized code that can work for any given dataframe.</p>\n<p>Is this possible on python?</p>\n",
        "formatted_input": {
            "qid": 67335759,
            "link": "https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python",
            "question": {
                "title": "Checking if column headers match PYTHON",
                "ques_desc": "I have two dataframes: df1: df2 I want to write a function that checks if the column headers are matching/the same as columns in df1. IF not we get a message telling us what column is missing. Example of the message given these dataframes: I want a generalized code that can work for any given dataframe. Is this possible on python? "
            },
            "io": [
                "      ID  Open High Low  \n       1  64   66   52   \n",
                "      ID Open High  Volume\n      1   33   45   30043\n"
            ],
            "answer": {
                "ans_desc": "You can have access to the column names via and then use set operations to check what you want: And it gives the expected output: ",
                "code": [
                    "import pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [64],\n        \"High\": [66],\n        \"Low\": [52]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [33],\n        \"High\": [45],\n        \"Volume\": [30043]\n    }\n)\n\ndf1_columns = set(df1.columns)\ndf2_columns = set(df2.columns)\n\ncommon_columns = df1_columns & df2_columns\n\ndf1_columns_only = df1_columns - common_columns\ndf2_columns_only = df2_columns - common_columns\n\nprint(\"Columns only available in df1\", df1_columns_only)\nprint(\"Columns only available in df2\", df2_columns_only)\n",
                    "Columns only available in df1 {'Low'}\nColumns only available in df2 {'Volume'}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 7018,
            "user_id": 1452759,
            "user_type": "registered",
            "accept_rate": 73,
            "profile_image": "https://www.gravatar.com/avatar/724bd9ac38fdf069f0b869b70ff0e753?s=128&d=identicon&r=PG",
            "display_name": "user1452759",
            "link": "https://stackoverflow.com/users/1452759/user1452759"
        },
        "is_answered": true,
        "view_count": 396687,
        "protected_date": 1562192230,
        "accepted_answer_id": 26838140,
        "answer_count": 8,
        "score": 290,
        "last_activity_date": 1619763697,
        "creation_date": 1415600966,
        "last_edit_date": 1540067939,
        "question_id": 26837998,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string",
        "title": "Pandas Replace NaN with blank/empty string",
        "body": "<p>I have a Pandas Dataframe as shown below:</p>\n\n<pre><code>    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n</code></pre>\n\n<p>I want to remove the NaN values with an empty string so that it looks like so:</p>\n\n<pre><code>    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n</code></pre>\n",
        "answer_body": "<pre><code>import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n</code></pre>\n\n<p>This might help. It will replace all NaNs with an empty string.</p>\n",
        "question_body": "<p>I have a Pandas Dataframe as shown below:</p>\n\n<pre><code>    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n</code></pre>\n\n<p>I want to remove the NaN values with an empty string so that it looks like so:</p>\n\n<pre><code>    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n</code></pre>\n",
        "formatted_input": {
            "qid": 26837998,
            "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string",
            "question": {
                "title": "Pandas Replace NaN with blank/empty string",
                "ques_desc": "I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so: "
            },
            "io": [
                "    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n",
                "    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n"
            ],
            "answer": {
                "ans_desc": " This might help. It will replace all NaNs with an empty string. ",
                "code": [
                    "import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 15817395,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyRFG42bJfN7WzpcIAmho4ZvAc1tTi_zySFK3_C=k-s128",
            "display_name": "Bernd Blase",
            "link": "https://stackoverflow.com/users/15817395/bernd-blase"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 67361868,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1619999183,
        "creation_date": 1619996303,
        "question_id": 67361824,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
        "title": "Convert dataframe objects to float by iterating over columns",
        "body": "<p>I want to convert data in Pandas.Series by iterating over Series</p>\n<p>DataFrame df looks like</p>\n<pre><code>   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n</code></pre>\n<p>'%' and '-' only values should be removed. Desired result:</p>\n<pre><code>   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n</code></pre>\n<p>If I call</p>\n<pre><code>df['c1']= df['c1'].str.replace('%', '')\ndf['c1']= df['c1'].str.replace('-%', '0')\ndf['c1']= df['c1'].astype(float)\n</code></pre>\n<p>it works.</p>\n<p>But if I try to iterate it does not:</p>\n<pre><code>col= [] \ncol.append(df['c1'])\ncol.append(df['c2'])\n\nfor i  in (col):\n    i = i.str.replace('%', '',regex=True)\n    i = i.str.replace('-$', '0',regex=True)\n</code></pre>\n<p>Thanks in advance</p>\n",
        "answer_body": "<p>EDIT: Improved regex</p>\n<pre><code># Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n</code></pre>\n<p>Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals.</p>\n<p><strong>Output</strong></p>\n<pre><code>    c1      c2\n0   0.0     75.0\n1   -5.5    65.8\n2   0.0     6.9\n</code></pre>\n<p><strong>Explanation</strong></p>\n<p>We can use regex over complete df, to replace the required symbols, we are replacing <code>%</code> with empty string and if a row consists of <code>-</code> at the end then replace it with 0.0.</p>\n",
        "question_body": "<p>I want to convert data in Pandas.Series by iterating over Series</p>\n<p>DataFrame df looks like</p>\n<pre><code>   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n</code></pre>\n<p>'%' and '-' only values should be removed. Desired result:</p>\n<pre><code>   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n</code></pre>\n<p>If I call</p>\n<pre><code>df['c1']= df['c1'].str.replace('%', '')\ndf['c1']= df['c1'].str.replace('-%', '0')\ndf['c1']= df['c1'].astype(float)\n</code></pre>\n<p>it works.</p>\n<p>But if I try to iterate it does not:</p>\n<pre><code>col= [] \ncol.append(df['c1'])\ncol.append(df['c2'])\n\nfor i  in (col):\n    i = i.str.replace('%', '',regex=True)\n    i = i.str.replace('-$', '0',regex=True)\n</code></pre>\n<p>Thanks in advance</p>\n",
        "formatted_input": {
            "qid": 67361824,
            "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
            "question": {
                "title": "Convert dataframe objects to float by iterating over columns",
                "ques_desc": "I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance "
            },
            "io": [
                "   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n",
                "   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n"
            ],
            "answer": {
                "ans_desc": "EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. ",
                "code": [
                    "# Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-apply"
        ],
        "owner": {
            "reputation": 777,
            "user_id": 4014825,
            "user_type": "registered",
            "accept_rate": 90,
            "profile_image": "https://www.gravatar.com/avatar/28ddcf15e4e55ae03685fcecee3c54f9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Inthu",
            "link": "https://stackoverflow.com/users/4014825/inthu"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 67358000,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1619969264,
        "creation_date": 1619967653,
        "last_edit_date": 1619968454,
        "question_id": 67357814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan",
        "title": "Python pandas dataframe apply result of function to multiple columns where NaN",
        "body": "<p>I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN.</p>\n<pre><code>def calculate(x):\n    return 1, 2\n\ndf = pd.DataFrame({'x':['a', 'b', 'c', 'd', 'e', 'f'], 'y':[np.NaN, np.NaN, np.NaN, 'a1', 'b2', 'c3'], 'z':[np.NaN, np.NaN, np.NaN, 'a2', 'b1', 'c4']})\n\n x    y    z\n0  a  NaN  NaN\n1  b  NaN  NaN\n2  c  NaN  NaN\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n\nmask = (df.isnull().any(axis=1))\n\ndf[['y', 'z']] = df[mask].apply(calculate, axis=1, result_type='expand')\n</code></pre>\n<p>However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong.</p>\n<pre><code>    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n</code></pre>\n<p>If the mask is inverted I get the following result:</p>\n<pre><code>df[['y', 'z']] = df[~mask].apply(calculate, axis=1, result_type='expand')\n    x   y   z\n0   a   NaN NaN\n1   b   NaN NaN\n2   c   NaN NaN\n3   d   1.0 2.0\n4   e   1.0 2.0\n5   f   1.0 2.0\n</code></pre>\n<p>Expected result:</p>\n<pre><code>   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n</code></pre>\n",
        "answer_body": "<p>you can fillna after calculating for the full dataframe and <code>set_axis</code></p>\n<pre><code>out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n</code></pre>\n<hr />\n<pre><code>print(out)\n\n   x   y   z\n0  a   1   2\n1  b   1   2\n2  c   1   2\n3  d  a1  a2\n4  e  b2  b1\n5  f  c3  c4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN.</p>\n<pre><code>def calculate(x):\n    return 1, 2\n\ndf = pd.DataFrame({'x':['a', 'b', 'c', 'd', 'e', 'f'], 'y':[np.NaN, np.NaN, np.NaN, 'a1', 'b2', 'c3'], 'z':[np.NaN, np.NaN, np.NaN, 'a2', 'b1', 'c4']})\n\n x    y    z\n0  a  NaN  NaN\n1  b  NaN  NaN\n2  c  NaN  NaN\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n\nmask = (df.isnull().any(axis=1))\n\ndf[['y', 'z']] = df[mask].apply(calculate, axis=1, result_type='expand')\n</code></pre>\n<p>However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong.</p>\n<pre><code>    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n</code></pre>\n<p>If the mask is inverted I get the following result:</p>\n<pre><code>df[['y', 'z']] = df[~mask].apply(calculate, axis=1, result_type='expand')\n    x   y   z\n0   a   NaN NaN\n1   b   NaN NaN\n2   c   NaN NaN\n3   d   1.0 2.0\n4   e   1.0 2.0\n5   f   1.0 2.0\n</code></pre>\n<p>Expected result:</p>\n<pre><code>   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67357814,
            "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan",
            "question": {
                "title": "Python pandas dataframe apply result of function to multiple columns where NaN",
                "ques_desc": "I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result: "
            },
            "io": [
                "    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n",
                "   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n"
            ],
            "answer": {
                "ans_desc": "you can fillna after calculating for the full dataframe and ",
                "code": [
                    "out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 361,
            "user_id": 15492238,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5bb9116f49e2fd92ba919ad7f1fd1141?s=128&d=identicon&r=PG&f=1",
            "display_name": "tj judge ",
            "link": "https://stackoverflow.com/users/15492238/tj-judge"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67336086,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1619843858,
        "creation_date": 1619793380,
        "last_edit_date": 1619843858,
        "question_id": 67335759,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python",
        "title": "Checking if column headers match PYTHON",
        "body": "<p>I have two dataframes:</p>\n<p>df1:</p>\n<pre><code>      ID  Open High Low  \n       1  64   66   52   \n</code></pre>\n<p>df2</p>\n<pre><code>      ID Open High  Volume\n      1   33   45   30043\n</code></pre>\n<p>I want to write a function that checks if the column headers are matching/the same as columns in df1.</p>\n<p>IF not we get a message telling us what column is missing.</p>\n<p>Example of the message given these dataframes:</p>\n<pre><code>  &quot;The column 'Low' is not selected in df2. The column 'Volume' is not selected in df1' \n</code></pre>\n<p>I want a generalized code that can work for any given dataframe.</p>\n<p>Is this possible on python?</p>\n",
        "answer_body": "<p>You can have access to the column names via <code>.columns</code> and then use set operations to check what you want:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        &quot;ID&quot;: [1],\n        &quot;Open&quot;: [64],\n        &quot;High&quot;: [66],\n        &quot;Low&quot;: [52]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        &quot;ID&quot;: [1],\n        &quot;Open&quot;: [33],\n        &quot;High&quot;: [45],\n        &quot;Volume&quot;: [30043]\n    }\n)\n\ndf1_columns = set(df1.columns)\ndf2_columns = set(df2.columns)\n\ncommon_columns = df1_columns &amp; df2_columns\n\ndf1_columns_only = df1_columns - common_columns\ndf2_columns_only = df2_columns - common_columns\n\nprint(&quot;Columns only available in df1&quot;, df1_columns_only)\nprint(&quot;Columns only available in df2&quot;, df2_columns_only)\n</code></pre>\n<p>And it gives the expected output:</p>\n<pre><code>Columns only available in df1 {'Low'}\nColumns only available in df2 {'Volume'}\n</code></pre>\n",
        "question_body": "<p>I have two dataframes:</p>\n<p>df1:</p>\n<pre><code>      ID  Open High Low  \n       1  64   66   52   \n</code></pre>\n<p>df2</p>\n<pre><code>      ID Open High  Volume\n      1   33   45   30043\n</code></pre>\n<p>I want to write a function that checks if the column headers are matching/the same as columns in df1.</p>\n<p>IF not we get a message telling us what column is missing.</p>\n<p>Example of the message given these dataframes:</p>\n<pre><code>  &quot;The column 'Low' is not selected in df2. The column 'Volume' is not selected in df1' \n</code></pre>\n<p>I want a generalized code that can work for any given dataframe.</p>\n<p>Is this possible on python?</p>\n",
        "formatted_input": {
            "qid": 67335759,
            "link": "https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python",
            "question": {
                "title": "Checking if column headers match PYTHON",
                "ques_desc": "I have two dataframes: df1: df2 I want to write a function that checks if the column headers are matching/the same as columns in df1. IF not we get a message telling us what column is missing. Example of the message given these dataframes: I want a generalized code that can work for any given dataframe. Is this possible on python? "
            },
            "io": [
                "      ID  Open High Low  \n       1  64   66   52   \n",
                "      ID Open High  Volume\n      1   33   45   30043\n"
            ],
            "answer": {
                "ans_desc": "You can have access to the column names via and then use set operations to check what you want: And it gives the expected output: ",
                "code": [
                    "import pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [64],\n        \"High\": [66],\n        \"Low\": [52]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [33],\n        \"High\": [45],\n        \"Volume\": [30043]\n    }\n)\n\ndf1_columns = set(df1.columns)\ndf2_columns = set(df2.columns)\n\ncommon_columns = df1_columns & df2_columns\n\ndf1_columns_only = df1_columns - common_columns\ndf2_columns_only = df2_columns - common_columns\n\nprint(\"Columns only available in df1\", df1_columns_only)\nprint(\"Columns only available in df2\", df2_columns_only)\n",
                    "Columns only available in df1 {'Low'}\nColumns only available in df2 {'Volume'}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 7018,
            "user_id": 1452759,
            "user_type": "registered",
            "accept_rate": 73,
            "profile_image": "https://www.gravatar.com/avatar/724bd9ac38fdf069f0b869b70ff0e753?s=128&d=identicon&r=PG",
            "display_name": "user1452759",
            "link": "https://stackoverflow.com/users/1452759/user1452759"
        },
        "is_answered": true,
        "view_count": 396687,
        "protected_date": 1562192230,
        "accepted_answer_id": 26838140,
        "answer_count": 8,
        "score": 290,
        "last_activity_date": 1619763697,
        "creation_date": 1415600966,
        "last_edit_date": 1540067939,
        "question_id": 26837998,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string",
        "title": "Pandas Replace NaN with blank/empty string",
        "body": "<p>I have a Pandas Dataframe as shown below:</p>\n\n<pre><code>    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n</code></pre>\n\n<p>I want to remove the NaN values with an empty string so that it looks like so:</p>\n\n<pre><code>    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n</code></pre>\n",
        "answer_body": "<pre><code>import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n</code></pre>\n\n<p>This might help. It will replace all NaNs with an empty string.</p>\n",
        "question_body": "<p>I have a Pandas Dataframe as shown below:</p>\n\n<pre><code>    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n</code></pre>\n\n<p>I want to remove the NaN values with an empty string so that it looks like so:</p>\n\n<pre><code>    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n</code></pre>\n",
        "formatted_input": {
            "qid": 26837998,
            "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string",
            "question": {
                "title": "Pandas Replace NaN with blank/empty string",
                "ques_desc": "I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so: "
            },
            "io": [
                "    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n",
                "    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n"
            ],
            "answer": {
                "ans_desc": " This might help. It will replace all NaNs with an empty string. ",
                "code": [
                    "import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 15778404,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GimYCkfz3pFkj7O8AJd9IIO7bm3ktBeyl9eehMGGQ=k-s128",
            "display_name": "rafazamp",
            "link": "https://stackoverflow.com/users/15778404/rafazamp"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67288740,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1619551535,
        "creation_date": 1619545844,
        "last_edit_date": 1619548270,
        "question_id": 67288220,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition",
        "title": "How can I add a new line in pandas dataframe based in a condition?",
        "body": "<p>I have this Dataframe that is populated from a file.\nThe first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition.</p>\n<pre><code>[1]   [2] [3]\n  1     30  2\n  1     30  1\n  1     30  3\n  1     90  3\n  1    370  3\n  1    430  3\n  1    705  3\n  1    805  3\n  1    880  2\n  1    905  3\n  1   1005  3\n  1   1170  3\n  1   1230  3\n  1   1970  3\n  1   2030  3\n  1   2970  3\n  1   3030  3\n  1   3970  3\n  1   4030  3\n  1   4423  3\n  1   4539  3\n  1   4575  3\n  1   4630  2\n  1   4635  3\n  1   4671  3\n  1   4787  3\n  1   4957  3\n  1   5057  3\n  1   5270  3\n  1   5330  3\n  1   5970  3\n  1   6030  3\n  1   6970  3\n  1   7030  3\n  1   7970  3\n  1   8030  3\n  1   8158  3\n  1   8257  3\n  1   8332  2\n  1   8357  3\n  1   8457  3\n  1   8970  3\n  1   9030  3\n  1   9970  3\n  1  10030  3\n  1  10970  3\n  1  11030  3\n  1  11470  3\n  1  11530  3\n  1  11853  3\n  1  11953  3\n</code></pre>\n<p>Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this..\nTaking for example the lines number 4 and 5:</p>\n<pre><code>  1     90  3\n  1    370  3\n 370 - 90 = 260 (260 is greater than 100)\n</code></pre>\n<p>So I need to add a new line with the last number + 100, and the last column needs to be zero:</p>\n<pre><code>  1     90  3\n  1    190  0\n  1    370  3\n</code></pre>\n<p>Any ideas how can I achieve that?\nThanks in advance.</p>\n<p>Edit: I just need to add the line once in the DataFrame.</p>\n",
        "answer_body": "<p>Try:</p>\n<pre><code>m = df[&quot;[2]&quot;].diff() &gt; 100\n\ndf.loc[m, &quot;[2]&quot;] = pd.Series(\n    [\n        [str(df.iloc[v - 1][&quot;[2]&quot;] + 100), df.iloc[v][&quot;[2]&quot;]]\n        for v in df.index[m]\n    ],\n    index=df.index[m],\n)\n\ndf = df.explode(&quot;[2]&quot;)\ndf[&quot;[3]&quot;] = np.where(\n    df[&quot;[2]&quot;].apply(lambda x: isinstance(x, str)), 0, df[&quot;[3]&quot;]\n)\ndf[&quot;[2]&quot;] = df[&quot;[2]&quot;].astype(int)\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>    [1]    [2]  [3]\n0     1     30    2\n1     1     30    1\n2     1     30    3\n3     1     90    3\n4     1    190    0\n4     1    370    3\n5     1    430    3\n6     1    530    0\n6     1    705    3\n7     1    805    3\n8     1    880    2\n9     1    905    3\n10    1   1005    3\n11    1   1105    0\n11    1   1170    3\n12    1   1230    3\n13    1   1330    0\n13    1   1970    3\n14    1   2030    3\n15    1   2130    0\n15    1   2970    3\n16    1   3030    3\n17    1   3130    0\n17    1   3970    3\n18    1   4030    3\n19    1   4130    0\n19    1   4423    3\n20    1   4523    0\n20    1   4539    3\n21    1   4575    3\n22    1   4630    2\n23    1   4635    3\n24    1   4671    3\n25    1   4771    0\n25    1   4787    3\n26    1   4887    0\n26    1   4957    3\n27    1   5057    3\n28    1   5157    0\n28    1   5270    3\n29    1   5330    3\n30    1   5430    0\n30    1   5970    3\n31    1   6030    3\n32    1   6130    0\n32    1   6970    3\n33    1   7030    3\n34    1   7130    0\n34    1   7970    3\n35    1   8030    3\n36    1   8130    0\n36    1   8158    3\n37    1   8257    3\n38    1   8332    2\n39    1   8357    3\n40    1   8457    3\n41    1   8557    0\n41    1   8970    3\n42    1   9030    3\n43    1   9130    0\n43    1   9970    3\n44    1  10030    3\n45    1  10130    0\n45    1  10970    3\n46    1  11030    3\n47    1  11130    0\n47    1  11470    3\n48    1  11530    3\n49    1  11630    0\n49    1  11853    3\n50    1  11953    3\n</code></pre>\n<hr />\n<p>EDIT: To change only one value:</p>\n<pre><code>mask = df[&quot;[2]&quot;].diff() &gt; 100\nif True in mask:\n    m = [False] * len(df)\n    m[mask.idxmax()] = True\n\n    df.loc[m, &quot;[2]&quot;] = pd.Series(\n        [\n            [str(df.iloc[v - 1][&quot;[2]&quot;] + 100), df.iloc[v][&quot;[2]&quot;]]\n            for v in df.index[m]\n        ],\n        index=df.index[m],\n    )\n\n    df = df.explode(&quot;[2]&quot;)\n    df[&quot;[3]&quot;] = np.where(\n        df[&quot;[2]&quot;].apply(lambda x: isinstance(x, str)), 0, df[&quot;[3]&quot;]\n    )\n    df[&quot;[2]&quot;] = df[&quot;[2]&quot;].astype(int)\n    print(df)\n</code></pre>\n",
        "question_body": "<p>I have this Dataframe that is populated from a file.\nThe first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition.</p>\n<pre><code>[1]   [2] [3]\n  1     30  2\n  1     30  1\n  1     30  3\n  1     90  3\n  1    370  3\n  1    430  3\n  1    705  3\n  1    805  3\n  1    880  2\n  1    905  3\n  1   1005  3\n  1   1170  3\n  1   1230  3\n  1   1970  3\n  1   2030  3\n  1   2970  3\n  1   3030  3\n  1   3970  3\n  1   4030  3\n  1   4423  3\n  1   4539  3\n  1   4575  3\n  1   4630  2\n  1   4635  3\n  1   4671  3\n  1   4787  3\n  1   4957  3\n  1   5057  3\n  1   5270  3\n  1   5330  3\n  1   5970  3\n  1   6030  3\n  1   6970  3\n  1   7030  3\n  1   7970  3\n  1   8030  3\n  1   8158  3\n  1   8257  3\n  1   8332  2\n  1   8357  3\n  1   8457  3\n  1   8970  3\n  1   9030  3\n  1   9970  3\n  1  10030  3\n  1  10970  3\n  1  11030  3\n  1  11470  3\n  1  11530  3\n  1  11853  3\n  1  11953  3\n</code></pre>\n<p>Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this..\nTaking for example the lines number 4 and 5:</p>\n<pre><code>  1     90  3\n  1    370  3\n 370 - 90 = 260 (260 is greater than 100)\n</code></pre>\n<p>So I need to add a new line with the last number + 100, and the last column needs to be zero:</p>\n<pre><code>  1     90  3\n  1    190  0\n  1    370  3\n</code></pre>\n<p>Any ideas how can I achieve that?\nThanks in advance.</p>\n<p>Edit: I just need to add the line once in the DataFrame.</p>\n",
        "formatted_input": {
            "qid": 67288220,
            "link": "https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition",
            "question": {
                "title": "How can I add a new line in pandas dataframe based in a condition?",
                "ques_desc": "I have this Dataframe that is populated from a file. The first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition. Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this.. Taking for example the lines number 4 and 5: So I need to add a new line with the last number + 100, and the last column needs to be zero: Any ideas how can I achieve that? Thanks in advance. Edit: I just need to add the line once in the DataFrame. "
            },
            "io": [
                "[1]   [2] [3]\n  1     30  2\n  1     30  1\n  1     30  3\n  1     90  3\n  1    370  3\n  1    430  3\n  1    705  3\n  1    805  3\n  1    880  2\n  1    905  3\n  1   1005  3\n  1   1170  3\n  1   1230  3\n  1   1970  3\n  1   2030  3\n  1   2970  3\n  1   3030  3\n  1   3970  3\n  1   4030  3\n  1   4423  3\n  1   4539  3\n  1   4575  3\n  1   4630  2\n  1   4635  3\n  1   4671  3\n  1   4787  3\n  1   4957  3\n  1   5057  3\n  1   5270  3\n  1   5330  3\n  1   5970  3\n  1   6030  3\n  1   6970  3\n  1   7030  3\n  1   7970  3\n  1   8030  3\n  1   8158  3\n  1   8257  3\n  1   8332  2\n  1   8357  3\n  1   8457  3\n  1   8970  3\n  1   9030  3\n  1   9970  3\n  1  10030  3\n  1  10970  3\n  1  11030  3\n  1  11470  3\n  1  11530  3\n  1  11853  3\n  1  11953  3\n",
                "  1     90  3\n  1    190  0\n  1    370  3\n"
            ],
            "answer": {
                "ans_desc": "Try: Prints: EDIT: To change only one value: ",
                "code": [
                    "m = df[\"[2]\"].diff() > 100\n\ndf.loc[m, \"[2]\"] = pd.Series(\n    [\n        [str(df.iloc[v - 1][\"[2]\"] + 100), df.iloc[v][\"[2]\"]]\n        for v in df.index[m]\n    ],\n    index=df.index[m],\n)\n\ndf = df.explode(\"[2]\")\ndf[\"[3]\"] = np.where(\n    df[\"[2]\"].apply(lambda x: isinstance(x, str)), 0, df[\"[3]\"]\n)\ndf[\"[2]\"] = df[\"[2]\"].astype(int)\nprint(df)\n",
                    "mask = df[\"[2]\"].diff() > 100\nif True in mask:\n    m = [False] * len(df)\n    m[mask.idxmax()] = True\n\n    df.loc[m, \"[2]\"] = pd.Series(\n        [\n            [str(df.iloc[v - 1][\"[2]\"] + 100), df.iloc[v][\"[2]\"]]\n            for v in df.index[m]\n        ],\n        index=df.index[m],\n    )\n\n    df = df.explode(\"[2]\")\n    df[\"[3]\"] = np.where(\n        df[\"[2]\"].apply(lambda x: isinstance(x, str)), 0, df[\"[3]\"]\n    )\n    df[\"[2]\"] = df[\"[2]\"].astype(int)\n    print(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 57,
            "user_id": 15415267,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/021c1306999edefd832f61302fe04f61?s=128&d=identicon&r=PG&f=1",
            "display_name": "muratmert41",
            "link": "https://stackoverflow.com/users/15415267/muratmert41"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 67258039,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1619423161,
        "creation_date": 1619381990,
        "last_edit_date": 1619423161,
        "question_id": 67257898,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column",
        "title": "How to add a value to a new column by referencing the values in a column",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5 \n 1  x1      100   15   10   20   25\n 2  x1      15    16   14   10   10\n 3  x4      10    50   40   30   25\n 4  x3      12    15   60   5    1\n 5  x1      80    15   10   20   25\n 6  x1      15    19   84   10   10\n 7  x4      90    40   90   30   25\n 8  x4      12    85   60   50   10\n</code></pre>\n<p>The <strong>xy</strong> column must be filled with the value of the column names in the <strong>reason</strong> column. Let's look at the first row. The <strong>reason column</strong> shows our value <strong>x1</strong>. So our value in column <strong>xy</strong>, will be the value of x1 column in the first row. Like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5   xy\n 1  x1      100   15   10   20   25   100\n 2  x1      15    16   14   10   10   15\n 3  x4      10    50   40   30   25   30\n 4  x3      12    15   60   5    1    60\n 5  x1      80    15   10   20   25   80\n 6  x1      15    19   84   10   10   15\n 7  x4      90    40   90   30   25   30\n 8  x4      12    85   60   50   10   50\n</code></pre>\n<p>Is there a way to do this?</p>\n",
        "answer_body": "<pre><code>df[&quot;xy&quot;] = df.apply(lambda x: x[x[&quot;reason&quot;]], axis=1)\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>   id reason   x1  x2  x3  x4  x5   xy\n0   1     x1  100  15  10  20  25  100\n1   2     x1   15  16  14  10  10   15\n2   3     x4   10  50  40  30  25   30\n3   4     x3   12  15  60   5   1   60\n4   5     x1   80  15  10  20  25   80\n5   6     x1   15  19  84  10  10   15\n6   7     x4   90  40  90  30  25   30\n7   8     x4   12  85  60  50  10   50\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5 \n 1  x1      100   15   10   20   25\n 2  x1      15    16   14   10   10\n 3  x4      10    50   40   30   25\n 4  x3      12    15   60   5    1\n 5  x1      80    15   10   20   25\n 6  x1      15    19   84   10   10\n 7  x4      90    40   90   30   25\n 8  x4      12    85   60   50   10\n</code></pre>\n<p>The <strong>xy</strong> column must be filled with the value of the column names in the <strong>reason</strong> column. Let's look at the first row. The <strong>reason column</strong> shows our value <strong>x1</strong>. So our value in column <strong>xy</strong>, will be the value of x1 column in the first row. Like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5   xy\n 1  x1      100   15   10   20   25   100\n 2  x1      15    16   14   10   10   15\n 3  x4      10    50   40   30   25   30\n 4  x3      12    15   60   5    1    60\n 5  x1      80    15   10   20   25   80\n 6  x1      15    19   84   10   10   15\n 7  x4      90    40   90   30   25   30\n 8  x4      12    85   60   50   10   50\n</code></pre>\n<p>Is there a way to do this?</p>\n",
        "formatted_input": {
            "qid": 67257898,
            "link": "https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column",
            "question": {
                "title": "How to add a value to a new column by referencing the values in a column",
                "ques_desc": "I have a dataframe like this: The xy column must be filled with the value of the column names in the reason column. Let's look at the first row. The reason column shows our value x1. So our value in column xy, will be the value of x1 column in the first row. Like this: Is there a way to do this? "
            },
            "io": [
                "id  reason  x1    x2   x3   x4   x5 \n 1  x1      100   15   10   20   25\n 2  x1      15    16   14   10   10\n 3  x4      10    50   40   30   25\n 4  x3      12    15   60   5    1\n 5  x1      80    15   10   20   25\n 6  x1      15    19   84   10   10\n 7  x4      90    40   90   30   25\n 8  x4      12    85   60   50   10\n",
                "id  reason  x1    x2   x3   x4   x5   xy\n 1  x1      100   15   10   20   25   100\n 2  x1      15    16   14   10   10   15\n 3  x4      10    50   40   30   25   30\n 4  x3      12    15   60   5    1    60\n 5  x1      80    15   10   20   25   80\n 6  x1      15    19   84   10   10   15\n 7  x4      90    40   90   30   25   30\n 8  x4      12    85   60   50   10   50\n"
            ],
            "answer": {
                "ans_desc": " Prints: ",
                "code": [
                    "df[\"xy\"] = df.apply(lambda x: x[x[\"reason\"]], axis=1)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 15762077,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/853af71cd55ed204dddebe32bbc8a571?s=128&d=identicon&r=PG&f=1",
            "display_name": "kdfjlasjdflaj",
            "link": "https://stackoverflow.com/users/15762077/kdfjlasjdflaj"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 67255996,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1619369716,
        "creation_date": 1619368167,
        "last_edit_date": 1619369473,
        "question_id": 67255732,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67255732/summation-of-operation-in-dataframe",
        "title": "Summation of operation in dataframe",
        "body": "<p>I want to implement a function that does the operation that you can see in the image:</p>\n<p><img src=\"https://i.stack.imgur.com/ocLbz.png\" alt=\"image\" /></p>\n<p>But i not sure how to implement the Summation for the moment i doing something like that:</p>\n<pre><code>def df_operation(df1: pd.DataFrame, df2: pd.DataFrame) -&gt; float:\n    return ((((abs(df1 - df2)).sum())/((df1+df2).sum()))*100)\n</code></pre>\n<p>And the problem is in the summation. If someone can help me.</p>\n<p>For example, i have two dataframes like that:</p>\n<pre><code>dataframe1 = pd.DataFrame({&quot;A&quot;:[8, 2],   \n               &quot;B&quot;:[26, 19]}) \n\ndataframe2 = pd.DataFrame({&quot;A&quot;:[3,6],   \n               &quot;B&quot;:[12,17]})  \n</code></pre>\n<p>Where for the abs operation we will obtain this:</p>\n<pre><code>   A  B\n0  5  14\n1  4  2\n</code></pre>\n<p>And for the sum:</p>\n<pre><code>   A   B\n0  11  38\n1  8   36\n</code></pre>\n<p>Finally we do the summation:</p>\n<pre><code>(25/93) * 100\n</code></pre>\n<p>where <code>25 = 5+14+4+2</code> and <code>93 = 11+8+38+36</code></p>\n",
        "answer_body": "<p>Use <code>.sum().sum()</code> to sum the dataframe across columns/rows:</p>\n<pre><code>result = (\n    dataframe1.sub(dataframe2).abs().sum().sum()\n    / dataframe1.add(dataframe2).sum().sum()\n) * 100\nprint(result)\n</code></pre>\n<p>Prints:</p>\n<pre><code>26.881720430107524\n</code></pre>\n",
        "question_body": "<p>I want to implement a function that does the operation that you can see in the image:</p>\n<p><img src=\"https://i.stack.imgur.com/ocLbz.png\" alt=\"image\" /></p>\n<p>But i not sure how to implement the Summation for the moment i doing something like that:</p>\n<pre><code>def df_operation(df1: pd.DataFrame, df2: pd.DataFrame) -&gt; float:\n    return ((((abs(df1 - df2)).sum())/((df1+df2).sum()))*100)\n</code></pre>\n<p>And the problem is in the summation. If someone can help me.</p>\n<p>For example, i have two dataframes like that:</p>\n<pre><code>dataframe1 = pd.DataFrame({&quot;A&quot;:[8, 2],   \n               &quot;B&quot;:[26, 19]}) \n\ndataframe2 = pd.DataFrame({&quot;A&quot;:[3,6],   \n               &quot;B&quot;:[12,17]})  \n</code></pre>\n<p>Where for the abs operation we will obtain this:</p>\n<pre><code>   A  B\n0  5  14\n1  4  2\n</code></pre>\n<p>And for the sum:</p>\n<pre><code>   A   B\n0  11  38\n1  8   36\n</code></pre>\n<p>Finally we do the summation:</p>\n<pre><code>(25/93) * 100\n</code></pre>\n<p>where <code>25 = 5+14+4+2</code> and <code>93 = 11+8+38+36</code></p>\n",
        "formatted_input": {
            "qid": 67255732,
            "link": "https://stackoverflow.com/questions/67255732/summation-of-operation-in-dataframe",
            "question": {
                "title": "Summation of operation in dataframe",
                "ques_desc": "I want to implement a function that does the operation that you can see in the image: But i not sure how to implement the Summation for the moment i doing something like that: And the problem is in the summation. If someone can help me. For example, i have two dataframes like that: Where for the abs operation we will obtain this: And for the sum: Finally we do the summation: where and "
            },
            "io": [
                "   A  B\n0  5  14\n1  4  2\n",
                "   A   B\n0  11  38\n1  8   36\n"
            ],
            "answer": {
                "ans_desc": "Use to sum the dataframe across columns/rows: Prints: ",
                "code": [
                    "result = (\n    dataframe1.sub(dataframe2).abs().sum().sum()\n    / dataframe1.add(dataframe2).sum().sum()\n) * 100\nprint(result)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 768,
            "user_id": 5356096,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/sKX9g.png?s=128&g=1",
            "display_name": "Jack Avante",
            "link": "https://stackoverflow.com/users/5356096/jack-avante"
        },
        "is_answered": true,
        "view_count": 56,
        "accepted_answer_id": 67245812,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1619348036,
        "creation_date": 1619269385,
        "last_edit_date": 1619286390,
        "question_id": 67243081,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas",
        "title": "Best way to change column data for all rows over multiple dataframes in pandas?",
        "body": "<p>Consider dataframes <code>df1</code>, <code>df2</code>, and <code>df3</code>.</p>\n<p><code>df1</code> and <code>df2</code> have an <code>id</code> column, and <code>df3</code> has a <code>from_id</code> and <code>to_id</code> column.</p>\n<p>I need to iterate over all rows of <code>df3</code>, and replace <code>from_id</code> and <code>to_id</code> with new unique randomly generated UUIDs, and then update those in <code>df1</code> and <code>df2</code> where <code>(id == from_id) | (id == to_id)</code> (before the change to UUID).</p>\n<p>I originally wanted to iterate over all rows of <code>df3</code> and simply check both <code>df1</code> and <code>df2</code> if they contain the original <code>from_id</code> or <code>to_id</code> inside the <code>id</code> column before replacing both, but <a href=\"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\">I found that iterating over pandas rows is a bad idea and slow.</a></p>\n<p>I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes.</p>\n<p>My current method that I believe to be slow and inefficient:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid1()\n\ndef update_ids(df_places: pd.DataFrame, df_transitions: pd.DataFrame, df_arcs: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    for i in range(len(df_arcs)):\n        new_uuid_from = __rand_uuid()\n        new_uuid_to = __rand_uuid()\n        new_uuid_arc = __rand_uuid()\n\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_places.loc[df_places.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_places.loc[df_places.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_arcs.iloc[i]['sourceId'] = new_uuid_from\n        df_arcs.iloc[i]['destinationId'] = new_uuid_to\n        df_arcs.iloc[i]['id'] = new_uuid_arc\n\n    return df_places, df_transitions, df_arcs\n</code></pre>\n<p>Here <code>df_places</code> and <code>df_transitions</code> are above mentioned <code>df1</code> and <code>df2</code>, and <code>df_arcs</code> is <code>df3</code></p>\n<p>Example <code>df_places</code></p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | a1 |\n+---+----+\n| 2 | c1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_transitions</code>:</p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | b1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_arcs</code>:</p>\n<pre><code>+---+----------+---------------+\n|   | sourceId | destinationId |\n+---+----------+---------------+\n| 1 | a1       | b1            |\n+---+----------+---------------+\n| 2 | b1       | c1            |\n+---+----------+---------------+\n</code></pre>\n",
        "answer_body": "<p>A very simple approach:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import itertools\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid4()\n\nrep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}\n\ndf3.replace(rep_dict, inplace=True)\ndf3.id = df3.id.map(lambda x: rand_uuid())\n\ndf1.replace(rep_dict, inplace=True)\ndf2.replace(rep_dict, inplace=True)\n</code></pre>\n",
        "question_body": "<p>Consider dataframes <code>df1</code>, <code>df2</code>, and <code>df3</code>.</p>\n<p><code>df1</code> and <code>df2</code> have an <code>id</code> column, and <code>df3</code> has a <code>from_id</code> and <code>to_id</code> column.</p>\n<p>I need to iterate over all rows of <code>df3</code>, and replace <code>from_id</code> and <code>to_id</code> with new unique randomly generated UUIDs, and then update those in <code>df1</code> and <code>df2</code> where <code>(id == from_id) | (id == to_id)</code> (before the change to UUID).</p>\n<p>I originally wanted to iterate over all rows of <code>df3</code> and simply check both <code>df1</code> and <code>df2</code> if they contain the original <code>from_id</code> or <code>to_id</code> inside the <code>id</code> column before replacing both, but <a href=\"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\">I found that iterating over pandas rows is a bad idea and slow.</a></p>\n<p>I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes.</p>\n<p>My current method that I believe to be slow and inefficient:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid1()\n\ndef update_ids(df_places: pd.DataFrame, df_transitions: pd.DataFrame, df_arcs: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    for i in range(len(df_arcs)):\n        new_uuid_from = __rand_uuid()\n        new_uuid_to = __rand_uuid()\n        new_uuid_arc = __rand_uuid()\n\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_places.loc[df_places.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_places.loc[df_places.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_arcs.iloc[i]['sourceId'] = new_uuid_from\n        df_arcs.iloc[i]['destinationId'] = new_uuid_to\n        df_arcs.iloc[i]['id'] = new_uuid_arc\n\n    return df_places, df_transitions, df_arcs\n</code></pre>\n<p>Here <code>df_places</code> and <code>df_transitions</code> are above mentioned <code>df1</code> and <code>df2</code>, and <code>df_arcs</code> is <code>df3</code></p>\n<p>Example <code>df_places</code></p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | a1 |\n+---+----+\n| 2 | c1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_transitions</code>:</p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | b1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_arcs</code>:</p>\n<pre><code>+---+----------+---------------+\n|   | sourceId | destinationId |\n+---+----------+---------------+\n| 1 | a1       | b1            |\n+---+----------+---------------+\n| 2 | b1       | c1            |\n+---+----------+---------------+\n</code></pre>\n",
        "formatted_input": {
            "qid": 67243081,
            "link": "https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas",
            "question": {
                "title": "Best way to change column data for all rows over multiple dataframes in pandas?",
                "ques_desc": "Consider dataframes , , and . and have an column, and has a and column. I need to iterate over all rows of , and replace and with new unique randomly generated UUIDs, and then update those in and where (before the change to UUID). I originally wanted to iterate over all rows of and simply check both and if they contain the original or inside the column before replacing both, but I found that iterating over pandas rows is a bad idea and slow. I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes. My current method that I believe to be slow and inefficient: Here and are above mentioned and , and is Example Example : Example : "
            },
            "io": [
                "+---+----+\n|   | id |\n+---+----+\n| 1 | a1 |\n+---+----+\n| 2 | c1 |\n+---+----+\n",
                "+---+----+\n|   | id |\n+---+----+\n| 1 | b1 |\n+---+----+\n"
            ],
            "answer": {
                "ans_desc": "A very simple approach: ",
                "code": [
                    "import itertools\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid4()\n\nrep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}\n\ndf3.replace(rep_dict, inplace=True)\ndf3.id = df3.id.map(lambda x: rand_uuid())\n\ndf1.replace(rep_dict, inplace=True)\ndf2.replace(rep_dict, inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 33,
            "user_id": 15817395,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a/AATXAJyRFG42bJfN7WzpcIAmho4ZvAc1tTi_zySFK3_C=k-s128",
            "display_name": "Bernd Blase",
            "link": "https://stackoverflow.com/users/15817395/bernd-blase"
        },
        "is_answered": true,
        "view_count": 41,
        "accepted_answer_id": 67361868,
        "answer_count": 1,
        "score": 3,
        "last_activity_date": 1619999183,
        "creation_date": 1619996303,
        "question_id": 67361824,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
        "title": "Convert dataframe objects to float by iterating over columns",
        "body": "<p>I want to convert data in Pandas.Series by iterating over Series</p>\n<p>DataFrame df looks like</p>\n<pre><code>   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n</code></pre>\n<p>'%' and '-' only values should be removed. Desired result:</p>\n<pre><code>   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n</code></pre>\n<p>If I call</p>\n<pre><code>df['c1']= df['c1'].str.replace('%', '')\ndf['c1']= df['c1'].str.replace('-%', '0')\ndf['c1']= df['c1'].astype(float)\n</code></pre>\n<p>it works.</p>\n<p>But if I try to iterate it does not:</p>\n<pre><code>col= [] \ncol.append(df['c1'])\ncol.append(df['c2'])\n\nfor i  in (col):\n    i = i.str.replace('%', '',regex=True)\n    i = i.str.replace('-$', '0',regex=True)\n</code></pre>\n<p>Thanks in advance</p>\n",
        "answer_body": "<p>EDIT: Improved regex</p>\n<pre><code># Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n</code></pre>\n<p>Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals.</p>\n<p><strong>Output</strong></p>\n<pre><code>    c1      c2\n0   0.0     75.0\n1   -5.5    65.8\n2   0.0     6.9\n</code></pre>\n<p><strong>Explanation</strong></p>\n<p>We can use regex over complete df, to replace the required symbols, we are replacing <code>%</code> with empty string and if a row consists of <code>-</code> at the end then replace it with 0.0.</p>\n",
        "question_body": "<p>I want to convert data in Pandas.Series by iterating over Series</p>\n<p>DataFrame df looks like</p>\n<pre><code>   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n</code></pre>\n<p>'%' and '-' only values should be removed. Desired result:</p>\n<pre><code>   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n</code></pre>\n<p>If I call</p>\n<pre><code>df['c1']= df['c1'].str.replace('%', '')\ndf['c1']= df['c1'].str.replace('-%', '0')\ndf['c1']= df['c1'].astype(float)\n</code></pre>\n<p>it works.</p>\n<p>But if I try to iterate it does not:</p>\n<pre><code>col= [] \ncol.append(df['c1'])\ncol.append(df['c2'])\n\nfor i  in (col):\n    i = i.str.replace('%', '',regex=True)\n    i = i.str.replace('-$', '0',regex=True)\n</code></pre>\n<p>Thanks in advance</p>\n",
        "formatted_input": {
            "qid": 67361824,
            "link": "https://stackoverflow.com/questions/67361824/convert-dataframe-objects-to-float-by-iterating-over-columns",
            "question": {
                "title": "Convert dataframe objects to float by iterating over columns",
                "ques_desc": "I want to convert data in Pandas.Series by iterating over Series DataFrame df looks like '%' and '-' only values should be removed. Desired result: If I call it works. But if I try to iterate it does not: Thanks in advance "
            },
            "io": [
                "   c1   c2\n0  -    75.0%\n1 -5.5% 65.8%\n.\nn  -    6.9%\n",
                "   c1    c2\n0  0.0   75.0\n1 -5.5   65.8\n.\nn  0.0    6.9\n"
            ],
            "answer": {
                "ans_desc": "EDIT: Improved regex Explanation - Since string-based data can often have random spaces. also you can just replace it with 0 since the subsequent float conversion will handle the decimals. Output Explanation We can use regex over complete df, to replace the required symbols, we are replacing with empty string and if a row consists of at the end then replace it with 0.0. ",
                "code": [
                    "# Thanks to @tdy\ndf.replace({'\\%':'', r'^\\s*-\\s*$':0}, regex=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "pandas-apply"
        ],
        "owner": {
            "reputation": 777,
            "user_id": 4014825,
            "user_type": "registered",
            "accept_rate": 90,
            "profile_image": "https://www.gravatar.com/avatar/28ddcf15e4e55ae03685fcecee3c54f9?s=128&d=identicon&r=PG&f=1",
            "display_name": "Inthu",
            "link": "https://stackoverflow.com/users/4014825/inthu"
        },
        "is_answered": true,
        "view_count": 38,
        "accepted_answer_id": 67358000,
        "answer_count": 2,
        "score": 2,
        "last_activity_date": 1619969264,
        "creation_date": 1619967653,
        "last_edit_date": 1619968454,
        "question_id": 67357814,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan",
        "title": "Python pandas dataframe apply result of function to multiple columns where NaN",
        "body": "<p>I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN.</p>\n<pre><code>def calculate(x):\n    return 1, 2\n\ndf = pd.DataFrame({'x':['a', 'b', 'c', 'd', 'e', 'f'], 'y':[np.NaN, np.NaN, np.NaN, 'a1', 'b2', 'c3'], 'z':[np.NaN, np.NaN, np.NaN, 'a2', 'b1', 'c4']})\n\n x    y    z\n0  a  NaN  NaN\n1  b  NaN  NaN\n2  c  NaN  NaN\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n\nmask = (df.isnull().any(axis=1))\n\ndf[['y', 'z']] = df[mask].apply(calculate, axis=1, result_type='expand')\n</code></pre>\n<p>However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong.</p>\n<pre><code>    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n</code></pre>\n<p>If the mask is inverted I get the following result:</p>\n<pre><code>df[['y', 'z']] = df[~mask].apply(calculate, axis=1, result_type='expand')\n    x   y   z\n0   a   NaN NaN\n1   b   NaN NaN\n2   c   NaN NaN\n3   d   1.0 2.0\n4   e   1.0 2.0\n5   f   1.0 2.0\n</code></pre>\n<p>Expected result:</p>\n<pre><code>   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n</code></pre>\n",
        "answer_body": "<p>you can fillna after calculating for the full dataframe and <code>set_axis</code></p>\n<pre><code>out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n</code></pre>\n<hr />\n<pre><code>print(out)\n\n   x   y   z\n0  a   1   2\n1  b   1   2\n2  c   1   2\n3  d  a1  a2\n4  e  b2  b1\n5  f  c3  c4\n</code></pre>\n",
        "question_body": "<p>I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN.</p>\n<pre><code>def calculate(x):\n    return 1, 2\n\ndf = pd.DataFrame({'x':['a', 'b', 'c', 'd', 'e', 'f'], 'y':[np.NaN, np.NaN, np.NaN, 'a1', 'b2', 'c3'], 'z':[np.NaN, np.NaN, np.NaN, 'a2', 'b1', 'c4']})\n\n x    y    z\n0  a  NaN  NaN\n1  b  NaN  NaN\n2  c  NaN  NaN\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n\nmask = (df.isnull().any(axis=1))\n\ndf[['y', 'z']] = df[mask].apply(calculate, axis=1, result_type='expand')\n</code></pre>\n<p>However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong.</p>\n<pre><code>    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n</code></pre>\n<p>If the mask is inverted I get the following result:</p>\n<pre><code>df[['y', 'z']] = df[~mask].apply(calculate, axis=1, result_type='expand')\n    x   y   z\n0   a   NaN NaN\n1   b   NaN NaN\n2   c   NaN NaN\n3   d   1.0 2.0\n4   e   1.0 2.0\n5   f   1.0 2.0\n</code></pre>\n<p>Expected result:</p>\n<pre><code>   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n</code></pre>\n",
        "formatted_input": {
            "qid": 67357814,
            "link": "https://stackoverflow.com/questions/67357814/python-pandas-dataframe-apply-result-of-function-to-multiple-columns-where-nan",
            "question": {
                "title": "Python pandas dataframe apply result of function to multiple columns where NaN",
                "ques_desc": "I have a dataframe with three columns and a function that calculates the values of column y and z given the value of column x. I need to only calculate the values if they are missing NaN. However, I get the following result, although I only apply to the masked set. Unsure what I'm doing wrong. If the mask is inverted I get the following result: Expected result: "
            },
            "io": [
                "    x   y   z\n0   a   1.0 2.0\n1   b   1.0 2.0\n2   c   1.0 2.0\n3   d   NaN NaN\n4   e   NaN NaN\n5   f   NaN NaN\n",
                "   x    y    z\n0  a  1.0   2.0\n1  b  1.0   2.0\n2  c  1.0   2.0\n3  d   a1   a2\n4  e   b2   b1\n5  f   c3   c4\n"
            ],
            "answer": {
                "ans_desc": "you can fillna after calculating for the full dataframe and ",
                "code": [
                    "out = (df.fillna(df.apply(calculate, axis=1, result_type='expand')\n                       .set_axis(['y','z'],inplace=False,axis=1)))\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 361,
            "user_id": 15492238,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/5bb9116f49e2fd92ba919ad7f1fd1141?s=128&d=identicon&r=PG&f=1",
            "display_name": "tj judge ",
            "link": "https://stackoverflow.com/users/15492238/tj-judge"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67336086,
        "answer_count": 1,
        "score": 2,
        "last_activity_date": 1619843858,
        "creation_date": 1619793380,
        "last_edit_date": 1619843858,
        "question_id": 67335759,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python",
        "title": "Checking if column headers match PYTHON",
        "body": "<p>I have two dataframes:</p>\n<p>df1:</p>\n<pre><code>      ID  Open High Low  \n       1  64   66   52   \n</code></pre>\n<p>df2</p>\n<pre><code>      ID Open High  Volume\n      1   33   45   30043\n</code></pre>\n<p>I want to write a function that checks if the column headers are matching/the same as columns in df1.</p>\n<p>IF not we get a message telling us what column is missing.</p>\n<p>Example of the message given these dataframes:</p>\n<pre><code>  &quot;The column 'Low' is not selected in df2. The column 'Volume' is not selected in df1' \n</code></pre>\n<p>I want a generalized code that can work for any given dataframe.</p>\n<p>Is this possible on python?</p>\n",
        "answer_body": "<p>You can have access to the column names via <code>.columns</code> and then use set operations to check what you want:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        &quot;ID&quot;: [1],\n        &quot;Open&quot;: [64],\n        &quot;High&quot;: [66],\n        &quot;Low&quot;: [52]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        &quot;ID&quot;: [1],\n        &quot;Open&quot;: [33],\n        &quot;High&quot;: [45],\n        &quot;Volume&quot;: [30043]\n    }\n)\n\ndf1_columns = set(df1.columns)\ndf2_columns = set(df2.columns)\n\ncommon_columns = df1_columns &amp; df2_columns\n\ndf1_columns_only = df1_columns - common_columns\ndf2_columns_only = df2_columns - common_columns\n\nprint(&quot;Columns only available in df1&quot;, df1_columns_only)\nprint(&quot;Columns only available in df2&quot;, df2_columns_only)\n</code></pre>\n<p>And it gives the expected output:</p>\n<pre><code>Columns only available in df1 {'Low'}\nColumns only available in df2 {'Volume'}\n</code></pre>\n",
        "question_body": "<p>I have two dataframes:</p>\n<p>df1:</p>\n<pre><code>      ID  Open High Low  \n       1  64   66   52   \n</code></pre>\n<p>df2</p>\n<pre><code>      ID Open High  Volume\n      1   33   45   30043\n</code></pre>\n<p>I want to write a function that checks if the column headers are matching/the same as columns in df1.</p>\n<p>IF not we get a message telling us what column is missing.</p>\n<p>Example of the message given these dataframes:</p>\n<pre><code>  &quot;The column 'Low' is not selected in df2. The column 'Volume' is not selected in df1' \n</code></pre>\n<p>I want a generalized code that can work for any given dataframe.</p>\n<p>Is this possible on python?</p>\n",
        "formatted_input": {
            "qid": 67335759,
            "link": "https://stackoverflow.com/questions/67335759/checking-if-column-headers-match-python",
            "question": {
                "title": "Checking if column headers match PYTHON",
                "ques_desc": "I have two dataframes: df1: df2 I want to write a function that checks if the column headers are matching/the same as columns in df1. IF not we get a message telling us what column is missing. Example of the message given these dataframes: I want a generalized code that can work for any given dataframe. Is this possible on python? "
            },
            "io": [
                "      ID  Open High Low  \n       1  64   66   52   \n",
                "      ID Open High  Volume\n      1   33   45   30043\n"
            ],
            "answer": {
                "ans_desc": "You can have access to the column names via and then use set operations to check what you want: And it gives the expected output: ",
                "code": [
                    "import pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [64],\n        \"High\": [66],\n        \"Low\": [52]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        \"ID\": [1],\n        \"Open\": [33],\n        \"High\": [45],\n        \"Volume\": [30043]\n    }\n)\n\ndf1_columns = set(df1.columns)\ndf2_columns = set(df2.columns)\n\ncommon_columns = df1_columns & df2_columns\n\ndf1_columns_only = df1_columns - common_columns\ndf2_columns_only = df2_columns - common_columns\n\nprint(\"Columns only available in df1\", df1_columns_only)\nprint(\"Columns only available in df2\", df2_columns_only)\n",
                    "Columns only available in df1 {'Low'}\nColumns only available in df2 {'Volume'}\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "nan"
        ],
        "owner": {
            "reputation": 7018,
            "user_id": 1452759,
            "user_type": "registered",
            "accept_rate": 73,
            "profile_image": "https://www.gravatar.com/avatar/724bd9ac38fdf069f0b869b70ff0e753?s=128&d=identicon&r=PG",
            "display_name": "user1452759",
            "link": "https://stackoverflow.com/users/1452759/user1452759"
        },
        "is_answered": true,
        "view_count": 396687,
        "protected_date": 1562192230,
        "accepted_answer_id": 26838140,
        "answer_count": 8,
        "score": 290,
        "last_activity_date": 1619763697,
        "creation_date": 1415600966,
        "last_edit_date": 1540067939,
        "question_id": 26837998,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string",
        "title": "Pandas Replace NaN with blank/empty string",
        "body": "<p>I have a Pandas Dataframe as shown below:</p>\n\n<pre><code>    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n</code></pre>\n\n<p>I want to remove the NaN values with an empty string so that it looks like so:</p>\n\n<pre><code>    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n</code></pre>\n",
        "answer_body": "<pre><code>import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n</code></pre>\n\n<p>This might help. It will replace all NaNs with an empty string.</p>\n",
        "question_body": "<p>I have a Pandas Dataframe as shown below:</p>\n\n<pre><code>    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n</code></pre>\n\n<p>I want to remove the NaN values with an empty string so that it looks like so:</p>\n\n<pre><code>    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n</code></pre>\n",
        "formatted_input": {
            "qid": 26837998,
            "link": "https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string",
            "question": {
                "title": "Pandas Replace NaN with blank/empty string",
                "ques_desc": "I have a Pandas Dataframe as shown below: I want to remove the NaN values with an empty string so that it looks like so: "
            },
            "io": [
                "    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n",
                "    1    2       3\n 0  a   \"\"    read\n 1  b    l  unread\n 2  c   \"\"    read\n"
            ],
            "answer": {
                "ans_desc": " This might help. It will replace all NaNs with an empty string. ",
                "code": [
                    "import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe",
            "numpy"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 15778404,
            "user_type": "registered",
            "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GimYCkfz3pFkj7O8AJd9IIO7bm3ktBeyl9eehMGGQ=k-s128",
            "display_name": "rafazamp",
            "link": "https://stackoverflow.com/users/15778404/rafazamp"
        },
        "is_answered": true,
        "view_count": 47,
        "accepted_answer_id": 67288740,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1619551535,
        "creation_date": 1619545844,
        "last_edit_date": 1619548270,
        "question_id": 67288220,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition",
        "title": "How can I add a new line in pandas dataframe based in a condition?",
        "body": "<p>I have this Dataframe that is populated from a file.\nThe first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition.</p>\n<pre><code>[1]   [2] [3]\n  1     30  2\n  1     30  1\n  1     30  3\n  1     90  3\n  1    370  3\n  1    430  3\n  1    705  3\n  1    805  3\n  1    880  2\n  1    905  3\n  1   1005  3\n  1   1170  3\n  1   1230  3\n  1   1970  3\n  1   2030  3\n  1   2970  3\n  1   3030  3\n  1   3970  3\n  1   4030  3\n  1   4423  3\n  1   4539  3\n  1   4575  3\n  1   4630  2\n  1   4635  3\n  1   4671  3\n  1   4787  3\n  1   4957  3\n  1   5057  3\n  1   5270  3\n  1   5330  3\n  1   5970  3\n  1   6030  3\n  1   6970  3\n  1   7030  3\n  1   7970  3\n  1   8030  3\n  1   8158  3\n  1   8257  3\n  1   8332  2\n  1   8357  3\n  1   8457  3\n  1   8970  3\n  1   9030  3\n  1   9970  3\n  1  10030  3\n  1  10970  3\n  1  11030  3\n  1  11470  3\n  1  11530  3\n  1  11853  3\n  1  11953  3\n</code></pre>\n<p>Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this..\nTaking for example the lines number 4 and 5:</p>\n<pre><code>  1     90  3\n  1    370  3\n 370 - 90 = 260 (260 is greater than 100)\n</code></pre>\n<p>So I need to add a new line with the last number + 100, and the last column needs to be zero:</p>\n<pre><code>  1     90  3\n  1    190  0\n  1    370  3\n</code></pre>\n<p>Any ideas how can I achieve that?\nThanks in advance.</p>\n<p>Edit: I just need to add the line once in the DataFrame.</p>\n",
        "answer_body": "<p>Try:</p>\n<pre><code>m = df[&quot;[2]&quot;].diff() &gt; 100\n\ndf.loc[m, &quot;[2]&quot;] = pd.Series(\n    [\n        [str(df.iloc[v - 1][&quot;[2]&quot;] + 100), df.iloc[v][&quot;[2]&quot;]]\n        for v in df.index[m]\n    ],\n    index=df.index[m],\n)\n\ndf = df.explode(&quot;[2]&quot;)\ndf[&quot;[3]&quot;] = np.where(\n    df[&quot;[2]&quot;].apply(lambda x: isinstance(x, str)), 0, df[&quot;[3]&quot;]\n)\ndf[&quot;[2]&quot;] = df[&quot;[2]&quot;].astype(int)\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>    [1]    [2]  [3]\n0     1     30    2\n1     1     30    1\n2     1     30    3\n3     1     90    3\n4     1    190    0\n4     1    370    3\n5     1    430    3\n6     1    530    0\n6     1    705    3\n7     1    805    3\n8     1    880    2\n9     1    905    3\n10    1   1005    3\n11    1   1105    0\n11    1   1170    3\n12    1   1230    3\n13    1   1330    0\n13    1   1970    3\n14    1   2030    3\n15    1   2130    0\n15    1   2970    3\n16    1   3030    3\n17    1   3130    0\n17    1   3970    3\n18    1   4030    3\n19    1   4130    0\n19    1   4423    3\n20    1   4523    0\n20    1   4539    3\n21    1   4575    3\n22    1   4630    2\n23    1   4635    3\n24    1   4671    3\n25    1   4771    0\n25    1   4787    3\n26    1   4887    0\n26    1   4957    3\n27    1   5057    3\n28    1   5157    0\n28    1   5270    3\n29    1   5330    3\n30    1   5430    0\n30    1   5970    3\n31    1   6030    3\n32    1   6130    0\n32    1   6970    3\n33    1   7030    3\n34    1   7130    0\n34    1   7970    3\n35    1   8030    3\n36    1   8130    0\n36    1   8158    3\n37    1   8257    3\n38    1   8332    2\n39    1   8357    3\n40    1   8457    3\n41    1   8557    0\n41    1   8970    3\n42    1   9030    3\n43    1   9130    0\n43    1   9970    3\n44    1  10030    3\n45    1  10130    0\n45    1  10970    3\n46    1  11030    3\n47    1  11130    0\n47    1  11470    3\n48    1  11530    3\n49    1  11630    0\n49    1  11853    3\n50    1  11953    3\n</code></pre>\n<hr />\n<p>EDIT: To change only one value:</p>\n<pre><code>mask = df[&quot;[2]&quot;].diff() &gt; 100\nif True in mask:\n    m = [False] * len(df)\n    m[mask.idxmax()] = True\n\n    df.loc[m, &quot;[2]&quot;] = pd.Series(\n        [\n            [str(df.iloc[v - 1][&quot;[2]&quot;] + 100), df.iloc[v][&quot;[2]&quot;]]\n            for v in df.index[m]\n        ],\n        index=df.index[m],\n    )\n\n    df = df.explode(&quot;[2]&quot;)\n    df[&quot;[3]&quot;] = np.where(\n        df[&quot;[2]&quot;].apply(lambda x: isinstance(x, str)), 0, df[&quot;[3]&quot;]\n    )\n    df[&quot;[2]&quot;] = df[&quot;[2]&quot;].astype(int)\n    print(df)\n</code></pre>\n",
        "question_body": "<p>I have this Dataframe that is populated from a file.\nThe first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition.</p>\n<pre><code>[1]   [2] [3]\n  1     30  2\n  1     30  1\n  1     30  3\n  1     90  3\n  1    370  3\n  1    430  3\n  1    705  3\n  1    805  3\n  1    880  2\n  1    905  3\n  1   1005  3\n  1   1170  3\n  1   1230  3\n  1   1970  3\n  1   2030  3\n  1   2970  3\n  1   3030  3\n  1   3970  3\n  1   4030  3\n  1   4423  3\n  1   4539  3\n  1   4575  3\n  1   4630  2\n  1   4635  3\n  1   4671  3\n  1   4787  3\n  1   4957  3\n  1   5057  3\n  1   5270  3\n  1   5330  3\n  1   5970  3\n  1   6030  3\n  1   6970  3\n  1   7030  3\n  1   7970  3\n  1   8030  3\n  1   8158  3\n  1   8257  3\n  1   8332  2\n  1   8357  3\n  1   8457  3\n  1   8970  3\n  1   9030  3\n  1   9970  3\n  1  10030  3\n  1  10970  3\n  1  11030  3\n  1  11470  3\n  1  11530  3\n  1  11853  3\n  1  11953  3\n</code></pre>\n<p>Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this..\nTaking for example the lines number 4 and 5:</p>\n<pre><code>  1     90  3\n  1    370  3\n 370 - 90 = 260 (260 is greater than 100)\n</code></pre>\n<p>So I need to add a new line with the last number + 100, and the last column needs to be zero:</p>\n<pre><code>  1     90  3\n  1    190  0\n  1    370  3\n</code></pre>\n<p>Any ideas how can I achieve that?\nThanks in advance.</p>\n<p>Edit: I just need to add the line once in the DataFrame.</p>\n",
        "formatted_input": {
            "qid": 67288220,
            "link": "https://stackoverflow.com/questions/67288220/how-can-i-add-a-new-line-in-pandas-dataframe-based-in-a-condition",
            "question": {
                "title": "How can I add a new line in pandas dataframe based in a condition?",
                "ques_desc": "I have this Dataframe that is populated from a file. The first column is always the same value, the second is dimension based (I got these values from a Cam file), and the third column is created by a else-if condition. Now I need to create a new row based in a calculation. I need to iterate each line to find a value that is greater than 100 to add a new line like this.. Taking for example the lines number 4 and 5: So I need to add a new line with the last number + 100, and the last column needs to be zero: Any ideas how can I achieve that? Thanks in advance. Edit: I just need to add the line once in the DataFrame. "
            },
            "io": [
                "[1]   [2] [3]\n  1     30  2\n  1     30  1\n  1     30  3\n  1     90  3\n  1    370  3\n  1    430  3\n  1    705  3\n  1    805  3\n  1    880  2\n  1    905  3\n  1   1005  3\n  1   1170  3\n  1   1230  3\n  1   1970  3\n  1   2030  3\n  1   2970  3\n  1   3030  3\n  1   3970  3\n  1   4030  3\n  1   4423  3\n  1   4539  3\n  1   4575  3\n  1   4630  2\n  1   4635  3\n  1   4671  3\n  1   4787  3\n  1   4957  3\n  1   5057  3\n  1   5270  3\n  1   5330  3\n  1   5970  3\n  1   6030  3\n  1   6970  3\n  1   7030  3\n  1   7970  3\n  1   8030  3\n  1   8158  3\n  1   8257  3\n  1   8332  2\n  1   8357  3\n  1   8457  3\n  1   8970  3\n  1   9030  3\n  1   9970  3\n  1  10030  3\n  1  10970  3\n  1  11030  3\n  1  11470  3\n  1  11530  3\n  1  11853  3\n  1  11953  3\n",
                "  1     90  3\n  1    190  0\n  1    370  3\n"
            ],
            "answer": {
                "ans_desc": "Try: Prints: EDIT: To change only one value: ",
                "code": [
                    "m = df[\"[2]\"].diff() > 100\n\ndf.loc[m, \"[2]\"] = pd.Series(\n    [\n        [str(df.iloc[v - 1][\"[2]\"] + 100), df.iloc[v][\"[2]\"]]\n        for v in df.index[m]\n    ],\n    index=df.index[m],\n)\n\ndf = df.explode(\"[2]\")\ndf[\"[3]\"] = np.where(\n    df[\"[2]\"].apply(lambda x: isinstance(x, str)), 0, df[\"[3]\"]\n)\ndf[\"[2]\"] = df[\"[2]\"].astype(int)\nprint(df)\n",
                    "mask = df[\"[2]\"].diff() > 100\nif True in mask:\n    m = [False] * len(df)\n    m[mask.idxmax()] = True\n\n    df.loc[m, \"[2]\"] = pd.Series(\n        [\n            [str(df.iloc[v - 1][\"[2]\"] + 100), df.iloc[v][\"[2]\"]]\n            for v in df.index[m]\n        ],\n        index=df.index[m],\n    )\n\n    df = df.explode(\"[2]\")\n    df[\"[3]\"] = np.where(\n        df[\"[2]\"].apply(lambda x: isinstance(x, str)), 0, df[\"[3]\"]\n    )\n    df[\"[2]\"] = df[\"[2]\"].astype(int)\n    print(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 57,
            "user_id": 15415267,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/021c1306999edefd832f61302fe04f61?s=128&d=identicon&r=PG&f=1",
            "display_name": "muratmert41",
            "link": "https://stackoverflow.com/users/15415267/muratmert41"
        },
        "is_answered": true,
        "view_count": 33,
        "accepted_answer_id": 67258039,
        "answer_count": 3,
        "score": 2,
        "last_activity_date": 1619423161,
        "creation_date": 1619381990,
        "last_edit_date": 1619423161,
        "question_id": 67257898,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column",
        "title": "How to add a value to a new column by referencing the values in a column",
        "body": "<p>I have a dataframe like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5 \n 1  x1      100   15   10   20   25\n 2  x1      15    16   14   10   10\n 3  x4      10    50   40   30   25\n 4  x3      12    15   60   5    1\n 5  x1      80    15   10   20   25\n 6  x1      15    19   84   10   10\n 7  x4      90    40   90   30   25\n 8  x4      12    85   60   50   10\n</code></pre>\n<p>The <strong>xy</strong> column must be filled with the value of the column names in the <strong>reason</strong> column. Let's look at the first row. The <strong>reason column</strong> shows our value <strong>x1</strong>. So our value in column <strong>xy</strong>, will be the value of x1 column in the first row. Like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5   xy\n 1  x1      100   15   10   20   25   100\n 2  x1      15    16   14   10   10   15\n 3  x4      10    50   40   30   25   30\n 4  x3      12    15   60   5    1    60\n 5  x1      80    15   10   20   25   80\n 6  x1      15    19   84   10   10   15\n 7  x4      90    40   90   30   25   30\n 8  x4      12    85   60   50   10   50\n</code></pre>\n<p>Is there a way to do this?</p>\n",
        "answer_body": "<pre><code>df[&quot;xy&quot;] = df.apply(lambda x: x[x[&quot;reason&quot;]], axis=1)\nprint(df)\n</code></pre>\n<p>Prints:</p>\n<pre class=\"lang-none prettyprint-override\"><code>   id reason   x1  x2  x3  x4  x5   xy\n0   1     x1  100  15  10  20  25  100\n1   2     x1   15  16  14  10  10   15\n2   3     x4   10  50  40  30  25   30\n3   4     x3   12  15  60   5   1   60\n4   5     x1   80  15  10  20  25   80\n5   6     x1   15  19  84  10  10   15\n6   7     x4   90  40  90  30  25   30\n7   8     x4   12  85  60  50  10   50\n</code></pre>\n",
        "question_body": "<p>I have a dataframe like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5 \n 1  x1      100   15   10   20   25\n 2  x1      15    16   14   10   10\n 3  x4      10    50   40   30   25\n 4  x3      12    15   60   5    1\n 5  x1      80    15   10   20   25\n 6  x1      15    19   84   10   10\n 7  x4      90    40   90   30   25\n 8  x4      12    85   60   50   10\n</code></pre>\n<p>The <strong>xy</strong> column must be filled with the value of the column names in the <strong>reason</strong> column. Let's look at the first row. The <strong>reason column</strong> shows our value <strong>x1</strong>. So our value in column <strong>xy</strong>, will be the value of x1 column in the first row. Like this:</p>\n<pre><code>id  reason  x1    x2   x3   x4   x5   xy\n 1  x1      100   15   10   20   25   100\n 2  x1      15    16   14   10   10   15\n 3  x4      10    50   40   30   25   30\n 4  x3      12    15   60   5    1    60\n 5  x1      80    15   10   20   25   80\n 6  x1      15    19   84   10   10   15\n 7  x4      90    40   90   30   25   30\n 8  x4      12    85   60   50   10   50\n</code></pre>\n<p>Is there a way to do this?</p>\n",
        "formatted_input": {
            "qid": 67257898,
            "link": "https://stackoverflow.com/questions/67257898/how-to-add-a-value-to-a-new-column-by-referencing-the-values-in-a-column",
            "question": {
                "title": "How to add a value to a new column by referencing the values in a column",
                "ques_desc": "I have a dataframe like this: The xy column must be filled with the value of the column names in the reason column. Let's look at the first row. The reason column shows our value x1. So our value in column xy, will be the value of x1 column in the first row. Like this: Is there a way to do this? "
            },
            "io": [
                "id  reason  x1    x2   x3   x4   x5 \n 1  x1      100   15   10   20   25\n 2  x1      15    16   14   10   10\n 3  x4      10    50   40   30   25\n 4  x3      12    15   60   5    1\n 5  x1      80    15   10   20   25\n 6  x1      15    19   84   10   10\n 7  x4      90    40   90   30   25\n 8  x4      12    85   60   50   10\n",
                "id  reason  x1    x2   x3   x4   x5   xy\n 1  x1      100   15   10   20   25   100\n 2  x1      15    16   14   10   10   15\n 3  x4      10    50   40   30   25   30\n 4  x3      12    15   60   5    1    60\n 5  x1      80    15   10   20   25   80\n 6  x1      15    19   84   10   10   15\n 7  x4      90    40   90   30   25   30\n 8  x4      12    85   60   50   10   50\n"
            ],
            "answer": {
                "ans_desc": " Prints: ",
                "code": [
                    "df[\"xy\"] = df.apply(lambda x: x[x[\"reason\"]], axis=1)\nprint(df)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 13,
            "user_id": 15762077,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/853af71cd55ed204dddebe32bbc8a571?s=128&d=identicon&r=PG&f=1",
            "display_name": "kdfjlasjdflaj",
            "link": "https://stackoverflow.com/users/15762077/kdfjlasjdflaj"
        },
        "is_answered": true,
        "view_count": 22,
        "accepted_answer_id": 67255996,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1619369716,
        "creation_date": 1619368167,
        "last_edit_date": 1619369473,
        "question_id": 67255732,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67255732/summation-of-operation-in-dataframe",
        "title": "Summation of operation in dataframe",
        "body": "<p>I want to implement a function that does the operation that you can see in the image:</p>\n<p><img src=\"https://i.stack.imgur.com/ocLbz.png\" alt=\"image\" /></p>\n<p>But i not sure how to implement the Summation for the moment i doing something like that:</p>\n<pre><code>def df_operation(df1: pd.DataFrame, df2: pd.DataFrame) -&gt; float:\n    return ((((abs(df1 - df2)).sum())/((df1+df2).sum()))*100)\n</code></pre>\n<p>And the problem is in the summation. If someone can help me.</p>\n<p>For example, i have two dataframes like that:</p>\n<pre><code>dataframe1 = pd.DataFrame({&quot;A&quot;:[8, 2],   \n               &quot;B&quot;:[26, 19]}) \n\ndataframe2 = pd.DataFrame({&quot;A&quot;:[3,6],   \n               &quot;B&quot;:[12,17]})  \n</code></pre>\n<p>Where for the abs operation we will obtain this:</p>\n<pre><code>   A  B\n0  5  14\n1  4  2\n</code></pre>\n<p>And for the sum:</p>\n<pre><code>   A   B\n0  11  38\n1  8   36\n</code></pre>\n<p>Finally we do the summation:</p>\n<pre><code>(25/93) * 100\n</code></pre>\n<p>where <code>25 = 5+14+4+2</code> and <code>93 = 11+8+38+36</code></p>\n",
        "answer_body": "<p>Use <code>.sum().sum()</code> to sum the dataframe across columns/rows:</p>\n<pre><code>result = (\n    dataframe1.sub(dataframe2).abs().sum().sum()\n    / dataframe1.add(dataframe2).sum().sum()\n) * 100\nprint(result)\n</code></pre>\n<p>Prints:</p>\n<pre><code>26.881720430107524\n</code></pre>\n",
        "question_body": "<p>I want to implement a function that does the operation that you can see in the image:</p>\n<p><img src=\"https://i.stack.imgur.com/ocLbz.png\" alt=\"image\" /></p>\n<p>But i not sure how to implement the Summation for the moment i doing something like that:</p>\n<pre><code>def df_operation(df1: pd.DataFrame, df2: pd.DataFrame) -&gt; float:\n    return ((((abs(df1 - df2)).sum())/((df1+df2).sum()))*100)\n</code></pre>\n<p>And the problem is in the summation. If someone can help me.</p>\n<p>For example, i have two dataframes like that:</p>\n<pre><code>dataframe1 = pd.DataFrame({&quot;A&quot;:[8, 2],   \n               &quot;B&quot;:[26, 19]}) \n\ndataframe2 = pd.DataFrame({&quot;A&quot;:[3,6],   \n               &quot;B&quot;:[12,17]})  \n</code></pre>\n<p>Where for the abs operation we will obtain this:</p>\n<pre><code>   A  B\n0  5  14\n1  4  2\n</code></pre>\n<p>And for the sum:</p>\n<pre><code>   A   B\n0  11  38\n1  8   36\n</code></pre>\n<p>Finally we do the summation:</p>\n<pre><code>(25/93) * 100\n</code></pre>\n<p>where <code>25 = 5+14+4+2</code> and <code>93 = 11+8+38+36</code></p>\n",
        "formatted_input": {
            "qid": 67255732,
            "link": "https://stackoverflow.com/questions/67255732/summation-of-operation-in-dataframe",
            "question": {
                "title": "Summation of operation in dataframe",
                "ques_desc": "I want to implement a function that does the operation that you can see in the image: But i not sure how to implement the Summation for the moment i doing something like that: And the problem is in the summation. If someone can help me. For example, i have two dataframes like that: Where for the abs operation we will obtain this: And for the sum: Finally we do the summation: where and "
            },
            "io": [
                "   A  B\n0  5  14\n1  4  2\n",
                "   A   B\n0  11  38\n1  8   36\n"
            ],
            "answer": {
                "ans_desc": "Use to sum the dataframe across columns/rows: Prints: ",
                "code": [
                    "result = (\n    dataframe1.sub(dataframe2).abs().sum().sum()\n    / dataframe1.add(dataframe2).sum().sum()\n) * 100\nprint(result)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 768,
            "user_id": 5356096,
            "user_type": "registered",
            "profile_image": "https://i.stack.imgur.com/sKX9g.png?s=128&g=1",
            "display_name": "Jack Avante",
            "link": "https://stackoverflow.com/users/5356096/jack-avante"
        },
        "is_answered": true,
        "view_count": 56,
        "accepted_answer_id": 67245812,
        "answer_count": 1,
        "score": 0,
        "last_activity_date": 1619348036,
        "creation_date": 1619269385,
        "last_edit_date": 1619286390,
        "question_id": 67243081,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas",
        "title": "Best way to change column data for all rows over multiple dataframes in pandas?",
        "body": "<p>Consider dataframes <code>df1</code>, <code>df2</code>, and <code>df3</code>.</p>\n<p><code>df1</code> and <code>df2</code> have an <code>id</code> column, and <code>df3</code> has a <code>from_id</code> and <code>to_id</code> column.</p>\n<p>I need to iterate over all rows of <code>df3</code>, and replace <code>from_id</code> and <code>to_id</code> with new unique randomly generated UUIDs, and then update those in <code>df1</code> and <code>df2</code> where <code>(id == from_id) | (id == to_id)</code> (before the change to UUID).</p>\n<p>I originally wanted to iterate over all rows of <code>df3</code> and simply check both <code>df1</code> and <code>df2</code> if they contain the original <code>from_id</code> or <code>to_id</code> inside the <code>id</code> column before replacing both, but <a href=\"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\">I found that iterating over pandas rows is a bad idea and slow.</a></p>\n<p>I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes.</p>\n<p>My current method that I believe to be slow and inefficient:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid1()\n\ndef update_ids(df_places: pd.DataFrame, df_transitions: pd.DataFrame, df_arcs: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    for i in range(len(df_arcs)):\n        new_uuid_from = __rand_uuid()\n        new_uuid_to = __rand_uuid()\n        new_uuid_arc = __rand_uuid()\n\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_places.loc[df_places.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_places.loc[df_places.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_arcs.iloc[i]['sourceId'] = new_uuid_from\n        df_arcs.iloc[i]['destinationId'] = new_uuid_to\n        df_arcs.iloc[i]['id'] = new_uuid_arc\n\n    return df_places, df_transitions, df_arcs\n</code></pre>\n<p>Here <code>df_places</code> and <code>df_transitions</code> are above mentioned <code>df1</code> and <code>df2</code>, and <code>df_arcs</code> is <code>df3</code></p>\n<p>Example <code>df_places</code></p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | a1 |\n+---+----+\n| 2 | c1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_transitions</code>:</p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | b1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_arcs</code>:</p>\n<pre><code>+---+----------+---------------+\n|   | sourceId | destinationId |\n+---+----------+---------------+\n| 1 | a1       | b1            |\n+---+----------+---------------+\n| 2 | b1       | c1            |\n+---+----------+---------------+\n</code></pre>\n",
        "answer_body": "<p>A very simple approach:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import itertools\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid4()\n\nrep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}\n\ndf3.replace(rep_dict, inplace=True)\ndf3.id = df3.id.map(lambda x: rand_uuid())\n\ndf1.replace(rep_dict, inplace=True)\ndf2.replace(rep_dict, inplace=True)\n</code></pre>\n",
        "question_body": "<p>Consider dataframes <code>df1</code>, <code>df2</code>, and <code>df3</code>.</p>\n<p><code>df1</code> and <code>df2</code> have an <code>id</code> column, and <code>df3</code> has a <code>from_id</code> and <code>to_id</code> column.</p>\n<p>I need to iterate over all rows of <code>df3</code>, and replace <code>from_id</code> and <code>to_id</code> with new unique randomly generated UUIDs, and then update those in <code>df1</code> and <code>df2</code> where <code>(id == from_id) | (id == to_id)</code> (before the change to UUID).</p>\n<p>I originally wanted to iterate over all rows of <code>df3</code> and simply check both <code>df1</code> and <code>df2</code> if they contain the original <code>from_id</code> or <code>to_id</code> inside the <code>id</code> column before replacing both, but <a href=\"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\">I found that iterating over pandas rows is a bad idea and slow.</a></p>\n<p>I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes.</p>\n<p>My current method that I believe to be slow and inefficient:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid1()\n\ndef update_ids(df_places: pd.DataFrame, df_transitions: pd.DataFrame, df_arcs: pd.DataFrame) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    for i in range(len(df_arcs)):\n        new_uuid_from = __rand_uuid()\n        new_uuid_to = __rand_uuid()\n        new_uuid_arc = __rand_uuid()\n\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_transitions.loc[df_transitions.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_places.loc[df_places.id == df_arcs.iloc[i]['sourceId'], 'id'] = new_uuid_from\n        df_places.loc[df_places.id == df_arcs.iloc[i]['destinationId'], 'id'] = new_uuid_to\n\n        df_arcs.iloc[i]['sourceId'] = new_uuid_from\n        df_arcs.iloc[i]['destinationId'] = new_uuid_to\n        df_arcs.iloc[i]['id'] = new_uuid_arc\n\n    return df_places, df_transitions, df_arcs\n</code></pre>\n<p>Here <code>df_places</code> and <code>df_transitions</code> are above mentioned <code>df1</code> and <code>df2</code>, and <code>df_arcs</code> is <code>df3</code></p>\n<p>Example <code>df_places</code></p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | a1 |\n+---+----+\n| 2 | c1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_transitions</code>:</p>\n<pre><code>+---+----+\n|   | id |\n+---+----+\n| 1 | b1 |\n+---+----+\n</code></pre>\n<p>Example <code>df_arcs</code>:</p>\n<pre><code>+---+----------+---------------+\n|   | sourceId | destinationId |\n+---+----------+---------------+\n| 1 | a1       | b1            |\n+---+----------+---------------+\n| 2 | b1       | c1            |\n+---+----------+---------------+\n</code></pre>\n",
        "formatted_input": {
            "qid": 67243081,
            "link": "https://stackoverflow.com/questions/67243081/best-way-to-change-column-data-for-all-rows-over-multiple-dataframes-in-pandas",
            "question": {
                "title": "Best way to change column data for all rows over multiple dataframes in pandas?",
                "ques_desc": "Consider dataframes , , and . and have an column, and has a and column. I need to iterate over all rows of , and replace and with new unique randomly generated UUIDs, and then update those in and where (before the change to UUID). I originally wanted to iterate over all rows of and simply check both and if they contain the original or inside the column before replacing both, but I found that iterating over pandas rows is a bad idea and slow. I'm not sure how I can apply the other mentioned methods in that post to this problem since I'm not applying a simple function or calculating anything, and I think the way I had intended to do it would be too slow for big dataframes. My current method that I believe to be slow and inefficient: Here and are above mentioned and , and is Example Example : Example : "
            },
            "io": [
                "+---+----+\n|   | id |\n+---+----+\n| 1 | a1 |\n+---+----+\n| 2 | c1 |\n+---+----+\n",
                "+---+----+\n|   | id |\n+---+----+\n| 1 | b1 |\n+---+----+\n"
            ],
            "answer": {
                "ans_desc": "A very simple approach: ",
                "code": [
                    "import itertools\nimport uuid\n\ndef rand_uuid():\n    return uuid.uuid4()\n\nrep_dict = {i: rand_uuid() for i in itertools.chain(df1.id, df2.id)}\n\ndf3.replace(rep_dict, inplace=True)\ndf3.id = df3.id.map(lambda x: rand_uuid())\n\ndf1.replace(rep_dict, inplace=True)\ndf2.replace(rep_dict, inplace=True)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "python-3.x",
            "pandas",
            "dataframe",
            "csv"
        ],
        "owner": {
            "reputation": 359,
            "user_id": 8546104,
            "user_type": "registered",
            "accept_rate": 78,
            "profile_image": "https://www.gravatar.com/avatar/d33f56c42c6594e69a0d6e8a83e5b91b?s=128&d=identicon&r=PG&f=1",
            "display_name": "codeDojo",
            "link": "https://stackoverflow.com/users/8546104/codedojo"
        },
        "is_answered": true,
        "view_count": 39,
        "accepted_answer_id": 67246882,
        "answer_count": 1,
        "score": 1,
        "last_activity_date": 1619295624,
        "creation_date": 1619293827,
        "question_id": 67246859,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67246859/how-to-convert-rows-into-columns-and-filter-using-the-id",
        "title": "How to convert rows into columns and filter using the ID",
        "body": "<p>I have a CSV file that looks like this:</p>\n<pre><code>customer_id |  key_id.  |  quantity |\n1           |    777    |    3      |\n1           |    888    |    2      |\n1           |    999    |    3      |\n2           |    777    |    6      |\n2           |    888    |    1      |\n</code></pre>\n<p>and I would like to use simple python or pandas to:</p>\n<ol>\n<li>Make each unique customer id in a separate row</li>\n<li>convert key_id to the columns titles and the values are the quantity</li>\n</ol>\n<p>The output table should look like this:</p>\n<pre><code>            |  777    |  888  |   999  | \n1           |   3     |   2   |    3   |\n2           |   6     |   1   |    0   |\n</code></pre>\n<p>I have been struggling to find a good data structure to do this but I couldn't. and using pandas I also couldn't filter using 2 ids. Any tips?</p>\n",
        "answer_body": "<p>You can pivot into <code>key_id</code> columns using <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot_table.html\" rel=\"nofollow noreferrer\"><strong><code>pivot_table()</code></strong></a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.pivot_table(index='customer_id', columns='key_id', values='quantity').fillna(0)\n\n# key_id       777  888  999\n# customer_id               \n# 1            3.0  2.0  3.0\n# 2            6.0  1.0  0.0\n</code></pre>\n<hr />\n<p>To handle duplicates, <code>pivot_table()</code> averages them by default. To override this aggregation method, you can set the <code>aggfunc</code> param (<code>max</code>, <code>min</code>, <code>first</code>, <code>last</code>, <code>sum</code>, etc.):</p>\n<pre class=\"lang-py prettyprint-override\"><code>df.pivot_table(\n    index='customer_id',\n    columns='key_id',\n    values='quantity',\n    aggfunc='max',\n).fillna(0)\n</code></pre>\n",
        "question_body": "<p>I have a CSV file that looks like this:</p>\n<pre><code>customer_id |  key_id.  |  quantity |\n1           |    777    |    3      |\n1           |    888    |    2      |\n1           |    999    |    3      |\n2           |    777    |    6      |\n2           |    888    |    1      |\n</code></pre>\n<p>and I would like to use simple python or pandas to:</p>\n<ol>\n<li>Make each unique customer id in a separate row</li>\n<li>convert key_id to the columns titles and the values are the quantity</li>\n</ol>\n<p>The output table should look like this:</p>\n<pre><code>            |  777    |  888  |   999  | \n1           |   3     |   2   |    3   |\n2           |   6     |   1   |    0   |\n</code></pre>\n<p>I have been struggling to find a good data structure to do this but I couldn't. and using pandas I also couldn't filter using 2 ids. Any tips?</p>\n",
        "formatted_input": {
            "qid": 67246859,
            "link": "https://stackoverflow.com/questions/67246859/how-to-convert-rows-into-columns-and-filter-using-the-id",
            "question": {
                "title": "How to convert rows into columns and filter using the ID",
                "ques_desc": "I have a CSV file that looks like this: and I would like to use simple python or pandas to: Make each unique customer id in a separate row convert key_id to the columns titles and the values are the quantity The output table should look like this: I have been struggling to find a good data structure to do this but I couldn't. and using pandas I also couldn't filter using 2 ids. Any tips? "
            },
            "io": [
                "customer_id |  key_id.  |  quantity |\n1           |    777    |    3      |\n1           |    888    |    2      |\n1           |    999    |    3      |\n2           |    777    |    6      |\n2           |    888    |    1      |\n",
                "            |  777    |  888  |   999  | \n1           |   3     |   2   |    3   |\n2           |   6     |   1   |    0   |\n"
            ],
            "answer": {
                "ans_desc": "You can pivot into columns using : To handle duplicates, averages them by default. To override this aggregation method, you can set the param (, , , , , etc.): ",
                "code": [
                    "df.pivot_table(\n    index='customer_id',\n    columns='key_id',\n    values='quantity',\n    aggfunc='max',\n).fillna(0)\n"
                ]
            }
        }
    },
    {
        "tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "owner": {
            "reputation": 21,
            "user_id": 14714831,
            "user_type": "registered",
            "profile_image": "https://www.gravatar.com/avatar/af4c1da9e055882b5ec057200d936150?s=128&d=identicon&r=PG&f=1",
            "display_name": "csantos",
            "link": "https://stackoverflow.com/users/14714831/csantos"
        },
        "is_answered": true,
        "view_count": 49,
        "accepted_answer_id": 67215226,
        "answer_count": 2,
        "score": 0,
        "last_activity_date": 1619110665,
        "creation_date": 1619097058,
        "question_id": 67213950,
        "content_license": "CC BY-SA 4.0",
        "link": "https://stackoverflow.com/questions/67213950/how-can-i-compare-each-row-from-a-dataframe-against-every-row-from-another-dataf",
        "title": "How can I compare each row from a dataframe against every row from another dataframe and see the difference between values?",
        "body": "<p>I have two dataframes:</p>\n<p>df1</p>\n<pre><code>     Code     Number\n0   ABC123      1\n1   DEF456      2\n2   GHI789      3\n3   DEA456      4\n</code></pre>\n<p>df2</p>\n<pre><code>     Code \n0   ABD123\n1   DEA458\n2   GHI789\n</code></pre>\n<p>df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be:</p>\n<p>ABD123 = 1 (because it has 1 different character from ABC123)</p>\n<p>DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one)</p>\n<p>GHI789 = 3 (because it has an equivalent at df1)</p>\n<p>I know how to check for the differences of each code individually and save the &quot;length&quot; of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way?</p>\n",
        "answer_body": "<blockquote>\n<p>don't know how to compare each row from df2 against all rows from df1.</p>\n</blockquote>\n<p>Nested loops will work.  If you had a function named <code>compare</code> it would look like this...</p>\n<pre><code>for index2, row2 in df2.iterrows():\n    for index1, row1 in df1.iterrows():\n        difference = compare(row2,row1)\n        #do something with the difference.\n</code></pre>\n<hr />\n<p>Nested loops are usually not ideal when working with Pandas or Numpy but they do work.  There may be better solutions.</p>\n<hr />\n<p><a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html\" rel=\"nofollow noreferrer\" title=\"Pandas docs\">DataFrame.iterrows()</a></p>\n",
        "question_body": "<p>I have two dataframes:</p>\n<p>df1</p>\n<pre><code>     Code     Number\n0   ABC123      1\n1   DEF456      2\n2   GHI789      3\n3   DEA456      4\n</code></pre>\n<p>df2</p>\n<pre><code>     Code \n0   ABD123\n1   DEA458\n2   GHI789\n</code></pre>\n<p>df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be:</p>\n<p>ABD123 = 1 (because it has 1 different character from ABC123)</p>\n<p>DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one)</p>\n<p>GHI789 = 3 (because it has an equivalent at df1)</p>\n<p>I know how to check for the differences of each code individually and save the &quot;length&quot; of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way?</p>\n",
        "formatted_input": {
            "qid": 67213950,
            "link": "https://stackoverflow.com/questions/67213950/how-can-i-compare-each-row-from-a-dataframe-against-every-row-from-another-dataf",
            "question": {
                "title": "How can I compare each row from a dataframe against every row from another dataframe and see the difference between values?",
                "ques_desc": "I have two dataframes: df1 df2 df1 acts like a dictionary, from which I can get the respective number for each item by checking their code. There are, however, unregistered codes, and in case I find an unregistered code, I'm supposed to look for the codes that look the most like them. So, the outcome should to be: ABD123 = 1 (because it has 1 different character from ABC123) DEA456 = 4 (because it has 1 different character from DEA456, and 2 from DEF456, so it chooses the closest one) GHI789 = 3 (because it has an equivalent at df1) I know how to check for the differences of each code individually and save the \"length\" of characters that differ, but I don't know how to apply this code as I don't know how to compare each row from df2 against all rows from df1. Is there a way? "
            },
            "io": [
                "     Code     Number\n0   ABC123      1\n1   DEF456      2\n2   GHI789      3\n3   DEA456      4\n",
                "     Code \n0   ABD123\n1   DEA458\n2   GHI789\n"
            ],
            "answer": {
                "ans_desc": " don't know how to compare each row from df2 against all rows from df1. Nested loops will work. If you had a function named it would look like this... Nested loops are usually not ideal when working with Pandas or Numpy but they do work. There may be better solutions. DataFrame.iterrows() ",
                "code": [
                    "for index2, row2 in df2.iterrows():\n    for index1, row1 in df1.iterrows():\n        difference = compare(row2,row1)\n        #do something with the difference.\n"
                ]
            }
        }
    }
]